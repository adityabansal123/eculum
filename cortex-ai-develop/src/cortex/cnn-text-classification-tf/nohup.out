
Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=64
CHECKPOINT_EVERY=100
DEV_SAMPLE_PERCENTAGE=0.1
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=128
EVALUATE_EVERY=100
FILTER_SIZES=3,4,5
L2_REG_LAMBDA=0.0
LOG_DEVICE_PLACEMENT=False
NEGATIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.neg
NUM_CHECKPOINTS=5
NUM_EPOCHS=200
NUM_FILTERS=128
POSITIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.pos

Loading data...
Vocabulary Size: 18758
Train/Dev split: 9596/1066
Writing to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352

2017-08-08T16:22:33.510615: step 1, loss 2.19485, acc 0.453125
2017-08-08T16:22:33.763361: step 2, loss 1.43934, acc 0.546875
2017-08-08T16:22:34.077961: step 3, loss 2.09918, acc 0.421875
2017-08-08T16:22:34.360926: step 4, loss 1.91098, acc 0.421875
2017-08-08T16:22:34.760217: step 5, loss 1.61625, acc 0.546875
2017-08-08T16:22:34.988244: step 6, loss 2.18771, acc 0.4375
2017-08-08T16:22:35.190074: step 7, loss 2.36195, acc 0.296875
2017-08-08T16:22:35.545546: step 8, loss 1.82266, acc 0.5
2017-08-08T16:22:35.708303: step 9, loss 1.96423, acc 0.515625
2017-08-08T16:22:36.000681: step 10, loss 1.56376, acc 0.546875
2017-08-08T16:22:36.267876: step 11, loss 1.54155, acc 0.578125
2017-08-08T16:22:36.769342: step 12, loss 1.3894, acc 0.5625
2017-08-08T16:22:37.239399: step 13, loss 1.423, acc 0.65625
2017-08-08T16:22:37.556041: step 14, loss 2.16625, acc 0.46875
2017-08-08T16:22:37.779509: step 15, loss 1.27758, acc 0.625
2017-08-08T16:22:38.161808: step 16, loss 2.14294, acc 0.5
2017-08-08T16:22:38.448014: step 17, loss 1.93438, acc 0.5
2017-08-08T16:22:38.770416: step 18, loss 1.804, acc 0.390625
2017-08-08T16:22:39.155643: step 19, loss 1.90745, acc 0.53125
2017-08-08T16:22:39.523660: step 20, loss 1.55718, acc 0.5625
2017-08-08T16:22:39.865392: step 21, loss 1.21635, acc 0.640625
2017-08-08T16:22:40.152319: step 22, loss 1.86746, acc 0.5625
2017-08-08T16:22:40.360944: step 23, loss 1.80063, acc 0.53125
2017-08-08T16:22:40.803133: step 24, loss 2.12138, acc 0.390625
2017-08-08T16:22:41.056481: step 25, loss 1.66732, acc 0.4375
2017-08-08T16:22:41.338026: step 26, loss 2.26429, acc 0.421875
2017-08-08T16:22:41.634618: step 27, loss 1.1822, acc 0.546875
2017-08-08T16:22:42.049295: step 28, loss 1.99389, acc 0.4375
2017-08-08T16:22:42.455832: step 29, loss 1.99334, acc 0.5
2017-08-08T16:22:42.750636: step 30, loss 1.79604, acc 0.453125
2017-08-08T16:22:43.003592: step 31, loss 1.68508, acc 0.59375
2017-08-08T16:22:43.211086: step 32, loss 1.8018, acc 0.5
2017-08-08T16:22:43.627224: step 33, loss 1.98898, acc 0.484375
2017-08-08T16:22:43.956823: step 34, loss 1.63847, acc 0.484375
2017-08-08T16:22:44.247228: step 35, loss 1.94261, acc 0.4375
2017-08-08T16:22:44.508156: step 36, loss 1.67299, acc 0.5625
2017-08-08T16:22:44.935361: step 37, loss 1.49879, acc 0.640625
2017-08-08T16:22:45.257722: step 38, loss 1.6616, acc 0.4375
2017-08-08T16:22:45.615776: step 39, loss 1.55872, acc 0.53125
2017-08-08T16:22:45.907578: step 40, loss 2.19183, acc 0.4375
2017-08-08T16:22:46.128042: step 41, loss 1.71322, acc 0.546875
2017-08-08T16:22:46.557667: step 42, loss 1.6695, acc 0.484375
2017-08-08T16:22:46.853722: step 43, loss 1.73894, acc 0.4375
2017-08-08T16:22:47.123896: step 44, loss 1.66991, acc 0.5
2017-08-08T16:22:47.377889: step 45, loss 1.30881, acc 0.578125
2017-08-08T16:22:47.684496: step 46, loss 1.51066, acc 0.546875
2017-08-08T16:22:48.145673: step 47, loss 1.33287, acc 0.53125
2017-08-08T16:22:48.553877: step 48, loss 2.04489, acc 0.46875
2017-08-08T16:22:48.808699: step 49, loss 1.79577, acc 0.515625
2017-08-08T16:22:49.042738: step 50, loss 1.88588, acc 0.4375
2017-08-08T16:22:49.405814: step 51, loss 1.28976, acc 0.59375
2017-08-08T16:22:49.648039: step 52, loss 1.44231, acc 0.5
2017-08-08T16:22:49.870600: step 53, loss 1.58583, acc 0.515625
2017-08-08T16:22:50.093323: step 54, loss 1.51838, acc 0.515625
2017-08-08T16:22:50.469311: step 55, loss 1.6778, acc 0.53125
2017-08-08T16:22:50.873234: step 56, loss 1.41631, acc 0.5625
2017-08-08T16:22:51.116212: step 57, loss 1.62658, acc 0.484375
2017-08-08T16:22:51.349355: step 58, loss 1.15221, acc 0.625
2017-08-08T16:22:51.758576: step 59, loss 1.25638, acc 0.515625
2017-08-08T16:22:52.105027: step 60, loss 1.30292, acc 0.59375
2017-08-08T16:22:52.361376: step 61, loss 1.68312, acc 0.484375
2017-08-08T16:22:52.680797: step 62, loss 1.7058, acc 0.484375
2017-08-08T16:22:53.009527: step 63, loss 1.26181, acc 0.46875
2017-08-08T16:22:53.410867: step 64, loss 1.21769, acc 0.53125
2017-08-08T16:22:53.932176: step 65, loss 1.26199, acc 0.578125
2017-08-08T16:22:54.179614: step 66, loss 1.55987, acc 0.515625
2017-08-08T16:22:54.415836: step 67, loss 1.50433, acc 0.484375
2017-08-08T16:22:54.710510: step 68, loss 1.17172, acc 0.609375
2017-08-08T16:22:54.982631: step 69, loss 1.06164, acc 0.671875
2017-08-08T16:22:55.222146: step 70, loss 1.4315, acc 0.46875
2017-08-08T16:22:55.604612: step 71, loss 1.29433, acc 0.53125
2017-08-08T16:22:55.936966: step 72, loss 1.56032, acc 0.53125
2017-08-08T16:22:56.236889: step 73, loss 1.69416, acc 0.515625
2017-08-08T16:22:56.488436: step 74, loss 1.49134, acc 0.609375
2017-08-08T16:22:56.855537: step 75, loss 1.52104, acc 0.53125
2017-08-08T16:22:57.118416: step 76, loss 2.05535, acc 0.4375
2017-08-08T16:22:57.342608: step 77, loss 1.6908, acc 0.515625
2017-08-08T16:22:57.578653: step 78, loss 1.25599, acc 0.59375
2017-08-08T16:22:57.941453: step 79, loss 1.62313, acc 0.5
2017-08-08T16:22:58.271865: step 80, loss 1.62737, acc 0.515625
2017-08-08T16:22:58.525401: step 81, loss 1.76224, acc 0.484375
2017-08-08T16:22:58.729052: step 82, loss 1.03959, acc 0.5625
2017-08-08T16:22:59.053152: step 83, loss 1.21233, acc 0.53125
2017-08-08T16:22:59.278308: step 84, loss 1.32783, acc 0.625
2017-08-08T16:22:59.522986: step 85, loss 1.73357, acc 0.40625
2017-08-08T16:22:59.753679: step 86, loss 1.40727, acc 0.5
2017-08-08T16:23:00.226710: step 87, loss 1.55689, acc 0.484375
2017-08-08T16:23:00.564835: step 88, loss 1.75315, acc 0.453125
2017-08-08T16:23:00.850981: step 89, loss 1.5545, acc 0.53125
2017-08-08T16:23:01.073659: step 90, loss 1.3917, acc 0.46875
2017-08-08T16:23:01.289790: step 91, loss 1.25627, acc 0.59375
2017-08-08T16:23:01.733153: step 92, loss 1.23146, acc 0.484375
2017-08-08T16:23:02.075706: step 93, loss 1.24131, acc 0.625
2017-08-08T16:23:02.456512: step 94, loss 1.16348, acc 0.5
2017-08-08T16:23:02.785007: step 95, loss 1.47768, acc 0.5
2017-08-08T16:23:03.282160: step 96, loss 1.13541, acc 0.578125
2017-08-08T16:23:03.705411: step 97, loss 1.53867, acc 0.515625
2017-08-08T16:23:04.035737: step 98, loss 1.28005, acc 0.53125
2017-08-08T16:23:04.280551: step 99, loss 1.70302, acc 0.53125
2017-08-08T16:23:04.689743: step 100, loss 1.39537, acc 0.40625

Evaluation:
2017-08-08T16:23:05.532280: step 100, loss 0.847804, acc 0.555347

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-100

2017-08-08T16:23:06.127191: step 101, loss 1.47201, acc 0.453125
2017-08-08T16:23:06.498818: step 102, loss 1.45128, acc 0.5
2017-08-08T16:23:06.808649: step 103, loss 1.04829, acc 0.59375
2017-08-08T16:23:07.044674: step 104, loss 1.21098, acc 0.65625
2017-08-08T16:23:07.290535: step 105, loss 1.15172, acc 0.53125
2017-08-08T16:23:07.664868: step 106, loss 1.18318, acc 0.578125
2017-08-08T16:23:07.911681: step 107, loss 1.1379, acc 0.515625
2017-08-08T16:23:08.212695: step 108, loss 1.14371, acc 0.515625
2017-08-08T16:23:08.488100: step 109, loss 1.24383, acc 0.53125
2017-08-08T16:23:08.925384: step 110, loss 1.06716, acc 0.640625
2017-08-08T16:23:09.257557: step 111, loss 1.16204, acc 0.578125
2017-08-08T16:23:09.519269: step 112, loss 1.13446, acc 0.53125
2017-08-08T16:23:09.716910: step 113, loss 0.940909, acc 0.671875
2017-08-08T16:23:10.028810: step 114, loss 1.03602, acc 0.625
2017-08-08T16:23:10.303572: step 115, loss 1.1122, acc 0.5
2017-08-08T16:23:10.566551: step 116, loss 1.0346, acc 0.53125
2017-08-08T16:23:10.827595: step 117, loss 1.40447, acc 0.453125
2017-08-08T16:23:11.237363: step 118, loss 1.10593, acc 0.59375
2017-08-08T16:23:11.640224: step 119, loss 1.59962, acc 0.5625
2017-08-08T16:23:11.956265: step 120, loss 1.06028, acc 0.59375
2017-08-08T16:23:12.292856: step 121, loss 1.28274, acc 0.546875
2017-08-08T16:23:12.511062: step 122, loss 1.46014, acc 0.46875
2017-08-08T16:23:12.773366: step 123, loss 1.17441, acc 0.578125
2017-08-08T16:23:13.030040: step 124, loss 1.40394, acc 0.578125
2017-08-08T16:23:13.285380: step 125, loss 1.12889, acc 0.5
2017-08-08T16:23:13.477035: step 126, loss 1.10135, acc 0.5
2017-08-08T16:23:13.777371: step 127, loss 1.34279, acc 0.578125
2017-08-08T16:23:14.221994: step 128, loss 1.03338, acc 0.515625
2017-08-08T16:23:14.641387: step 129, loss 1.35273, acc 0.4375
2017-08-08T16:23:14.923892: step 130, loss 0.906579, acc 0.640625
2017-08-08T16:23:15.170771: step 131, loss 0.980642, acc 0.546875
2017-08-08T16:23:15.521406: step 132, loss 1.3281, acc 0.421875
2017-08-08T16:23:15.725322: step 133, loss 0.978669, acc 0.625
2017-08-08T16:23:15.936127: step 134, loss 1.79865, acc 0.453125
2017-08-08T16:23:16.241376: step 135, loss 1.10946, acc 0.515625
2017-08-08T16:23:16.638781: step 136, loss 1.03199, acc 0.59375
2017-08-08T16:23:16.930220: step 137, loss 1.43602, acc 0.515625
2017-08-08T16:23:17.146321: step 138, loss 1.06082, acc 0.609375
2017-08-08T16:23:17.332321: step 139, loss 0.88308, acc 0.59375
2017-08-08T16:23:17.641023: step 140, loss 1.20401, acc 0.53125
2017-08-08T16:23:17.832411: step 141, loss 1.18959, acc 0.59375
2017-08-08T16:23:18.092977: step 142, loss 1.28426, acc 0.453125
2017-08-08T16:23:18.299122: step 143, loss 1.40288, acc 0.53125
2017-08-08T16:23:18.649063: step 144, loss 1.11606, acc 0.5625
2017-08-08T16:23:19.054694: step 145, loss 1.13993, acc 0.515625
2017-08-08T16:23:19.382311: step 146, loss 1.0393, acc 0.59375
2017-08-08T16:23:19.638012: step 147, loss 1.20343, acc 0.515625
2017-08-08T16:23:19.810959: step 148, loss 1.13964, acc 0.546875
2017-08-08T16:23:20.078398: step 149, loss 1.453, acc 0.453125
2017-08-08T16:23:20.355386: step 150, loss 1.36055, acc 0.516667
2017-08-08T16:23:20.576237: step 151, loss 0.759101, acc 0.625
2017-08-08T16:23:20.790605: step 152, loss 0.782178, acc 0.65625
2017-08-08T16:23:21.157377: step 153, loss 0.975656, acc 0.578125
2017-08-08T16:23:21.443809: step 154, loss 0.97769, acc 0.5625
2017-08-08T16:23:21.705990: step 155, loss 0.994581, acc 0.5625
2017-08-08T16:23:21.924635: step 156, loss 1.03497, acc 0.546875
2017-08-08T16:23:22.109052: step 157, loss 0.799237, acc 0.671875
2017-08-08T16:23:22.499640: step 158, loss 1.13077, acc 0.5625
2017-08-08T16:23:22.774878: step 159, loss 1.18262, acc 0.5
2017-08-08T16:23:23.038999: step 160, loss 1.08853, acc 0.625
2017-08-08T16:23:23.298414: step 161, loss 1.0023, acc 0.65625
2017-08-08T16:23:23.716796: step 162, loss 0.708609, acc 0.65625
2017-08-08T16:23:24.101488: step 163, loss 0.811171, acc 0.625
2017-08-08T16:23:24.471093: step 164, loss 0.739577, acc 0.6875
2017-08-08T16:23:24.761505: step 165, loss 0.653478, acc 0.703125
2017-08-08T16:23:25.076506: step 166, loss 0.992597, acc 0.625
2017-08-08T16:23:25.582804: step 167, loss 1.01631, acc 0.484375
2017-08-08T16:23:25.893773: step 168, loss 0.494176, acc 0.796875
2017-08-08T16:23:26.192876: step 169, loss 0.766641, acc 0.734375
2017-08-08T16:23:26.542920: step 170, loss 0.937061, acc 0.59375
2017-08-08T16:23:26.954422: step 171, loss 0.796589, acc 0.609375
2017-08-08T16:23:27.359406: step 172, loss 0.954254, acc 0.640625
2017-08-08T16:23:27.614417: step 173, loss 1.30014, acc 0.515625
2017-08-08T16:23:27.882774: step 174, loss 1.04131, acc 0.5
2017-08-08T16:23:28.375709: step 175, loss 0.94933, acc 0.5625
2017-08-08T16:23:28.652400: step 176, loss 0.971425, acc 0.578125
2017-08-08T16:23:28.878413: step 177, loss 0.927517, acc 0.59375
2017-08-08T16:23:29.102811: step 178, loss 1.02172, acc 0.59375
2017-08-08T16:23:29.622942: step 179, loss 0.85437, acc 0.59375
2017-08-08T16:23:29.926175: step 180, loss 0.848347, acc 0.640625
2017-08-08T16:23:30.171744: step 181, loss 0.8378, acc 0.5625
2017-08-08T16:23:30.382183: step 182, loss 0.921747, acc 0.625
2017-08-08T16:23:30.797386: step 183, loss 0.838084, acc 0.640625
2017-08-08T16:23:31.028647: step 184, loss 0.81186, acc 0.640625
2017-08-08T16:23:31.249260: step 185, loss 0.972807, acc 0.5625
2017-08-08T16:23:31.517793: step 186, loss 0.781349, acc 0.671875
2017-08-08T16:23:31.958294: step 187, loss 0.745201, acc 0.671875
2017-08-08T16:23:32.226161: step 188, loss 0.65663, acc 0.625
2017-08-08T16:23:32.583813: step 189, loss 0.831835, acc 0.59375
2017-08-08T16:23:32.835801: step 190, loss 0.869575, acc 0.65625
2017-08-08T16:23:33.185147: step 191, loss 0.6293, acc 0.703125
2017-08-08T16:23:33.556544: step 192, loss 1.07912, acc 0.59375
2017-08-08T16:23:33.826872: step 193, loss 0.694079, acc 0.671875
2017-08-08T16:23:34.118325: step 194, loss 1.04638, acc 0.5625
2017-08-08T16:23:34.408277: step 195, loss 0.779898, acc 0.625
2017-08-08T16:23:34.869362: step 196, loss 0.696174, acc 0.71875
2017-08-08T16:23:35.302216: step 197, loss 0.993829, acc 0.515625
2017-08-08T16:23:35.599586: step 198, loss 0.979345, acc 0.59375
2017-08-08T16:23:35.856523: step 199, loss 1.04205, acc 0.59375
2017-08-08T16:23:36.231704: step 200, loss 0.775544, acc 0.640625

Evaluation:
2017-08-08T16:23:36.804035: step 200, loss 0.695131, acc 0.597561

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-200

2017-08-08T16:23:37.300333: step 201, loss 0.980245, acc 0.59375
2017-08-08T16:23:37.709764: step 202, loss 0.948501, acc 0.5625
2017-08-08T16:23:38.032412: step 203, loss 0.912922, acc 0.5625
2017-08-08T16:23:38.264667: step 204, loss 0.825986, acc 0.578125
2017-08-08T16:23:38.453321: step 205, loss 0.938324, acc 0.515625
2017-08-08T16:23:38.812980: step 206, loss 1.10252, acc 0.578125
2017-08-08T16:23:38.995959: step 207, loss 0.822357, acc 0.671875
2017-08-08T16:23:39.234699: step 208, loss 0.886948, acc 0.59375
2017-08-08T16:23:39.551013: step 209, loss 0.850349, acc 0.65625
2017-08-08T16:23:39.887667: step 210, loss 0.868994, acc 0.5625
2017-08-08T16:23:40.201690: step 211, loss 0.963182, acc 0.53125
2017-08-08T16:23:40.409740: step 212, loss 0.772851, acc 0.6875
2017-08-08T16:23:40.647872: step 213, loss 0.840773, acc 0.609375
2017-08-08T16:23:40.881336: step 214, loss 0.731676, acc 0.625
2017-08-08T16:23:41.066721: step 215, loss 1.01386, acc 0.546875
2017-08-08T16:23:41.245575: step 216, loss 0.975933, acc 0.59375
2017-08-08T16:23:41.511666: step 217, loss 0.92464, acc 0.53125
2017-08-08T16:23:41.724748: step 218, loss 1.26288, acc 0.46875
2017-08-08T16:23:41.948434: step 219, loss 1.1519, acc 0.484375
2017-08-08T16:23:42.158630: step 220, loss 0.659646, acc 0.625
2017-08-08T16:23:42.330483: step 221, loss 0.621364, acc 0.71875
2017-08-08T16:23:42.545370: step 222, loss 0.722469, acc 0.6875
2017-08-08T16:23:42.797730: step 223, loss 1.06769, acc 0.53125
2017-08-08T16:23:42.978359: step 224, loss 0.816085, acc 0.671875
2017-08-08T16:23:43.185066: step 225, loss 1.07358, acc 0.53125
2017-08-08T16:23:43.429352: step 226, loss 0.675682, acc 0.65625
2017-08-08T16:23:43.686263: step 227, loss 1.05928, acc 0.46875
2017-08-08T16:23:44.037403: step 228, loss 0.939164, acc 0.578125
2017-08-08T16:23:44.286313: step 229, loss 0.836232, acc 0.5625
2017-08-08T16:23:44.545309: step 230, loss 0.859217, acc 0.5625
2017-08-08T16:23:45.030988: step 231, loss 0.778248, acc 0.65625
2017-08-08T16:23:45.333898: step 232, loss 0.992568, acc 0.546875
2017-08-08T16:23:45.656860: step 233, loss 0.812213, acc 0.59375
2017-08-08T16:23:45.914784: step 234, loss 0.769556, acc 0.59375
2017-08-08T16:23:46.387120: step 235, loss 0.943146, acc 0.625
2017-08-08T16:23:46.771488: step 236, loss 0.740086, acc 0.609375
2017-08-08T16:23:46.996917: step 237, loss 0.868922, acc 0.515625
2017-08-08T16:23:47.233985: step 238, loss 0.707325, acc 0.625
2017-08-08T16:23:47.715646: step 239, loss 0.782549, acc 0.640625
2017-08-08T16:23:47.984919: step 240, loss 0.647871, acc 0.625
2017-08-08T16:23:48.221524: step 241, loss 0.899131, acc 0.5625
2017-08-08T16:23:48.500466: step 242, loss 1.07451, acc 0.484375
2017-08-08T16:23:48.905223: step 243, loss 0.766666, acc 0.59375
2017-08-08T16:23:49.223211: step 244, loss 0.864697, acc 0.609375
2017-08-08T16:23:49.554847: step 245, loss 0.99413, acc 0.515625
2017-08-08T16:23:49.832152: step 246, loss 0.791388, acc 0.625
2017-08-08T16:23:50.304896: step 247, loss 0.843987, acc 0.59375
2017-08-08T16:23:50.540722: step 248, loss 0.826299, acc 0.59375
2017-08-08T16:23:50.736971: step 249, loss 0.892407, acc 0.578125
2017-08-08T16:23:51.008504: step 250, loss 0.678079, acc 0.640625
2017-08-08T16:23:51.373628: step 251, loss 0.749967, acc 0.578125
2017-08-08T16:23:51.647135: step 252, loss 0.831838, acc 0.59375
2017-08-08T16:23:51.877105: step 253, loss 0.987921, acc 0.5
2017-08-08T16:23:52.104811: step 254, loss 0.780204, acc 0.609375
2017-08-08T16:23:52.421407: step 255, loss 0.797844, acc 0.578125
2017-08-08T16:23:52.644977: step 256, loss 0.711495, acc 0.640625
2017-08-08T16:23:52.861332: step 257, loss 0.868852, acc 0.5625
2017-08-08T16:23:53.073804: step 258, loss 0.794487, acc 0.625
2017-08-08T16:23:53.381366: step 259, loss 0.83912, acc 0.5625
2017-08-08T16:23:53.734511: step 260, loss 0.794469, acc 0.609375
2017-08-08T16:23:54.135671: step 261, loss 0.715781, acc 0.75
2017-08-08T16:23:54.445849: step 262, loss 0.800386, acc 0.609375
2017-08-08T16:23:54.664715: step 263, loss 0.691128, acc 0.71875
2017-08-08T16:23:55.001369: step 264, loss 0.623198, acc 0.65625
2017-08-08T16:23:55.323581: step 265, loss 0.775882, acc 0.640625
2017-08-08T16:23:55.537207: step 266, loss 0.976802, acc 0.515625
2017-08-08T16:23:55.801417: step 267, loss 0.746139, acc 0.578125
2017-08-08T16:23:56.164257: step 268, loss 0.817719, acc 0.5625
2017-08-08T16:23:56.620008: step 269, loss 0.693662, acc 0.640625
2017-08-08T16:23:56.937559: step 270, loss 0.736585, acc 0.71875
2017-08-08T16:23:57.176450: step 271, loss 0.777243, acc 0.640625
2017-08-08T16:23:57.426115: step 272, loss 0.644651, acc 0.59375
2017-08-08T16:23:57.852598: step 273, loss 0.653125, acc 0.6875
2017-08-08T16:23:58.077672: step 274, loss 0.790263, acc 0.609375
2017-08-08T16:23:58.353633: step 275, loss 0.743628, acc 0.640625
2017-08-08T16:23:58.624272: step 276, loss 0.712361, acc 0.625
2017-08-08T16:23:58.964633: step 277, loss 0.841731, acc 0.625
2017-08-08T16:23:59.377369: step 278, loss 0.732731, acc 0.65625
2017-08-08T16:23:59.824396: step 279, loss 0.570958, acc 0.765625
2017-08-08T16:24:00.138430: step 280, loss 0.797335, acc 0.59375
2017-08-08T16:24:00.384836: step 281, loss 0.696767, acc 0.71875
2017-08-08T16:24:00.773481: step 282, loss 0.662198, acc 0.65625
2017-08-08T16:24:01.036473: step 283, loss 0.613548, acc 0.703125
2017-08-08T16:24:01.300598: step 284, loss 0.866929, acc 0.5625
2017-08-08T16:24:01.607228: step 285, loss 0.86297, acc 0.640625
2017-08-08T16:24:02.128035: step 286, loss 0.654189, acc 0.71875
2017-08-08T16:24:02.602251: step 287, loss 0.640819, acc 0.671875
2017-08-08T16:24:03.009877: step 288, loss 0.7146, acc 0.65625
2017-08-08T16:24:03.383581: step 289, loss 0.674394, acc 0.65625
2017-08-08T16:24:03.616224: step 290, loss 0.557179, acc 0.609375
2017-08-08T16:24:04.012850: step 291, loss 0.855582, acc 0.578125
2017-08-08T16:24:04.304748: step 292, loss 0.792863, acc 0.484375
2017-08-08T16:24:04.573898: step 293, loss 0.842997, acc 0.578125
2017-08-08T16:24:04.834785: step 294, loss 0.703735, acc 0.640625
2017-08-08T16:24:05.202994: step 295, loss 0.599996, acc 0.671875
2017-08-08T16:24:05.619188: step 296, loss 0.740628, acc 0.578125
2017-08-08T16:24:05.966844: step 297, loss 0.732222, acc 0.65625
2017-08-08T16:24:06.199239: step 298, loss 0.818187, acc 0.609375
2017-08-08T16:24:06.521400: step 299, loss 0.996847, acc 0.546875
2017-08-08T16:24:06.734007: step 300, loss 0.677827, acc 0.6

Evaluation:
2017-08-08T16:24:07.324376: step 300, loss 0.64453, acc 0.621951

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-300

2017-08-08T16:24:07.865944: step 301, loss 0.632005, acc 0.6875
2017-08-08T16:24:08.184365: step 302, loss 0.660052, acc 0.703125
2017-08-08T16:24:08.390709: step 303, loss 0.613049, acc 0.6875
2017-08-08T16:24:08.591659: step 304, loss 0.495672, acc 0.75
2017-08-08T16:24:08.940528: step 305, loss 0.827814, acc 0.609375
2017-08-08T16:24:09.218419: step 306, loss 0.733585, acc 0.59375
2017-08-08T16:24:09.509318: step 307, loss 0.562595, acc 0.65625
2017-08-08T16:24:09.897415: step 308, loss 0.780943, acc 0.578125
2017-08-08T16:24:10.397397: step 309, loss 0.703664, acc 0.609375
2017-08-08T16:24:10.694472: step 310, loss 0.953527, acc 0.546875
2017-08-08T16:24:10.977861: step 311, loss 0.892714, acc 0.59375
2017-08-08T16:24:11.361360: step 312, loss 0.542053, acc 0.734375
2017-08-08T16:24:11.622030: step 313, loss 0.683436, acc 0.625
2017-08-08T16:24:11.848959: step 314, loss 0.568909, acc 0.75
2017-08-08T16:24:12.061933: step 315, loss 0.768366, acc 0.546875
2017-08-08T16:24:12.331571: step 316, loss 0.778342, acc 0.65625
2017-08-08T16:24:12.632460: step 317, loss 0.672454, acc 0.578125
2017-08-08T16:24:12.959384: step 318, loss 0.622602, acc 0.65625
2017-08-08T16:24:13.193990: step 319, loss 0.745706, acc 0.609375
2017-08-08T16:24:13.400313: step 320, loss 0.745937, acc 0.609375
2017-08-08T16:24:13.672704: step 321, loss 0.80037, acc 0.53125
2017-08-08T16:24:13.998614: step 322, loss 0.660036, acc 0.671875
2017-08-08T16:24:14.289954: step 323, loss 0.660303, acc 0.703125
2017-08-08T16:24:14.547115: step 324, loss 0.598007, acc 0.6875
2017-08-08T16:24:14.840525: step 325, loss 0.596329, acc 0.765625
2017-08-08T16:24:15.063309: step 326, loss 0.622034, acc 0.703125
2017-08-08T16:24:15.293312: step 327, loss 0.570787, acc 0.78125
2017-08-08T16:24:15.532547: step 328, loss 0.689612, acc 0.640625
2017-08-08T16:24:15.839903: step 329, loss 0.700152, acc 0.59375
2017-08-08T16:24:16.199691: step 330, loss 0.788761, acc 0.59375
2017-08-08T16:24:16.414667: step 331, loss 0.574713, acc 0.6875
2017-08-08T16:24:16.624673: step 332, loss 0.738924, acc 0.546875
2017-08-08T16:24:16.985087: step 333, loss 0.706924, acc 0.609375
2017-08-08T16:24:17.316400: step 334, loss 0.629671, acc 0.640625
2017-08-08T16:24:17.706531: step 335, loss 0.641643, acc 0.6875
2017-08-08T16:24:18.029899: step 336, loss 0.642646, acc 0.671875
2017-08-08T16:24:18.254257: step 337, loss 0.738196, acc 0.640625
2017-08-08T16:24:18.478060: step 338, loss 0.739495, acc 0.5625
2017-08-08T16:24:18.876339: step 339, loss 0.796139, acc 0.59375
2017-08-08T16:24:19.149918: step 340, loss 0.65223, acc 0.65625
2017-08-08T16:24:19.442869: step 341, loss 0.657467, acc 0.625
2017-08-08T16:24:19.728571: step 342, loss 0.799892, acc 0.546875
2017-08-08T16:24:20.159264: step 343, loss 0.700473, acc 0.640625
2017-08-08T16:24:20.508392: step 344, loss 0.71226, acc 0.625
2017-08-08T16:24:20.813768: step 345, loss 0.442293, acc 0.8125
2017-08-08T16:24:21.048064: step 346, loss 0.617617, acc 0.6875
2017-08-08T16:24:21.334953: step 347, loss 0.554852, acc 0.703125
2017-08-08T16:24:21.690271: step 348, loss 0.57388, acc 0.6875
2017-08-08T16:24:21.887783: step 349, loss 0.750628, acc 0.625
2017-08-08T16:24:22.101502: step 350, loss 0.649539, acc 0.671875
2017-08-08T16:24:22.316162: step 351, loss 0.621561, acc 0.65625
2017-08-08T16:24:22.780382: step 352, loss 0.529461, acc 0.703125
2017-08-08T16:24:23.137642: step 353, loss 0.756262, acc 0.59375
2017-08-08T16:24:23.376393: step 354, loss 0.719641, acc 0.625
2017-08-08T16:24:23.583765: step 355, loss 0.556836, acc 0.734375
2017-08-08T16:24:23.890069: step 356, loss 0.695497, acc 0.578125
2017-08-08T16:24:24.072082: step 357, loss 0.760067, acc 0.71875
2017-08-08T16:24:24.289945: step 358, loss 0.579118, acc 0.703125
2017-08-08T16:24:24.499568: step 359, loss 0.694882, acc 0.59375
2017-08-08T16:24:24.877293: step 360, loss 0.570466, acc 0.734375
2017-08-08T16:24:25.177900: step 361, loss 0.680273, acc 0.671875
2017-08-08T16:24:25.411442: step 362, loss 0.668271, acc 0.640625
2017-08-08T16:24:25.593273: step 363, loss 0.628501, acc 0.65625
2017-08-08T16:24:25.858799: step 364, loss 0.605765, acc 0.71875
2017-08-08T16:24:26.106824: step 365, loss 0.570093, acc 0.671875
2017-08-08T16:24:26.322213: step 366, loss 0.63558, acc 0.625
2017-08-08T16:24:26.652360: step 367, loss 0.665535, acc 0.65625
2017-08-08T16:24:26.993317: step 368, loss 0.568626, acc 0.6875
2017-08-08T16:24:27.305814: step 369, loss 0.643577, acc 0.703125
2017-08-08T16:24:27.554682: step 370, loss 0.535035, acc 0.75
2017-08-08T16:24:27.739911: step 371, loss 0.658681, acc 0.65625
2017-08-08T16:24:28.001320: step 372, loss 0.61633, acc 0.65625
2017-08-08T16:24:28.205335: step 373, loss 0.698504, acc 0.671875
2017-08-08T16:24:28.447565: step 374, loss 0.804117, acc 0.53125
2017-08-08T16:24:28.695338: step 375, loss 0.618326, acc 0.703125
2017-08-08T16:24:29.063849: step 376, loss 0.664642, acc 0.703125
2017-08-08T16:24:29.426018: step 377, loss 0.456419, acc 0.8125
2017-08-08T16:24:29.792941: step 378, loss 0.528457, acc 0.75
2017-08-08T16:24:30.092485: step 379, loss 0.655953, acc 0.640625
2017-08-08T16:24:30.343419: step 380, loss 0.519952, acc 0.78125
2017-08-08T16:24:30.822198: step 381, loss 0.534679, acc 0.671875
2017-08-08T16:24:31.108330: step 382, loss 0.608303, acc 0.625
2017-08-08T16:24:31.382559: step 383, loss 0.657339, acc 0.640625
2017-08-08T16:24:31.684062: step 384, loss 0.603469, acc 0.703125
2017-08-08T16:24:32.109359: step 385, loss 0.598303, acc 0.65625
2017-08-08T16:24:32.633503: step 386, loss 0.570919, acc 0.6875
2017-08-08T16:24:32.981691: step 387, loss 0.584071, acc 0.71875
2017-08-08T16:24:33.222034: step 388, loss 0.71142, acc 0.5
2017-08-08T16:24:33.649371: step 389, loss 0.677771, acc 0.65625
2017-08-08T16:24:33.951753: step 390, loss 0.620478, acc 0.640625
2017-08-08T16:24:34.202221: step 391, loss 0.670549, acc 0.640625
2017-08-08T16:24:34.433734: step 392, loss 0.606365, acc 0.609375
2017-08-08T16:24:34.764656: step 393, loss 0.70694, acc 0.625
2017-08-08T16:24:35.172669: step 394, loss 0.668961, acc 0.640625
2017-08-08T16:24:35.537604: step 395, loss 0.660678, acc 0.671875
2017-08-08T16:24:35.779865: step 396, loss 0.740441, acc 0.609375
2017-08-08T16:24:36.085374: step 397, loss 0.621706, acc 0.65625
2017-08-08T16:24:36.421206: step 398, loss 0.588187, acc 0.625
2017-08-08T16:24:36.705523: step 399, loss 0.62165, acc 0.6875
2017-08-08T16:24:36.947188: step 400, loss 0.625022, acc 0.671875

Evaluation:
2017-08-08T16:24:37.684221: step 400, loss 0.661038, acc 0.598499

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-400

2017-08-08T16:24:38.257176: step 401, loss 0.597156, acc 0.734375
2017-08-08T16:24:38.521384: step 402, loss 0.676582, acc 0.609375
2017-08-08T16:24:38.842810: step 403, loss 0.539998, acc 0.71875
2017-08-08T16:24:39.232304: step 404, loss 0.581686, acc 0.640625
2017-08-08T16:24:39.508801: step 405, loss 0.613726, acc 0.65625
2017-08-08T16:24:39.780929: step 406, loss 0.517544, acc 0.71875
2017-08-08T16:24:40.157385: step 407, loss 0.713129, acc 0.546875
2017-08-08T16:24:40.473392: step 408, loss 0.623317, acc 0.640625
2017-08-08T16:24:40.800037: step 409, loss 0.665415, acc 0.671875
2017-08-08T16:24:41.121417: step 410, loss 0.70274, acc 0.625
2017-08-08T16:24:41.450767: step 411, loss 0.610729, acc 0.6875
2017-08-08T16:24:41.761921: step 412, loss 0.650592, acc 0.609375
2017-08-08T16:24:42.001141: step 413, loss 0.651157, acc 0.65625
2017-08-08T16:24:42.235484: step 414, loss 0.755058, acc 0.625
2017-08-08T16:24:42.517871: step 415, loss 0.717101, acc 0.546875
2017-08-08T16:24:42.835636: step 416, loss 0.586061, acc 0.6875
2017-08-08T16:24:43.181471: step 417, loss 0.638547, acc 0.609375
2017-08-08T16:24:43.446637: step 418, loss 0.652251, acc 0.625
2017-08-08T16:24:43.690091: step 419, loss 0.792886, acc 0.578125
2017-08-08T16:24:44.050053: step 420, loss 0.746948, acc 0.65625
2017-08-08T16:24:44.245610: step 421, loss 0.637157, acc 0.65625
2017-08-08T16:24:44.412658: step 422, loss 0.665361, acc 0.609375
2017-08-08T16:24:44.693344: step 423, loss 0.649897, acc 0.625
2017-08-08T16:24:45.054714: step 424, loss 0.508765, acc 0.75
2017-08-08T16:24:45.353720: step 425, loss 0.6555, acc 0.65625
2017-08-08T16:24:45.578737: step 426, loss 0.743282, acc 0.671875
2017-08-08T16:24:45.906615: step 427, loss 0.762768, acc 0.515625
2017-08-08T16:24:46.167191: step 428, loss 0.62756, acc 0.65625
2017-08-08T16:24:46.406351: step 429, loss 0.6188, acc 0.640625
2017-08-08T16:24:46.599072: step 430, loss 0.620086, acc 0.671875
2017-08-08T16:24:46.862689: step 431, loss 0.667192, acc 0.6875
2017-08-08T16:24:47.220403: step 432, loss 0.584229, acc 0.703125
2017-08-08T16:24:47.489437: step 433, loss 0.599296, acc 0.65625
2017-08-08T16:24:47.662955: step 434, loss 0.737119, acc 0.625
2017-08-08T16:24:47.920649: step 435, loss 0.636971, acc 0.671875
2017-08-08T16:24:48.269221: step 436, loss 0.602692, acc 0.734375
2017-08-08T16:24:48.481531: step 437, loss 0.658128, acc 0.640625
2017-08-08T16:24:48.662507: step 438, loss 0.631471, acc 0.71875
2017-08-08T16:24:49.005138: step 439, loss 0.544936, acc 0.703125
2017-08-08T16:24:49.409332: step 440, loss 0.70928, acc 0.65625
2017-08-08T16:24:49.683119: step 441, loss 0.627336, acc 0.65625
2017-08-08T16:24:49.874334: step 442, loss 0.574618, acc 0.734375
2017-08-08T16:24:50.182709: step 443, loss 0.701094, acc 0.625
2017-08-08T16:24:50.495595: step 444, loss 0.630635, acc 0.609375
2017-08-08T16:24:50.764264: step 445, loss 0.627031, acc 0.640625
2017-08-08T16:24:51.004061: step 446, loss 0.552789, acc 0.65625
2017-08-08T16:24:51.498326: step 447, loss 0.576522, acc 0.71875
2017-08-08T16:24:51.883635: step 448, loss 0.537184, acc 0.671875
2017-08-08T16:24:52.214137: step 449, loss 0.562891, acc 0.734375
2017-08-08T16:24:52.453420: step 450, loss 0.671262, acc 0.616667
2017-08-08T16:24:52.731123: step 451, loss 0.630445, acc 0.703125
2017-08-08T16:24:52.927985: step 452, loss 0.572708, acc 0.703125
2017-08-08T16:24:53.212233: step 453, loss 0.580457, acc 0.6875
2017-08-08T16:24:53.568873: step 454, loss 0.560152, acc 0.6875
2017-08-08T16:24:53.823952: step 455, loss 0.496082, acc 0.75
2017-08-08T16:24:54.030422: step 456, loss 0.578587, acc 0.734375
2017-08-08T16:24:54.222561: step 457, loss 0.575327, acc 0.625
2017-08-08T16:24:54.467888: step 458, loss 0.516875, acc 0.734375
2017-08-08T16:24:54.915044: step 459, loss 0.646268, acc 0.75
2017-08-08T16:24:55.208810: step 460, loss 0.709927, acc 0.671875
2017-08-08T16:24:55.481401: step 461, loss 0.58586, acc 0.6875
2017-08-08T16:24:55.689809: step 462, loss 0.492534, acc 0.796875
2017-08-08T16:24:56.026584: step 463, loss 0.505266, acc 0.75
2017-08-08T16:24:56.310307: step 464, loss 0.656194, acc 0.640625
2017-08-08T16:24:56.556176: step 465, loss 0.49333, acc 0.75
2017-08-08T16:24:56.758181: step 466, loss 0.624138, acc 0.65625
2017-08-08T16:24:57.050072: step 467, loss 0.555788, acc 0.671875
2017-08-08T16:24:57.417125: step 468, loss 0.600739, acc 0.671875
2017-08-08T16:24:57.628207: step 469, loss 0.613732, acc 0.703125
2017-08-08T16:24:57.893446: step 470, loss 0.601098, acc 0.671875
2017-08-08T16:24:58.310856: step 471, loss 0.561991, acc 0.71875
2017-08-08T16:24:58.617335: step 472, loss 0.56631, acc 0.640625
2017-08-08T16:24:58.854543: step 473, loss 0.582603, acc 0.671875
2017-08-08T16:24:59.138017: step 474, loss 0.609272, acc 0.671875
2017-08-08T16:24:59.402298: step 475, loss 0.596149, acc 0.734375
2017-08-08T16:24:59.589317: step 476, loss 0.619625, acc 0.640625
2017-08-08T16:24:59.822709: step 477, loss 0.574378, acc 0.6875
2017-08-08T16:25:00.250048: step 478, loss 0.552348, acc 0.703125
2017-08-08T16:25:00.600713: step 479, loss 0.550583, acc 0.734375
2017-08-08T16:25:00.910250: step 480, loss 0.679101, acc 0.640625
2017-08-08T16:25:01.189949: step 481, loss 0.549314, acc 0.703125
2017-08-08T16:25:01.537839: step 482, loss 0.499691, acc 0.75
2017-08-08T16:25:01.859289: step 483, loss 0.498944, acc 0.78125
2017-08-08T16:25:02.126801: step 484, loss 0.566754, acc 0.734375
2017-08-08T16:25:02.411711: step 485, loss 0.566584, acc 0.71875
2017-08-08T16:25:02.729197: step 486, loss 0.578748, acc 0.765625
2017-08-08T16:25:03.257907: step 487, loss 0.573988, acc 0.65625
2017-08-08T16:25:03.702946: step 488, loss 0.5187, acc 0.71875
2017-08-08T16:25:04.020247: step 489, loss 0.546382, acc 0.71875
2017-08-08T16:25:04.259172: step 490, loss 0.576983, acc 0.6875
2017-08-08T16:25:04.542340: step 491, loss 0.59403, acc 0.703125
2017-08-08T16:25:04.964829: step 492, loss 0.538048, acc 0.75
2017-08-08T16:25:05.234732: step 493, loss 0.61184, acc 0.6875
2017-08-08T16:25:05.557656: step 494, loss 0.584268, acc 0.71875
2017-08-08T16:25:05.889777: step 495, loss 0.52685, acc 0.671875
2017-08-08T16:25:06.316030: step 496, loss 0.758005, acc 0.609375
2017-08-08T16:25:06.729258: step 497, loss 0.526043, acc 0.75
2017-08-08T16:25:06.967277: step 498, loss 0.515514, acc 0.765625
2017-08-08T16:25:07.199392: step 499, loss 0.587463, acc 0.734375
2017-08-08T16:25:07.527518: step 500, loss 0.531808, acc 0.75

Evaluation:
2017-08-08T16:25:08.153688: step 500, loss 0.632462, acc 0.635084

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-500

2017-08-08T16:25:08.609188: step 501, loss 0.505301, acc 0.8125
2017-08-08T16:25:08.947448: step 502, loss 0.678791, acc 0.609375
2017-08-08T16:25:09.308074: step 503, loss 0.506292, acc 0.71875
2017-08-08T16:25:09.660842: step 504, loss 0.477408, acc 0.828125
2017-08-08T16:25:09.882060: step 505, loss 0.605683, acc 0.71875
2017-08-08T16:25:10.115925: step 506, loss 0.570739, acc 0.6875
2017-08-08T16:25:10.509328: step 507, loss 0.671005, acc 0.640625
2017-08-08T16:25:10.768145: step 508, loss 0.662531, acc 0.671875
2017-08-08T16:25:11.031016: step 509, loss 0.492766, acc 0.734375
2017-08-08T16:25:11.304990: step 510, loss 0.454566, acc 0.84375
2017-08-08T16:25:11.705331: step 511, loss 0.584142, acc 0.71875
2017-08-08T16:25:11.985358: step 512, loss 0.526807, acc 0.71875
2017-08-08T16:25:12.325374: step 513, loss 0.565904, acc 0.65625
2017-08-08T16:25:12.544302: step 514, loss 0.635882, acc 0.6875
2017-08-08T16:25:12.841506: step 515, loss 0.492947, acc 0.796875
2017-08-08T16:25:13.123573: step 516, loss 0.646572, acc 0.671875
2017-08-08T16:25:13.375803: step 517, loss 0.619285, acc 0.671875
2017-08-08T16:25:13.606062: step 518, loss 0.464197, acc 0.78125
2017-08-08T16:25:13.901309: step 519, loss 0.70137, acc 0.5625
2017-08-08T16:25:14.428331: step 520, loss 0.558009, acc 0.734375
2017-08-08T16:25:14.861698: step 521, loss 0.476431, acc 0.8125
2017-08-08T16:25:15.173911: step 522, loss 0.53472, acc 0.703125
2017-08-08T16:25:15.486404: step 523, loss 0.601839, acc 0.59375
2017-08-08T16:25:15.853613: step 524, loss 0.451936, acc 0.796875
2017-08-08T16:25:16.065980: step 525, loss 0.627596, acc 0.65625
2017-08-08T16:25:16.323054: step 526, loss 0.600451, acc 0.71875
2017-08-08T16:25:16.561341: step 527, loss 0.554344, acc 0.75
2017-08-08T16:25:17.033255: step 528, loss 0.546632, acc 0.65625
2017-08-08T16:25:17.378533: step 529, loss 0.504836, acc 0.75
2017-08-08T16:25:17.712551: step 530, loss 0.650467, acc 0.65625
2017-08-08T16:25:17.983743: step 531, loss 0.654519, acc 0.59375
2017-08-08T16:25:18.399353: step 532, loss 0.64763, acc 0.71875
2017-08-08T16:25:18.767560: step 533, loss 0.605709, acc 0.609375
2017-08-08T16:25:19.034510: step 534, loss 0.610653, acc 0.640625
2017-08-08T16:25:19.287181: step 535, loss 0.536061, acc 0.796875
2017-08-08T16:25:19.474822: step 536, loss 0.706507, acc 0.5625
2017-08-08T16:25:19.771569: step 537, loss 0.415909, acc 0.8125
2017-08-08T16:25:20.039603: step 538, loss 0.595531, acc 0.6875
2017-08-08T16:25:20.298169: step 539, loss 0.583315, acc 0.671875
2017-08-08T16:25:20.507238: step 540, loss 0.516021, acc 0.796875
2017-08-08T16:25:20.838749: step 541, loss 0.517282, acc 0.71875
2017-08-08T16:25:21.155638: step 542, loss 0.559575, acc 0.765625
2017-08-08T16:25:21.417761: step 543, loss 0.513955, acc 0.734375
2017-08-08T16:25:21.681527: step 544, loss 0.59143, acc 0.65625
2017-08-08T16:25:22.157354: step 545, loss 0.634003, acc 0.65625
2017-08-08T16:25:22.503729: step 546, loss 0.488316, acc 0.796875
2017-08-08T16:25:22.719315: step 547, loss 0.570674, acc 0.6875
2017-08-08T16:25:22.887498: step 548, loss 0.475856, acc 0.765625
2017-08-08T16:25:23.237440: step 549, loss 0.591909, acc 0.71875
2017-08-08T16:25:23.446920: step 550, loss 0.49464, acc 0.75
2017-08-08T16:25:23.641551: step 551, loss 0.532194, acc 0.75
2017-08-08T16:25:23.829437: step 552, loss 0.588968, acc 0.65625
2017-08-08T16:25:24.029777: step 553, loss 0.668312, acc 0.65625
2017-08-08T16:25:24.340593: step 554, loss 0.563299, acc 0.703125
2017-08-08T16:25:24.587602: step 555, loss 0.490805, acc 0.75
2017-08-08T16:25:24.819989: step 556, loss 0.591814, acc 0.75
2017-08-08T16:25:24.975025: step 557, loss 0.608179, acc 0.734375
2017-08-08T16:25:25.209363: step 558, loss 0.504485, acc 0.75
2017-08-08T16:25:25.483996: step 559, loss 0.480652, acc 0.75
2017-08-08T16:25:25.700878: step 560, loss 0.612578, acc 0.6875
2017-08-08T16:25:25.883089: step 561, loss 0.458878, acc 0.78125
2017-08-08T16:25:26.094203: step 562, loss 0.491458, acc 0.734375
2017-08-08T16:25:26.517173: step 563, loss 0.536173, acc 0.6875
2017-08-08T16:25:26.825862: step 564, loss 0.65438, acc 0.625
2017-08-08T16:25:27.131824: step 565, loss 0.611598, acc 0.734375
2017-08-08T16:25:27.345774: step 566, loss 0.471715, acc 0.796875
2017-08-08T16:25:27.563017: step 567, loss 0.584765, acc 0.734375
2017-08-08T16:25:27.883723: step 568, loss 0.5744, acc 0.65625
2017-08-08T16:25:28.123087: step 569, loss 0.533444, acc 0.78125
2017-08-08T16:25:28.318106: step 570, loss 0.704365, acc 0.625
2017-08-08T16:25:28.583042: step 571, loss 0.508625, acc 0.734375
2017-08-08T16:25:28.934120: step 572, loss 0.615395, acc 0.703125
2017-08-08T16:25:29.286024: step 573, loss 0.556159, acc 0.75
2017-08-08T16:25:29.547931: step 574, loss 0.467051, acc 0.78125
2017-08-08T16:25:29.765322: step 575, loss 0.591086, acc 0.6875
2017-08-08T16:25:30.077843: step 576, loss 0.539068, acc 0.75
2017-08-08T16:25:30.307797: step 577, loss 0.564144, acc 0.734375
2017-08-08T16:25:30.575849: step 578, loss 0.688995, acc 0.609375
2017-08-08T16:25:30.833205: step 579, loss 0.623937, acc 0.6875
2017-08-08T16:25:31.079531: step 580, loss 0.602867, acc 0.625
2017-08-08T16:25:31.360778: step 581, loss 0.628699, acc 0.6875
2017-08-08T16:25:31.577654: step 582, loss 0.620634, acc 0.71875
2017-08-08T16:25:31.912207: step 583, loss 0.716117, acc 0.578125
2017-08-08T16:25:32.152352: step 584, loss 0.597929, acc 0.734375
2017-08-08T16:25:32.523880: step 585, loss 0.507731, acc 0.734375
2017-08-08T16:25:32.833206: step 586, loss 0.584124, acc 0.703125
2017-08-08T16:25:33.025011: step 587, loss 0.483319, acc 0.796875
2017-08-08T16:25:33.279593: step 588, loss 0.487622, acc 0.78125
2017-08-08T16:25:33.567732: step 589, loss 0.542696, acc 0.71875
2017-08-08T16:25:33.901634: step 590, loss 0.641793, acc 0.65625
2017-08-08T16:25:34.276306: step 591, loss 0.729853, acc 0.59375
2017-08-08T16:25:34.553982: step 592, loss 0.444929, acc 0.8125
2017-08-08T16:25:34.912584: step 593, loss 0.650141, acc 0.578125
2017-08-08T16:25:35.242314: step 594, loss 0.554607, acc 0.6875
2017-08-08T16:25:35.517364: step 595, loss 0.579033, acc 0.6875
2017-08-08T16:25:35.959936: step 596, loss 0.618099, acc 0.6875
2017-08-08T16:25:36.236729: step 597, loss 0.436047, acc 0.828125
2017-08-08T16:25:36.681351: step 598, loss 0.478849, acc 0.765625
2017-08-08T16:25:37.004955: step 599, loss 0.555605, acc 0.71875
2017-08-08T16:25:37.352773: step 600, loss 0.544465, acc 0.766667

Evaluation:
2017-08-08T16:25:38.156057: step 600, loss 0.705316, acc 0.571295

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-600

2017-08-08T16:25:38.764062: step 601, loss 0.598553, acc 0.734375
2017-08-08T16:25:39.099125: step 602, loss 0.562876, acc 0.703125
2017-08-08T16:25:39.481066: step 603, loss 0.561756, acc 0.75
2017-08-08T16:25:39.748020: step 604, loss 0.534951, acc 0.703125
2017-08-08T16:25:40.157172: step 605, loss 0.487207, acc 0.75
2017-08-08T16:25:40.411822: step 606, loss 0.359796, acc 0.84375
2017-08-08T16:25:40.765936: step 607, loss 0.597375, acc 0.671875
2017-08-08T16:25:41.090628: step 608, loss 0.648762, acc 0.703125
2017-08-08T16:25:41.358217: step 609, loss 0.744899, acc 0.625
2017-08-08T16:25:41.747648: step 610, loss 0.472936, acc 0.765625
2017-08-08T16:25:42.071114: step 611, loss 0.588994, acc 0.6875
2017-08-08T16:25:42.330963: step 612, loss 0.384456, acc 0.875
2017-08-08T16:25:42.619016: step 613, loss 0.623506, acc 0.671875
2017-08-08T16:25:42.912974: step 614, loss 0.611209, acc 0.71875
2017-08-08T16:25:43.295099: step 615, loss 0.564592, acc 0.703125
2017-08-08T16:25:43.700011: step 616, loss 0.431946, acc 0.828125
2017-08-08T16:25:44.052328: step 617, loss 0.604443, acc 0.640625
2017-08-08T16:25:44.323617: step 618, loss 0.447873, acc 0.78125
2017-08-08T16:25:44.665559: step 619, loss 0.425573, acc 0.828125
2017-08-08T16:25:45.034259: step 620, loss 0.710743, acc 0.65625
2017-08-08T16:25:45.283954: step 621, loss 0.521432, acc 0.71875
2017-08-08T16:25:45.531931: step 622, loss 0.503209, acc 0.765625
2017-08-08T16:25:45.769355: step 623, loss 0.433573, acc 0.8125
2017-08-08T16:25:46.155221: step 624, loss 0.576427, acc 0.71875
2017-08-08T16:25:46.439934: step 625, loss 0.421087, acc 0.8125
2017-08-08T16:25:46.686331: step 626, loss 0.516921, acc 0.78125
2017-08-08T16:25:46.864661: step 627, loss 0.515599, acc 0.6875
2017-08-08T16:25:47.127851: step 628, loss 0.499255, acc 0.78125
2017-08-08T16:25:47.503989: step 629, loss 0.533907, acc 0.6875
2017-08-08T16:25:47.705380: step 630, loss 0.447594, acc 0.796875
2017-08-08T16:25:47.928020: step 631, loss 0.456947, acc 0.78125
2017-08-08T16:25:48.170372: step 632, loss 0.522759, acc 0.765625
2017-08-08T16:25:48.648487: step 633, loss 0.439131, acc 0.78125
2017-08-08T16:25:48.954453: step 634, loss 0.56694, acc 0.734375
2017-08-08T16:25:49.222235: step 635, loss 0.493796, acc 0.796875
2017-08-08T16:25:49.503918: step 636, loss 0.547659, acc 0.734375
2017-08-08T16:25:49.890006: step 637, loss 0.498782, acc 0.734375
2017-08-08T16:25:50.186724: step 638, loss 0.451023, acc 0.78125
2017-08-08T16:25:50.500002: step 639, loss 0.551415, acc 0.671875
2017-08-08T16:25:50.875014: step 640, loss 0.541622, acc 0.75
2017-08-08T16:25:51.230012: step 641, loss 0.498772, acc 0.78125
2017-08-08T16:25:51.601985: step 642, loss 0.572836, acc 0.703125
2017-08-08T16:25:51.857244: step 643, loss 0.536687, acc 0.71875
2017-08-08T16:25:52.175443: step 644, loss 0.521344, acc 0.75
2017-08-08T16:25:52.588002: step 645, loss 0.49027, acc 0.75
2017-08-08T16:25:52.886071: step 646, loss 0.561323, acc 0.703125
2017-08-08T16:25:53.111595: step 647, loss 0.502428, acc 0.78125
2017-08-08T16:25:53.369720: step 648, loss 0.458235, acc 0.828125
2017-08-08T16:25:53.669434: step 649, loss 0.440485, acc 0.765625
2017-08-08T16:25:53.995181: step 650, loss 0.471205, acc 0.8125
2017-08-08T16:25:54.358883: step 651, loss 0.504554, acc 0.765625
2017-08-08T16:25:54.603285: step 652, loss 0.489711, acc 0.71875
2017-08-08T16:25:54.990427: step 653, loss 0.483571, acc 0.734375
2017-08-08T16:25:55.370970: step 654, loss 0.466897, acc 0.8125
2017-08-08T16:25:55.647012: step 655, loss 0.475523, acc 0.765625
2017-08-08T16:25:55.915624: step 656, loss 0.558465, acc 0.703125
2017-08-08T16:25:56.270083: step 657, loss 0.558752, acc 0.703125
2017-08-08T16:25:56.641389: step 658, loss 0.517111, acc 0.703125
2017-08-08T16:25:56.993043: step 659, loss 0.463313, acc 0.796875
2017-08-08T16:25:57.178201: step 660, loss 0.537452, acc 0.734375
2017-08-08T16:25:57.369899: step 661, loss 0.540833, acc 0.8125
2017-08-08T16:25:57.631125: step 662, loss 0.470372, acc 0.765625
2017-08-08T16:25:57.806795: step 663, loss 0.520994, acc 0.703125
2017-08-08T16:25:57.962440: step 664, loss 0.511188, acc 0.703125
2017-08-08T16:25:58.244264: step 665, loss 0.655468, acc 0.671875
2017-08-08T16:25:58.486710: step 666, loss 0.598164, acc 0.75
2017-08-08T16:25:58.716014: step 667, loss 0.525737, acc 0.75
2017-08-08T16:25:58.897149: step 668, loss 0.520382, acc 0.765625
2017-08-08T16:25:59.181151: step 669, loss 0.57341, acc 0.71875
2017-08-08T16:25:59.346057: step 670, loss 0.459799, acc 0.765625
2017-08-08T16:25:59.522481: step 671, loss 0.609029, acc 0.71875
2017-08-08T16:25:59.717311: step 672, loss 0.493482, acc 0.765625
2017-08-08T16:25:59.971756: step 673, loss 0.559288, acc 0.6875
2017-08-08T16:26:00.299928: step 674, loss 0.390098, acc 0.796875
2017-08-08T16:26:00.749445: step 675, loss 0.453818, acc 0.828125
2017-08-08T16:26:00.989498: step 676, loss 0.612233, acc 0.640625
2017-08-08T16:26:01.219230: step 677, loss 0.453146, acc 0.765625
2017-08-08T16:26:01.458161: step 678, loss 0.476921, acc 0.796875
2017-08-08T16:26:01.774986: step 679, loss 0.477316, acc 0.703125
2017-08-08T16:26:02.094969: step 680, loss 0.459556, acc 0.78125
2017-08-08T16:26:02.340406: step 681, loss 0.651729, acc 0.6875
2017-08-08T16:26:02.696330: step 682, loss 0.453139, acc 0.78125
2017-08-08T16:26:03.123249: step 683, loss 0.526159, acc 0.75
2017-08-08T16:26:03.537463: step 684, loss 0.725487, acc 0.578125
2017-08-08T16:26:03.772312: step 685, loss 0.588355, acc 0.703125
2017-08-08T16:26:03.984135: step 686, loss 0.540233, acc 0.71875
2017-08-08T16:26:04.357363: step 687, loss 0.537248, acc 0.6875
2017-08-08T16:26:04.647843: step 688, loss 0.579611, acc 0.6875
2017-08-08T16:26:04.864376: step 689, loss 0.508655, acc 0.765625
2017-08-08T16:26:05.121246: step 690, loss 0.504549, acc 0.78125
2017-08-08T16:26:05.438124: step 691, loss 0.434, acc 0.828125
2017-08-08T16:26:05.769348: step 692, loss 0.60114, acc 0.703125
2017-08-08T16:26:06.216633: step 693, loss 0.417785, acc 0.828125
2017-08-08T16:26:06.432599: step 694, loss 0.452399, acc 0.84375
2017-08-08T16:26:06.588682: step 695, loss 0.474659, acc 0.734375
2017-08-08T16:26:06.827118: step 696, loss 0.431352, acc 0.796875
2017-08-08T16:26:07.218130: step 697, loss 0.49383, acc 0.78125
2017-08-08T16:26:07.484360: step 698, loss 0.5405, acc 0.78125
2017-08-08T16:26:07.742581: step 699, loss 0.533242, acc 0.65625
2017-08-08T16:26:08.032563: step 700, loss 0.502367, acc 0.6875

Evaluation:
2017-08-08T16:26:09.088026: step 700, loss 0.603493, acc 0.672608

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-700

2017-08-08T16:26:09.519825: step 701, loss 0.576772, acc 0.6875
2017-08-08T16:26:09.820249: step 702, loss 0.499238, acc 0.78125
2017-08-08T16:26:10.169400: step 703, loss 0.534269, acc 0.71875
2017-08-08T16:26:10.336129: step 704, loss 0.447211, acc 0.78125
2017-08-08T16:26:10.536493: step 705, loss 0.434704, acc 0.828125
2017-08-08T16:26:10.747368: step 706, loss 0.684874, acc 0.640625
2017-08-08T16:26:11.063472: step 707, loss 0.440965, acc 0.796875
2017-08-08T16:26:11.401365: step 708, loss 0.496352, acc 0.796875
2017-08-08T16:26:11.683032: step 709, loss 0.591671, acc 0.6875
2017-08-08T16:26:11.884636: step 710, loss 0.579386, acc 0.6875
2017-08-08T16:26:12.075889: step 711, loss 0.540122, acc 0.75
2017-08-08T16:26:12.412794: step 712, loss 0.457211, acc 0.78125
2017-08-08T16:26:12.613762: step 713, loss 0.504719, acc 0.75
2017-08-08T16:26:12.815483: step 714, loss 0.607667, acc 0.6875
2017-08-08T16:26:13.016330: step 715, loss 0.50707, acc 0.765625
2017-08-08T16:26:13.275490: step 716, loss 0.434816, acc 0.78125
2017-08-08T16:26:13.603334: step 717, loss 0.405845, acc 0.8125
2017-08-08T16:26:13.900784: step 718, loss 0.472154, acc 0.8125
2017-08-08T16:26:14.233359: step 719, loss 0.411302, acc 0.8125
2017-08-08T16:26:14.481054: step 720, loss 0.51608, acc 0.75
2017-08-08T16:26:14.722379: step 721, loss 0.505068, acc 0.75
2017-08-08T16:26:15.172214: step 722, loss 0.47588, acc 0.765625
2017-08-08T16:26:15.452973: step 723, loss 0.573574, acc 0.703125
2017-08-08T16:26:15.681188: step 724, loss 0.437627, acc 0.828125
2017-08-08T16:26:15.940697: step 725, loss 0.750522, acc 0.6875
2017-08-08T16:26:16.331154: step 726, loss 0.687521, acc 0.6875
2017-08-08T16:26:16.641361: step 727, loss 0.464188, acc 0.75
2017-08-08T16:26:16.941627: step 728, loss 0.510416, acc 0.796875
2017-08-08T16:26:17.175507: step 729, loss 0.579998, acc 0.6875
2017-08-08T16:26:17.412832: step 730, loss 0.505236, acc 0.734375
2017-08-08T16:26:17.713471: step 731, loss 0.621307, acc 0.625
2017-08-08T16:26:17.963824: step 732, loss 0.613776, acc 0.640625
2017-08-08T16:26:18.209268: step 733, loss 0.399159, acc 0.8125
2017-08-08T16:26:18.401415: step 734, loss 0.49275, acc 0.75
2017-08-08T16:26:18.821356: step 735, loss 0.458531, acc 0.765625
2017-08-08T16:26:19.220261: step 736, loss 0.539114, acc 0.75
2017-08-08T16:26:19.481886: step 737, loss 0.621692, acc 0.71875
2017-08-08T16:26:19.786353: step 738, loss 0.400296, acc 0.859375
2017-08-08T16:26:20.145435: step 739, loss 0.560132, acc 0.734375
2017-08-08T16:26:20.428011: step 740, loss 0.499059, acc 0.796875
2017-08-08T16:26:20.726519: step 741, loss 0.514144, acc 0.71875
2017-08-08T16:26:21.059496: step 742, loss 0.619488, acc 0.671875
2017-08-08T16:26:21.480208: step 743, loss 0.500862, acc 0.75
2017-08-08T16:26:21.753312: step 744, loss 0.563297, acc 0.765625
2017-08-08T16:26:21.996660: step 745, loss 0.52791, acc 0.71875
2017-08-08T16:26:22.170015: step 746, loss 0.52694, acc 0.671875
2017-08-08T16:26:22.484133: step 747, loss 0.303803, acc 0.875
2017-08-08T16:26:22.748522: step 748, loss 0.48173, acc 0.796875
2017-08-08T16:26:22.963188: step 749, loss 0.54793, acc 0.671875
2017-08-08T16:26:23.156300: step 750, loss 0.463831, acc 0.816667
2017-08-08T16:26:23.458793: step 751, loss 0.412619, acc 0.8125
2017-08-08T16:26:23.804705: step 752, loss 0.416065, acc 0.8125
2017-08-08T16:26:24.093996: step 753, loss 0.491087, acc 0.71875
2017-08-08T16:26:24.313750: step 754, loss 0.419019, acc 0.796875
2017-08-08T16:26:24.578927: step 755, loss 0.456068, acc 0.78125
2017-08-08T16:26:24.919244: step 756, loss 0.416397, acc 0.796875
2017-08-08T16:26:25.123581: step 757, loss 0.426981, acc 0.84375
2017-08-08T16:26:25.331141: step 758, loss 0.537161, acc 0.71875
2017-08-08T16:26:25.578561: step 759, loss 0.613614, acc 0.6875
2017-08-08T16:26:25.880984: step 760, loss 0.439084, acc 0.828125
2017-08-08T16:26:26.227990: step 761, loss 0.450331, acc 0.84375
2017-08-08T16:26:26.537368: step 762, loss 0.560057, acc 0.75
2017-08-08T16:26:26.812688: step 763, loss 0.499765, acc 0.703125
2017-08-08T16:26:27.021845: step 764, loss 0.471401, acc 0.765625
2017-08-08T16:26:27.349517: step 765, loss 0.45652, acc 0.796875
2017-08-08T16:26:27.561945: step 766, loss 0.368325, acc 0.84375
2017-08-08T16:26:27.773148: step 767, loss 0.366703, acc 0.828125
2017-08-08T16:26:27.980284: step 768, loss 0.513735, acc 0.765625
2017-08-08T16:26:28.207491: step 769, loss 0.333998, acc 0.890625
2017-08-08T16:26:28.601494: step 770, loss 0.565103, acc 0.734375
2017-08-08T16:26:29.023491: step 771, loss 0.348381, acc 0.90625
2017-08-08T16:26:29.288489: step 772, loss 0.488183, acc 0.796875
2017-08-08T16:26:29.531482: step 773, loss 0.559773, acc 0.6875
2017-08-08T16:26:29.851522: step 774, loss 0.400281, acc 0.875
2017-08-08T16:26:30.134473: step 775, loss 0.344105, acc 0.84375
2017-08-08T16:26:30.339616: step 776, loss 0.453579, acc 0.78125
2017-08-08T16:26:30.593873: step 777, loss 0.489751, acc 0.75
2017-08-08T16:26:31.053352: step 778, loss 0.387977, acc 0.78125
2017-08-08T16:26:31.501052: step 779, loss 0.462855, acc 0.8125
2017-08-08T16:26:31.831962: step 780, loss 0.400038, acc 0.84375
2017-08-08T16:26:32.091973: step 781, loss 0.517662, acc 0.75
2017-08-08T16:26:32.446536: step 782, loss 0.377492, acc 0.890625
2017-08-08T16:26:32.771589: step 783, loss 0.379754, acc 0.8125
2017-08-08T16:26:33.054086: step 784, loss 0.429814, acc 0.78125
2017-08-08T16:26:33.336243: step 785, loss 0.378524, acc 0.84375
2017-08-08T16:26:33.712523: step 786, loss 0.419412, acc 0.75
2017-08-08T16:26:34.134796: step 787, loss 0.459137, acc 0.765625
2017-08-08T16:26:34.502202: step 788, loss 0.479761, acc 0.765625
2017-08-08T16:26:34.784979: step 789, loss 0.447135, acc 0.859375
2017-08-08T16:26:35.149367: step 790, loss 0.424789, acc 0.828125
2017-08-08T16:26:35.518863: step 791, loss 0.39966, acc 0.796875
2017-08-08T16:26:35.798151: step 792, loss 0.464116, acc 0.75
2017-08-08T16:26:36.088844: step 793, loss 0.432975, acc 0.796875
2017-08-08T16:26:36.427496: step 794, loss 0.446047, acc 0.78125
2017-08-08T16:26:36.840849: step 795, loss 0.382553, acc 0.796875
2017-08-08T16:26:37.246303: step 796, loss 0.370334, acc 0.890625
2017-08-08T16:26:37.547707: step 797, loss 0.552884, acc 0.734375
2017-08-08T16:26:37.800719: step 798, loss 0.417626, acc 0.796875
2017-08-08T16:26:38.060959: step 799, loss 0.45938, acc 0.78125
2017-08-08T16:26:38.537376: step 800, loss 0.442261, acc 0.8125

Evaluation:
2017-08-08T16:26:39.196182: step 800, loss 0.59407, acc 0.679174

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-800

2017-08-08T16:26:39.693375: step 801, loss 0.421387, acc 0.8125
2017-08-08T16:26:40.034554: step 802, loss 0.551563, acc 0.703125
2017-08-08T16:26:40.353378: step 803, loss 0.368284, acc 0.828125
2017-08-08T16:26:40.624043: step 804, loss 0.367726, acc 0.828125
2017-08-08T16:26:40.839095: step 805, loss 0.416532, acc 0.8125
2017-08-08T16:26:41.050334: step 806, loss 0.455812, acc 0.796875
2017-08-08T16:26:41.406920: step 807, loss 0.489635, acc 0.765625
2017-08-08T16:26:41.644074: step 808, loss 0.46985, acc 0.828125
2017-08-08T16:26:41.900982: step 809, loss 0.486187, acc 0.75
2017-08-08T16:26:42.134927: step 810, loss 0.595357, acc 0.6875
2017-08-08T16:26:42.465831: step 811, loss 0.487193, acc 0.734375
2017-08-08T16:26:42.753325: step 812, loss 0.466123, acc 0.765625
2017-08-08T16:26:43.017283: step 813, loss 0.401495, acc 0.828125
2017-08-08T16:26:43.195879: step 814, loss 0.429228, acc 0.8125
2017-08-08T16:26:43.485366: step 815, loss 0.415036, acc 0.765625
2017-08-08T16:26:43.755767: step 816, loss 0.478524, acc 0.796875
2017-08-08T16:26:44.066924: step 817, loss 0.388707, acc 0.859375
2017-08-08T16:26:44.372683: step 818, loss 0.446536, acc 0.8125
2017-08-08T16:26:44.771415: step 819, loss 0.540148, acc 0.765625
2017-08-08T16:26:45.158748: step 820, loss 0.403978, acc 0.78125
2017-08-08T16:26:45.362496: step 821, loss 0.58087, acc 0.703125
2017-08-08T16:26:45.565835: step 822, loss 0.508845, acc 0.734375
2017-08-08T16:26:45.866938: step 823, loss 0.409594, acc 0.828125
2017-08-08T16:26:46.073287: step 824, loss 0.416561, acc 0.828125
2017-08-08T16:26:46.263880: step 825, loss 0.490831, acc 0.78125
2017-08-08T16:26:46.486342: step 826, loss 0.352902, acc 0.828125
2017-08-08T16:26:46.800291: step 827, loss 0.497285, acc 0.703125
2017-08-08T16:26:47.015097: step 828, loss 0.365558, acc 0.8125
2017-08-08T16:26:47.216650: step 829, loss 0.384242, acc 0.875
2017-08-08T16:26:47.398519: step 830, loss 0.343374, acc 0.890625
2017-08-08T16:26:47.745640: step 831, loss 0.404108, acc 0.78125
2017-08-08T16:26:47.930688: step 832, loss 0.461425, acc 0.734375
2017-08-08T16:26:48.152064: step 833, loss 0.410308, acc 0.84375
2017-08-08T16:26:48.553365: step 834, loss 0.340161, acc 0.828125
2017-08-08T16:26:49.009383: step 835, loss 0.476791, acc 0.765625
2017-08-08T16:26:49.406810: step 836, loss 0.478069, acc 0.796875
2017-08-08T16:26:49.713444: step 837, loss 0.423197, acc 0.859375
2017-08-08T16:26:50.005755: step 838, loss 0.499118, acc 0.828125
2017-08-08T16:26:50.400125: step 839, loss 0.305833, acc 0.921875
2017-08-08T16:26:50.677936: step 840, loss 0.455767, acc 0.84375
2017-08-08T16:26:50.932858: step 841, loss 0.523134, acc 0.734375
2017-08-08T16:26:51.214115: step 842, loss 0.57048, acc 0.765625
2017-08-08T16:26:51.555471: step 843, loss 0.379591, acc 0.828125
2017-08-08T16:26:51.906214: step 844, loss 0.379443, acc 0.8125
2017-08-08T16:26:52.194132: step 845, loss 0.48998, acc 0.703125
2017-08-08T16:26:52.421259: step 846, loss 0.356221, acc 0.84375
2017-08-08T16:26:52.797396: step 847, loss 0.410045, acc 0.78125
2017-08-08T16:26:53.082290: step 848, loss 0.432563, acc 0.828125
2017-08-08T16:26:53.403779: step 849, loss 0.425577, acc 0.796875
2017-08-08T16:26:53.759850: step 850, loss 0.499263, acc 0.765625
2017-08-08T16:26:54.215223: step 851, loss 0.53025, acc 0.765625
2017-08-08T16:26:54.665492: step 852, loss 0.560997, acc 0.75
2017-08-08T16:26:54.884451: step 853, loss 0.488147, acc 0.8125
2017-08-08T16:26:55.084986: step 854, loss 0.505821, acc 0.765625
2017-08-08T16:26:55.409321: step 855, loss 0.572224, acc 0.765625
2017-08-08T16:26:55.708326: step 856, loss 0.563448, acc 0.75
2017-08-08T16:26:55.997026: step 857, loss 0.398809, acc 0.78125
2017-08-08T16:26:56.321447: step 858, loss 0.361713, acc 0.828125
2017-08-08T16:26:56.564400: step 859, loss 0.461364, acc 0.8125
2017-08-08T16:26:56.834020: step 860, loss 0.325979, acc 0.875
2017-08-08T16:26:57.050844: step 861, loss 0.478492, acc 0.78125
2017-08-08T16:26:57.234181: step 862, loss 0.388551, acc 0.8125
2017-08-08T16:26:57.423858: step 863, loss 0.399832, acc 0.84375
2017-08-08T16:26:57.725476: step 864, loss 0.508895, acc 0.75
2017-08-08T16:26:57.992495: step 865, loss 0.43288, acc 0.78125
2017-08-08T16:26:58.337988: step 866, loss 0.418102, acc 0.796875
2017-08-08T16:26:58.658867: step 867, loss 0.485086, acc 0.765625
2017-08-08T16:26:58.874861: step 868, loss 0.392208, acc 0.84375
2017-08-08T16:26:59.215533: step 869, loss 0.428444, acc 0.796875
2017-08-08T16:26:59.416038: step 870, loss 0.455665, acc 0.765625
2017-08-08T16:26:59.660414: step 871, loss 0.376303, acc 0.796875
2017-08-08T16:26:59.937847: step 872, loss 0.352111, acc 0.90625
2017-08-08T16:27:00.309952: step 873, loss 0.406245, acc 0.84375
2017-08-08T16:27:00.687807: step 874, loss 0.489527, acc 0.734375
2017-08-08T16:27:00.988470: step 875, loss 0.40025, acc 0.78125
2017-08-08T16:27:01.191021: step 876, loss 0.366789, acc 0.859375
2017-08-08T16:27:01.636456: step 877, loss 0.4409, acc 0.765625
2017-08-08T16:27:02.078328: step 878, loss 0.39033, acc 0.8125
2017-08-08T16:27:02.422631: step 879, loss 0.543774, acc 0.78125
2017-08-08T16:27:02.702633: step 880, loss 0.470349, acc 0.796875
2017-08-08T16:27:03.172837: step 881, loss 0.401305, acc 0.828125
2017-08-08T16:27:03.601272: step 882, loss 0.484819, acc 0.78125
2017-08-08T16:27:03.951331: step 883, loss 0.526445, acc 0.703125
2017-08-08T16:27:04.204821: step 884, loss 0.368938, acc 0.8125
2017-08-08T16:27:04.432291: step 885, loss 0.675985, acc 0.65625
2017-08-08T16:27:04.844026: step 886, loss 0.588988, acc 0.65625
2017-08-08T16:27:05.081046: step 887, loss 0.482173, acc 0.703125
2017-08-08T16:27:05.314834: step 888, loss 0.514136, acc 0.765625
2017-08-08T16:27:05.602936: step 889, loss 0.313773, acc 0.875
2017-08-08T16:27:06.033363: step 890, loss 0.31628, acc 0.875
2017-08-08T16:27:06.426042: step 891, loss 0.485183, acc 0.734375
2017-08-08T16:27:06.701732: step 892, loss 0.591082, acc 0.71875
2017-08-08T16:27:06.958614: step 893, loss 0.447643, acc 0.8125
2017-08-08T16:27:07.241565: step 894, loss 0.480209, acc 0.75
2017-08-08T16:27:07.460358: step 895, loss 0.337519, acc 0.859375
2017-08-08T16:27:07.665297: step 896, loss 0.421577, acc 0.765625
2017-08-08T16:27:07.953425: step 897, loss 0.478704, acc 0.765625
2017-08-08T16:27:08.442720: step 898, loss 0.382338, acc 0.8125
2017-08-08T16:27:08.824970: step 899, loss 0.426667, acc 0.8125
2017-08-08T16:27:09.181162: step 900, loss 0.424721, acc 0.816667

Evaluation:
2017-08-08T16:27:09.883594: step 900, loss 0.579426, acc 0.689493

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-900

2017-08-08T16:27:10.530225: step 901, loss 0.416406, acc 0.765625
2017-08-08T16:27:10.824711: step 902, loss 0.434913, acc 0.8125
2017-08-08T16:27:11.097347: step 903, loss 0.277861, acc 0.84375
2017-08-08T16:27:11.478440: step 904, loss 0.41915, acc 0.875
2017-08-08T16:27:11.882607: step 905, loss 0.344932, acc 0.84375
2017-08-08T16:27:12.226760: step 906, loss 0.405187, acc 0.796875
2017-08-08T16:27:12.508536: step 907, loss 0.493222, acc 0.75
2017-08-08T16:27:12.980540: step 908, loss 0.351247, acc 0.859375
2017-08-08T16:27:13.271367: step 909, loss 0.507328, acc 0.75
2017-08-08T16:27:13.581737: step 910, loss 0.405444, acc 0.8125
2017-08-08T16:27:13.851197: step 911, loss 0.325306, acc 0.859375
2017-08-08T16:27:14.275158: step 912, loss 0.389593, acc 0.765625
2017-08-08T16:27:14.679966: step 913, loss 0.378811, acc 0.828125
2017-08-08T16:27:14.963651: step 914, loss 0.325529, acc 0.84375
2017-08-08T16:27:15.171634: step 915, loss 0.337863, acc 0.859375
2017-08-08T16:27:15.370139: step 916, loss 0.404979, acc 0.796875
2017-08-08T16:27:15.665337: step 917, loss 0.459715, acc 0.796875
2017-08-08T16:27:15.893390: step 918, loss 0.345135, acc 0.875
2017-08-08T16:27:16.119680: step 919, loss 0.341909, acc 0.859375
2017-08-08T16:27:16.407711: step 920, loss 0.327991, acc 0.875
2017-08-08T16:27:16.673139: step 921, loss 0.354048, acc 0.828125
2017-08-08T16:27:17.024797: step 922, loss 0.277087, acc 0.875
2017-08-08T16:27:17.293492: step 923, loss 0.44064, acc 0.75
2017-08-08T16:27:17.563694: step 924, loss 0.251991, acc 0.90625
2017-08-08T16:27:17.807174: step 925, loss 0.308584, acc 0.859375
2017-08-08T16:27:18.011820: step 926, loss 0.394982, acc 0.78125
2017-08-08T16:27:18.257449: step 927, loss 0.450182, acc 0.796875
2017-08-08T16:27:18.451416: step 928, loss 0.310697, acc 0.875
2017-08-08T16:27:18.715816: step 929, loss 0.392589, acc 0.859375
2017-08-08T16:27:18.954245: step 930, loss 0.298439, acc 0.859375
2017-08-08T16:27:19.339555: step 931, loss 0.398994, acc 0.796875
2017-08-08T16:27:19.707573: step 932, loss 0.474893, acc 0.828125
2017-08-08T16:27:20.055263: step 933, loss 0.243931, acc 0.9375
2017-08-08T16:27:20.326028: step 934, loss 0.357358, acc 0.84375
2017-08-08T16:27:20.499766: step 935, loss 0.363302, acc 0.828125
2017-08-08T16:27:20.825448: step 936, loss 0.327666, acc 0.875
2017-08-08T16:27:21.114581: step 937, loss 0.432723, acc 0.765625
2017-08-08T16:27:21.318807: step 938, loss 0.475977, acc 0.796875
2017-08-08T16:27:21.541681: step 939, loss 0.296592, acc 0.875
2017-08-08T16:27:21.795689: step 940, loss 0.347693, acc 0.859375
2017-08-08T16:27:22.121164: step 941, loss 0.403005, acc 0.78125
2017-08-08T16:27:22.398798: step 942, loss 0.489079, acc 0.8125
2017-08-08T16:27:22.590030: step 943, loss 0.362537, acc 0.828125
2017-08-08T16:27:22.930653: step 944, loss 0.408642, acc 0.8125
2017-08-08T16:27:23.101652: step 945, loss 0.33702, acc 0.84375
2017-08-08T16:27:23.297956: step 946, loss 0.408311, acc 0.84375
2017-08-08T16:27:23.488407: step 947, loss 0.415868, acc 0.8125
2017-08-08T16:27:23.757453: step 948, loss 0.391272, acc 0.828125
2017-08-08T16:27:24.056748: step 949, loss 0.349174, acc 0.859375
2017-08-08T16:27:24.424308: step 950, loss 0.325383, acc 0.9375
2017-08-08T16:27:24.595724: step 951, loss 0.407272, acc 0.828125
2017-08-08T16:27:24.902659: step 952, loss 0.339939, acc 0.875
2017-08-08T16:27:25.095083: step 953, loss 0.259272, acc 0.859375
2017-08-08T16:27:25.300349: step 954, loss 0.342593, acc 0.828125
2017-08-08T16:27:25.511885: step 955, loss 0.387343, acc 0.796875
2017-08-08T16:27:25.845375: step 956, loss 0.400412, acc 0.859375
2017-08-08T16:27:26.254321: step 957, loss 0.459667, acc 0.6875
2017-08-08T16:27:26.567188: step 958, loss 0.288786, acc 0.90625
2017-08-08T16:27:26.868873: step 959, loss 0.362055, acc 0.828125
2017-08-08T16:27:27.104633: step 960, loss 0.337362, acc 0.84375
2017-08-08T16:27:27.532736: step 961, loss 0.288119, acc 0.875
2017-08-08T16:27:27.818014: step 962, loss 0.230255, acc 0.890625
2017-08-08T16:27:28.124991: step 963, loss 0.245488, acc 0.953125
2017-08-08T16:27:28.528294: step 964, loss 0.349613, acc 0.859375
2017-08-08T16:27:28.963587: step 965, loss 0.360218, acc 0.796875
2017-08-08T16:27:29.328285: step 966, loss 0.507581, acc 0.703125
2017-08-08T16:27:29.667460: step 967, loss 0.354411, acc 0.84375
2017-08-08T16:27:29.942591: step 968, loss 0.369483, acc 0.828125
2017-08-08T16:27:30.450914: step 969, loss 0.466915, acc 0.84375
2017-08-08T16:27:30.742801: step 970, loss 0.541568, acc 0.78125
2017-08-08T16:27:31.034659: step 971, loss 0.430911, acc 0.828125
2017-08-08T16:27:31.418934: step 972, loss 0.332643, acc 0.828125
2017-08-08T16:27:31.783399: step 973, loss 0.340672, acc 0.890625
2017-08-08T16:27:32.133864: step 974, loss 0.292866, acc 0.84375
2017-08-08T16:27:32.456968: step 975, loss 0.321983, acc 0.875
2017-08-08T16:27:32.705719: step 976, loss 0.308215, acc 0.828125
2017-08-08T16:27:33.116908: step 977, loss 0.430933, acc 0.765625
2017-08-08T16:27:33.357138: step 978, loss 0.316186, acc 0.8125
2017-08-08T16:27:33.583859: step 979, loss 0.292192, acc 0.90625
2017-08-08T16:27:33.824304: step 980, loss 0.299275, acc 0.84375
2017-08-08T16:27:34.150328: step 981, loss 0.3516, acc 0.84375
2017-08-08T16:27:34.461626: step 982, loss 0.310248, acc 0.890625
2017-08-08T16:27:34.695849: step 983, loss 0.349591, acc 0.828125
2017-08-08T16:27:34.922926: step 984, loss 0.359105, acc 0.859375
2017-08-08T16:27:35.195858: step 985, loss 0.367234, acc 0.84375
2017-08-08T16:27:35.549110: step 986, loss 0.349869, acc 0.859375
2017-08-08T16:27:35.754779: step 987, loss 0.38324, acc 0.828125
2017-08-08T16:27:35.955912: step 988, loss 0.376772, acc 0.828125
2017-08-08T16:27:36.282871: step 989, loss 0.480995, acc 0.765625
2017-08-08T16:27:36.585682: step 990, loss 0.272474, acc 0.890625
2017-08-08T16:27:36.916449: step 991, loss 0.379697, acc 0.765625
2017-08-08T16:27:37.109698: step 992, loss 0.417569, acc 0.78125
2017-08-08T16:27:37.305661: step 993, loss 0.367201, acc 0.859375
2017-08-08T16:27:37.656386: step 994, loss 0.363168, acc 0.859375
2017-08-08T16:27:37.959649: step 995, loss 0.324102, acc 0.90625
2017-08-08T16:27:38.261364: step 996, loss 0.343403, acc 0.828125
2017-08-08T16:27:38.551761: step 997, loss 0.219733, acc 0.9375
2017-08-08T16:27:38.941658: step 998, loss 0.362577, acc 0.859375
2017-08-08T16:27:39.329387: step 999, loss 0.368262, acc 0.828125
2017-08-08T16:27:39.720724: step 1000, loss 0.345868, acc 0.84375

Evaluation:
2017-08-08T16:27:40.416093: step 1000, loss 0.582793, acc 0.688555

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-1000

2017-08-08T16:27:40.957277: step 1001, loss 0.400688, acc 0.84375
2017-08-08T16:27:41.245821: step 1002, loss 0.318041, acc 0.890625
2017-08-08T16:27:41.534830: step 1003, loss 0.35561, acc 0.875
2017-08-08T16:27:41.914858: step 1004, loss 0.305159, acc 0.90625
2017-08-08T16:27:42.297404: step 1005, loss 0.444425, acc 0.78125
2017-08-08T16:27:42.658557: step 1006, loss 0.233249, acc 0.9375
2017-08-08T16:27:42.963636: step 1007, loss 0.285001, acc 0.875
2017-08-08T16:27:43.249377: step 1008, loss 0.439709, acc 0.75
2017-08-08T16:27:43.552400: step 1009, loss 0.399979, acc 0.84375
2017-08-08T16:27:43.836093: step 1010, loss 0.275236, acc 0.9375
2017-08-08T16:27:44.159947: step 1011, loss 0.346906, acc 0.84375
2017-08-08T16:27:44.500375: step 1012, loss 0.462473, acc 0.8125
2017-08-08T16:27:44.896365: step 1013, loss 0.446101, acc 0.78125
2017-08-08T16:27:45.257322: step 1014, loss 0.363936, acc 0.765625
2017-08-08T16:27:45.453366: step 1015, loss 0.421361, acc 0.828125
2017-08-08T16:27:45.652432: step 1016, loss 0.50286, acc 0.78125
2017-08-08T16:27:46.084531: step 1017, loss 0.40448, acc 0.78125
2017-08-08T16:27:46.291477: step 1018, loss 0.363187, acc 0.828125
2017-08-08T16:27:46.590180: step 1019, loss 0.340618, acc 0.859375
2017-08-08T16:27:46.945365: step 1020, loss 0.377486, acc 0.8125
2017-08-08T16:27:47.380651: step 1021, loss 0.364683, acc 0.84375
2017-08-08T16:27:47.738927: step 1022, loss 0.419043, acc 0.796875
2017-08-08T16:27:47.990399: step 1023, loss 0.455849, acc 0.8125
2017-08-08T16:27:48.188441: step 1024, loss 0.312959, acc 0.890625
2017-08-08T16:27:48.587566: step 1025, loss 0.537297, acc 0.703125
2017-08-08T16:27:48.875683: step 1026, loss 0.521879, acc 0.75
2017-08-08T16:27:49.135835: step 1027, loss 0.579447, acc 0.703125
2017-08-08T16:27:49.363477: step 1028, loss 0.460475, acc 0.734375
2017-08-08T16:27:49.599520: step 1029, loss 0.418275, acc 0.8125
2017-08-08T16:27:49.856066: step 1030, loss 0.316355, acc 0.859375
2017-08-08T16:27:50.120748: step 1031, loss 0.464345, acc 0.84375
2017-08-08T16:27:50.323075: step 1032, loss 0.379139, acc 0.8125
2017-08-08T16:27:50.491288: step 1033, loss 0.393607, acc 0.828125
2017-08-08T16:27:50.823830: step 1034, loss 0.266013, acc 0.921875
2017-08-08T16:27:51.027275: step 1035, loss 0.362429, acc 0.84375
2017-08-08T16:27:51.253467: step 1036, loss 0.314597, acc 0.8125
2017-08-08T16:27:51.514042: step 1037, loss 0.476569, acc 0.84375
2017-08-08T16:27:51.864803: step 1038, loss 0.347963, acc 0.8125
2017-08-08T16:27:52.140261: step 1039, loss 0.612883, acc 0.734375
2017-08-08T16:27:52.320381: step 1040, loss 0.279621, acc 0.875
2017-08-08T16:27:52.540827: step 1041, loss 0.436867, acc 0.796875
2017-08-08T16:27:52.800679: step 1042, loss 0.278171, acc 0.90625
2017-08-08T16:27:52.986617: step 1043, loss 0.298956, acc 0.90625
2017-08-08T16:27:53.217369: step 1044, loss 0.470856, acc 0.734375
2017-08-08T16:27:53.539943: step 1045, loss 0.292914, acc 0.875
2017-08-08T16:27:54.010520: step 1046, loss 0.319421, acc 0.90625
2017-08-08T16:27:54.373017: step 1047, loss 0.413129, acc 0.8125
2017-08-08T16:27:54.620077: step 1048, loss 0.407122, acc 0.84375
2017-08-08T16:27:54.980381: step 1049, loss 0.328305, acc 0.890625
2017-08-08T16:27:55.212336: step 1050, loss 0.413643, acc 0.833333
2017-08-08T16:27:55.400000: step 1051, loss 0.336153, acc 0.875
2017-08-08T16:27:55.644520: step 1052, loss 0.363872, acc 0.796875
2017-08-08T16:27:55.899580: step 1053, loss 0.329258, acc 0.84375
2017-08-08T16:27:56.186107: step 1054, loss 0.292044, acc 0.859375
2017-08-08T16:27:56.471018: step 1055, loss 0.386461, acc 0.8125
2017-08-08T16:27:56.768119: step 1056, loss 0.33011, acc 0.890625
2017-08-08T16:27:56.985500: step 1057, loss 0.247118, acc 0.921875
2017-08-08T16:27:57.165319: step 1058, loss 0.342911, acc 0.875
2017-08-08T16:27:57.440308: step 1059, loss 0.353273, acc 0.875
2017-08-08T16:27:57.609305: step 1060, loss 0.260843, acc 0.875
2017-08-08T16:27:57.817919: step 1061, loss 0.310936, acc 0.875
2017-08-08T16:27:58.068741: step 1062, loss 0.336743, acc 0.84375
2017-08-08T16:27:58.431841: step 1063, loss 0.38697, acc 0.828125
2017-08-08T16:27:58.803761: step 1064, loss 0.281124, acc 0.875
2017-08-08T16:27:59.128169: step 1065, loss 0.336669, acc 0.859375
2017-08-08T16:27:59.362093: step 1066, loss 0.272055, acc 0.875
2017-08-08T16:27:59.691801: step 1067, loss 0.264038, acc 0.9375
2017-08-08T16:28:00.033533: step 1068, loss 0.258955, acc 0.921875
2017-08-08T16:28:00.317128: step 1069, loss 0.276348, acc 0.890625
2017-08-08T16:28:00.631934: step 1070, loss 0.358852, acc 0.84375
2017-08-08T16:28:01.005838: step 1071, loss 0.266953, acc 0.84375
2017-08-08T16:28:01.418167: step 1072, loss 0.406247, acc 0.8125
2017-08-08T16:28:01.711075: step 1073, loss 0.375108, acc 0.828125
2017-08-08T16:28:01.993391: step 1074, loss 0.3191, acc 0.84375
2017-08-08T16:28:02.189267: step 1075, loss 0.3223, acc 0.859375
2017-08-08T16:28:02.628711: step 1076, loss 0.284222, acc 0.875
2017-08-08T16:28:02.999866: step 1077, loss 0.284204, acc 0.921875
2017-08-08T16:28:03.272192: step 1078, loss 0.358517, acc 0.859375
2017-08-08T16:28:03.597366: step 1079, loss 0.262302, acc 0.90625
2017-08-08T16:28:04.047622: step 1080, loss 0.30597, acc 0.859375
2017-08-08T16:28:04.417173: step 1081, loss 0.263444, acc 0.921875
2017-08-08T16:28:04.702930: step 1082, loss 0.254535, acc 0.90625
2017-08-08T16:28:04.944844: step 1083, loss 0.291313, acc 0.875
2017-08-08T16:28:05.216265: step 1084, loss 0.275858, acc 0.875
2017-08-08T16:28:05.499125: step 1085, loss 0.277732, acc 0.859375
2017-08-08T16:28:05.741967: step 1086, loss 0.241028, acc 0.9375
2017-08-08T16:28:05.973433: step 1087, loss 0.358748, acc 0.859375
2017-08-08T16:28:06.391575: step 1088, loss 0.246504, acc 0.921875
2017-08-08T16:28:06.724297: step 1089, loss 0.164991, acc 0.953125
2017-08-08T16:28:06.963814: step 1090, loss 0.339243, acc 0.875
2017-08-08T16:28:07.164460: step 1091, loss 0.254764, acc 0.921875
2017-08-08T16:28:07.492336: step 1092, loss 0.319341, acc 0.890625
2017-08-08T16:28:07.729471: step 1093, loss 0.233141, acc 0.90625
2017-08-08T16:28:07.935611: step 1094, loss 0.242975, acc 0.890625
2017-08-08T16:28:08.185312: step 1095, loss 0.249892, acc 0.890625
2017-08-08T16:28:08.441313: step 1096, loss 0.328019, acc 0.875
2017-08-08T16:28:08.678351: step 1097, loss 0.330156, acc 0.84375
2017-08-08T16:28:08.882167: step 1098, loss 0.231854, acc 0.9375
2017-08-08T16:28:09.058868: step 1099, loss 0.308536, acc 0.890625
2017-08-08T16:28:09.341393: step 1100, loss 0.286932, acc 0.875

Evaluation:
2017-08-08T16:28:09.893849: step 1100, loss 0.588986, acc 0.712008

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-1100

2017-08-08T16:28:10.462476: step 1101, loss 0.315741, acc 0.84375
2017-08-08T16:28:10.840583: step 1102, loss 0.228569, acc 0.90625
2017-08-08T16:28:11.210578: step 1103, loss 0.330706, acc 0.875
2017-08-08T16:28:11.544632: step 1104, loss 0.368806, acc 0.828125
2017-08-08T16:28:11.774781: step 1105, loss 0.194301, acc 0.9375
2017-08-08T16:28:12.112076: step 1106, loss 0.273419, acc 0.921875
2017-08-08T16:28:12.375188: step 1107, loss 0.266927, acc 0.859375
2017-08-08T16:28:12.643893: step 1108, loss 0.293003, acc 0.875
2017-08-08T16:28:12.963294: step 1109, loss 0.340899, acc 0.875
2017-08-08T16:28:13.214673: step 1110, loss 0.306802, acc 0.859375
2017-08-08T16:28:13.602018: step 1111, loss 0.399413, acc 0.828125
2017-08-08T16:28:14.025242: step 1112, loss 0.306121, acc 0.84375
2017-08-08T16:28:14.336993: step 1113, loss 0.368386, acc 0.84375
2017-08-08T16:28:14.562241: step 1114, loss 0.291082, acc 0.890625
2017-08-08T16:28:14.898720: step 1115, loss 0.247342, acc 0.921875
2017-08-08T16:28:15.204740: step 1116, loss 0.309117, acc 0.875
2017-08-08T16:28:15.463643: step 1117, loss 0.420606, acc 0.828125
2017-08-08T16:28:15.731134: step 1118, loss 0.268301, acc 0.921875
2017-08-08T16:28:16.057647: step 1119, loss 0.215116, acc 0.890625
2017-08-08T16:28:16.363382: step 1120, loss 0.410587, acc 0.828125
2017-08-08T16:28:16.651427: step 1121, loss 0.314006, acc 0.90625
2017-08-08T16:28:16.899628: step 1122, loss 0.276254, acc 0.890625
2017-08-08T16:28:17.145975: step 1123, loss 0.222042, acc 0.875
2017-08-08T16:28:17.523352: step 1124, loss 0.273557, acc 0.875
2017-08-08T16:28:17.840298: step 1125, loss 0.247885, acc 0.890625
2017-08-08T16:28:18.032457: step 1126, loss 0.251706, acc 0.890625
2017-08-08T16:28:18.261405: step 1127, loss 0.315484, acc 0.859375
2017-08-08T16:28:18.570168: step 1128, loss 0.435548, acc 0.8125
2017-08-08T16:28:18.879606: step 1129, loss 0.265008, acc 0.90625
2017-08-08T16:28:19.176167: step 1130, loss 0.225232, acc 0.9375
2017-08-08T16:28:19.578219: step 1131, loss 0.256316, acc 0.90625
2017-08-08T16:28:19.812373: step 1132, loss 0.386952, acc 0.875
2017-08-08T16:28:19.992026: step 1133, loss 0.222274, acc 0.90625
2017-08-08T16:28:20.257365: step 1134, loss 0.330452, acc 0.890625
2017-08-08T16:28:20.526536: step 1135, loss 0.308745, acc 0.828125
2017-08-08T16:28:20.744403: step 1136, loss 0.239211, acc 0.9375
2017-08-08T16:28:20.944737: step 1137, loss 0.379487, acc 0.84375
2017-08-08T16:28:21.205398: step 1138, loss 0.149589, acc 0.96875
2017-08-08T16:28:21.478902: step 1139, loss 0.495165, acc 0.765625
2017-08-08T16:28:21.695920: step 1140, loss 0.207232, acc 0.90625
2017-08-08T16:28:21.895626: step 1141, loss 0.279452, acc 0.90625
2017-08-08T16:28:22.131603: step 1142, loss 0.281708, acc 0.921875
2017-08-08T16:28:22.419397: step 1143, loss 0.314728, acc 0.859375
2017-08-08T16:28:22.625545: step 1144, loss 0.314186, acc 0.859375
2017-08-08T16:28:22.844128: step 1145, loss 0.273166, acc 0.875
2017-08-08T16:28:23.063573: step 1146, loss 0.377772, acc 0.84375
2017-08-08T16:28:23.420188: step 1147, loss 0.251682, acc 0.890625
2017-08-08T16:28:23.649336: step 1148, loss 0.318149, acc 0.859375
2017-08-08T16:28:23.881356: step 1149, loss 0.393018, acc 0.890625
2017-08-08T16:28:24.055309: step 1150, loss 0.198051, acc 0.9375
2017-08-08T16:28:24.230177: step 1151, loss 0.286206, acc 0.828125
2017-08-08T16:28:24.488273: step 1152, loss 0.372283, acc 0.828125
2017-08-08T16:28:24.929987: step 1153, loss 0.481962, acc 0.828125
2017-08-08T16:28:25.193572: step 1154, loss 0.250679, acc 0.859375
2017-08-08T16:28:25.443270: step 1155, loss 0.409287, acc 0.859375
2017-08-08T16:28:25.756837: step 1156, loss 0.176987, acc 0.96875
2017-08-08T16:28:26.181332: step 1157, loss 0.200134, acc 0.890625
2017-08-08T16:28:26.475838: step 1158, loss 0.225987, acc 0.90625
2017-08-08T16:28:26.688280: step 1159, loss 0.298805, acc 0.875
2017-08-08T16:28:26.878295: step 1160, loss 0.502185, acc 0.734375
2017-08-08T16:28:27.129391: step 1161, loss 0.370368, acc 0.875
2017-08-08T16:28:27.367027: step 1162, loss 0.293897, acc 0.90625
2017-08-08T16:28:27.606137: step 1163, loss 0.339781, acc 0.84375
2017-08-08T16:28:27.852502: step 1164, loss 0.435353, acc 0.78125
2017-08-08T16:28:28.327131: step 1165, loss 0.232899, acc 0.90625
2017-08-08T16:28:28.743777: step 1166, loss 0.282429, acc 0.875
2017-08-08T16:28:29.011909: step 1167, loss 0.499798, acc 0.75
2017-08-08T16:28:29.206637: step 1168, loss 0.242308, acc 0.890625
2017-08-08T16:28:29.477353: step 1169, loss 0.315674, acc 0.84375
2017-08-08T16:28:29.746162: step 1170, loss 0.267864, acc 0.90625
2017-08-08T16:28:29.975027: step 1171, loss 0.295439, acc 0.859375
2017-08-08T16:28:30.243941: step 1172, loss 0.234253, acc 0.921875
2017-08-08T16:28:30.493705: step 1173, loss 0.273568, acc 0.921875
2017-08-08T16:28:30.821495: step 1174, loss 0.332249, acc 0.875
2017-08-08T16:28:31.116383: step 1175, loss 0.205463, acc 0.921875
2017-08-08T16:28:31.392405: step 1176, loss 0.21223, acc 0.9375
2017-08-08T16:28:31.697830: step 1177, loss 0.248121, acc 0.90625
2017-08-08T16:28:31.885609: step 1178, loss 0.301602, acc 0.828125
2017-08-08T16:28:32.166273: step 1179, loss 0.264088, acc 0.921875
2017-08-08T16:28:32.420922: step 1180, loss 0.32173, acc 0.828125
2017-08-08T16:28:32.634645: step 1181, loss 0.384409, acc 0.875
2017-08-08T16:28:32.838606: step 1182, loss 0.446369, acc 0.84375
2017-08-08T16:28:33.107222: step 1183, loss 0.318462, acc 0.921875
2017-08-08T16:28:33.425340: step 1184, loss 0.302818, acc 0.875
2017-08-08T16:28:33.738159: step 1185, loss 0.475588, acc 0.828125
2017-08-08T16:28:33.958277: step 1186, loss 0.369443, acc 0.84375
2017-08-08T16:28:34.149344: step 1187, loss 0.251781, acc 0.90625
2017-08-08T16:28:34.470719: step 1188, loss 0.357942, acc 0.828125
2017-08-08T16:28:34.710512: step 1189, loss 0.319522, acc 0.828125
2017-08-08T16:28:34.958117: step 1190, loss 0.334717, acc 0.875
2017-08-08T16:28:35.181970: step 1191, loss 0.284078, acc 0.90625
2017-08-08T16:28:35.456042: step 1192, loss 0.351301, acc 0.859375
2017-08-08T16:28:35.739968: step 1193, loss 0.347459, acc 0.890625
2017-08-08T16:28:36.166383: step 1194, loss 0.354538, acc 0.8125
2017-08-08T16:28:36.457839: step 1195, loss 0.375538, acc 0.8125
2017-08-08T16:28:36.651294: step 1196, loss 0.346845, acc 0.78125
2017-08-08T16:28:36.994714: step 1197, loss 0.356837, acc 0.84375
2017-08-08T16:28:37.315829: step 1198, loss 0.305469, acc 0.890625
2017-08-08T16:28:37.590134: step 1199, loss 0.325193, acc 0.859375
2017-08-08T16:28:37.852526: step 1200, loss 0.282843, acc 0.916667

Evaluation:
2017-08-08T16:28:38.695429: step 1200, loss 0.577772, acc 0.723265

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-1200

2017-08-08T16:28:39.341360: step 1201, loss 0.212374, acc 0.9375
2017-08-08T16:28:39.625289: step 1202, loss 0.23354, acc 0.90625
2017-08-08T16:28:39.943677: step 1203, loss 0.185291, acc 0.9375
2017-08-08T16:28:40.358525: step 1204, loss 0.158204, acc 0.96875
2017-08-08T16:28:40.668609: step 1205, loss 0.202264, acc 0.90625
2017-08-08T16:28:40.964015: step 1206, loss 0.269033, acc 0.890625
2017-08-08T16:28:41.344482: step 1207, loss 0.167999, acc 0.953125
2017-08-08T16:28:41.685356: step 1208, loss 0.260184, acc 0.90625
2017-08-08T16:28:42.067910: step 1209, loss 0.156512, acc 0.921875
2017-08-08T16:28:42.379220: step 1210, loss 0.229928, acc 0.90625
2017-08-08T16:28:42.696614: step 1211, loss 0.288496, acc 0.828125
2017-08-08T16:28:43.008138: step 1212, loss 0.199996, acc 0.953125
2017-08-08T16:28:43.449866: step 1213, loss 0.185147, acc 0.890625
2017-08-08T16:28:43.740855: step 1214, loss 0.194966, acc 0.9375
2017-08-08T16:28:44.014088: step 1215, loss 0.196743, acc 0.921875
2017-08-08T16:28:44.285370: step 1216, loss 0.265352, acc 0.890625
2017-08-08T16:28:44.750272: step 1217, loss 0.236149, acc 0.9375
2017-08-08T16:28:45.056315: step 1218, loss 0.214253, acc 0.9375
2017-08-08T16:28:45.378979: step 1219, loss 0.231527, acc 0.90625
2017-08-08T16:28:45.720388: step 1220, loss 0.158729, acc 0.953125
2017-08-08T16:28:46.053822: step 1221, loss 0.228323, acc 0.921875
2017-08-08T16:28:46.300287: step 1222, loss 0.167903, acc 0.9375
2017-08-08T16:28:46.502186: step 1223, loss 0.244165, acc 0.890625
2017-08-08T16:28:46.706296: step 1224, loss 0.199129, acc 0.90625
2017-08-08T16:28:47.122204: step 1225, loss 0.20191, acc 0.90625
2017-08-08T16:28:47.522523: step 1226, loss 0.178051, acc 0.984375
2017-08-08T16:28:47.783788: step 1227, loss 0.257728, acc 0.90625
2017-08-08T16:28:48.036563: step 1228, loss 0.276012, acc 0.859375
2017-08-08T16:28:48.284212: step 1229, loss 0.209496, acc 0.9375
2017-08-08T16:28:48.760118: step 1230, loss 0.135769, acc 0.96875
2017-08-08T16:28:49.049543: step 1231, loss 0.254754, acc 0.859375
2017-08-08T16:28:49.351211: step 1232, loss 0.256957, acc 0.890625
2017-08-08T16:28:49.644862: step 1233, loss 0.214431, acc 0.921875
2017-08-08T16:28:50.115638: step 1234, loss 0.207729, acc 0.921875
2017-08-08T16:28:50.489411: step 1235, loss 0.175694, acc 0.921875
2017-08-08T16:28:50.750323: step 1236, loss 0.177188, acc 0.953125
2017-08-08T16:28:50.968933: step 1237, loss 0.205933, acc 0.921875
2017-08-08T16:28:51.365665: step 1238, loss 0.212147, acc 0.921875
2017-08-08T16:28:51.682068: step 1239, loss 0.220382, acc 0.875
2017-08-08T16:28:51.907174: step 1240, loss 0.196591, acc 0.90625
2017-08-08T16:28:52.103851: step 1241, loss 0.230635, acc 0.921875
2017-08-08T16:28:52.346318: step 1242, loss 0.170652, acc 0.921875
2017-08-08T16:28:52.702424: step 1243, loss 0.220509, acc 0.9375
2017-08-08T16:28:52.946494: step 1244, loss 0.300055, acc 0.828125
2017-08-08T16:28:53.198347: step 1245, loss 0.220881, acc 0.921875
2017-08-08T16:28:53.442780: step 1246, loss 0.17985, acc 0.9375
2017-08-08T16:28:53.697353: step 1247, loss 0.27259, acc 0.875
2017-08-08T16:28:54.098875: step 1248, loss 0.24954, acc 0.90625
2017-08-08T16:28:54.296447: step 1249, loss 0.246744, acc 0.875
2017-08-08T16:28:54.509297: step 1250, loss 0.141746, acc 0.953125
2017-08-08T16:28:54.722561: step 1251, loss 0.245336, acc 0.84375
2017-08-08T16:28:55.147031: step 1252, loss 0.210948, acc 0.90625
2017-08-08T16:28:55.513511: step 1253, loss 0.212686, acc 0.90625
2017-08-08T16:28:55.824406: step 1254, loss 0.245772, acc 0.90625
2017-08-08T16:28:56.074062: step 1255, loss 0.287152, acc 0.875
2017-08-08T16:28:56.309387: step 1256, loss 0.334991, acc 0.921875
2017-08-08T16:28:56.692338: step 1257, loss 0.318913, acc 0.90625
2017-08-08T16:28:56.896304: step 1258, loss 0.275075, acc 0.859375
2017-08-08T16:28:57.136706: step 1259, loss 0.355229, acc 0.875
2017-08-08T16:28:57.313233: step 1260, loss 0.15058, acc 0.96875
2017-08-08T16:28:57.608503: step 1261, loss 0.264978, acc 0.828125
2017-08-08T16:28:57.833383: step 1262, loss 0.21273, acc 0.90625
2017-08-08T16:28:58.090233: step 1263, loss 0.255318, acc 0.84375
2017-08-08T16:28:58.314964: step 1264, loss 0.270718, acc 0.875
2017-08-08T16:28:58.598534: step 1265, loss 0.269538, acc 0.921875
2017-08-08T16:28:58.951008: step 1266, loss 0.317573, acc 0.828125
2017-08-08T16:28:59.134100: step 1267, loss 0.358379, acc 0.859375
2017-08-08T16:28:59.353353: step 1268, loss 0.198559, acc 0.90625
2017-08-08T16:28:59.589362: step 1269, loss 0.230806, acc 0.890625
2017-08-08T16:28:59.963785: step 1270, loss 0.251455, acc 0.890625
2017-08-08T16:29:00.297265: step 1271, loss 0.328986, acc 0.859375
2017-08-08T16:29:00.553039: step 1272, loss 0.163697, acc 0.9375
2017-08-08T16:29:00.787887: step 1273, loss 0.27461, acc 0.90625
2017-08-08T16:29:01.198777: step 1274, loss 0.346354, acc 0.84375
2017-08-08T16:29:01.458767: step 1275, loss 0.227936, acc 0.90625
2017-08-08T16:29:01.728886: step 1276, loss 0.190464, acc 0.9375
2017-08-08T16:29:02.021238: step 1277, loss 0.232197, acc 0.875
2017-08-08T16:29:02.312246: step 1278, loss 0.279794, acc 0.890625
2017-08-08T16:29:02.754073: step 1279, loss 0.188913, acc 0.921875
2017-08-08T16:29:03.178810: step 1280, loss 0.23645, acc 0.890625
2017-08-08T16:29:03.501600: step 1281, loss 0.236516, acc 0.875
2017-08-08T16:29:03.794215: step 1282, loss 0.357948, acc 0.8125
2017-08-08T16:29:04.137348: step 1283, loss 0.200355, acc 0.90625
2017-08-08T16:29:04.533380: step 1284, loss 0.242365, acc 0.90625
2017-08-08T16:29:04.831840: step 1285, loss 0.296267, acc 0.90625
2017-08-08T16:29:05.140263: step 1286, loss 0.18228, acc 0.921875
2017-08-08T16:29:05.541479: step 1287, loss 0.375849, acc 0.828125
2017-08-08T16:29:05.954162: step 1288, loss 0.175282, acc 0.953125
2017-08-08T16:29:06.372105: step 1289, loss 0.23502, acc 0.90625
2017-08-08T16:29:06.582575: step 1290, loss 0.0963054, acc 0.9375
2017-08-08T16:29:06.809370: step 1291, loss 0.205658, acc 0.9375
2017-08-08T16:29:07.234505: step 1292, loss 0.186505, acc 0.953125
2017-08-08T16:29:07.468931: step 1293, loss 0.182865, acc 0.921875
2017-08-08T16:29:07.750374: step 1294, loss 0.190226, acc 0.9375
2017-08-08T16:29:08.010805: step 1295, loss 0.241418, acc 0.890625
2017-08-08T16:29:08.433678: step 1296, loss 0.186201, acc 0.875
2017-08-08T16:29:08.841537: step 1297, loss 0.229083, acc 0.9375
2017-08-08T16:29:09.119462: step 1298, loss 0.27868, acc 0.875
2017-08-08T16:29:09.440090: step 1299, loss 0.344576, acc 0.859375
2017-08-08T16:29:09.837702: step 1300, loss 0.274163, acc 0.875

Evaluation:
2017-08-08T16:29:10.305252: step 1300, loss 0.598845, acc 0.727017

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-1300

2017-08-08T16:29:10.635540: step 1301, loss 0.225086, acc 0.921875
2017-08-08T16:29:10.991880: step 1302, loss 0.169996, acc 0.953125
2017-08-08T16:29:11.378568: step 1303, loss 0.157389, acc 0.9375
2017-08-08T16:29:11.663142: step 1304, loss 0.282967, acc 0.890625
2017-08-08T16:29:11.857004: step 1305, loss 0.300897, acc 0.875
2017-08-08T16:29:12.105809: step 1306, loss 0.230122, acc 0.90625
2017-08-08T16:29:12.345164: step 1307, loss 0.280452, acc 0.890625
2017-08-08T16:29:12.524806: step 1308, loss 0.258107, acc 0.90625
2017-08-08T16:29:12.704968: step 1309, loss 0.319996, acc 0.875
2017-08-08T16:29:12.874347: step 1310, loss 0.168268, acc 0.953125
2017-08-08T16:29:13.262879: step 1311, loss 0.324788, acc 0.890625
2017-08-08T16:29:13.601059: step 1312, loss 0.110549, acc 0.984375
2017-08-08T16:29:13.979056: step 1313, loss 0.264754, acc 0.890625
2017-08-08T16:29:14.223681: step 1314, loss 0.198216, acc 0.90625
2017-08-08T16:29:14.523660: step 1315, loss 0.295595, acc 0.90625
2017-08-08T16:29:14.772036: step 1316, loss 0.30536, acc 0.859375
2017-08-08T16:29:15.026668: step 1317, loss 0.228547, acc 0.90625
2017-08-08T16:29:15.319023: step 1318, loss 0.186978, acc 0.9375
2017-08-08T16:29:15.704527: step 1319, loss 0.179897, acc 0.953125
2017-08-08T16:29:16.005322: step 1320, loss 0.381638, acc 0.84375
2017-08-08T16:29:16.296239: step 1321, loss 0.274873, acc 0.859375
2017-08-08T16:29:16.533677: step 1322, loss 0.240801, acc 0.921875
2017-08-08T16:29:16.806502: step 1323, loss 0.195888, acc 0.921875
2017-08-08T16:29:17.250040: step 1324, loss 0.355652, acc 0.90625
2017-08-08T16:29:17.509523: step 1325, loss 0.286209, acc 0.859375
2017-08-08T16:29:17.717696: step 1326, loss 0.283416, acc 0.875
2017-08-08T16:29:17.986872: step 1327, loss 0.201514, acc 0.890625
2017-08-08T16:29:18.425366: step 1328, loss 0.212872, acc 0.953125
2017-08-08T16:29:18.778723: step 1329, loss 0.253928, acc 0.9375
2017-08-08T16:29:19.064318: step 1330, loss 0.296725, acc 0.859375
2017-08-08T16:29:19.292164: step 1331, loss 0.214804, acc 0.90625
2017-08-08T16:29:19.678709: step 1332, loss 0.200947, acc 0.921875
2017-08-08T16:29:19.933636: step 1333, loss 0.230725, acc 0.90625
2017-08-08T16:29:20.139028: step 1334, loss 0.250006, acc 0.90625
2017-08-08T16:29:20.351121: step 1335, loss 0.196535, acc 0.90625
2017-08-08T16:29:20.650885: step 1336, loss 0.122852, acc 0.96875
2017-08-08T16:29:21.000714: step 1337, loss 0.265699, acc 0.875
2017-08-08T16:29:21.364102: step 1338, loss 0.197678, acc 0.9375
2017-08-08T16:29:21.643425: step 1339, loss 0.370431, acc 0.8125
2017-08-08T16:29:21.887493: step 1340, loss 0.308893, acc 0.8125
2017-08-08T16:29:22.251730: step 1341, loss 0.129096, acc 0.953125
2017-08-08T16:29:22.559741: step 1342, loss 0.26146, acc 0.921875
2017-08-08T16:29:22.848100: step 1343, loss 0.280991, acc 0.875
2017-08-08T16:29:23.155348: step 1344, loss 0.225309, acc 0.90625
2017-08-08T16:29:23.450079: step 1345, loss 0.409027, acc 0.796875
2017-08-08T16:29:23.778312: step 1346, loss 0.156771, acc 0.953125
2017-08-08T16:29:24.158459: step 1347, loss 0.229124, acc 0.90625
2017-08-08T16:29:24.407213: step 1348, loss 0.315222, acc 0.875
2017-08-08T16:29:24.595606: step 1349, loss 0.192675, acc 0.96875
2017-08-08T16:29:24.930792: step 1350, loss 0.225697, acc 0.9
2017-08-08T16:29:25.114710: step 1351, loss 0.130097, acc 0.984375
2017-08-08T16:29:25.345183: step 1352, loss 0.163817, acc 0.921875
2017-08-08T16:29:25.632403: step 1353, loss 0.216036, acc 0.890625
2017-08-08T16:29:26.085469: step 1354, loss 0.168716, acc 0.9375
2017-08-08T16:29:26.382832: step 1355, loss 0.235863, acc 0.890625
2017-08-08T16:29:26.637756: step 1356, loss 0.1606, acc 0.9375
2017-08-08T16:29:26.837187: step 1357, loss 0.180051, acc 0.9375
2017-08-08T16:29:27.189628: step 1358, loss 0.183693, acc 0.90625
2017-08-08T16:29:27.381529: step 1359, loss 0.208157, acc 0.890625
2017-08-08T16:29:27.615787: step 1360, loss 0.14304, acc 0.953125
2017-08-08T16:29:27.966391: step 1361, loss 0.113711, acc 0.953125
2017-08-08T16:29:28.350823: step 1362, loss 0.100222, acc 0.984375
2017-08-08T16:29:28.690330: step 1363, loss 0.203639, acc 0.921875
2017-08-08T16:29:28.945732: step 1364, loss 0.231575, acc 0.90625
2017-08-08T16:29:29.185318: step 1365, loss 0.217232, acc 0.921875
2017-08-08T16:29:29.580046: step 1366, loss 0.172938, acc 0.921875
2017-08-08T16:29:29.876013: step 1367, loss 0.156084, acc 0.953125
2017-08-08T16:29:30.155364: step 1368, loss 0.175503, acc 0.96875
2017-08-08T16:29:30.417712: step 1369, loss 0.15014, acc 0.921875
2017-08-08T16:29:30.861885: step 1370, loss 0.245229, acc 0.875
2017-08-08T16:29:31.139439: step 1371, loss 0.145599, acc 0.96875
2017-08-08T16:29:31.460432: step 1372, loss 0.206255, acc 0.890625
2017-08-08T16:29:31.662691: step 1373, loss 0.130017, acc 0.984375
2017-08-08T16:29:31.899131: step 1374, loss 0.130213, acc 0.96875
2017-08-08T16:29:32.233513: step 1375, loss 0.247311, acc 0.90625
2017-08-08T16:29:32.439543: step 1376, loss 0.140795, acc 0.96875
2017-08-08T16:29:32.649899: step 1377, loss 0.12969, acc 0.921875
2017-08-08T16:29:32.851481: step 1378, loss 0.222673, acc 0.90625
2017-08-08T16:29:33.143935: step 1379, loss 0.160278, acc 0.9375
2017-08-08T16:29:33.442650: step 1380, loss 0.187754, acc 0.96875
2017-08-08T16:29:33.604888: step 1381, loss 0.267104, acc 0.890625
2017-08-08T16:29:33.775393: step 1382, loss 0.160656, acc 0.921875
2017-08-08T16:29:34.028265: step 1383, loss 0.204452, acc 0.953125
2017-08-08T16:29:34.199770: step 1384, loss 0.139335, acc 0.953125
2017-08-08T16:29:34.413122: step 1385, loss 0.193393, acc 0.90625
2017-08-08T16:29:34.653565: step 1386, loss 0.227745, acc 0.90625
2017-08-08T16:29:35.052670: step 1387, loss 0.137626, acc 0.96875
2017-08-08T16:29:35.314390: step 1388, loss 0.165911, acc 0.953125
2017-08-08T16:29:35.633256: step 1389, loss 0.135116, acc 0.953125
2017-08-08T16:29:35.855429: step 1390, loss 0.244883, acc 0.859375
2017-08-08T16:29:36.062947: step 1391, loss 0.189911, acc 0.921875
2017-08-08T16:29:36.298042: step 1392, loss 0.231604, acc 0.890625
2017-08-08T16:29:36.567967: step 1393, loss 0.164683, acc 0.953125
2017-08-08T16:29:36.818282: step 1394, loss 0.213145, acc 0.859375
2017-08-08T16:29:37.063079: step 1395, loss 0.186177, acc 0.90625
2017-08-08T16:29:37.395711: step 1396, loss 0.30526, acc 0.890625
2017-08-08T16:29:37.766596: step 1397, loss 0.212821, acc 0.9375
2017-08-08T16:29:38.046262: step 1398, loss 0.113887, acc 0.984375
2017-08-08T16:29:38.294695: step 1399, loss 0.217149, acc 0.90625
2017-08-08T16:29:38.501367: step 1400, loss 0.219908, acc 0.9375

Evaluation:
2017-08-08T16:29:39.143414: step 1400, loss 0.62217, acc 0.723265

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-1400

2017-08-08T16:29:39.449917: step 1401, loss 0.122325, acc 0.953125
2017-08-08T16:29:39.767069: step 1402, loss 0.223141, acc 0.9375
2017-08-08T16:29:40.017361: step 1403, loss 0.132019, acc 0.96875
2017-08-08T16:29:40.290972: step 1404, loss 0.221267, acc 0.90625
2017-08-08T16:29:40.483764: step 1405, loss 0.0819732, acc 0.96875
2017-08-08T16:29:40.778139: step 1406, loss 0.226241, acc 0.890625
2017-08-08T16:29:41.060595: step 1407, loss 0.332381, acc 0.859375
2017-08-08T16:29:41.296985: step 1408, loss 0.201104, acc 0.921875
2017-08-08T16:29:41.525324: step 1409, loss 0.193504, acc 0.90625
2017-08-08T16:29:41.737363: step 1410, loss 0.163115, acc 0.953125
2017-08-08T16:29:42.050610: step 1411, loss 0.175588, acc 0.9375
2017-08-08T16:29:42.385340: step 1412, loss 0.205546, acc 0.890625
2017-08-08T16:29:42.662725: step 1413, loss 0.102022, acc 0.953125
2017-08-08T16:29:42.889738: step 1414, loss 0.158178, acc 0.9375
2017-08-08T16:29:43.078302: step 1415, loss 0.155643, acc 0.9375
2017-08-08T16:29:43.345533: step 1416, loss 0.181271, acc 0.953125
2017-08-08T16:29:43.609337: step 1417, loss 0.154848, acc 0.9375
2017-08-08T16:29:43.844031: step 1418, loss 0.244919, acc 0.890625
2017-08-08T16:29:44.102288: step 1419, loss 0.158595, acc 0.921875
2017-08-08T16:29:44.516290: step 1420, loss 0.138832, acc 0.953125
2017-08-08T16:29:44.842503: step 1421, loss 0.272961, acc 0.921875
2017-08-08T16:29:45.043112: step 1422, loss 0.124258, acc 0.96875
2017-08-08T16:29:45.256646: step 1423, loss 0.150922, acc 0.9375
2017-08-08T16:29:45.609673: step 1424, loss 0.168449, acc 0.9375
2017-08-08T16:29:45.893846: step 1425, loss 0.167534, acc 0.9375
2017-08-08T16:29:46.141396: step 1426, loss 0.109946, acc 0.953125
2017-08-08T16:29:46.490705: step 1427, loss 0.160176, acc 0.890625
2017-08-08T16:29:46.818353: step 1428, loss 0.167407, acc 0.9375
2017-08-08T16:29:47.133827: step 1429, loss 0.307922, acc 0.90625
2017-08-08T16:29:47.431135: step 1430, loss 0.267346, acc 0.875
2017-08-08T16:29:47.821346: step 1431, loss 0.139837, acc 0.9375
2017-08-08T16:29:48.145220: step 1432, loss 0.193212, acc 0.90625
2017-08-08T16:29:48.399663: step 1433, loss 0.134264, acc 0.96875
2017-08-08T16:29:48.673191: step 1434, loss 0.181211, acc 0.953125
2017-08-08T16:29:49.070804: step 1435, loss 0.160853, acc 0.953125
2017-08-08T16:29:49.454086: step 1436, loss 0.16632, acc 0.921875
2017-08-08T16:29:49.869353: step 1437, loss 0.15829, acc 0.921875
2017-08-08T16:29:50.176851: step 1438, loss 0.203377, acc 0.90625
2017-08-08T16:29:50.462715: step 1439, loss 0.110376, acc 0.953125
2017-08-08T16:29:50.906618: step 1440, loss 0.230562, acc 0.9375
2017-08-08T16:29:51.154173: step 1441, loss 0.24656, acc 0.859375
2017-08-08T16:29:51.409372: step 1442, loss 0.183106, acc 0.953125
2017-08-08T16:29:51.671409: step 1443, loss 0.346679, acc 0.875
2017-08-08T16:29:52.077564: step 1444, loss 0.155967, acc 0.9375
2017-08-08T16:29:52.465579: step 1445, loss 0.177283, acc 0.9375
2017-08-08T16:29:52.844294: step 1446, loss 0.100155, acc 0.984375
2017-08-08T16:29:53.067010: step 1447, loss 0.240693, acc 0.890625
2017-08-08T16:29:53.277771: step 1448, loss 0.252904, acc 0.90625
2017-08-08T16:29:53.584351: step 1449, loss 0.388439, acc 0.859375
2017-08-08T16:29:53.815710: step 1450, loss 0.124086, acc 0.953125
2017-08-08T16:29:54.154233: step 1451, loss 0.254666, acc 0.890625
2017-08-08T16:29:54.392466: step 1452, loss 0.184927, acc 0.9375
2017-08-08T16:29:54.744928: step 1453, loss 0.114301, acc 0.9375
2017-08-08T16:29:55.001406: step 1454, loss 0.245106, acc 0.90625
2017-08-08T16:29:55.329358: step 1455, loss 0.22016, acc 0.90625
2017-08-08T16:29:55.564135: step 1456, loss 0.0777288, acc 1
2017-08-08T16:29:55.825827: step 1457, loss 0.206622, acc 0.90625
2017-08-08T16:29:56.161548: step 1458, loss 0.156319, acc 0.953125
2017-08-08T16:29:56.441113: step 1459, loss 0.129691, acc 0.9375
2017-08-08T16:29:56.710929: step 1460, loss 0.202884, acc 0.90625
2017-08-08T16:29:57.155769: step 1461, loss 0.200471, acc 0.90625
2017-08-08T16:29:57.376698: step 1462, loss 0.26441, acc 0.875
2017-08-08T16:29:57.641313: step 1463, loss 0.156693, acc 0.90625
2017-08-08T16:29:57.826123: step 1464, loss 0.315243, acc 0.84375
2017-08-08T16:29:58.099136: step 1465, loss 0.211831, acc 0.890625
2017-08-08T16:29:58.404020: step 1466, loss 0.12599, acc 0.9375
2017-08-08T16:29:58.625725: step 1467, loss 0.145861, acc 0.9375
2017-08-08T16:29:59.053346: step 1468, loss 0.315421, acc 0.890625
2017-08-08T16:29:59.343245: step 1469, loss 0.182199, acc 0.921875
2017-08-08T16:29:59.669545: step 1470, loss 0.167952, acc 0.921875
2017-08-08T16:30:00.024221: step 1471, loss 0.227283, acc 0.875
2017-08-08T16:30:00.300434: step 1472, loss 0.13018, acc 0.96875
2017-08-08T16:30:00.737621: step 1473, loss 0.199938, acc 0.90625
2017-08-08T16:30:01.028535: step 1474, loss 0.189834, acc 0.921875
2017-08-08T16:30:01.411649: step 1475, loss 0.301921, acc 0.84375
2017-08-08T16:30:01.791010: step 1476, loss 0.165829, acc 0.9375
2017-08-08T16:30:02.125647: step 1477, loss 0.200552, acc 0.921875
2017-08-08T16:30:02.589196: step 1478, loss 0.173394, acc 0.9375
2017-08-08T16:30:02.939748: step 1479, loss 0.217326, acc 0.890625
2017-08-08T16:30:03.365353: step 1480, loss 0.187445, acc 0.890625
2017-08-08T16:30:03.745062: step 1481, loss 0.108349, acc 0.96875
2017-08-08T16:30:04.085828: step 1482, loss 0.202644, acc 0.921875
2017-08-08T16:30:04.519456: step 1483, loss 0.281757, acc 0.84375
2017-08-08T16:30:04.789525: step 1484, loss 0.393924, acc 0.8125
2017-08-08T16:30:05.071120: step 1485, loss 0.117312, acc 0.9375
2017-08-08T16:30:05.475253: step 1486, loss 0.237367, acc 0.9375
2017-08-08T16:30:05.716448: step 1487, loss 0.239316, acc 0.921875
2017-08-08T16:30:05.998170: step 1488, loss 0.167925, acc 0.953125
2017-08-08T16:30:06.275649: step 1489, loss 0.19445, acc 0.921875
2017-08-08T16:30:06.665573: step 1490, loss 0.177097, acc 0.90625
2017-08-08T16:30:07.069247: step 1491, loss 0.103528, acc 0.984375
2017-08-08T16:30:07.534068: step 1492, loss 0.195992, acc 0.9375
2017-08-08T16:30:07.874600: step 1493, loss 0.219331, acc 0.984375
2017-08-08T16:30:08.191496: step 1494, loss 0.247271, acc 0.921875
2017-08-08T16:30:08.623391: step 1495, loss 0.298591, acc 0.875
2017-08-08T16:30:08.905205: step 1496, loss 0.13659, acc 0.96875
2017-08-08T16:30:09.184057: step 1497, loss 0.194996, acc 0.921875
2017-08-08T16:30:09.463617: step 1498, loss 0.122354, acc 0.953125
2017-08-08T16:30:09.917931: step 1499, loss 0.339683, acc 0.890625
2017-08-08T16:30:10.269592: step 1500, loss 0.120209, acc 0.966667

Evaluation:
2017-08-08T16:30:11.059157: step 1500, loss 0.63203, acc 0.728893

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-1500

2017-08-08T16:30:11.716068: step 1501, loss 0.162648, acc 0.90625
2017-08-08T16:30:11.939431: step 1502, loss 0.175346, acc 0.921875
2017-08-08T16:30:12.238840: step 1503, loss 0.101032, acc 0.96875
2017-08-08T16:30:12.538323: step 1504, loss 0.140946, acc 0.9375
2017-08-08T16:30:12.961940: step 1505, loss 0.131771, acc 0.953125
2017-08-08T16:30:13.391770: step 1506, loss 0.178587, acc 0.921875
2017-08-08T16:30:13.770000: step 1507, loss 0.14281, acc 0.953125
2017-08-08T16:30:14.074838: step 1508, loss 0.240274, acc 0.875
2017-08-08T16:30:14.279736: step 1509, loss 0.14074, acc 0.953125
2017-08-08T16:30:14.712463: step 1510, loss 0.108032, acc 0.953125
2017-08-08T16:30:15.019143: step 1511, loss 0.0741949, acc 1
2017-08-08T16:30:15.223588: step 1512, loss 0.272357, acc 0.90625
2017-08-08T16:30:15.500548: step 1513, loss 0.151835, acc 0.953125
2017-08-08T16:30:15.864909: step 1514, loss 0.196632, acc 0.890625
2017-08-08T16:30:16.306486: step 1515, loss 0.09274, acc 0.984375
2017-08-08T16:30:16.677947: step 1516, loss 0.143768, acc 0.953125
2017-08-08T16:30:17.012213: step 1517, loss 0.144038, acc 0.9375
2017-08-08T16:30:17.276956: step 1518, loss 0.101789, acc 0.953125
2017-08-08T16:30:17.652160: step 1519, loss 0.108323, acc 0.96875
2017-08-08T16:30:18.009782: step 1520, loss 0.184305, acc 0.921875
2017-08-08T16:30:18.300907: step 1521, loss 0.0991213, acc 0.96875
2017-08-08T16:30:18.637981: step 1522, loss 0.0968285, acc 0.984375
2017-08-08T16:30:19.053731: step 1523, loss 0.149076, acc 0.921875
2017-08-08T16:30:19.478361: step 1524, loss 0.125099, acc 0.9375
2017-08-08T16:30:19.833529: step 1525, loss 0.0765969, acc 0.984375
2017-08-08T16:30:20.117663: step 1526, loss 0.091945, acc 0.96875
2017-08-08T16:30:20.341320: step 1527, loss 0.076651, acc 0.984375
2017-08-08T16:30:20.734766: step 1528, loss 0.199809, acc 0.875
2017-08-08T16:30:20.954445: step 1529, loss 0.0989679, acc 0.96875
2017-08-08T16:30:21.180455: step 1530, loss 0.108202, acc 0.96875
2017-08-08T16:30:21.388331: step 1531, loss 0.10609, acc 0.953125
2017-08-08T16:30:21.711474: step 1532, loss 0.102377, acc 0.984375
2017-08-08T16:30:22.051553: step 1533, loss 0.244643, acc 0.875
2017-08-08T16:30:22.403989: step 1534, loss 0.237756, acc 0.90625
2017-08-08T16:30:22.661577: step 1535, loss 0.120292, acc 0.96875
2017-08-08T16:30:23.059082: step 1536, loss 0.226677, acc 0.9375
2017-08-08T16:30:23.400923: step 1537, loss 0.149575, acc 0.96875
2017-08-08T16:30:23.693907: step 1538, loss 0.0725074, acc 1
2017-08-08T16:30:23.974040: step 1539, loss 0.0871017, acc 0.984375
2017-08-08T16:30:24.258234: step 1540, loss 0.0915708, acc 0.96875
2017-08-08T16:30:24.697112: step 1541, loss 0.112156, acc 0.953125
2017-08-08T16:30:25.075403: step 1542, loss 0.0459886, acc 1
2017-08-08T16:30:25.411010: step 1543, loss 0.150522, acc 0.9375
2017-08-08T16:30:25.673347: step 1544, loss 0.0835327, acc 0.984375
2017-08-08T16:30:26.012713: step 1545, loss 0.118755, acc 0.96875
2017-08-08T16:30:26.298247: step 1546, loss 0.134416, acc 0.9375
2017-08-08T16:30:26.528701: step 1547, loss 0.121522, acc 0.953125
2017-08-08T16:30:26.891998: step 1548, loss 0.209471, acc 0.921875
2017-08-08T16:30:27.306168: step 1549, loss 0.143689, acc 0.9375
2017-08-08T16:30:27.665255: step 1550, loss 0.202903, acc 0.921875
2017-08-08T16:30:27.919715: step 1551, loss 0.134996, acc 0.953125
2017-08-08T16:30:28.218715: step 1552, loss 0.146951, acc 0.9375
2017-08-08T16:30:28.598036: step 1553, loss 0.15243, acc 0.9375
2017-08-08T16:30:28.808437: step 1554, loss 0.220751, acc 0.90625
2017-08-08T16:30:29.045505: step 1555, loss 0.0811395, acc 0.984375
2017-08-08T16:30:29.338098: step 1556, loss 0.0856521, acc 0.96875
2017-08-08T16:30:29.708810: step 1557, loss 0.0951661, acc 0.96875
2017-08-08T16:30:30.062593: step 1558, loss 0.129368, acc 0.9375
2017-08-08T16:30:30.344156: step 1559, loss 0.0859589, acc 0.96875
2017-08-08T16:30:30.539303: step 1560, loss 0.127482, acc 0.953125
2017-08-08T16:30:30.810051: step 1561, loss 0.105968, acc 0.9375
2017-08-08T16:30:30.989619: step 1562, loss 0.0815444, acc 0.96875
2017-08-08T16:30:31.243272: step 1563, loss 0.188434, acc 0.921875
2017-08-08T16:30:31.436375: step 1564, loss 0.164428, acc 0.90625
2017-08-08T16:30:31.738667: step 1565, loss 0.181871, acc 0.90625
2017-08-08T16:30:32.007253: step 1566, loss 0.157405, acc 0.9375
2017-08-08T16:30:32.281402: step 1567, loss 0.189493, acc 0.953125
2017-08-08T16:30:32.497188: step 1568, loss 0.150742, acc 0.9375
2017-08-08T16:30:32.703831: step 1569, loss 0.120711, acc 0.96875
2017-08-08T16:30:33.094550: step 1570, loss 0.15555, acc 0.953125
2017-08-08T16:30:33.325022: step 1571, loss 0.253936, acc 0.90625
2017-08-08T16:30:33.570747: step 1572, loss 0.149779, acc 0.90625
2017-08-08T16:30:33.849609: step 1573, loss 0.171287, acc 0.921875
2017-08-08T16:30:34.229312: step 1574, loss 0.154277, acc 0.9375
2017-08-08T16:30:34.611612: step 1575, loss 0.191334, acc 0.90625
2017-08-08T16:30:34.906265: step 1576, loss 0.211685, acc 0.890625
2017-08-08T16:30:35.114145: step 1577, loss 0.0683655, acc 0.984375
2017-08-08T16:30:35.325303: step 1578, loss 0.19249, acc 0.953125
2017-08-08T16:30:35.668165: step 1579, loss 0.168338, acc 0.9375
2017-08-08T16:30:35.871748: step 1580, loss 0.185366, acc 0.90625
2017-08-08T16:30:36.083808: step 1581, loss 0.105121, acc 0.96875
2017-08-08T16:30:36.382692: step 1582, loss 0.170586, acc 0.9375
2017-08-08T16:30:36.726269: step 1583, loss 0.242886, acc 0.921875
2017-08-08T16:30:37.049455: step 1584, loss 0.165467, acc 0.921875
2017-08-08T16:30:37.271227: step 1585, loss 0.148298, acc 0.953125
2017-08-08T16:30:37.467329: step 1586, loss 0.108168, acc 0.9375
2017-08-08T16:30:37.785011: step 1587, loss 0.173756, acc 0.953125
2017-08-08T16:30:37.984002: step 1588, loss 0.191127, acc 0.90625
2017-08-08T16:30:38.196140: step 1589, loss 0.112277, acc 0.953125
2017-08-08T16:30:38.421042: step 1590, loss 0.176582, acc 0.921875
2017-08-08T16:30:38.721542: step 1591, loss 0.184599, acc 0.921875
2017-08-08T16:30:39.129280: step 1592, loss 0.116376, acc 0.953125
2017-08-08T16:30:39.492981: step 1593, loss 0.192249, acc 0.890625
2017-08-08T16:30:39.713444: step 1594, loss 0.142848, acc 0.921875
2017-08-08T16:30:39.989730: step 1595, loss 0.176394, acc 0.921875
2017-08-08T16:30:40.301676: step 1596, loss 0.0746262, acc 0.984375
2017-08-08T16:30:40.525321: step 1597, loss 0.153311, acc 0.9375
2017-08-08T16:30:40.741876: step 1598, loss 0.134837, acc 0.9375
2017-08-08T16:30:41.038567: step 1599, loss 0.18245, acc 0.90625
2017-08-08T16:30:41.369614: step 1600, loss 0.181106, acc 0.890625

Evaluation:
2017-08-08T16:30:41.926034: step 1600, loss 0.658643, acc 0.74015

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-1600

2017-08-08T16:30:42.405267: step 1601, loss 0.077439, acc 0.96875
2017-08-08T16:30:42.601489: step 1602, loss 0.165301, acc 0.953125
2017-08-08T16:30:42.805884: step 1603, loss 0.167653, acc 0.9375
2017-08-08T16:30:43.037320: step 1604, loss 0.086048, acc 0.96875
2017-08-08T16:30:43.421311: step 1605, loss 0.0988982, acc 0.953125
2017-08-08T16:30:43.694936: step 1606, loss 0.187909, acc 0.90625
2017-08-08T16:30:43.976461: step 1607, loss 0.152715, acc 0.9375
2017-08-08T16:30:44.262700: step 1608, loss 0.0837179, acc 0.96875
2017-08-08T16:30:44.532546: step 1609, loss 0.132875, acc 0.921875
2017-08-08T16:30:44.735937: step 1610, loss 0.279701, acc 0.875
2017-08-08T16:30:44.941924: step 1611, loss 0.187935, acc 0.953125
2017-08-08T16:30:45.183819: step 1612, loss 0.0823795, acc 0.984375
2017-08-08T16:30:45.449320: step 1613, loss 0.101117, acc 0.953125
2017-08-08T16:30:45.680227: step 1614, loss 0.200112, acc 0.9375
2017-08-08T16:30:45.897995: step 1615, loss 0.168964, acc 0.9375
2017-08-08T16:30:46.152383: step 1616, loss 0.130572, acc 0.9375
2017-08-08T16:30:46.672629: step 1617, loss 0.140258, acc 0.953125
2017-08-08T16:30:46.944733: step 1618, loss 0.14524, acc 0.953125
2017-08-08T16:30:47.220598: step 1619, loss 0.193884, acc 0.9375
2017-08-08T16:30:47.536122: step 1620, loss 0.0993381, acc 0.984375
2017-08-08T16:30:47.917061: step 1621, loss 0.152519, acc 0.96875
2017-08-08T16:30:48.252431: step 1622, loss 0.16162, acc 0.953125
2017-08-08T16:30:48.470961: step 1623, loss 0.107884, acc 0.96875
2017-08-08T16:30:48.740491: step 1624, loss 0.0712475, acc 1
2017-08-08T16:30:49.103443: step 1625, loss 0.119002, acc 0.953125
2017-08-08T16:30:49.399524: step 1626, loss 0.116225, acc 0.953125
2017-08-08T16:30:49.657949: step 1627, loss 0.0661105, acc 1
2017-08-08T16:30:49.867369: step 1628, loss 0.234481, acc 0.90625
2017-08-08T16:30:50.308987: step 1629, loss 0.181871, acc 0.9375
2017-08-08T16:30:50.649783: step 1630, loss 0.155635, acc 0.921875
2017-08-08T16:30:50.920162: step 1631, loss 0.175326, acc 0.9375
2017-08-08T16:30:51.294233: step 1632, loss 0.139259, acc 0.953125
2017-08-08T16:30:51.662400: step 1633, loss 0.113428, acc 0.953125
2017-08-08T16:30:51.896132: step 1634, loss 0.243445, acc 0.90625
2017-08-08T16:30:52.090049: step 1635, loss 0.140787, acc 0.9375
2017-08-08T16:30:52.365351: step 1636, loss 0.197486, acc 0.890625
2017-08-08T16:30:52.773361: step 1637, loss 0.178241, acc 0.921875
2017-08-08T16:30:53.153309: step 1638, loss 0.124592, acc 0.953125
2017-08-08T16:30:53.377976: step 1639, loss 0.190719, acc 0.890625
2017-08-08T16:30:53.641765: step 1640, loss 0.265604, acc 0.859375
2017-08-08T16:30:54.055980: step 1641, loss 0.135219, acc 0.921875
2017-08-08T16:30:54.366228: step 1642, loss 0.0724116, acc 0.984375
2017-08-08T16:30:54.716280: step 1643, loss 0.101564, acc 0.953125
2017-08-08T16:30:55.147187: step 1644, loss 0.0802367, acc 0.984375
2017-08-08T16:30:55.510272: step 1645, loss 0.115177, acc 0.953125
2017-08-08T16:30:55.785381: step 1646, loss 0.155552, acc 0.9375
2017-08-08T16:30:56.103063: step 1647, loss 0.134588, acc 0.9375
2017-08-08T16:30:56.320897: step 1648, loss 0.128328, acc 0.953125
2017-08-08T16:30:56.617328: step 1649, loss 0.112465, acc 0.921875
2017-08-08T16:30:57.000078: step 1650, loss 0.286048, acc 0.916667
2017-08-08T16:30:57.290519: step 1651, loss 0.160367, acc 0.921875
2017-08-08T16:30:57.580474: step 1652, loss 0.146935, acc 0.9375
2017-08-08T16:30:57.966501: step 1653, loss 0.0938587, acc 0.953125
2017-08-08T16:30:58.321377: step 1654, loss 0.141928, acc 0.953125
2017-08-08T16:30:58.695294: step 1655, loss 0.142329, acc 0.953125
2017-08-08T16:30:58.941932: step 1656, loss 0.166336, acc 0.921875
2017-08-08T16:30:59.208708: step 1657, loss 0.155283, acc 0.921875
2017-08-08T16:30:59.603455: step 1658, loss 0.178941, acc 0.9375
2017-08-08T16:30:59.847232: step 1659, loss 0.0918506, acc 0.953125
2017-08-08T16:31:00.096070: step 1660, loss 0.0665183, acc 0.984375
2017-08-08T16:31:00.337513: step 1661, loss 0.0536492, acc 1
2017-08-08T16:31:00.641372: step 1662, loss 0.0785159, acc 1
2017-08-08T16:31:01.045307: step 1663, loss 0.055179, acc 1
2017-08-08T16:31:01.437878: step 1664, loss 0.131864, acc 0.953125
2017-08-08T16:31:01.785147: step 1665, loss 0.0915477, acc 0.96875
2017-08-08T16:31:02.095724: step 1666, loss 0.0736368, acc 0.96875
2017-08-08T16:31:02.481187: step 1667, loss 0.125047, acc 0.96875
2017-08-08T16:31:02.873456: step 1668, loss 0.0533416, acc 1
2017-08-08T16:31:03.148869: step 1669, loss 0.113866, acc 0.953125
2017-08-08T16:31:03.446658: step 1670, loss 0.102414, acc 0.953125
2017-08-08T16:31:03.709365: step 1671, loss 0.0547823, acc 1
2017-08-08T16:31:04.143729: step 1672, loss 0.0659598, acc 0.984375
2017-08-08T16:31:04.522415: step 1673, loss 0.165854, acc 0.953125
2017-08-08T16:31:04.882741: step 1674, loss 0.123817, acc 0.96875
2017-08-08T16:31:05.083485: step 1675, loss 0.0997002, acc 0.9375
2017-08-08T16:31:05.351245: step 1676, loss 0.0699269, acc 0.96875
2017-08-08T16:31:05.805365: step 1677, loss 0.136464, acc 0.96875
2017-08-08T16:31:06.089379: step 1678, loss 0.0716819, acc 0.984375
2017-08-08T16:31:06.352712: step 1679, loss 0.0756622, acc 0.96875
2017-08-08T16:31:06.568300: step 1680, loss 0.105872, acc 0.96875
2017-08-08T16:31:06.962175: step 1681, loss 0.0511733, acc 1
2017-08-08T16:31:07.282197: step 1682, loss 0.138082, acc 0.953125
2017-08-08T16:31:07.564338: step 1683, loss 0.114812, acc 0.921875
2017-08-08T16:31:07.769574: step 1684, loss 0.0702727, acc 0.984375
2017-08-08T16:31:08.016575: step 1685, loss 0.0865892, acc 0.984375
2017-08-08T16:31:08.488653: step 1686, loss 0.13994, acc 0.921875
2017-08-08T16:31:08.740019: step 1687, loss 0.0639114, acc 0.984375
2017-08-08T16:31:09.018642: step 1688, loss 0.0924128, acc 0.984375
2017-08-08T16:31:09.306289: step 1689, loss 0.0440948, acc 0.984375
2017-08-08T16:31:09.661460: step 1690, loss 0.0571568, acc 0.984375
2017-08-08T16:31:10.045446: step 1691, loss 0.0666112, acc 0.984375
2017-08-08T16:31:10.361332: step 1692, loss 0.113513, acc 0.96875
2017-08-08T16:31:10.651241: step 1693, loss 0.0839416, acc 0.984375
2017-08-08T16:31:10.889544: step 1694, loss 0.0917618, acc 0.96875
2017-08-08T16:31:11.191313: step 1695, loss 0.126313, acc 0.9375
2017-08-08T16:31:11.415874: step 1696, loss 0.109386, acc 0.953125
2017-08-08T16:31:11.620415: step 1697, loss 0.0904055, acc 0.953125
2017-08-08T16:31:11.914773: step 1698, loss 0.0501635, acc 1
2017-08-08T16:31:12.232567: step 1699, loss 0.0453514, acc 1
2017-08-08T16:31:12.545305: step 1700, loss 0.166313, acc 0.921875

Evaluation:
2017-08-08T16:31:13.207146: step 1700, loss 0.69613, acc 0.722326

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-1700

2017-08-08T16:31:13.635840: step 1701, loss 0.143874, acc 0.9375
2017-08-08T16:31:14.027475: step 1702, loss 0.0743633, acc 0.96875
2017-08-08T16:31:14.328598: step 1703, loss 0.0707452, acc 0.96875
2017-08-08T16:31:14.649730: step 1704, loss 0.153788, acc 0.9375
2017-08-08T16:31:14.993291: step 1705, loss 0.158109, acc 0.953125
2017-08-08T16:31:15.410004: step 1706, loss 0.0519076, acc 0.984375
2017-08-08T16:31:15.768992: step 1707, loss 0.127723, acc 0.921875
2017-08-08T16:31:16.052539: step 1708, loss 0.136803, acc 0.9375
2017-08-08T16:31:16.397451: step 1709, loss 0.0710056, acc 0.984375
2017-08-08T16:31:16.736427: step 1710, loss 0.0835744, acc 0.96875
2017-08-08T16:31:17.023043: step 1711, loss 0.139592, acc 0.9375
2017-08-08T16:31:17.272534: step 1712, loss 0.111348, acc 0.953125
2017-08-08T16:31:17.619805: step 1713, loss 0.150991, acc 0.9375
2017-08-08T16:31:17.957706: step 1714, loss 0.21491, acc 0.921875
2017-08-08T16:31:18.261941: step 1715, loss 0.1571, acc 0.9375
2017-08-08T16:31:18.462500: step 1716, loss 0.199104, acc 0.90625
2017-08-08T16:31:18.783546: step 1717, loss 0.112577, acc 0.9375
2017-08-08T16:31:19.074224: step 1718, loss 0.0787685, acc 0.96875
2017-08-08T16:31:19.277156: step 1719, loss 0.216304, acc 0.90625
2017-08-08T16:31:19.526136: step 1720, loss 0.0900511, acc 0.96875
2017-08-08T16:31:19.817357: step 1721, loss 0.066245, acc 0.984375
2017-08-08T16:31:20.318413: step 1722, loss 0.143851, acc 0.9375
2017-08-08T16:31:20.683120: step 1723, loss 0.116649, acc 0.9375
2017-08-08T16:31:21.012924: step 1724, loss 0.150044, acc 0.921875
2017-08-08T16:31:21.233743: step 1725, loss 0.0571339, acc 0.984375
2017-08-08T16:31:21.589353: step 1726, loss 0.153604, acc 0.953125
2017-08-08T16:31:22.003627: step 1727, loss 0.150361, acc 0.953125
2017-08-08T16:31:22.281022: step 1728, loss 0.137669, acc 0.953125
2017-08-08T16:31:22.575516: step 1729, loss 0.0864424, acc 0.96875
2017-08-08T16:31:22.952476: step 1730, loss 0.0884766, acc 0.984375
2017-08-08T16:31:23.417366: step 1731, loss 0.118518, acc 0.921875
2017-08-08T16:31:23.845841: step 1732, loss 0.0730426, acc 0.96875
2017-08-08T16:31:24.102713: step 1733, loss 0.182363, acc 0.9375
2017-08-08T16:31:24.329393: step 1734, loss 0.0903478, acc 0.96875
2017-08-08T16:31:24.780447: step 1735, loss 0.118722, acc 0.96875
2017-08-08T16:31:25.090390: step 1736, loss 0.0857858, acc 0.953125
2017-08-08T16:31:25.368985: step 1737, loss 0.117662, acc 0.9375
2017-08-08T16:31:25.829354: step 1738, loss 0.157562, acc 0.921875
2017-08-08T16:31:26.199717: step 1739, loss 0.0829897, acc 0.96875
2017-08-08T16:31:26.504965: step 1740, loss 0.105319, acc 0.96875
2017-08-08T16:31:26.780332: step 1741, loss 0.2056, acc 0.90625
2017-08-08T16:31:27.176200: step 1742, loss 0.0806111, acc 0.96875
2017-08-08T16:31:27.421730: step 1743, loss 0.0786435, acc 0.96875
2017-08-08T16:31:27.716112: step 1744, loss 0.0889347, acc 0.96875
2017-08-08T16:31:28.037406: step 1745, loss 0.10353, acc 0.953125
2017-08-08T16:31:28.381354: step 1746, loss 0.138426, acc 0.984375
2017-08-08T16:31:28.760248: step 1747, loss 0.206512, acc 0.9375
2017-08-08T16:31:29.086092: step 1748, loss 0.113746, acc 0.9375
2017-08-08T16:31:29.263502: step 1749, loss 0.16926, acc 0.9375
2017-08-08T16:31:29.574814: step 1750, loss 0.0621268, acc 0.984375
2017-08-08T16:31:29.843704: step 1751, loss 0.0853374, acc 0.984375
2017-08-08T16:31:30.055210: step 1752, loss 0.0514482, acc 0.984375
2017-08-08T16:31:30.271237: step 1753, loss 0.0786423, acc 0.984375
2017-08-08T16:31:30.718704: step 1754, loss 0.0959264, acc 0.984375
2017-08-08T16:31:31.176949: step 1755, loss 0.197701, acc 0.90625
2017-08-08T16:31:31.519190: step 1756, loss 0.0650538, acc 0.984375
2017-08-08T16:31:31.778445: step 1757, loss 0.0721386, acc 1
2017-08-08T16:31:32.098681: step 1758, loss 0.129968, acc 0.9375
2017-08-08T16:31:32.449118: step 1759, loss 0.0832663, acc 0.96875
2017-08-08T16:31:32.726147: step 1760, loss 0.110875, acc 0.96875
2017-08-08T16:31:32.967552: step 1761, loss 0.0804334, acc 0.984375
2017-08-08T16:31:33.266063: step 1762, loss 0.0717573, acc 0.984375
2017-08-08T16:31:33.693552: step 1763, loss 0.0672232, acc 1
2017-08-08T16:31:34.144619: step 1764, loss 0.110116, acc 0.953125
2017-08-08T16:31:34.467821: step 1765, loss 0.130407, acc 0.953125
2017-08-08T16:31:34.703861: step 1766, loss 0.127237, acc 0.953125
2017-08-08T16:31:34.978271: step 1767, loss 0.173105, acc 0.9375
2017-08-08T16:31:35.272277: step 1768, loss 0.161752, acc 0.90625
2017-08-08T16:31:35.488397: step 1769, loss 0.247841, acc 0.921875
2017-08-08T16:31:35.716092: step 1770, loss 0.0444589, acc 0.984375
2017-08-08T16:31:36.054680: step 1771, loss 0.18441, acc 0.90625
2017-08-08T16:31:36.341272: step 1772, loss 0.0716276, acc 0.984375
2017-08-08T16:31:36.621509: step 1773, loss 0.127628, acc 0.9375
2017-08-08T16:31:36.807215: step 1774, loss 0.106917, acc 0.96875
2017-08-08T16:31:37.009330: step 1775, loss 0.0586748, acc 0.984375
2017-08-08T16:31:37.347393: step 1776, loss 0.135589, acc 0.9375
2017-08-08T16:31:37.550262: step 1777, loss 0.149621, acc 0.890625
2017-08-08T16:31:37.797345: step 1778, loss 0.0684631, acc 0.96875
2017-08-08T16:31:38.166490: step 1779, loss 0.186786, acc 0.921875
2017-08-08T16:31:38.509323: step 1780, loss 0.116637, acc 0.953125
2017-08-08T16:31:38.850071: step 1781, loss 0.0330219, acc 1
2017-08-08T16:31:39.103822: step 1782, loss 0.0971836, acc 0.984375
2017-08-08T16:31:39.325207: step 1783, loss 0.101252, acc 0.953125
2017-08-08T16:31:39.741368: step 1784, loss 0.127727, acc 0.953125
2017-08-08T16:31:40.039930: step 1785, loss 0.10283, acc 0.96875
2017-08-08T16:31:40.321725: step 1786, loss 0.0745273, acc 0.96875
2017-08-08T16:31:40.591781: step 1787, loss 0.103966, acc 0.96875
2017-08-08T16:31:41.001379: step 1788, loss 0.0461618, acc 1
2017-08-08T16:31:41.437623: step 1789, loss 0.0587836, acc 1
2017-08-08T16:31:41.764584: step 1790, loss 0.21563, acc 0.921875
2017-08-08T16:31:42.058585: step 1791, loss 0.130461, acc 0.953125
2017-08-08T16:31:42.402867: step 1792, loss 0.0757364, acc 0.984375
2017-08-08T16:31:42.767577: step 1793, loss 0.129084, acc 0.921875
2017-08-08T16:31:43.049246: step 1794, loss 0.163969, acc 0.953125
2017-08-08T16:31:43.352605: step 1795, loss 0.106626, acc 0.9375
2017-08-08T16:31:43.613363: step 1796, loss 0.1193, acc 0.953125
2017-08-08T16:31:44.093373: step 1797, loss 0.13157, acc 0.953125
2017-08-08T16:31:44.534116: step 1798, loss 0.0957063, acc 0.984375
2017-08-08T16:31:44.902745: step 1799, loss 0.166757, acc 0.96875
2017-08-08T16:31:45.169587: step 1800, loss 0.0947877, acc 0.983333

Evaluation:
2017-08-08T16:31:46.047133: step 1800, loss 0.717245, acc 0.729831

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-1800

2017-08-08T16:31:46.461322: step 1801, loss 0.0809599, acc 0.96875
2017-08-08T16:31:46.689340: step 1802, loss 0.0257845, acc 1
2017-08-08T16:31:46.977342: step 1803, loss 0.0877771, acc 0.96875
2017-08-08T16:31:47.224048: step 1804, loss 0.0613282, acc 0.984375
2017-08-08T16:31:47.483989: step 1805, loss 0.0709883, acc 0.984375
2017-08-08T16:31:47.693357: step 1806, loss 0.0728919, acc 0.96875
2017-08-08T16:31:47.919585: step 1807, loss 0.0516066, acc 0.984375
2017-08-08T16:31:48.165747: step 1808, loss 0.141138, acc 0.9375
2017-08-08T16:31:48.339830: step 1809, loss 0.047255, acc 0.96875
2017-08-08T16:31:48.556241: step 1810, loss 0.106267, acc 0.953125
2017-08-08T16:31:48.832461: step 1811, loss 0.0706945, acc 0.984375
2017-08-08T16:31:49.086704: step 1812, loss 0.138229, acc 0.9375
2017-08-08T16:31:49.374990: step 1813, loss 0.0536036, acc 0.984375
2017-08-08T16:31:49.605043: step 1814, loss 0.201145, acc 0.921875
2017-08-08T16:31:49.845379: step 1815, loss 0.0466712, acc 0.984375
2017-08-08T16:31:50.308756: step 1816, loss 0.0822057, acc 0.953125
2017-08-08T16:31:50.542771: step 1817, loss 0.0801907, acc 0.96875
2017-08-08T16:31:50.763425: step 1818, loss 0.137504, acc 0.90625
2017-08-08T16:31:51.093315: step 1819, loss 0.130588, acc 0.9375
2017-08-08T16:31:51.495729: step 1820, loss 0.126869, acc 0.921875
2017-08-08T16:31:51.787723: step 1821, loss 0.0774357, acc 0.96875
2017-08-08T16:31:52.129401: step 1822, loss 0.0444925, acc 0.984375
2017-08-08T16:31:52.449848: step 1823, loss 0.0779102, acc 0.96875
2017-08-08T16:31:52.781811: step 1824, loss 0.15459, acc 0.953125
2017-08-08T16:31:53.062207: step 1825, loss 0.0837172, acc 0.96875
2017-08-08T16:31:53.321522: step 1826, loss 0.0494109, acc 0.984375
2017-08-08T16:31:53.648600: step 1827, loss 0.0548219, acc 0.984375
2017-08-08T16:31:54.037323: step 1828, loss 0.0328302, acc 1
2017-08-08T16:31:54.364871: step 1829, loss 0.121607, acc 0.953125
2017-08-08T16:31:54.612525: step 1830, loss 0.0476295, acc 1
2017-08-08T16:31:54.825325: step 1831, loss 0.0872369, acc 0.953125
2017-08-08T16:31:55.129407: step 1832, loss 0.0455157, acc 0.984375
2017-08-08T16:31:55.328584: step 1833, loss 0.0908239, acc 0.96875
2017-08-08T16:31:55.571530: step 1834, loss 0.112102, acc 0.921875
2017-08-08T16:31:55.791537: step 1835, loss 0.101053, acc 0.96875
2017-08-08T16:31:56.134027: step 1836, loss 0.060548, acc 0.96875
2017-08-08T16:31:56.428962: step 1837, loss 0.0736536, acc 0.984375
2017-08-08T16:31:56.656246: step 1838, loss 0.126763, acc 0.9375
2017-08-08T16:31:56.831446: step 1839, loss 0.0658255, acc 0.96875
2017-08-08T16:31:57.169367: step 1840, loss 0.0362489, acc 1
2017-08-08T16:31:57.413255: step 1841, loss 0.0966704, acc 0.96875
2017-08-08T16:31:57.609438: step 1842, loss 0.152052, acc 0.90625
2017-08-08T16:31:57.805576: step 1843, loss 0.113016, acc 0.953125
2017-08-08T16:31:58.105323: step 1844, loss 0.0811599, acc 0.96875
2017-08-08T16:31:58.465461: step 1845, loss 0.0687961, acc 0.96875
2017-08-08T16:31:58.917422: step 1846, loss 0.0770946, acc 0.96875
2017-08-08T16:31:59.178359: step 1847, loss 0.0626629, acc 0.953125
2017-08-08T16:31:59.481496: step 1848, loss 0.113423, acc 0.96875
2017-08-08T16:31:59.834117: step 1849, loss 0.167556, acc 0.9375
2017-08-08T16:32:00.130147: step 1850, loss 0.0463431, acc 0.984375
2017-08-08T16:32:00.389472: step 1851, loss 0.0724492, acc 0.984375
2017-08-08T16:32:00.707944: step 1852, loss 0.0564648, acc 0.984375
2017-08-08T16:32:01.059188: step 1853, loss 0.0800688, acc 0.953125
2017-08-08T16:32:01.437422: step 1854, loss 0.0589181, acc 0.96875
2017-08-08T16:32:01.735747: step 1855, loss 0.145559, acc 0.90625
2017-08-08T16:32:01.990588: step 1856, loss 0.0835148, acc 0.953125
2017-08-08T16:32:02.464263: step 1857, loss 0.134483, acc 0.953125
2017-08-08T16:32:02.743097: step 1858, loss 0.12508, acc 0.96875
2017-08-08T16:32:03.011174: step 1859, loss 0.057816, acc 0.984375
2017-08-08T16:32:03.291918: step 1860, loss 0.0268847, acc 1
2017-08-08T16:32:03.714607: step 1861, loss 0.10192, acc 0.96875
2017-08-08T16:32:04.096509: step 1862, loss 0.086703, acc 0.953125
2017-08-08T16:32:04.435433: step 1863, loss 0.0426631, acc 1
2017-08-08T16:32:04.688193: step 1864, loss 0.0748972, acc 0.984375
2017-08-08T16:32:04.974336: step 1865, loss 0.118078, acc 0.921875
2017-08-08T16:32:05.444805: step 1866, loss 0.0928302, acc 0.96875
2017-08-08T16:32:05.724043: step 1867, loss 0.0118995, acc 1
2017-08-08T16:32:05.975860: step 1868, loss 0.0591058, acc 0.984375
2017-08-08T16:32:06.250123: step 1869, loss 0.0561092, acc 0.96875
2017-08-08T16:32:06.567186: step 1870, loss 0.022128, acc 1
2017-08-08T16:32:06.986074: step 1871, loss 0.147187, acc 0.9375
2017-08-08T16:32:07.307564: step 1872, loss 0.0933878, acc 0.953125
2017-08-08T16:32:07.529735: step 1873, loss 0.100417, acc 0.953125
2017-08-08T16:32:07.831635: step 1874, loss 0.128177, acc 0.9375
2017-08-08T16:32:08.198410: step 1875, loss 0.128586, acc 0.953125
2017-08-08T16:32:08.465506: step 1876, loss 0.0986084, acc 0.96875
2017-08-08T16:32:08.657365: step 1877, loss 0.0647214, acc 0.96875
2017-08-08T16:32:08.867135: step 1878, loss 0.169177, acc 0.9375
2017-08-08T16:32:09.291684: step 1879, loss 0.0494635, acc 0.984375
2017-08-08T16:32:09.639877: step 1880, loss 0.117723, acc 0.953125
2017-08-08T16:32:09.890867: step 1881, loss 0.045987, acc 0.984375
2017-08-08T16:32:10.090256: step 1882, loss 0.0464805, acc 0.984375
2017-08-08T16:32:10.481188: step 1883, loss 0.0641509, acc 0.984375
2017-08-08T16:32:10.858563: step 1884, loss 0.0808455, acc 0.96875
2017-08-08T16:32:11.090472: step 1885, loss 0.0993163, acc 0.953125
2017-08-08T16:32:11.311767: step 1886, loss 0.143467, acc 0.953125
2017-08-08T16:32:11.676080: step 1887, loss 0.0706545, acc 0.96875
2017-08-08T16:32:12.041372: step 1888, loss 0.080218, acc 0.96875
2017-08-08T16:32:12.338007: step 1889, loss 0.0613163, acc 0.96875
2017-08-08T16:32:12.569061: step 1890, loss 0.0624017, acc 0.984375
2017-08-08T16:32:12.774749: step 1891, loss 0.0618588, acc 0.984375
2017-08-08T16:32:13.153617: step 1892, loss 0.128343, acc 0.953125
2017-08-08T16:32:13.560250: step 1893, loss 0.0801662, acc 0.953125
2017-08-08T16:32:13.797033: step 1894, loss 0.0912838, acc 0.96875
2017-08-08T16:32:14.108871: step 1895, loss 0.125375, acc 0.953125
2017-08-08T16:32:14.506940: step 1896, loss 0.100434, acc 0.953125
2017-08-08T16:32:14.813389: step 1897, loss 0.0581103, acc 0.984375
2017-08-08T16:32:15.066951: step 1898, loss 0.0367407, acc 0.984375
2017-08-08T16:32:15.268914: step 1899, loss 0.0735389, acc 0.984375
2017-08-08T16:32:15.642097: step 1900, loss 0.154405, acc 0.953125

Evaluation:
2017-08-08T16:32:16.223442: step 1900, loss 0.728733, acc 0.730769

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-1900

2017-08-08T16:32:16.837443: step 1901, loss 0.0771223, acc 0.984375
2017-08-08T16:32:17.164129: step 1902, loss 0.0871769, acc 0.984375
2017-08-08T16:32:17.440212: step 1903, loss 0.0817243, acc 0.984375
2017-08-08T16:32:17.654980: step 1904, loss 0.0656526, acc 0.96875
2017-08-08T16:32:17.894555: step 1905, loss 0.0931486, acc 0.953125
2017-08-08T16:32:18.261495: step 1906, loss 0.0613154, acc 0.984375
2017-08-08T16:32:18.532934: step 1907, loss 0.0944612, acc 0.96875
2017-08-08T16:32:18.816107: step 1908, loss 0.0695957, acc 0.96875
2017-08-08T16:32:19.068199: step 1909, loss 0.154546, acc 0.953125
2017-08-08T16:32:19.512746: step 1910, loss 0.0524381, acc 0.984375
2017-08-08T16:32:19.838479: step 1911, loss 0.0369308, acc 1
2017-08-08T16:32:20.192853: step 1912, loss 0.0555172, acc 0.984375
2017-08-08T16:32:20.479027: step 1913, loss 0.0484998, acc 1
2017-08-08T16:32:20.840671: step 1914, loss 0.0671295, acc 0.96875
2017-08-08T16:32:21.174554: step 1915, loss 0.0675547, acc 0.96875
2017-08-08T16:32:21.415462: step 1916, loss 0.0582878, acc 0.984375
2017-08-08T16:32:21.720291: step 1917, loss 0.077136, acc 0.984375
2017-08-08T16:32:22.135161: step 1918, loss 0.0429667, acc 0.984375
2017-08-08T16:32:22.521059: step 1919, loss 0.0807381, acc 0.96875
2017-08-08T16:32:22.833468: step 1920, loss 0.083944, acc 0.96875
2017-08-08T16:32:23.031787: step 1921, loss 0.074212, acc 0.984375
2017-08-08T16:32:23.263422: step 1922, loss 0.0797256, acc 0.96875
2017-08-08T16:32:23.524460: step 1923, loss 0.0447852, acc 0.984375
2017-08-08T16:32:23.799000: step 1924, loss 0.100026, acc 0.953125
2017-08-08T16:32:24.028218: step 1925, loss 0.153538, acc 0.953125
2017-08-08T16:32:24.221683: step 1926, loss 0.0918483, acc 0.953125
2017-08-08T16:32:24.625552: step 1927, loss 0.135111, acc 0.96875
2017-08-08T16:32:24.950438: step 1928, loss 0.0974201, acc 0.953125
2017-08-08T16:32:25.257195: step 1929, loss 0.0424983, acc 1
2017-08-08T16:32:25.499310: step 1930, loss 0.145922, acc 0.90625
2017-08-08T16:32:25.717797: step 1931, loss 0.0368527, acc 1
2017-08-08T16:32:26.189385: step 1932, loss 0.0649247, acc 0.96875
2017-08-08T16:32:26.457443: step 1933, loss 0.0533243, acc 0.984375
2017-08-08T16:32:26.709747: step 1934, loss 0.0751542, acc 0.96875
2017-08-08T16:32:26.986156: step 1935, loss 0.0932971, acc 0.953125
2017-08-08T16:32:27.445376: step 1936, loss 0.108666, acc 0.953125
2017-08-08T16:32:27.948265: step 1937, loss 0.0628757, acc 0.984375
2017-08-08T16:32:28.314195: step 1938, loss 0.077075, acc 0.96875
2017-08-08T16:32:28.577183: step 1939, loss 0.0855712, acc 0.984375
2017-08-08T16:32:28.948653: step 1940, loss 0.066225, acc 0.96875
2017-08-08T16:32:29.163809: step 1941, loss 0.0805078, acc 0.953125
2017-08-08T16:32:29.419309: step 1942, loss 0.0827817, acc 0.984375
2017-08-08T16:32:29.807098: step 1943, loss 0.106754, acc 0.96875
2017-08-08T16:32:30.240898: step 1944, loss 0.116414, acc 0.953125
2017-08-08T16:32:30.597904: step 1945, loss 0.0762456, acc 0.984375
2017-08-08T16:32:30.845689: step 1946, loss 0.0428694, acc 1
2017-08-08T16:32:31.247816: step 1947, loss 0.0398854, acc 1
2017-08-08T16:32:31.439741: step 1948, loss 0.0647305, acc 0.953125
2017-08-08T16:32:31.679523: step 1949, loss 0.10251, acc 0.9375
2017-08-08T16:32:31.857829: step 1950, loss 0.130796, acc 0.95
2017-08-08T16:32:32.147106: step 1951, loss 0.0188999, acc 1
2017-08-08T16:32:32.432456: step 1952, loss 0.0418866, acc 1
2017-08-08T16:32:32.780178: step 1953, loss 0.123761, acc 0.96875
2017-08-08T16:32:33.019165: step 1954, loss 0.021634, acc 1
2017-08-08T16:32:33.246864: step 1955, loss 0.070378, acc 0.96875
2017-08-08T16:32:33.608097: step 1956, loss 0.0222574, acc 1
2017-08-08T16:32:33.807507: step 1957, loss 0.05166, acc 0.96875
2017-08-08T16:32:34.008792: step 1958, loss 0.0537802, acc 0.984375
2017-08-08T16:32:34.319865: step 1959, loss 0.0689396, acc 0.96875
2017-08-08T16:32:34.701380: step 1960, loss 0.0871926, acc 0.96875
2017-08-08T16:32:35.035014: step 1961, loss 0.0467162, acc 0.984375
2017-08-08T16:32:35.250000: step 1962, loss 0.0437165, acc 0.984375
2017-08-08T16:32:35.469580: step 1963, loss 0.0992403, acc 0.953125
2017-08-08T16:32:35.899653: step 1964, loss 0.0579766, acc 0.984375
2017-08-08T16:32:36.161155: step 1965, loss 0.07108, acc 0.96875
2017-08-08T16:32:36.518616: step 1966, loss 0.0274493, acc 1
2017-08-08T16:32:36.828236: step 1967, loss 0.0422698, acc 0.984375
2017-08-08T16:32:37.191895: step 1968, loss 0.019549, acc 1
2017-08-08T16:32:37.587123: step 1969, loss 0.0785283, acc 0.96875
2017-08-08T16:32:37.909356: step 1970, loss 0.0727396, acc 0.96875
2017-08-08T16:32:38.126186: step 1971, loss 0.102175, acc 0.96875
2017-08-08T16:32:38.390941: step 1972, loss 0.0811969, acc 0.984375
2017-08-08T16:32:38.770964: step 1973, loss 0.031305, acc 0.984375
2017-08-08T16:32:39.088203: step 1974, loss 0.0313792, acc 0.984375
2017-08-08T16:32:39.379373: step 1975, loss 0.0871996, acc 0.96875
2017-08-08T16:32:39.684536: step 1976, loss 0.0202178, acc 1
2017-08-08T16:32:40.057368: step 1977, loss 0.0713698, acc 0.984375
2017-08-08T16:32:40.336294: step 1978, loss 0.104594, acc 0.96875
2017-08-08T16:32:40.581379: step 1979, loss 0.046922, acc 1
2017-08-08T16:32:40.783691: step 1980, loss 0.163602, acc 0.953125
2017-08-08T16:32:41.107292: step 1981, loss 0.0583846, acc 0.984375
2017-08-08T16:32:41.449209: step 1982, loss 0.0319568, acc 0.984375
2017-08-08T16:32:41.729159: step 1983, loss 0.0412455, acc 0.96875
2017-08-08T16:32:41.998967: step 1984, loss 0.096003, acc 0.953125
2017-08-08T16:32:42.423374: step 1985, loss 0.0346955, acc 1
2017-08-08T16:32:42.752485: step 1986, loss 0.0845405, acc 0.953125
2017-08-08T16:32:43.045556: step 1987, loss 0.0665044, acc 0.96875
2017-08-08T16:32:43.292402: step 1988, loss 0.0380654, acc 1
2017-08-08T16:32:43.653363: step 1989, loss 0.092227, acc 0.96875
2017-08-08T16:32:43.965125: step 1990, loss 0.0983694, acc 0.9375
2017-08-08T16:32:44.233856: step 1991, loss 0.147739, acc 0.921875
2017-08-08T16:32:44.530013: step 1992, loss 0.0357689, acc 1
2017-08-08T16:32:44.995180: step 1993, loss 0.0451041, acc 0.984375
2017-08-08T16:32:45.469317: step 1994, loss 0.0264068, acc 1
2017-08-08T16:32:45.803022: step 1995, loss 0.134284, acc 0.9375
2017-08-08T16:32:46.009416: step 1996, loss 0.0605572, acc 0.984375
2017-08-08T16:32:46.399501: step 1997, loss 0.0366101, acc 1
2017-08-08T16:32:46.681323: step 1998, loss 0.0452621, acc 0.984375
2017-08-08T16:32:46.894493: step 1999, loss 0.0704596, acc 0.96875
2017-08-08T16:32:47.127488: step 2000, loss 0.109742, acc 0.953125

Evaluation:
2017-08-08T16:32:47.901826: step 2000, loss 0.827535, acc 0.717636

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-2000

2017-08-08T16:32:48.284642: step 2001, loss 0.0147019, acc 1
2017-08-08T16:32:48.599919: step 2002, loss 0.0884778, acc 0.96875
2017-08-08T16:32:48.890252: step 2003, loss 0.0832648, acc 0.984375
2017-08-08T16:32:49.089312: step 2004, loss 0.0601033, acc 0.984375
2017-08-08T16:32:49.309170: step 2005, loss 0.086517, acc 0.96875
2017-08-08T16:32:49.677313: step 2006, loss 0.0440239, acc 0.984375
2017-08-08T16:32:50.039795: step 2007, loss 0.0381208, acc 0.984375
2017-08-08T16:32:50.349367: step 2008, loss 0.0940217, acc 0.953125
2017-08-08T16:32:50.592442: step 2009, loss 0.0869504, acc 0.96875
2017-08-08T16:32:50.980218: step 2010, loss 0.0921992, acc 0.96875
2017-08-08T16:32:51.334664: step 2011, loss 0.0253723, acc 1
2017-08-08T16:32:51.618064: step 2012, loss 0.059294, acc 0.984375
2017-08-08T16:32:51.893417: step 2013, loss 0.0687345, acc 0.96875
2017-08-08T16:32:52.352056: step 2014, loss 0.0903336, acc 0.984375
2017-08-08T16:32:52.574959: step 2015, loss 0.0454523, acc 1
2017-08-08T16:32:52.879406: step 2016, loss 0.0671265, acc 0.953125
2017-08-08T16:32:53.104575: step 2017, loss 0.0681393, acc 1
2017-08-08T16:32:53.461386: step 2018, loss 0.124526, acc 0.9375
2017-08-08T16:32:53.831262: step 2019, loss 0.0738669, acc 0.96875
2017-08-08T16:32:54.102776: step 2020, loss 0.0784286, acc 0.953125
2017-08-08T16:32:54.397900: step 2021, loss 0.0259412, acc 0.984375
2017-08-08T16:32:54.801285: step 2022, loss 0.0260754, acc 1
2017-08-08T16:32:55.213412: step 2023, loss 0.0685024, acc 0.96875
2017-08-08T16:32:55.529400: step 2024, loss 0.0393889, acc 0.984375
2017-08-08T16:32:55.810616: step 2025, loss 0.112362, acc 0.921875
2017-08-08T16:32:56.174832: step 2026, loss 0.0225692, acc 1
2017-08-08T16:32:56.569755: step 2027, loss 0.0742164, acc 0.984375
2017-08-08T16:32:56.806504: step 2028, loss 0.0256975, acc 1
2017-08-08T16:32:57.044198: step 2029, loss 0.0660407, acc 0.984375
2017-08-08T16:32:57.285342: step 2030, loss 0.0886387, acc 0.96875
2017-08-08T16:32:57.609587: step 2031, loss 0.0326927, acc 1
2017-08-08T16:32:58.034744: step 2032, loss 0.0696848, acc 0.984375
2017-08-08T16:32:58.298045: step 2033, loss 0.0646875, acc 1
2017-08-08T16:32:58.489798: step 2034, loss 0.0548027, acc 0.984375
2017-08-08T16:32:58.787636: step 2035, loss 0.19574, acc 0.9375
2017-08-08T16:32:59.035971: step 2036, loss 0.0378102, acc 0.984375
2017-08-08T16:32:59.461335: step 2037, loss 0.097167, acc 0.96875
2017-08-08T16:32:59.719327: step 2038, loss 0.0813136, acc 0.953125
2017-08-08T16:33:00.109583: step 2039, loss 0.084822, acc 0.96875
2017-08-08T16:33:00.385475: step 2040, loss 0.0168281, acc 1
2017-08-08T16:33:00.629374: step 2041, loss 0.0274458, acc 0.984375
2017-08-08T16:33:00.909002: step 2042, loss 0.0456718, acc 1
2017-08-08T16:33:01.153387: step 2043, loss 0.0361302, acc 1
2017-08-08T16:33:01.685410: step 2044, loss 0.0449015, acc 1
2017-08-08T16:33:02.149040: step 2045, loss 0.0910712, acc 0.96875
2017-08-08T16:33:02.512015: step 2046, loss 0.0307485, acc 1
2017-08-08T16:33:02.811091: step 2047, loss 0.0607729, acc 0.984375
2017-08-08T16:33:03.173421: step 2048, loss 0.0223713, acc 1
2017-08-08T16:33:03.490164: step 2049, loss 0.0871368, acc 0.96875
2017-08-08T16:33:03.781390: step 2050, loss 0.0330733, acc 0.984375
2017-08-08T16:33:04.029387: step 2051, loss 0.0676747, acc 0.953125
2017-08-08T16:33:04.385895: step 2052, loss 0.118932, acc 0.953125
2017-08-08T16:33:04.715818: step 2053, loss 0.0899287, acc 0.96875
2017-08-08T16:33:05.027531: step 2054, loss 0.0575372, acc 0.953125
2017-08-08T16:33:05.358749: step 2055, loss 0.0591458, acc 0.96875
2017-08-08T16:33:05.623514: step 2056, loss 0.0699732, acc 0.984375
2017-08-08T16:33:05.906160: step 2057, loss 0.0850833, acc 0.984375
2017-08-08T16:33:06.345805: step 2058, loss 0.101565, acc 0.96875
2017-08-08T16:33:06.610664: step 2059, loss 0.0402758, acc 0.984375
2017-08-08T16:33:06.913226: step 2060, loss 0.0711602, acc 0.96875
2017-08-08T16:33:07.209451: step 2061, loss 0.172441, acc 0.96875
2017-08-08T16:33:07.645308: step 2062, loss 0.0490723, acc 0.96875
2017-08-08T16:33:08.041320: step 2063, loss 0.0555023, acc 0.984375
2017-08-08T16:33:08.289941: step 2064, loss 0.102595, acc 0.96875
2017-08-08T16:33:08.527823: step 2065, loss 0.105236, acc 0.96875
2017-08-08T16:33:08.841235: step 2066, loss 0.0462124, acc 0.984375
2017-08-08T16:33:09.021399: step 2067, loss 0.0599737, acc 0.953125
2017-08-08T16:33:09.241380: step 2068, loss 0.13731, acc 0.921875
2017-08-08T16:33:09.586096: step 2069, loss 0.0421006, acc 0.984375
2017-08-08T16:33:09.982964: step 2070, loss 0.0465537, acc 0.984375
2017-08-08T16:33:10.317454: step 2071, loss 0.0401548, acc 0.984375
2017-08-08T16:33:10.675915: step 2072, loss 0.129174, acc 0.9375
2017-08-08T16:33:10.897324: step 2073, loss 0.0706245, acc 0.984375
2017-08-08T16:33:11.115301: step 2074, loss 0.069122, acc 0.96875
2017-08-08T16:33:11.419140: step 2075, loss 0.0303434, acc 1
2017-08-08T16:33:11.588024: step 2076, loss 0.104823, acc 0.953125
2017-08-08T16:33:11.803846: step 2077, loss 0.0254388, acc 1
2017-08-08T16:33:12.027470: step 2078, loss 0.0527788, acc 0.953125
2017-08-08T16:33:12.440610: step 2079, loss 0.0422535, acc 0.984375
2017-08-08T16:33:12.761585: step 2080, loss 0.130681, acc 0.953125
2017-08-08T16:33:13.020555: step 2081, loss 0.0952417, acc 0.96875
2017-08-08T16:33:13.206338: step 2082, loss 0.0404772, acc 0.984375
2017-08-08T16:33:13.488424: step 2083, loss 0.114764, acc 0.953125
2017-08-08T16:33:13.779571: step 2084, loss 0.0211921, acc 1
2017-08-08T16:33:13.985807: step 2085, loss 0.0616438, acc 1
2017-08-08T16:33:14.219961: step 2086, loss 0.0664327, acc 0.96875
2017-08-08T16:33:14.447016: step 2087, loss 0.0629694, acc 0.96875
2017-08-08T16:33:14.833026: step 2088, loss 0.0243494, acc 1
2017-08-08T16:33:15.248135: step 2089, loss 0.0607333, acc 0.984375
2017-08-08T16:33:15.594599: step 2090, loss 0.118098, acc 0.96875
2017-08-08T16:33:15.859377: step 2091, loss 0.0915852, acc 0.96875
2017-08-08T16:33:16.117409: step 2092, loss 0.102806, acc 0.96875
2017-08-08T16:33:16.475928: step 2093, loss 0.0454297, acc 1
2017-08-08T16:33:16.710302: step 2094, loss 0.100794, acc 0.9375
2017-08-08T16:33:16.970525: step 2095, loss 0.0200064, acc 1
2017-08-08T16:33:17.258010: step 2096, loss 0.133798, acc 0.96875
2017-08-08T16:33:17.681512: step 2097, loss 0.0804955, acc 0.984375
2017-08-08T16:33:18.071670: step 2098, loss 0.030764, acc 1
2017-08-08T16:33:18.442487: step 2099, loss 0.0255541, acc 1
2017-08-08T16:33:18.713268: step 2100, loss 0.0631084, acc 0.983333

Evaluation:
2017-08-08T16:33:19.300447: step 2100, loss 0.801619, acc 0.725141

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-2100

2017-08-08T16:33:19.645664: step 2101, loss 0.0291072, acc 0.984375
2017-08-08T16:33:19.901403: step 2102, loss 0.0276714, acc 1
2017-08-08T16:33:20.177951: step 2103, loss 0.0604773, acc 0.984375
2017-08-08T16:33:20.411429: step 2104, loss 0.0175603, acc 1
2017-08-08T16:33:20.636804: step 2105, loss 0.072646, acc 0.96875
2017-08-08T16:33:20.869299: step 2106, loss 0.0901163, acc 0.953125
2017-08-08T16:33:21.319659: step 2107, loss 0.027256, acc 0.984375
2017-08-08T16:33:21.539315: step 2108, loss 0.0464657, acc 0.96875
2017-08-08T16:33:21.800622: step 2109, loss 0.0209596, acc 1
2017-08-08T16:33:22.102551: step 2110, loss 0.0217626, acc 1
2017-08-08T16:33:22.529417: step 2111, loss 0.0463185, acc 1
2017-08-08T16:33:22.824478: step 2112, loss 0.106292, acc 0.96875
2017-08-08T16:33:22.994893: step 2113, loss 0.0345528, acc 1
2017-08-08T16:33:23.182495: step 2114, loss 0.0621083, acc 0.984375
2017-08-08T16:33:23.482711: step 2115, loss 0.0315813, acc 1
2017-08-08T16:33:23.747624: step 2116, loss 0.0197961, acc 1
2017-08-08T16:33:24.049013: step 2117, loss 0.0360211, acc 1
2017-08-08T16:33:24.357955: step 2118, loss 0.0343304, acc 1
2017-08-08T16:33:24.789377: step 2119, loss 0.0347135, acc 0.984375
2017-08-08T16:33:25.213376: step 2120, loss 0.0619397, acc 0.984375
2017-08-08T16:33:25.512690: step 2121, loss 0.0382612, acc 1
2017-08-08T16:33:25.747837: step 2122, loss 0.13136, acc 0.953125
2017-08-08T16:33:26.009333: step 2123, loss 0.0290696, acc 0.984375
2017-08-08T16:33:26.460736: step 2124, loss 0.0280115, acc 1
2017-08-08T16:33:26.720851: step 2125, loss 0.0764815, acc 0.96875
2017-08-08T16:33:26.976420: step 2126, loss 0.0228356, acc 1
2017-08-08T16:33:27.326503: step 2127, loss 0.0471039, acc 0.96875
2017-08-08T16:33:27.701466: step 2128, loss 0.0172721, acc 1
2017-08-08T16:33:28.001428: step 2129, loss 0.0483464, acc 0.984375
2017-08-08T16:33:28.236653: step 2130, loss 0.0730651, acc 0.953125
2017-08-08T16:33:28.475161: step 2131, loss 0.0393122, acc 0.984375
2017-08-08T16:33:28.797572: step 2132, loss 0.0291808, acc 1
2017-08-08T16:33:29.120862: step 2133, loss 0.0253624, acc 1
2017-08-08T16:33:29.377627: step 2134, loss 0.0238007, acc 1
2017-08-08T16:33:29.664879: step 2135, loss 0.0350866, acc 0.984375
2017-08-08T16:33:29.973459: step 2136, loss 0.0911872, acc 0.96875
2017-08-08T16:33:30.337550: step 2137, loss 0.116496, acc 0.96875
2017-08-08T16:33:30.607488: step 2138, loss 0.0808531, acc 0.96875
2017-08-08T16:33:30.889426: step 2139, loss 0.034634, acc 1
2017-08-08T16:33:31.308592: step 2140, loss 0.0969753, acc 0.984375
2017-08-08T16:33:31.546457: step 2141, loss 0.0282975, acc 1
2017-08-08T16:33:31.775388: step 2142, loss 0.0644532, acc 0.984375
2017-08-08T16:33:32.044287: step 2143, loss 0.0585076, acc 0.96875
2017-08-08T16:33:32.313425: step 2144, loss 0.0795574, acc 0.984375
2017-08-08T16:33:32.668715: step 2145, loss 0.0439112, acc 0.984375
2017-08-08T16:33:32.859182: step 2146, loss 0.0537439, acc 0.96875
2017-08-08T16:33:33.175268: step 2147, loss 0.128857, acc 0.984375
2017-08-08T16:33:33.446174: step 2148, loss 0.0618898, acc 0.96875
2017-08-08T16:33:33.717704: step 2149, loss 0.0418604, acc 0.984375
2017-08-08T16:33:33.997406: step 2150, loss 0.0167128, acc 1
2017-08-08T16:33:34.395142: step 2151, loss 0.103467, acc 0.953125
2017-08-08T16:33:34.733313: step 2152, loss 0.0356832, acc 0.984375
2017-08-08T16:33:34.983803: step 2153, loss 0.0339807, acc 1
2017-08-08T16:33:35.249102: step 2154, loss 0.0370285, acc 0.984375
2017-08-08T16:33:35.490878: step 2155, loss 0.0442647, acc 0.984375
2017-08-08T16:33:35.950034: step 2156, loss 0.130744, acc 0.953125
2017-08-08T16:33:36.217967: step 2157, loss 0.0435563, acc 0.984375
2017-08-08T16:33:36.434337: step 2158, loss 0.0468235, acc 0.984375
2017-08-08T16:33:36.657394: step 2159, loss 0.02741, acc 1
2017-08-08T16:33:37.066098: step 2160, loss 0.0304516, acc 0.984375
2017-08-08T16:33:37.424042: step 2161, loss 0.03462, acc 1
2017-08-08T16:33:37.763127: step 2162, loss 0.0762766, acc 0.984375
2017-08-08T16:33:37.926134: step 2163, loss 0.0727493, acc 0.96875
2017-08-08T16:33:38.205567: step 2164, loss 0.0131314, acc 1
2017-08-08T16:33:38.500020: step 2165, loss 0.144428, acc 0.96875
2017-08-08T16:33:38.798884: step 2166, loss 0.0463472, acc 1
2017-08-08T16:33:39.098730: step 2167, loss 0.0201571, acc 1
2017-08-08T16:33:39.519657: step 2168, loss 0.045259, acc 0.984375
2017-08-08T16:33:39.929420: step 2169, loss 0.0303814, acc 0.984375
2017-08-08T16:33:40.248401: step 2170, loss 0.0351877, acc 0.984375
2017-08-08T16:33:40.445625: step 2171, loss 0.0488075, acc 0.984375
2017-08-08T16:33:40.746741: step 2172, loss 0.039618, acc 1
2017-08-08T16:33:40.997353: step 2173, loss 0.0255836, acc 1
2017-08-08T16:33:41.291848: step 2174, loss 0.0263527, acc 0.984375
2017-08-08T16:33:41.520423: step 2175, loss 0.0353387, acc 1
2017-08-08T16:33:41.789470: step 2176, loss 0.0125149, acc 1
2017-08-08T16:33:42.019110: step 2177, loss 0.031972, acc 1
2017-08-08T16:33:42.240589: step 2178, loss 0.102271, acc 0.953125
2017-08-08T16:33:42.431953: step 2179, loss 0.0492619, acc 0.984375
2017-08-08T16:33:42.617327: step 2180, loss 0.0477627, acc 0.984375
2017-08-08T16:33:42.868986: step 2181, loss 0.0546093, acc 0.984375
2017-08-08T16:33:43.047834: step 2182, loss 0.0294428, acc 0.984375
2017-08-08T16:33:43.269325: step 2183, loss 0.0500023, acc 1
2017-08-08T16:33:43.542041: step 2184, loss 0.0501271, acc 0.96875
2017-08-08T16:33:43.811966: step 2185, loss 0.136284, acc 0.921875
2017-08-08T16:33:43.987934: step 2186, loss 0.0454519, acc 0.984375
2017-08-08T16:33:44.182812: step 2187, loss 0.0351648, acc 1
2017-08-08T16:33:44.570854: step 2188, loss 0.0309998, acc 1
2017-08-08T16:33:44.862249: step 2189, loss 0.0384288, acc 0.984375
2017-08-08T16:33:45.056161: step 2190, loss 0.0122751, acc 1
2017-08-08T16:33:45.396941: step 2191, loss 0.122626, acc 0.953125
2017-08-08T16:33:45.637369: step 2192, loss 0.039378, acc 1
2017-08-08T16:33:45.923219: step 2193, loss 0.042096, acc 1
2017-08-08T16:33:46.146072: step 2194, loss 0.0781971, acc 0.953125
2017-08-08T16:33:46.397054: step 2195, loss 0.0297885, acc 1
2017-08-08T16:33:46.841850: step 2196, loss 0.0435922, acc 0.984375
2017-08-08T16:33:47.098527: step 2197, loss 0.0284933, acc 0.984375
2017-08-08T16:33:47.327003: step 2198, loss 0.0514153, acc 0.984375
2017-08-08T16:33:47.571253: step 2199, loss 0.0467677, acc 0.984375
2017-08-08T16:33:47.916095: step 2200, loss 0.0882643, acc 0.96875

Evaluation:
2017-08-08T16:33:48.617510: step 2200, loss 0.831726, acc 0.73546

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-2200

2017-08-08T16:33:49.021116: step 2201, loss 0.0256325, acc 0.984375
2017-08-08T16:33:49.418450: step 2202, loss 0.0499296, acc 1
2017-08-08T16:33:49.765816: step 2203, loss 0.133035, acc 0.9375
2017-08-08T16:33:50.050679: step 2204, loss 0.0165347, acc 1
2017-08-08T16:33:50.321714: step 2205, loss 0.119242, acc 0.953125
2017-08-08T16:33:50.622852: step 2206, loss 0.0353892, acc 0.984375
2017-08-08T16:33:50.898551: step 2207, loss 0.130658, acc 0.921875
2017-08-08T16:33:51.172585: step 2208, loss 0.0347891, acc 0.96875
2017-08-08T16:33:51.346121: step 2209, loss 0.0600397, acc 0.953125
2017-08-08T16:33:51.626365: step 2210, loss 0.0942029, acc 0.9375
2017-08-08T16:33:52.071826: step 2211, loss 0.0485763, acc 1
2017-08-08T16:33:52.369819: step 2212, loss 0.0877745, acc 0.953125
2017-08-08T16:33:52.639242: step 2213, loss 0.0302698, acc 0.984375
2017-08-08T16:33:52.981350: step 2214, loss 0.0523457, acc 0.984375
2017-08-08T16:33:53.326772: step 2215, loss 0.0844399, acc 0.953125
2017-08-08T16:33:53.666742: step 2216, loss 0.103035, acc 0.953125
2017-08-08T16:33:53.898855: step 2217, loss 0.0410056, acc 1
2017-08-08T16:33:54.209756: step 2218, loss 0.0414854, acc 0.984375
2017-08-08T16:33:54.512007: step 2219, loss 0.0571918, acc 0.984375
2017-08-08T16:33:54.797792: step 2220, loss 0.0591092, acc 0.984375
2017-08-08T16:33:55.053389: step 2221, loss 0.0621454, acc 0.984375
2017-08-08T16:33:55.325258: step 2222, loss 0.0514684, acc 0.984375
2017-08-08T16:33:55.642183: step 2223, loss 0.056649, acc 0.96875
2017-08-08T16:33:55.927286: step 2224, loss 0.119242, acc 0.96875
2017-08-08T16:33:56.184041: step 2225, loss 0.0439558, acc 0.984375
2017-08-08T16:33:56.342922: step 2226, loss 0.0162545, acc 1
2017-08-08T16:33:56.589412: step 2227, loss 0.0420461, acc 0.96875
2017-08-08T16:33:56.854023: step 2228, loss 0.0182825, acc 1
2017-08-08T16:33:57.086058: step 2229, loss 0.043014, acc 0.984375
2017-08-08T16:33:57.269803: step 2230, loss 0.0828331, acc 0.96875
2017-08-08T16:33:57.539488: step 2231, loss 0.0866139, acc 0.953125
2017-08-08T16:33:57.811930: step 2232, loss 0.0460873, acc 0.984375
2017-08-08T16:33:58.113363: step 2233, loss 0.0414058, acc 0.984375
2017-08-08T16:33:58.346915: step 2234, loss 0.0510697, acc 0.96875
2017-08-08T16:33:58.561645: step 2235, loss 0.0153496, acc 1
2017-08-08T16:33:58.897034: step 2236, loss 0.0161123, acc 1
2017-08-08T16:33:59.103363: step 2237, loss 0.0333405, acc 1
2017-08-08T16:33:59.321393: step 2238, loss 0.024082, acc 1
2017-08-08T16:33:59.580299: step 2239, loss 0.0128057, acc 1
2017-08-08T16:33:59.905359: step 2240, loss 0.0600521, acc 0.96875
2017-08-08T16:34:00.246372: step 2241, loss 0.11144, acc 0.953125
2017-08-08T16:34:00.505950: step 2242, loss 0.0269444, acc 1
2017-08-08T16:34:00.699675: step 2243, loss 0.0411582, acc 1
2017-08-08T16:34:01.021308: step 2244, loss 0.0260271, acc 1
2017-08-08T16:34:01.295577: step 2245, loss 0.0843585, acc 0.96875
2017-08-08T16:34:01.580328: step 2246, loss 0.0349933, acc 0.984375
2017-08-08T16:34:01.882278: step 2247, loss 0.113029, acc 0.953125
2017-08-08T16:34:02.177264: step 2248, loss 0.0636288, acc 0.984375
2017-08-08T16:34:02.542704: step 2249, loss 0.110568, acc 0.96875
2017-08-08T16:34:02.896880: step 2250, loss 0.0403011, acc 0.983333
2017-08-08T16:34:03.316409: step 2251, loss 0.00837094, acc 1
2017-08-08T16:34:03.617512: step 2252, loss 0.042324, acc 0.984375
2017-08-08T16:34:03.873456: step 2253, loss 0.0230636, acc 1
2017-08-08T16:34:04.310138: step 2254, loss 0.0229658, acc 1
2017-08-08T16:34:04.691251: step 2255, loss 0.0273273, acc 1
2017-08-08T16:34:04.940471: step 2256, loss 0.0570023, acc 0.984375
2017-08-08T16:34:05.231653: step 2257, loss 0.0186979, acc 1
2017-08-08T16:34:05.581362: step 2258, loss 0.0297624, acc 1
2017-08-08T16:34:06.045189: step 2259, loss 0.0191697, acc 1
2017-08-08T16:34:06.412996: step 2260, loss 0.0200186, acc 1
2017-08-08T16:34:06.667148: step 2261, loss 0.0418147, acc 0.984375
2017-08-08T16:34:06.913392: step 2262, loss 0.0215773, acc 1
2017-08-08T16:34:07.339098: step 2263, loss 0.0775749, acc 0.96875
2017-08-08T16:34:07.681648: step 2264, loss 0.0216505, acc 1
2017-08-08T16:34:07.953926: step 2265, loss 0.0281429, acc 1
2017-08-08T16:34:08.320462: step 2266, loss 0.0188307, acc 1
2017-08-08T16:34:08.683325: step 2267, loss 0.0632172, acc 0.96875
2017-08-08T16:34:08.980547: step 2268, loss 0.0201451, acc 1
2017-08-08T16:34:09.287881: step 2269, loss 0.0443576, acc 0.984375
2017-08-08T16:34:09.524059: step 2270, loss 0.0380194, acc 1
2017-08-08T16:34:09.813157: step 2271, loss 0.0249127, acc 0.984375
2017-08-08T16:34:10.162812: step 2272, loss 0.0511627, acc 0.984375
2017-08-08T16:34:10.476826: step 2273, loss 0.0244646, acc 1
2017-08-08T16:34:10.786034: step 2274, loss 0.0637893, acc 0.953125
2017-08-08T16:34:11.134591: step 2275, loss 0.0225962, acc 1
2017-08-08T16:34:11.561398: step 2276, loss 0.0290511, acc 1
2017-08-08T16:34:11.873470: step 2277, loss 0.033634, acc 0.984375
2017-08-08T16:34:12.096247: step 2278, loss 0.02249, acc 1
2017-08-08T16:34:12.273369: step 2279, loss 0.0668679, acc 0.96875
2017-08-08T16:34:12.574557: step 2280, loss 0.0180843, acc 1
2017-08-08T16:34:12.948594: step 2281, loss 0.0115791, acc 1
2017-08-08T16:34:13.234996: step 2282, loss 0.0311687, acc 1
2017-08-08T16:34:13.531351: step 2283, loss 0.0297045, acc 0.984375
2017-08-08T16:34:13.930451: step 2284, loss 0.0171116, acc 1
2017-08-08T16:34:14.289663: step 2285, loss 0.0193859, acc 1
2017-08-08T16:34:14.720401: step 2286, loss 0.0520849, acc 0.984375
2017-08-08T16:34:15.055789: step 2287, loss 0.0441897, acc 0.984375
2017-08-08T16:34:15.311468: step 2288, loss 0.0495474, acc 0.96875
2017-08-08T16:34:15.608502: step 2289, loss 0.0410537, acc 0.984375
2017-08-08T16:34:15.952779: step 2290, loss 0.0917205, acc 0.96875
2017-08-08T16:34:16.191315: step 2291, loss 0.0349528, acc 0.984375
2017-08-08T16:34:16.469478: step 2292, loss 0.103835, acc 0.96875
2017-08-08T16:34:16.800007: step 2293, loss 0.00601803, acc 1
2017-08-08T16:34:17.235678: step 2294, loss 0.0611203, acc 0.96875
2017-08-08T16:34:17.570879: step 2295, loss 0.0634263, acc 0.96875
2017-08-08T16:34:17.852870: step 2296, loss 0.0239628, acc 1
2017-08-08T16:34:18.126991: step 2297, loss 0.0218512, acc 1
2017-08-08T16:34:18.506178: step 2298, loss 0.0569876, acc 0.96875
2017-08-08T16:34:18.680636: step 2299, loss 0.0508563, acc 0.96875
2017-08-08T16:34:18.883801: step 2300, loss 0.07016, acc 0.96875

Evaluation:
2017-08-08T16:34:19.610669: step 2300, loss 0.869597, acc 0.726079

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-2300

2017-08-08T16:34:20.183705: step 2301, loss 0.0185138, acc 1
2017-08-08T16:34:20.476997: step 2302, loss 0.0438191, acc 0.984375
2017-08-08T16:34:20.691624: step 2303, loss 0.0504343, acc 0.984375
2017-08-08T16:34:20.917348: step 2304, loss 0.0148155, acc 1
2017-08-08T16:34:21.134362: step 2305, loss 0.160626, acc 0.984375
2017-08-08T16:34:21.331332: step 2306, loss 0.0475554, acc 0.984375
2017-08-08T16:34:21.606780: step 2307, loss 0.0172479, acc 1
2017-08-08T16:34:21.800579: step 2308, loss 0.0798325, acc 0.96875
2017-08-08T16:34:22.097403: step 2309, loss 0.0140918, acc 1
2017-08-08T16:34:22.420080: step 2310, loss 0.0177743, acc 1
2017-08-08T16:34:22.616653: step 2311, loss 0.0561077, acc 0.96875
2017-08-08T16:34:22.968097: step 2312, loss 0.106312, acc 0.96875
2017-08-08T16:34:23.192589: step 2313, loss 0.0324314, acc 0.984375
2017-08-08T16:34:23.539371: step 2314, loss 0.146768, acc 0.9375
2017-08-08T16:34:23.732917: step 2315, loss 0.0134894, acc 1
2017-08-08T16:34:23.980175: step 2316, loss 0.0251715, acc 1
2017-08-08T16:34:24.310925: step 2317, loss 0.0509804, acc 0.984375
2017-08-08T16:34:24.560197: step 2318, loss 0.0147183, acc 1
2017-08-08T16:34:25.001315: step 2319, loss 0.0290183, acc 1
2017-08-08T16:34:25.237672: step 2320, loss 0.0413882, acc 0.984375
2017-08-08T16:34:25.491247: step 2321, loss 0.00975427, acc 1
2017-08-08T16:34:25.916313: step 2322, loss 0.0100784, acc 1
2017-08-08T16:34:26.118190: step 2323, loss 0.0508674, acc 0.96875
2017-08-08T16:34:26.367854: step 2324, loss 0.0575126, acc 0.96875
2017-08-08T16:34:26.589656: step 2325, loss 0.0136549, acc 1
2017-08-08T16:34:26.842866: step 2326, loss 0.0150785, acc 1
2017-08-08T16:34:27.147252: step 2327, loss 0.0393234, acc 0.984375
2017-08-08T16:34:27.372619: step 2328, loss 0.0266712, acc 1
2017-08-08T16:34:27.660068: step 2329, loss 0.0339564, acc 1
2017-08-08T16:34:27.890513: step 2330, loss 0.0528054, acc 0.984375
2017-08-08T16:34:28.135159: step 2331, loss 0.0255544, acc 1
2017-08-08T16:34:28.571554: step 2332, loss 0.0321969, acc 0.984375
2017-08-08T16:34:28.916143: step 2333, loss 0.0231969, acc 0.984375
2017-08-08T16:34:29.277353: step 2334, loss 0.0222171, acc 1
2017-08-08T16:34:29.469526: step 2335, loss 0.0129449, acc 1
2017-08-08T16:34:29.669141: step 2336, loss 0.15375, acc 0.9375
2017-08-08T16:34:29.941521: step 2337, loss 0.0404469, acc 0.96875
2017-08-08T16:34:30.200731: step 2338, loss 0.0222594, acc 1
2017-08-08T16:34:30.505282: step 2339, loss 0.0178425, acc 1
2017-08-08T16:34:30.718193: step 2340, loss 0.00656276, acc 1
2017-08-08T16:34:30.935862: step 2341, loss 0.0298951, acc 1
2017-08-08T16:34:31.269578: step 2342, loss 0.0475348, acc 0.984375
2017-08-08T16:34:31.503923: step 2343, loss 0.0163242, acc 1
2017-08-08T16:34:31.706475: step 2344, loss 0.0234859, acc 0.984375
2017-08-08T16:34:31.913543: step 2345, loss 0.0111321, acc 1
2017-08-08T16:34:32.208585: step 2346, loss 0.0592283, acc 0.984375
2017-08-08T16:34:32.492949: step 2347, loss 0.113572, acc 0.953125
2017-08-08T16:34:32.845310: step 2348, loss 0.0158899, acc 1
2017-08-08T16:34:33.074090: step 2349, loss 0.0256994, acc 1
2017-08-08T16:34:33.269457: step 2350, loss 0.0320328, acc 1
2017-08-08T16:34:33.601424: step 2351, loss 0.0756277, acc 0.96875
2017-08-08T16:34:33.808213: step 2352, loss 0.0606167, acc 0.96875
2017-08-08T16:34:34.091437: step 2353, loss 0.0334729, acc 0.984375
2017-08-08T16:34:34.357341: step 2354, loss 0.0237481, acc 0.984375
2017-08-08T16:34:34.684210: step 2355, loss 0.0312877, acc 0.984375
2017-08-08T16:34:34.969315: step 2356, loss 0.026248, acc 0.984375
2017-08-08T16:34:35.203871: step 2357, loss 0.0571016, acc 0.984375
2017-08-08T16:34:35.423980: step 2358, loss 0.0210595, acc 1
2017-08-08T16:34:35.772162: step 2359, loss 0.0534393, acc 0.96875
2017-08-08T16:34:36.072348: step 2360, loss 0.0255257, acc 1
2017-08-08T16:34:36.345518: step 2361, loss 0.0235778, acc 1
2017-08-08T16:34:36.568054: step 2362, loss 0.0192255, acc 1
2017-08-08T16:34:36.833341: step 2363, loss 0.0655184, acc 0.96875
2017-08-08T16:34:37.261372: step 2364, loss 0.0499007, acc 0.96875
2017-08-08T16:34:37.622734: step 2365, loss 0.0362612, acc 1
2017-08-08T16:34:37.845660: step 2366, loss 0.0527289, acc 0.984375
2017-08-08T16:34:38.052994: step 2367, loss 0.0515432, acc 0.984375
2017-08-08T16:34:38.247510: step 2368, loss 0.0205719, acc 1
2017-08-08T16:34:38.592452: step 2369, loss 0.0731116, acc 0.96875
2017-08-08T16:34:38.871548: step 2370, loss 0.0360046, acc 0.984375
2017-08-08T16:34:39.150830: step 2371, loss 0.0225773, acc 1
2017-08-08T16:34:39.456344: step 2372, loss 0.0220363, acc 1
2017-08-08T16:34:39.930371: step 2373, loss 0.032948, acc 0.984375
2017-08-08T16:34:40.354206: step 2374, loss 0.0203152, acc 1
2017-08-08T16:34:40.675775: step 2375, loss 0.0521554, acc 0.984375
2017-08-08T16:34:40.894713: step 2376, loss 0.0452264, acc 0.984375
2017-08-08T16:34:41.121884: step 2377, loss 0.019801, acc 0.984375
2017-08-08T16:34:41.586747: step 2378, loss 0.0335387, acc 0.984375
2017-08-08T16:34:41.871390: step 2379, loss 0.0207687, acc 1
2017-08-08T16:34:42.160125: step 2380, loss 0.0383536, acc 0.984375
2017-08-08T16:34:42.480194: step 2381, loss 0.0179089, acc 1
2017-08-08T16:34:42.868595: step 2382, loss 0.0468022, acc 0.96875
2017-08-08T16:34:43.276241: step 2383, loss 0.0882362, acc 0.953125
2017-08-08T16:34:43.574773: step 2384, loss 0.0455931, acc 0.984375
2017-08-08T16:34:43.815242: step 2385, loss 0.0374214, acc 0.984375
2017-08-08T16:34:44.132775: step 2386, loss 0.030797, acc 1
2017-08-08T16:34:44.410408: step 2387, loss 0.061373, acc 0.984375
2017-08-08T16:34:44.667503: step 2388, loss 0.0302002, acc 0.984375
2017-08-08T16:34:44.888261: step 2389, loss 0.0166843, acc 1
2017-08-08T16:34:45.172489: step 2390, loss 0.0261388, acc 0.984375
2017-08-08T16:34:45.578382: step 2391, loss 0.116358, acc 0.953125
2017-08-08T16:34:45.945297: step 2392, loss 0.0446611, acc 0.96875
2017-08-08T16:34:46.334015: step 2393, loss 0.0318833, acc 1
2017-08-08T16:34:46.604211: step 2394, loss 0.0894467, acc 0.96875
2017-08-08T16:34:46.934769: step 2395, loss 0.0274729, acc 0.984375
2017-08-08T16:34:47.330056: step 2396, loss 0.027704, acc 0.984375
2017-08-08T16:34:47.652405: step 2397, loss 0.0170991, acc 1
2017-08-08T16:34:47.934799: step 2398, loss 0.0464632, acc 0.984375
2017-08-08T16:34:48.356746: step 2399, loss 0.0299707, acc 1
2017-08-08T16:34:48.782622: step 2400, loss 0.0379006, acc 1

Evaluation:
2017-08-08T16:34:49.601632: step 2400, loss 0.893201, acc 0.726079

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-2400

2017-08-08T16:34:50.206790: step 2401, loss 0.0190673, acc 0.984375
2017-08-08T16:34:50.469706: step 2402, loss 0.0195887, acc 1
2017-08-08T16:34:50.693128: step 2403, loss 0.00512046, acc 1
2017-08-08T16:34:51.015283: step 2404, loss 0.00246388, acc 1
2017-08-08T16:34:51.386330: step 2405, loss 0.0393183, acc 0.984375
2017-08-08T16:34:51.850222: step 2406, loss 0.0614944, acc 0.984375
2017-08-08T16:34:52.187973: step 2407, loss 0.0760854, acc 0.96875
2017-08-08T16:34:52.495982: step 2408, loss 0.0119689, acc 1
2017-08-08T16:34:52.833652: step 2409, loss 0.0163056, acc 1
2017-08-08T16:34:53.136806: step 2410, loss 0.0186575, acc 1
2017-08-08T16:34:53.421269: step 2411, loss 0.0332047, acc 1
2017-08-08T16:34:53.731085: step 2412, loss 0.0212236, acc 0.984375
2017-08-08T16:34:54.142625: step 2413, loss 0.0438938, acc 0.984375
2017-08-08T16:34:54.650911: step 2414, loss 0.0113568, acc 1
2017-08-08T16:34:54.896996: step 2415, loss 0.0562514, acc 0.984375
2017-08-08T16:34:55.126793: step 2416, loss 0.0963999, acc 0.96875
2017-08-08T16:34:55.462390: step 2417, loss 0.0195671, acc 0.984375
2017-08-08T16:34:55.792573: step 2418, loss 0.120591, acc 0.90625
2017-08-08T16:34:56.033330: step 2419, loss 0.00422659, acc 1
2017-08-08T16:34:56.272484: step 2420, loss 0.0184901, acc 1
2017-08-08T16:34:56.488449: step 2421, loss 0.0206998, acc 1
2017-08-08T16:34:56.878757: step 2422, loss 0.0617641, acc 0.984375
2017-08-08T16:34:57.293371: step 2423, loss 0.0357121, acc 0.984375
2017-08-08T16:34:57.619417: step 2424, loss 0.0953714, acc 0.96875
2017-08-08T16:34:57.880609: step 2425, loss 0.0679502, acc 0.96875
2017-08-08T16:34:58.141379: step 2426, loss 0.0228416, acc 1
2017-08-08T16:34:58.525753: step 2427, loss 0.0334614, acc 0.984375
2017-08-08T16:34:58.793053: step 2428, loss 0.0497839, acc 0.984375
2017-08-08T16:34:59.110039: step 2429, loss 0.0553958, acc 0.984375
2017-08-08T16:34:59.427322: step 2430, loss 0.0407933, acc 0.984375
2017-08-08T16:34:59.864607: step 2431, loss 0.0686319, acc 0.953125
2017-08-08T16:35:00.319540: step 2432, loss 0.0367345, acc 0.984375
2017-08-08T16:35:00.647308: step 2433, loss 0.0219651, acc 1
2017-08-08T16:35:00.902413: step 2434, loss 0.030486, acc 0.984375
2017-08-08T16:35:01.319021: step 2435, loss 0.0147558, acc 1
2017-08-08T16:35:01.637206: step 2436, loss 0.0135917, acc 1
2017-08-08T16:35:01.992895: step 2437, loss 0.0205507, acc 1
2017-08-08T16:35:02.296169: step 2438, loss 0.0434624, acc 0.984375
2017-08-08T16:35:02.813217: step 2439, loss 0.0804695, acc 0.953125
2017-08-08T16:35:03.438087: step 2440, loss 0.0247956, acc 0.984375
2017-08-08T16:35:03.866817: step 2441, loss 0.0421781, acc 0.984375
2017-08-08T16:35:04.113594: step 2442, loss 0.0212365, acc 1
2017-08-08T16:35:04.513517: step 2443, loss 0.0200568, acc 1
2017-08-08T16:35:04.857705: step 2444, loss 0.0462295, acc 0.984375
2017-08-08T16:35:05.167683: step 2445, loss 0.0473481, acc 0.96875
2017-08-08T16:35:05.461041: step 2446, loss 0.0298062, acc 0.984375
2017-08-08T16:35:05.714865: step 2447, loss 0.0486938, acc 0.984375
2017-08-08T16:35:06.123100: step 2448, loss 0.0101251, acc 1
2017-08-08T16:35:06.560078: step 2449, loss 0.0181763, acc 1
2017-08-08T16:35:06.955038: step 2450, loss 0.0432613, acc 0.984375
2017-08-08T16:35:07.198727: step 2451, loss 0.0515681, acc 0.96875
2017-08-08T16:35:07.460579: step 2452, loss 0.0324946, acc 1
2017-08-08T16:35:07.769805: step 2453, loss 0.0283563, acc 0.984375
2017-08-08T16:35:08.081330: step 2454, loss 0.00712611, acc 1
2017-08-08T16:35:08.350207: step 2455, loss 0.0273547, acc 1
2017-08-08T16:35:08.633422: step 2456, loss 0.0484142, acc 0.96875
2017-08-08T16:35:08.906997: step 2457, loss 0.0513292, acc 0.984375
2017-08-08T16:35:09.353188: step 2458, loss 0.0243566, acc 1
2017-08-08T16:35:09.845593: step 2459, loss 0.0107106, acc 1
2017-08-08T16:35:10.188108: step 2460, loss 0.0148373, acc 1
2017-08-08T16:35:10.438246: step 2461, loss 0.0744631, acc 0.96875
2017-08-08T16:35:10.877339: step 2462, loss 0.0315855, acc 1
2017-08-08T16:35:11.257916: step 2463, loss 0.0191058, acc 1
2017-08-08T16:35:11.598538: step 2464, loss 0.106433, acc 0.96875
2017-08-08T16:35:11.878875: step 2465, loss 0.0270903, acc 1
2017-08-08T16:35:12.331210: step 2466, loss 0.028993, acc 1
2017-08-08T16:35:12.739835: step 2467, loss 0.0164747, acc 1
2017-08-08T16:35:13.078092: step 2468, loss 0.0498862, acc 0.96875
2017-08-08T16:35:13.347781: step 2469, loss 0.0133819, acc 1
2017-08-08T16:35:13.610270: step 2470, loss 0.0142847, acc 1
2017-08-08T16:35:14.042964: step 2471, loss 0.0697023, acc 0.96875
2017-08-08T16:35:14.367596: step 2472, loss 0.0341011, acc 1
2017-08-08T16:35:14.642719: step 2473, loss 0.00412863, acc 1
2017-08-08T16:35:15.017152: step 2474, loss 0.00576827, acc 1
2017-08-08T16:35:15.544635: step 2475, loss 0.0311582, acc 0.984375
2017-08-08T16:35:15.892826: step 2476, loss 0.042613, acc 0.984375
2017-08-08T16:35:16.152298: step 2477, loss 0.0341763, acc 0.984375
2017-08-08T16:35:16.437424: step 2478, loss 0.0509323, acc 0.984375
2017-08-08T16:35:16.931826: step 2479, loss 0.0735764, acc 0.96875
2017-08-08T16:35:17.255087: step 2480, loss 0.0357394, acc 0.984375
2017-08-08T16:35:17.523996: step 2481, loss 0.0153691, acc 1
2017-08-08T16:35:17.777584: step 2482, loss 0.0199988, acc 1
2017-08-08T16:35:18.066558: step 2483, loss 0.014431, acc 1
2017-08-08T16:35:18.412375: step 2484, loss 0.0173843, acc 1
2017-08-08T16:35:18.763509: step 2485, loss 0.0201179, acc 1
2017-08-08T16:35:19.031562: step 2486, loss 0.00710988, acc 1
2017-08-08T16:35:19.301559: step 2487, loss 0.0339329, acc 0.984375
2017-08-08T16:35:19.698824: step 2488, loss 0.0347894, acc 0.96875
2017-08-08T16:35:19.933846: step 2489, loss 0.0301388, acc 1
2017-08-08T16:35:20.194053: step 2490, loss 0.00848639, acc 1
2017-08-08T16:35:20.458801: step 2491, loss 0.0178919, acc 1
2017-08-08T16:35:20.974536: step 2492, loss 0.0300357, acc 0.984375
2017-08-08T16:35:21.389835: step 2493, loss 0.00830411, acc 1
2017-08-08T16:35:21.748886: step 2494, loss 0.0179943, acc 1
2017-08-08T16:35:21.959981: step 2495, loss 0.0294944, acc 1
2017-08-08T16:35:22.265369: step 2496, loss 0.0266957, acc 0.984375
2017-08-08T16:35:22.562027: step 2497, loss 0.0294439, acc 0.984375
2017-08-08T16:35:22.761325: step 2498, loss 0.0282553, acc 1
2017-08-08T16:35:22.990665: step 2499, loss 0.043216, acc 0.96875
2017-08-08T16:35:23.304850: step 2500, loss 0.0288982, acc 0.984375

Evaluation:
2017-08-08T16:35:24.102464: step 2500, loss 0.924885, acc 0.733584

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-2500

2017-08-08T16:35:24.478753: step 2501, loss 0.0199163, acc 1
2017-08-08T16:35:24.753367: step 2502, loss 0.0187101, acc 0.984375
2017-08-08T16:35:24.944272: step 2503, loss 0.0501189, acc 0.984375
2017-08-08T16:35:25.159936: step 2504, loss 0.0298766, acc 0.984375
2017-08-08T16:35:25.381369: step 2505, loss 0.0161805, acc 1
2017-08-08T16:35:25.671702: step 2506, loss 0.0428127, acc 0.96875
2017-08-08T16:35:26.101896: step 2507, loss 0.0238394, acc 1
2017-08-08T16:35:26.332859: step 2508, loss 0.0222233, acc 0.984375
2017-08-08T16:35:26.526205: step 2509, loss 0.0235575, acc 1
2017-08-08T16:35:26.780571: step 2510, loss 0.028333, acc 0.984375
2017-08-08T16:35:27.080802: step 2511, loss 0.0253133, acc 1
2017-08-08T16:35:27.304548: step 2512, loss 0.0147986, acc 1
2017-08-08T16:35:27.616965: step 2513, loss 0.0606017, acc 0.96875
2017-08-08T16:35:27.889398: step 2514, loss 0.0323669, acc 0.984375
2017-08-08T16:35:28.311460: step 2515, loss 0.0229101, acc 0.984375
2017-08-08T16:35:28.725366: step 2516, loss 0.0506595, acc 0.96875
2017-08-08T16:35:29.113009: step 2517, loss 0.0529474, acc 0.984375
2017-08-08T16:35:29.374452: step 2518, loss 0.0261221, acc 0.984375
2017-08-08T16:35:29.589348: step 2519, loss 0.0444341, acc 0.96875
2017-08-08T16:35:29.937521: step 2520, loss 0.0552935, acc 0.984375
2017-08-08T16:35:30.100217: step 2521, loss 0.0643927, acc 0.953125
2017-08-08T16:35:30.319213: step 2522, loss 0.0341795, acc 0.96875
2017-08-08T16:35:30.553319: step 2523, loss 0.0239724, acc 1
2017-08-08T16:35:30.898950: step 2524, loss 0.0223209, acc 1
2017-08-08T16:35:31.177341: step 2525, loss 0.0189268, acc 1
2017-08-08T16:35:31.447035: step 2526, loss 0.0575223, acc 0.984375
2017-08-08T16:35:31.676908: step 2527, loss 0.10009, acc 0.953125
2017-08-08T16:35:31.860328: step 2528, loss 0.0175987, acc 1
2017-08-08T16:35:32.125715: step 2529, loss 0.0259919, acc 1
2017-08-08T16:35:32.376975: step 2530, loss 0.0770962, acc 0.984375
2017-08-08T16:35:32.591337: step 2531, loss 0.028975, acc 0.984375
2017-08-08T16:35:32.824259: step 2532, loss 0.0558786, acc 0.984375
2017-08-08T16:35:33.101456: step 2533, loss 0.028643, acc 1
2017-08-08T16:35:33.591819: step 2534, loss 0.0920614, acc 0.984375
2017-08-08T16:35:33.964111: step 2535, loss 0.0726023, acc 0.984375
2017-08-08T16:35:34.355688: step 2536, loss 0.00781403, acc 1
2017-08-08T16:35:34.595728: step 2537, loss 0.0209927, acc 1
2017-08-08T16:35:34.862385: step 2538, loss 0.0141493, acc 1
2017-08-08T16:35:35.316795: step 2539, loss 0.0580551, acc 0.96875
2017-08-08T16:35:35.590493: step 2540, loss 0.0455875, acc 0.96875
2017-08-08T16:35:35.869470: step 2541, loss 0.0432242, acc 0.984375
2017-08-08T16:35:36.269345: step 2542, loss 0.0270206, acc 1
2017-08-08T16:35:36.540660: step 2543, loss 0.0332276, acc 0.984375
2017-08-08T16:35:36.802113: step 2544, loss 0.0110666, acc 1
2017-08-08T16:35:37.078535: step 2545, loss 0.0290149, acc 1
2017-08-08T16:35:37.455047: step 2546, loss 0.063273, acc 0.96875
2017-08-08T16:35:37.734476: step 2547, loss 0.0566329, acc 0.96875
2017-08-08T16:35:38.020770: step 2548, loss 0.0211721, acc 1
2017-08-08T16:35:38.452334: step 2549, loss 0.0257925, acc 0.984375
2017-08-08T16:35:38.792624: step 2550, loss 0.0780567, acc 0.95
2017-08-08T16:35:39.166290: step 2551, loss 0.0442437, acc 0.984375
2017-08-08T16:35:39.419220: step 2552, loss 0.00923294, acc 1
2017-08-08T16:35:39.640230: step 2553, loss 0.00926995, acc 1
2017-08-08T16:35:40.126212: step 2554, loss 0.00406831, acc 1
2017-08-08T16:35:40.430540: step 2555, loss 0.0667334, acc 0.984375
2017-08-08T16:35:40.696600: step 2556, loss 0.0092577, acc 1
2017-08-08T16:35:40.975995: step 2557, loss 0.00766998, acc 1
2017-08-08T16:35:41.436219: step 2558, loss 0.0132417, acc 1
2017-08-08T16:35:41.850985: step 2559, loss 0.0349419, acc 0.984375
2017-08-08T16:35:42.162691: step 2560, loss 0.0235178, acc 0.984375
2017-08-08T16:35:42.422561: step 2561, loss 0.0551684, acc 0.96875
2017-08-08T16:35:42.677276: step 2562, loss 0.0830349, acc 0.96875
2017-08-08T16:35:43.158070: step 2563, loss 0.0180094, acc 1
2017-08-08T16:35:43.448975: step 2564, loss 0.0339282, acc 0.984375
2017-08-08T16:35:43.738552: step 2565, loss 0.0745895, acc 0.984375
2017-08-08T16:35:44.105411: step 2566, loss 0.00664853, acc 1
2017-08-08T16:35:44.586338: step 2567, loss 0.0138286, acc 1
2017-08-08T16:35:44.950925: step 2568, loss 0.00716554, acc 1
2017-08-08T16:35:45.244399: step 2569, loss 0.0100005, acc 1
2017-08-08T16:35:45.519169: step 2570, loss 0.0187983, acc 1
2017-08-08T16:35:45.737394: step 2571, loss 0.0157759, acc 1
2017-08-08T16:35:46.135375: step 2572, loss 0.0404976, acc 0.984375
2017-08-08T16:35:46.396336: step 2573, loss 0.00697154, acc 1
2017-08-08T16:35:46.642029: step 2574, loss 0.0099986, acc 1
2017-08-08T16:35:46.875830: step 2575, loss 0.0100326, acc 1
2017-08-08T16:35:47.313391: step 2576, loss 0.0714424, acc 0.953125
2017-08-08T16:35:47.661379: step 2577, loss 0.0127154, acc 1
2017-08-08T16:35:47.904261: step 2578, loss 0.0333393, acc 0.984375
2017-08-08T16:35:48.162252: step 2579, loss 0.0263259, acc 0.984375
2017-08-08T16:35:48.425615: step 2580, loss 0.0125777, acc 1
2017-08-08T16:35:48.624406: step 2581, loss 0.0191131, acc 1
2017-08-08T16:35:48.861494: step 2582, loss 0.00466845, acc 1
2017-08-08T16:35:49.138980: step 2583, loss 0.0101892, acc 1
2017-08-08T16:35:49.495300: step 2584, loss 0.022853, acc 1
2017-08-08T16:35:49.813369: step 2585, loss 0.0145546, acc 1
2017-08-08T16:35:50.101661: step 2586, loss 0.0153152, acc 1
2017-08-08T16:35:50.507373: step 2587, loss 0.00731794, acc 1
2017-08-08T16:35:50.857478: step 2588, loss 0.0135516, acc 1
2017-08-08T16:35:51.177231: step 2589, loss 0.0196782, acc 1
2017-08-08T16:35:51.470057: step 2590, loss 0.0361103, acc 0.984375
2017-08-08T16:35:51.905404: step 2591, loss 0.0107526, acc 1
2017-08-08T16:35:52.321305: step 2592, loss 0.0557577, acc 0.984375
2017-08-08T16:35:52.627897: step 2593, loss 0.0130718, acc 1
2017-08-08T16:35:52.970747: step 2594, loss 0.033883, acc 0.984375
2017-08-08T16:35:53.418256: step 2595, loss 0.0372682, acc 0.984375
2017-08-08T16:35:53.715446: step 2596, loss 0.0359363, acc 0.984375
2017-08-08T16:35:53.970634: step 2597, loss 0.0504206, acc 0.984375
2017-08-08T16:35:54.409360: step 2598, loss 0.0699104, acc 0.953125
2017-08-08T16:35:54.813345: step 2599, loss 0.0775368, acc 0.96875
2017-08-08T16:35:55.165343: step 2600, loss 0.00431488, acc 1

Evaluation:
2017-08-08T16:35:55.814455: step 2600, loss 0.959295, acc 0.717636

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-2600

2017-08-08T16:35:56.426658: step 2601, loss 0.0154781, acc 1
2017-08-08T16:35:56.701717: step 2602, loss 0.0276667, acc 0.984375
2017-08-08T16:35:56.998105: step 2603, loss 0.0516999, acc 0.984375
2017-08-08T16:35:57.339171: step 2604, loss 0.0319877, acc 0.984375
2017-08-08T16:35:57.753362: step 2605, loss 0.013178, acc 1
2017-08-08T16:35:58.175291: step 2606, loss 0.0152949, acc 1
2017-08-08T16:35:58.469015: step 2607, loss 0.00636499, acc 1
2017-08-08T16:35:58.769623: step 2608, loss 0.010616, acc 1
2017-08-08T16:35:59.225361: step 2609, loss 0.00377715, acc 1
2017-08-08T16:35:59.518866: step 2610, loss 0.0078026, acc 1
2017-08-08T16:35:59.807304: step 2611, loss 0.0150146, acc 1
2017-08-08T16:36:00.233357: step 2612, loss 0.0304217, acc 0.984375
2017-08-08T16:36:00.653043: step 2613, loss 0.00666083, acc 1
2017-08-08T16:36:01.042777: step 2614, loss 0.0138246, acc 1
2017-08-08T16:36:01.361213: step 2615, loss 0.0448665, acc 1
2017-08-08T16:36:01.623343: step 2616, loss 0.0400758, acc 0.96875
2017-08-08T16:36:01.997532: step 2617, loss 0.0322665, acc 0.96875
2017-08-08T16:36:02.377767: step 2618, loss 0.0321686, acc 1
2017-08-08T16:36:02.632154: step 2619, loss 0.00790105, acc 1
2017-08-08T16:36:02.921419: step 2620, loss 0.0195566, acc 1
2017-08-08T16:36:03.207942: step 2621, loss 0.0578729, acc 0.96875
2017-08-08T16:36:03.745020: step 2622, loss 0.0309338, acc 0.984375
2017-08-08T16:36:04.171127: step 2623, loss 0.00343414, acc 1
2017-08-08T16:36:04.545664: step 2624, loss 0.0377479, acc 0.984375
2017-08-08T16:36:04.761348: step 2625, loss 0.0441389, acc 0.96875
2017-08-08T16:36:05.014780: step 2626, loss 0.00995913, acc 1
2017-08-08T16:36:05.425614: step 2627, loss 0.00911357, acc 1
2017-08-08T16:36:05.693860: step 2628, loss 0.0111092, acc 1
2017-08-08T16:36:05.985158: step 2629, loss 0.0264777, acc 1
2017-08-08T16:36:06.397689: step 2630, loss 0.00907043, acc 1
2017-08-08T16:36:06.809981: step 2631, loss 0.0291226, acc 0.984375
2017-08-08T16:36:07.180611: step 2632, loss 0.0166135, acc 0.984375
2017-08-08T16:36:07.509246: step 2633, loss 0.0987111, acc 0.96875
2017-08-08T16:36:07.797114: step 2634, loss 0.0214009, acc 1
2017-08-08T16:36:08.171588: step 2635, loss 0.0482966, acc 0.96875
2017-08-08T16:36:08.441643: step 2636, loss 0.0284488, acc 0.984375
2017-08-08T16:36:08.770558: step 2637, loss 0.00994728, acc 1
2017-08-08T16:36:09.057398: step 2638, loss 0.025021, acc 0.984375
2017-08-08T16:36:09.503474: step 2639, loss 0.00719809, acc 1
2017-08-08T16:36:09.805376: step 2640, loss 0.00931759, acc 1
2017-08-08T16:36:10.079790: step 2641, loss 0.00834618, acc 1
2017-08-08T16:36:10.247875: step 2642, loss 0.0882921, acc 0.96875
2017-08-08T16:36:10.473138: step 2643, loss 0.0208548, acc 1
2017-08-08T16:36:10.825153: step 2644, loss 0.0302805, acc 0.984375
2017-08-08T16:36:11.080717: step 2645, loss 0.0276907, acc 0.984375
2017-08-08T16:36:11.360361: step 2646, loss 0.0100647, acc 1
2017-08-08T16:36:11.724079: step 2647, loss 0.040667, acc 0.984375
2017-08-08T16:36:12.049479: step 2648, loss 0.0147042, acc 1
2017-08-08T16:36:12.343992: step 2649, loss 0.025574, acc 0.984375
2017-08-08T16:36:12.665066: step 2650, loss 0.00934975, acc 1
2017-08-08T16:36:12.916962: step 2651, loss 0.0146844, acc 1
2017-08-08T16:36:13.314373: step 2652, loss 0.0188845, acc 1
2017-08-08T16:36:13.712536: step 2653, loss 0.0507802, acc 0.96875
2017-08-08T16:36:14.010781: step 2654, loss 0.0184843, acc 1
2017-08-08T16:36:14.289417: step 2655, loss 0.00542661, acc 1
2017-08-08T16:36:14.733030: step 2656, loss 0.0136923, acc 1
2017-08-08T16:36:15.161956: step 2657, loss 0.0135918, acc 1
2017-08-08T16:36:15.446302: step 2658, loss 0.00335281, acc 1
2017-08-08T16:36:15.661479: step 2659, loss 0.013703, acc 1
2017-08-08T16:36:15.865148: step 2660, loss 0.020214, acc 1
2017-08-08T16:36:16.272472: step 2661, loss 0.0388417, acc 0.984375
2017-08-08T16:36:16.569629: step 2662, loss 0.0194187, acc 1
2017-08-08T16:36:16.877411: step 2663, loss 0.0180849, acc 0.984375
2017-08-08T16:36:17.229400: step 2664, loss 0.0129907, acc 1
2017-08-08T16:36:17.581390: step 2665, loss 0.027307, acc 0.984375
2017-08-08T16:36:17.907198: step 2666, loss 0.0438924, acc 0.96875
2017-08-08T16:36:18.197792: step 2667, loss 0.0364494, acc 0.96875
2017-08-08T16:36:18.383437: step 2668, loss 0.0405382, acc 0.984375
2017-08-08T16:36:18.695392: step 2669, loss 0.0148907, acc 1
2017-08-08T16:36:18.948829: step 2670, loss 0.0625947, acc 0.984375
2017-08-08T16:36:19.204482: step 2671, loss 0.0396665, acc 0.984375
2017-08-08T16:36:19.508467: step 2672, loss 0.161637, acc 0.953125
2017-08-08T16:36:19.719050: step 2673, loss 0.0278234, acc 0.984375
2017-08-08T16:36:20.141213: step 2674, loss 0.0155504, acc 1
2017-08-08T16:36:20.465538: step 2675, loss 0.0795054, acc 0.96875
2017-08-08T16:36:20.724431: step 2676, loss 0.0339454, acc 0.984375
2017-08-08T16:36:21.009952: step 2677, loss 0.0109285, acc 1
2017-08-08T16:36:21.445359: step 2678, loss 0.00550539, acc 1
2017-08-08T16:36:21.730444: step 2679, loss 0.0343839, acc 0.96875
2017-08-08T16:36:21.974003: step 2680, loss 0.0647268, acc 0.953125
2017-08-08T16:36:22.319779: step 2681, loss 0.0321658, acc 0.984375
2017-08-08T16:36:22.731950: step 2682, loss 0.0135632, acc 1
2017-08-08T16:36:23.060337: step 2683, loss 0.0051562, acc 1
2017-08-08T16:36:23.341498: step 2684, loss 0.048117, acc 0.984375
2017-08-08T16:36:23.529186: step 2685, loss 0.0076494, acc 1
2017-08-08T16:36:23.824515: step 2686, loss 0.0333769, acc 0.984375
2017-08-08T16:36:24.197812: step 2687, loss 0.0460984, acc 0.984375
2017-08-08T16:36:24.525746: step 2688, loss 0.0269364, acc 0.984375
2017-08-08T16:36:24.768857: step 2689, loss 0.0408498, acc 0.984375
2017-08-08T16:36:25.049176: step 2690, loss 0.0288703, acc 0.984375
2017-08-08T16:36:25.384318: step 2691, loss 0.0384217, acc 0.984375
2017-08-08T16:36:25.773844: step 2692, loss 0.0672574, acc 0.984375
2017-08-08T16:36:26.069991: step 2693, loss 0.0108188, acc 1
2017-08-08T16:36:26.406087: step 2694, loss 0.0270227, acc 0.984375
2017-08-08T16:36:26.779047: step 2695, loss 0.0381098, acc 0.96875
2017-08-08T16:36:27.039163: step 2696, loss 0.0182047, acc 1
2017-08-08T16:36:27.321401: step 2697, loss 0.0254661, acc 0.984375
2017-08-08T16:36:27.723886: step 2698, loss 0.0777369, acc 0.96875
2017-08-08T16:36:28.132406: step 2699, loss 0.022669, acc 1
2017-08-08T16:36:28.430639: step 2700, loss 0.0189078, acc 1

Evaluation:
2017-08-08T16:36:29.331313: step 2700, loss 1.02677, acc 0.727955

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-2700

2017-08-08T16:36:29.811016: step 2701, loss 0.00733673, acc 1
2017-08-08T16:36:30.046032: step 2702, loss 0.0254953, acc 0.984375
2017-08-08T16:36:30.321374: step 2703, loss 0.0067692, acc 1
2017-08-08T16:36:30.704704: step 2704, loss 0.040926, acc 0.984375
2017-08-08T16:36:31.009165: step 2705, loss 0.0086795, acc 1
2017-08-08T16:36:31.216473: step 2706, loss 0.0666143, acc 0.96875
2017-08-08T16:36:31.537313: step 2707, loss 0.00821825, acc 1
2017-08-08T16:36:31.723166: step 2708, loss 0.00931345, acc 1
2017-08-08T16:36:31.954493: step 2709, loss 0.00744608, acc 1
2017-08-08T16:36:32.178801: step 2710, loss 0.0284631, acc 1
2017-08-08T16:36:32.507476: step 2711, loss 0.0381021, acc 0.96875
2017-08-08T16:36:32.837367: step 2712, loss 0.0204326, acc 0.984375
2017-08-08T16:36:33.074404: step 2713, loss 0.0139647, acc 1
2017-08-08T16:36:33.316644: step 2714, loss 0.0157937, acc 1
2017-08-08T16:36:33.631564: step 2715, loss 0.0247915, acc 1
2017-08-08T16:36:33.856979: step 2716, loss 0.0351831, acc 0.984375
2017-08-08T16:36:34.090088: step 2717, loss 0.0103586, acc 1
2017-08-08T16:36:34.387871: step 2718, loss 0.00923513, acc 1
2017-08-08T16:36:34.785754: step 2719, loss 0.0111523, acc 1
2017-08-08T16:36:35.120370: step 2720, loss 0.0382424, acc 0.984375
2017-08-08T16:36:35.361309: step 2721, loss 0.0278348, acc 1
2017-08-08T16:36:35.566197: step 2722, loss 0.00668821, acc 1
2017-08-08T16:36:35.873551: step 2723, loss 0.0111037, acc 1
2017-08-08T16:36:36.151657: step 2724, loss 0.0124192, acc 1
2017-08-08T16:36:36.367733: step 2725, loss 0.0304792, acc 0.984375
2017-08-08T16:36:36.580450: step 2726, loss 0.0286817, acc 1
2017-08-08T16:36:36.929899: step 2727, loss 0.0090067, acc 1
2017-08-08T16:36:37.218950: step 2728, loss 0.0279476, acc 0.984375
2017-08-08T16:36:37.453455: step 2729, loss 0.0179761, acc 1
2017-08-08T16:36:37.718744: step 2730, loss 0.0214038, acc 0.984375
2017-08-08T16:36:37.902185: step 2731, loss 0.0139227, acc 1
2017-08-08T16:36:38.221675: step 2732, loss 0.0108379, acc 1
2017-08-08T16:36:38.556259: step 2733, loss 0.00838634, acc 1
2017-08-08T16:36:38.837871: step 2734, loss 0.0328569, acc 0.984375
2017-08-08T16:36:39.075934: step 2735, loss 0.0192733, acc 1
2017-08-08T16:36:39.476789: step 2736, loss 0.0170395, acc 1
2017-08-08T16:36:39.888449: step 2737, loss 0.00396067, acc 1
2017-08-08T16:36:40.203713: step 2738, loss 0.0158789, acc 1
2017-08-08T16:36:40.466913: step 2739, loss 0.0466286, acc 0.96875
2017-08-08T16:36:40.831672: step 2740, loss 0.00923707, acc 1
2017-08-08T16:36:41.054120: step 2741, loss 0.0119563, acc 1
2017-08-08T16:36:41.255996: step 2742, loss 0.0134683, acc 1
2017-08-08T16:36:41.541634: step 2743, loss 0.0593991, acc 0.96875
2017-08-08T16:36:41.777333: step 2744, loss 0.0299858, acc 0.984375
2017-08-08T16:36:42.059821: step 2745, loss 0.0158981, acc 1
2017-08-08T16:36:42.407696: step 2746, loss 0.00492087, acc 1
2017-08-08T16:36:42.783704: step 2747, loss 0.0120008, acc 1
2017-08-08T16:36:43.141844: step 2748, loss 0.0103578, acc 1
2017-08-08T16:36:43.425674: step 2749, loss 0.00720807, acc 1
2017-08-08T16:36:43.734794: step 2750, loss 0.0233793, acc 1
2017-08-08T16:36:44.118813: step 2751, loss 0.0218325, acc 1
2017-08-08T16:36:44.490209: step 2752, loss 0.0102972, acc 1
2017-08-08T16:36:44.835147: step 2753, loss 0.0339546, acc 0.984375
2017-08-08T16:36:45.124772: step 2754, loss 0.027172, acc 0.984375
2017-08-08T16:36:45.384271: step 2755, loss 0.0325516, acc 0.984375
2017-08-08T16:36:45.822334: step 2756, loss 0.0127876, acc 1
2017-08-08T16:36:46.048498: step 2757, loss 0.0121663, acc 1
2017-08-08T16:36:46.286371: step 2758, loss 0.0168773, acc 1
2017-08-08T16:36:46.519252: step 2759, loss 0.0143377, acc 1
2017-08-08T16:36:46.840665: step 2760, loss 0.0366751, acc 0.984375
2017-08-08T16:36:47.188891: step 2761, loss 0.0144379, acc 1
2017-08-08T16:36:47.418428: step 2762, loss 0.010705, acc 1
2017-08-08T16:36:47.581596: step 2763, loss 0.0772375, acc 0.953125
2017-08-08T16:36:47.757508: step 2764, loss 0.00666242, acc 1
2017-08-08T16:36:48.024046: step 2765, loss 0.0176618, acc 1
2017-08-08T16:36:48.242768: step 2766, loss 0.0125933, acc 1
2017-08-08T16:36:48.436902: step 2767, loss 0.00793496, acc 1
2017-08-08T16:36:48.652382: step 2768, loss 0.0515314, acc 0.96875
2017-08-08T16:36:49.016860: step 2769, loss 0.00638117, acc 1
2017-08-08T16:36:49.482099: step 2770, loss 0.0290897, acc 1
2017-08-08T16:36:49.788499: step 2771, loss 0.0334176, acc 0.984375
2017-08-08T16:36:50.018880: step 2772, loss 0.0268738, acc 1
2017-08-08T16:36:50.458956: step 2773, loss 0.0352638, acc 0.984375
2017-08-08T16:36:50.695071: step 2774, loss 0.0294568, acc 0.96875
2017-08-08T16:36:50.939441: step 2775, loss 0.0789223, acc 0.984375
2017-08-08T16:36:51.170557: step 2776, loss 0.0472204, acc 0.984375
2017-08-08T16:36:51.503175: step 2777, loss 0.0121625, acc 1
2017-08-08T16:36:51.803719: step 2778, loss 0.00835835, acc 1
2017-08-08T16:36:52.100104: step 2779, loss 0.0706686, acc 0.96875
2017-08-08T16:36:52.342796: step 2780, loss 0.013069, acc 1
2017-08-08T16:36:52.548827: step 2781, loss 0.0663431, acc 0.953125
2017-08-08T16:36:52.756939: step 2782, loss 0.0247648, acc 0.984375
2017-08-08T16:36:53.097312: step 2783, loss 0.0286764, acc 0.984375
2017-08-08T16:36:53.311076: step 2784, loss 0.00329494, acc 1
2017-08-08T16:36:53.503705: step 2785, loss 0.0131266, acc 1
2017-08-08T16:36:53.754202: step 2786, loss 0.0269608, acc 0.984375
2017-08-08T16:36:54.162791: step 2787, loss 0.00502166, acc 1
2017-08-08T16:36:54.652534: step 2788, loss 0.0365719, acc 0.984375
2017-08-08T16:36:55.065246: step 2789, loss 0.0516664, acc 0.984375
2017-08-08T16:36:55.318127: step 2790, loss 0.00825948, acc 1
2017-08-08T16:36:55.709013: step 2791, loss 0.00779625, acc 1
2017-08-08T16:36:56.075338: step 2792, loss 0.00645958, acc 1
2017-08-08T16:36:56.395446: step 2793, loss 0.0114914, acc 1
2017-08-08T16:36:56.674657: step 2794, loss 0.053224, acc 0.96875
2017-08-08T16:36:56.995345: step 2795, loss 0.0212965, acc 0.984375
2017-08-08T16:36:57.413943: step 2796, loss 0.020158, acc 1
2017-08-08T16:36:57.753979: step 2797, loss 0.0156597, acc 1
2017-08-08T16:36:58.009247: step 2798, loss 0.00709399, acc 1
2017-08-08T16:36:58.369555: step 2799, loss 0.0144678, acc 1
2017-08-08T16:36:58.622140: step 2800, loss 0.013051, acc 1

Evaluation:
2017-08-08T16:36:59.291674: step 2800, loss 1.02988, acc 0.722326

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-2800

2017-08-08T16:36:59.884457: step 2801, loss 0.0211038, acc 1
2017-08-08T16:37:00.329029: step 2802, loss 0.00815103, acc 1
2017-08-08T16:37:00.720050: step 2803, loss 0.00418506, acc 1
2017-08-08T16:37:00.969392: step 2804, loss 0.0315375, acc 0.984375
2017-08-08T16:37:01.222739: step 2805, loss 0.0212406, acc 0.984375
2017-08-08T16:37:01.611905: step 2806, loss 0.0165656, acc 1
2017-08-08T16:37:01.921956: step 2807, loss 0.0134219, acc 1
2017-08-08T16:37:02.182840: step 2808, loss 0.0266653, acc 0.984375
2017-08-08T16:37:02.442500: step 2809, loss 0.0322119, acc 0.984375
2017-08-08T16:37:02.871741: step 2810, loss 0.00579973, acc 1
2017-08-08T16:37:03.359445: step 2811, loss 0.0515644, acc 0.96875
2017-08-08T16:37:03.711231: step 2812, loss 0.0239243, acc 1
2017-08-08T16:37:04.059810: step 2813, loss 0.0131121, acc 1
2017-08-08T16:37:04.263898: step 2814, loss 0.0114964, acc 1
2017-08-08T16:37:04.598317: step 2815, loss 0.00343722, acc 1
2017-08-08T16:37:04.848689: step 2816, loss 0.0128223, acc 1
2017-08-08T16:37:05.084294: step 2817, loss 0.0426827, acc 0.96875
2017-08-08T16:37:05.340168: step 2818, loss 0.0849671, acc 0.96875
2017-08-08T16:37:05.561918: step 2819, loss 0.0247385, acc 0.984375
2017-08-08T16:37:06.051658: step 2820, loss 0.0115541, acc 1
2017-08-08T16:37:06.389929: step 2821, loss 0.0119041, acc 1
2017-08-08T16:37:06.694808: step 2822, loss 0.0082599, acc 1
2017-08-08T16:37:06.909792: step 2823, loss 0.0796267, acc 0.96875
2017-08-08T16:37:07.213757: step 2824, loss 0.0277411, acc 0.984375
2017-08-08T16:37:07.448525: step 2825, loss 0.0121838, acc 1
2017-08-08T16:37:07.656762: step 2826, loss 0.0204055, acc 1
2017-08-08T16:37:07.922039: step 2827, loss 0.0123971, acc 1
2017-08-08T16:37:08.176432: step 2828, loss 0.0253381, acc 1
2017-08-08T16:37:08.489340: step 2829, loss 0.0493009, acc 0.984375
2017-08-08T16:37:08.914964: step 2830, loss 0.0325624, acc 0.984375
2017-08-08T16:37:09.178783: step 2831, loss 0.00914982, acc 1
2017-08-08T16:37:09.414149: step 2832, loss 0.0145798, acc 1
2017-08-08T16:37:09.692838: step 2833, loss 0.0532562, acc 0.984375
2017-08-08T16:37:10.099362: step 2834, loss 0.0194283, acc 0.984375
2017-08-08T16:37:10.311456: step 2835, loss 0.0290822, acc 0.984375
2017-08-08T16:37:10.576560: step 2836, loss 0.00292263, acc 1
2017-08-08T16:37:10.902889: step 2837, loss 0.022105, acc 1
2017-08-08T16:37:11.185166: step 2838, loss 0.044778, acc 0.984375
2017-08-08T16:37:11.413354: step 2839, loss 0.0659069, acc 0.953125
2017-08-08T16:37:11.612365: step 2840, loss 0.065035, acc 0.984375
2017-08-08T16:37:11.897049: step 2841, loss 0.0128675, acc 1
2017-08-08T16:37:12.151484: step 2842, loss 0.0269393, acc 0.984375
2017-08-08T16:37:12.363401: step 2843, loss 0.0102307, acc 1
2017-08-08T16:37:12.573018: step 2844, loss 0.0508775, acc 0.984375
2017-08-08T16:37:12.833328: step 2845, loss 0.0116388, acc 1
2017-08-08T16:37:13.179597: step 2846, loss 0.00979241, acc 1
2017-08-08T16:37:13.467940: step 2847, loss 0.00757705, acc 1
2017-08-08T16:37:13.760678: step 2848, loss 0.0151893, acc 1
2017-08-08T16:37:14.082910: step 2849, loss 0.008715, acc 1
2017-08-08T16:37:14.347399: step 2850, loss 0.0104382, acc 1
2017-08-08T16:37:14.605837: step 2851, loss 0.0101381, acc 1
2017-08-08T16:37:14.805988: step 2852, loss 0.00566513, acc 1
2017-08-08T16:37:15.209358: step 2853, loss 0.0197878, acc 0.984375
2017-08-08T16:37:15.573252: step 2854, loss 0.0186629, acc 0.984375
2017-08-08T16:37:15.794420: step 2855, loss 0.0105911, acc 1
2017-08-08T16:37:16.010209: step 2856, loss 0.00516803, acc 1
2017-08-08T16:37:16.358211: step 2857, loss 0.0496291, acc 0.96875
2017-08-08T16:37:16.557687: step 2858, loss 0.0121683, acc 1
2017-08-08T16:37:16.789531: step 2859, loss 0.0843836, acc 0.984375
2017-08-08T16:37:17.000611: step 2860, loss 0.0391946, acc 0.96875
2017-08-08T16:37:17.337685: step 2861, loss 0.0257435, acc 0.984375
2017-08-08T16:37:17.695201: step 2862, loss 0.0116653, acc 1
2017-08-08T16:37:18.112155: step 2863, loss 0.0258511, acc 1
2017-08-08T16:37:18.417502: step 2864, loss 0.0558525, acc 0.984375
2017-08-08T16:37:18.661471: step 2865, loss 0.0196827, acc 1
2017-08-08T16:37:19.097419: step 2866, loss 0.0112268, acc 1
2017-08-08T16:37:19.409891: step 2867, loss 0.0394451, acc 0.984375
2017-08-08T16:37:19.689282: step 2868, loss 0.0237066, acc 1
2017-08-08T16:37:19.987523: step 2869, loss 0.0561192, acc 0.96875
2017-08-08T16:37:20.251548: step 2870, loss 0.00972836, acc 1
2017-08-08T16:37:20.649406: step 2871, loss 0.00236549, acc 1
2017-08-08T16:37:21.077945: step 2872, loss 0.0172629, acc 1
2017-08-08T16:37:21.451891: step 2873, loss 0.0157751, acc 1
2017-08-08T16:37:21.683102: step 2874, loss 0.0209514, acc 0.984375
2017-08-08T16:37:21.995196: step 2875, loss 0.0571244, acc 0.984375
2017-08-08T16:37:22.285197: step 2876, loss 0.0098238, acc 1
2017-08-08T16:37:22.523776: step 2877, loss 0.0566801, acc 0.984375
2017-08-08T16:37:22.748110: step 2878, loss 0.0358783, acc 0.984375
2017-08-08T16:37:23.054722: step 2879, loss 0.00391052, acc 1
2017-08-08T16:37:23.275960: step 2880, loss 0.0108068, acc 1
2017-08-08T16:37:23.591487: step 2881, loss 0.0450635, acc 0.96875
2017-08-08T16:37:23.798338: step 2882, loss 0.00813303, acc 1
2017-08-08T16:37:24.037384: step 2883, loss 0.0173554, acc 0.984375
2017-08-08T16:37:24.433382: step 2884, loss 0.0080299, acc 1
2017-08-08T16:37:24.670643: step 2885, loss 0.0279291, acc 1
2017-08-08T16:37:24.904127: step 2886, loss 0.00772388, acc 1
2017-08-08T16:37:25.381394: step 2887, loss 0.0174824, acc 0.984375
2017-08-08T16:37:25.646066: step 2888, loss 0.00361038, acc 1
2017-08-08T16:37:25.870912: step 2889, loss 0.0293792, acc 0.984375
2017-08-08T16:37:26.083943: step 2890, loss 0.00760153, acc 1
2017-08-08T16:37:26.313706: step 2891, loss 0.00496242, acc 1
2017-08-08T16:37:26.672543: step 2892, loss 0.00841878, acc 1
2017-08-08T16:37:26.985235: step 2893, loss 0.0127667, acc 1
2017-08-08T16:37:27.252129: step 2894, loss 0.00915684, acc 1
2017-08-08T16:37:27.542194: step 2895, loss 0.0734605, acc 0.96875
2017-08-08T16:37:27.768134: step 2896, loss 0.0156526, acc 1
2017-08-08T16:37:28.253321: step 2897, loss 0.00995129, acc 1
2017-08-08T16:37:28.632213: step 2898, loss 0.0227771, acc 1
2017-08-08T16:37:28.932430: step 2899, loss 0.00521632, acc 1
2017-08-08T16:37:29.168559: step 2900, loss 0.0186152, acc 0.984375

Evaluation:
2017-08-08T16:37:30.017851: step 2900, loss 1.03719, acc 0.725141

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-2900

2017-08-08T16:37:30.366211: step 2901, loss 0.00649571, acc 1
2017-08-08T16:37:30.597051: step 2902, loss 0.0203499, acc 0.984375
2017-08-08T16:37:30.877757: step 2903, loss 0.0338922, acc 0.984375
2017-08-08T16:37:31.156726: step 2904, loss 0.0129538, acc 1
2017-08-08T16:37:31.479779: step 2905, loss 0.107271, acc 0.96875
2017-08-08T16:37:31.729706: step 2906, loss 0.00740845, acc 1
2017-08-08T16:37:31.912575: step 2907, loss 0.00704171, acc 1
2017-08-08T16:37:32.191509: step 2908, loss 0.0352914, acc 1
2017-08-08T16:37:32.461934: step 2909, loss 0.00934331, acc 1
2017-08-08T16:37:32.650133: step 2910, loss 0.0128972, acc 1
2017-08-08T16:37:32.926805: step 2911, loss 0.00750767, acc 1
2017-08-08T16:37:33.199106: step 2912, loss 0.0099387, acc 1
2017-08-08T16:37:33.503056: step 2913, loss 0.0242895, acc 0.984375
2017-08-08T16:37:33.841838: step 2914, loss 0.0135163, acc 1
2017-08-08T16:37:34.047516: step 2915, loss 0.00633548, acc 1
2017-08-08T16:37:34.336697: step 2916, loss 0.0187652, acc 0.984375
2017-08-08T16:37:34.652906: step 2917, loss 0.0176897, acc 1
2017-08-08T16:37:34.897562: step 2918, loss 0.0102892, acc 1
2017-08-08T16:37:35.183781: step 2919, loss 0.00703482, acc 1
2017-08-08T16:37:35.500721: step 2920, loss 0.0272785, acc 1
2017-08-08T16:37:35.868606: step 2921, loss 0.00184305, acc 1
2017-08-08T16:37:36.125711: step 2922, loss 0.00841561, acc 1
2017-08-08T16:37:36.290440: step 2923, loss 0.0108542, acc 1
2017-08-08T16:37:36.469034: step 2924, loss 0.0192466, acc 1
2017-08-08T16:37:36.788106: step 2925, loss 0.00712883, acc 1
2017-08-08T16:37:37.020353: step 2926, loss 0.00723171, acc 1
2017-08-08T16:37:37.273622: step 2927, loss 0.0111394, acc 1
2017-08-08T16:37:37.518393: step 2928, loss 0.0290764, acc 1
2017-08-08T16:37:37.882223: step 2929, loss 0.0126141, acc 1
2017-08-08T16:37:38.237931: step 2930, loss 0.083022, acc 0.96875
2017-08-08T16:37:38.539901: step 2931, loss 0.0349582, acc 0.984375
2017-08-08T16:37:38.797572: step 2932, loss 0.0102781, acc 1
2017-08-08T16:37:39.088022: step 2933, loss 0.0100122, acc 1
2017-08-08T16:37:39.265231: step 2934, loss 0.0135187, acc 1
2017-08-08T16:37:39.587698: step 2935, loss 0.00537203, acc 1
2017-08-08T16:37:39.813685: step 2936, loss 0.0127587, acc 1
2017-08-08T16:37:40.225398: step 2937, loss 0.0173451, acc 1
2017-08-08T16:37:40.652939: step 2938, loss 0.00677794, acc 1
2017-08-08T16:37:40.993475: step 2939, loss 0.0083222, acc 1
2017-08-08T16:37:41.181385: step 2940, loss 0.00983245, acc 1
2017-08-08T16:37:41.353264: step 2941, loss 0.00471597, acc 1
2017-08-08T16:37:41.744537: step 2942, loss 0.00219341, acc 1
2017-08-08T16:37:41.987242: step 2943, loss 0.0166452, acc 1
2017-08-08T16:37:42.210509: step 2944, loss 0.00793993, acc 1
2017-08-08T16:37:42.456909: step 2945, loss 0.0100144, acc 1
2017-08-08T16:37:42.869418: step 2946, loss 0.00604309, acc 1
2017-08-08T16:37:43.197422: step 2947, loss 0.0160351, acc 1
2017-08-08T16:37:43.429855: step 2948, loss 0.0168979, acc 1
2017-08-08T16:37:43.617868: step 2949, loss 0.0219089, acc 1
2017-08-08T16:37:43.946742: step 2950, loss 0.0501278, acc 0.96875
2017-08-08T16:37:44.250805: step 2951, loss 0.00909608, acc 1
2017-08-08T16:37:44.543160: step 2952, loss 0.00619585, acc 1
2017-08-08T16:37:44.833665: step 2953, loss 0.0128819, acc 1
2017-08-08T16:37:45.214866: step 2954, loss 0.0597249, acc 0.96875
2017-08-08T16:37:45.557052: step 2955, loss 0.0250401, acc 0.984375
2017-08-08T16:37:45.832212: step 2956, loss 0.019079, acc 1
2017-08-08T16:37:46.057192: step 2957, loss 0.0128598, acc 1
2017-08-08T16:37:46.413144: step 2958, loss 0.0388133, acc 0.984375
2017-08-08T16:37:46.628450: step 2959, loss 0.0208855, acc 0.984375
2017-08-08T16:37:46.817024: step 2960, loss 0.01687, acc 1
2017-08-08T16:37:47.051520: step 2961, loss 0.00634569, acc 1
2017-08-08T16:37:47.286407: step 2962, loss 0.00933473, acc 1
2017-08-08T16:37:47.769358: step 2963, loss 0.0100668, acc 1
2017-08-08T16:37:48.134237: step 2964, loss 0.00563832, acc 1
2017-08-08T16:37:48.470193: step 2965, loss 0.00671428, acc 1
2017-08-08T16:37:48.715261: step 2966, loss 0.0126981, acc 1
2017-08-08T16:37:49.081140: step 2967, loss 0.0140231, acc 1
2017-08-08T16:37:49.366148: step 2968, loss 0.0230491, acc 1
2017-08-08T16:37:49.562075: step 2969, loss 0.0431391, acc 0.984375
2017-08-08T16:37:49.796847: step 2970, loss 0.00177683, acc 1
2017-08-08T16:37:50.061312: step 2971, loss 0.00478563, acc 1
2017-08-08T16:37:50.470360: step 2972, loss 0.00266144, acc 1
2017-08-08T16:37:50.845371: step 2973, loss 0.0188089, acc 1
2017-08-08T16:37:51.213436: step 2974, loss 0.0671585, acc 0.96875
2017-08-08T16:37:51.456415: step 2975, loss 0.0163066, acc 1
2017-08-08T16:37:51.805394: step 2976, loss 0.0107734, acc 1
2017-08-08T16:37:52.106091: step 2977, loss 0.0126514, acc 1
2017-08-08T16:37:52.326423: step 2978, loss 0.0138759, acc 1
2017-08-08T16:37:52.551859: step 2979, loss 0.0496059, acc 0.96875
2017-08-08T16:37:52.789731: step 2980, loss 0.0443799, acc 0.984375
2017-08-08T16:37:53.212605: step 2981, loss 0.0111258, acc 1
2017-08-08T16:37:53.632737: step 2982, loss 0.0265201, acc 0.984375
2017-08-08T16:37:53.944550: step 2983, loss 0.00379841, acc 1
2017-08-08T16:37:54.134705: step 2984, loss 0.0147572, acc 1
2017-08-08T16:37:54.400456: step 2985, loss 0.0373401, acc 0.984375
2017-08-08T16:37:54.731014: step 2986, loss 0.00757218, acc 1
2017-08-08T16:37:54.956190: step 2987, loss 0.0739927, acc 0.984375
2017-08-08T16:37:55.198363: step 2988, loss 0.0108858, acc 1
2017-08-08T16:37:55.431906: step 2989, loss 0.00785984, acc 1
2017-08-08T16:37:55.748490: step 2990, loss 0.00774874, acc 1
2017-08-08T16:37:56.019012: step 2991, loss 0.0823162, acc 0.96875
2017-08-08T16:37:56.220858: step 2992, loss 0.00630893, acc 1
2017-08-08T16:37:56.476805: step 2993, loss 0.00541272, acc 1
2017-08-08T16:37:56.926215: step 2994, loss 0.0178972, acc 1
2017-08-08T16:37:57.188674: step 2995, loss 0.0159536, acc 1
2017-08-08T16:37:57.404176: step 2996, loss 0.0144867, acc 1
2017-08-08T16:37:57.626034: step 2997, loss 0.0309463, acc 0.984375
2017-08-08T16:37:58.092712: step 2998, loss 0.00362869, acc 1
2017-08-08T16:37:58.485176: step 2999, loss 0.0147603, acc 1
2017-08-08T16:37:58.842726: step 3000, loss 0.00936574, acc 1

Evaluation:
2017-08-08T16:37:59.635983: step 3000, loss 1.07627, acc 0.727017

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-3000

2017-08-08T16:38:00.214983: step 3001, loss 0.00759773, acc 1
2017-08-08T16:38:00.517699: step 3002, loss 0.00852837, acc 1
2017-08-08T16:38:00.881346: step 3003, loss 0.0143768, acc 1
2017-08-08T16:38:01.214249: step 3004, loss 0.0102089, acc 1
2017-08-08T16:38:01.611942: step 3005, loss 0.013719, acc 1
2017-08-08T16:38:01.973903: step 3006, loss 0.0423856, acc 0.984375
2017-08-08T16:38:02.224760: step 3007, loss 0.00597485, acc 1
2017-08-08T16:38:02.521972: step 3008, loss 0.0190361, acc 1
2017-08-08T16:38:02.966036: step 3009, loss 0.0118315, acc 1
2017-08-08T16:38:03.233686: step 3010, loss 0.00199469, acc 1
2017-08-08T16:38:03.582362: step 3011, loss 0.00662404, acc 1
2017-08-08T16:38:03.957495: step 3012, loss 0.00309212, acc 1
2017-08-08T16:38:04.325477: step 3013, loss 0.0176415, acc 1
2017-08-08T16:38:04.652440: step 3014, loss 0.0330253, acc 0.984375
2017-08-08T16:38:04.882937: step 3015, loss 0.00673999, acc 1
2017-08-08T16:38:05.169843: step 3016, loss 0.00256648, acc 1
2017-08-08T16:38:05.559136: step 3017, loss 0.0146692, acc 1
2017-08-08T16:38:05.789438: step 3018, loss 0.0243151, acc 1
2017-08-08T16:38:06.073296: step 3019, loss 0.00252956, acc 1
2017-08-08T16:38:06.310559: step 3020, loss 0.00854368, acc 1
2017-08-08T16:38:06.675460: step 3021, loss 0.00709041, acc 1
2017-08-08T16:38:07.035533: step 3022, loss 0.0246169, acc 0.984375
2017-08-08T16:38:07.289450: step 3023, loss 0.0133198, acc 1
2017-08-08T16:38:07.518383: step 3024, loss 0.0376635, acc 0.984375
2017-08-08T16:38:07.892617: step 3025, loss 0.00355389, acc 1
2017-08-08T16:38:08.127688: step 3026, loss 0.00652907, acc 1
2017-08-08T16:38:08.357304: step 3027, loss 0.00216122, acc 1
2017-08-08T16:38:08.553868: step 3028, loss 0.00340018, acc 1
2017-08-08T16:38:08.800058: step 3029, loss 0.00755909, acc 1
2017-08-08T16:38:09.141344: step 3030, loss 0.0148042, acc 1
2017-08-08T16:38:09.439720: step 3031, loss 0.0141909, acc 1
2017-08-08T16:38:09.624901: step 3032, loss 0.013277, acc 1
2017-08-08T16:38:09.967614: step 3033, loss 0.0106172, acc 1
2017-08-08T16:38:10.328423: step 3034, loss 0.00565876, acc 1
2017-08-08T16:38:10.619143: step 3035, loss 0.0176125, acc 0.984375
2017-08-08T16:38:10.857458: step 3036, loss 0.00726307, acc 1
2017-08-08T16:38:11.153752: step 3037, loss 0.00713421, acc 1
2017-08-08T16:38:11.527051: step 3038, loss 0.00465888, acc 1
2017-08-08T16:38:11.883834: step 3039, loss 0.00206249, acc 1
2017-08-08T16:38:12.169622: step 3040, loss 0.00895104, acc 1
2017-08-08T16:38:12.454443: step 3041, loss 0.00274286, acc 1
2017-08-08T16:38:12.903435: step 3042, loss 0.00489918, acc 1
2017-08-08T16:38:13.174878: step 3043, loss 0.0111768, acc 1
2017-08-08T16:38:13.444844: step 3044, loss 0.0149783, acc 1
2017-08-08T16:38:13.659480: step 3045, loss 0.0208763, acc 0.984375
2017-08-08T16:38:13.949313: step 3046, loss 0.00500899, acc 1
2017-08-08T16:38:14.319572: step 3047, loss 0.00424786, acc 1
2017-08-08T16:38:14.682550: step 3048, loss 0.0105809, acc 1
2017-08-08T16:38:14.923666: step 3049, loss 0.0477097, acc 0.96875
2017-08-08T16:38:15.166996: step 3050, loss 0.0402929, acc 0.984375
2017-08-08T16:38:15.429372: step 3051, loss 0.0242065, acc 0.984375
2017-08-08T16:38:15.743825: step 3052, loss 0.0152431, acc 1
2017-08-08T16:38:15.990055: step 3053, loss 0.0113514, acc 1
2017-08-08T16:38:16.254106: step 3054, loss 0.0189034, acc 1
2017-08-08T16:38:16.643623: step 3055, loss 0.0421907, acc 0.984375
2017-08-08T16:38:16.923321: step 3056, loss 0.012446, acc 1
2017-08-08T16:38:17.182426: step 3057, loss 0.00393115, acc 1
2017-08-08T16:38:17.372572: step 3058, loss 0.0170632, acc 1
2017-08-08T16:38:17.563262: step 3059, loss 0.0120776, acc 1
2017-08-08T16:38:17.931178: step 3060, loss 0.016335, acc 1
2017-08-08T16:38:18.177437: step 3061, loss 0.00494761, acc 1
2017-08-08T16:38:18.447360: step 3062, loss 0.00165909, acc 1
2017-08-08T16:38:18.677456: step 3063, loss 0.00230264, acc 1
2017-08-08T16:38:19.052542: step 3064, loss 0.0114967, acc 1
2017-08-08T16:38:19.390971: step 3065, loss 0.00371171, acc 1
2017-08-08T16:38:19.687907: step 3066, loss 0.0157761, acc 1
2017-08-08T16:38:19.961776: step 3067, loss 0.0145762, acc 1
2017-08-08T16:38:20.412240: step 3068, loss 0.00538102, acc 1
2017-08-08T16:38:20.608551: step 3069, loss 0.00282004, acc 1
2017-08-08T16:38:20.869571: step 3070, loss 0.00807851, acc 1
2017-08-08T16:38:21.127870: step 3071, loss 0.00532272, acc 1
2017-08-08T16:38:21.411896: step 3072, loss 0.0154129, acc 1
2017-08-08T16:38:21.748822: step 3073, loss 0.000915237, acc 1
2017-08-08T16:38:22.080000: step 3074, loss 0.028502, acc 0.984375
2017-08-08T16:38:22.320101: step 3075, loss 0.008318, acc 1
2017-08-08T16:38:22.577786: step 3076, loss 0.0077541, acc 1
2017-08-08T16:38:22.991405: step 3077, loss 0.00972618, acc 1
2017-08-08T16:38:23.294302: step 3078, loss 0.0645186, acc 0.984375
2017-08-08T16:38:23.588465: step 3079, loss 0.0123333, acc 1
2017-08-08T16:38:23.916075: step 3080, loss 0.0108543, acc 1
2017-08-08T16:38:24.309480: step 3081, loss 0.00603227, acc 1
2017-08-08T16:38:24.733261: step 3082, loss 0.00742241, acc 1
2017-08-08T16:38:25.113214: step 3083, loss 0.0060716, acc 1
2017-08-08T16:38:25.411188: step 3084, loss 0.0143173, acc 1
2017-08-08T16:38:25.707432: step 3085, loss 0.0179002, acc 1
2017-08-08T16:38:26.097787: step 3086, loss 0.0231702, acc 0.984375
2017-08-08T16:38:26.408401: step 3087, loss 0.0234272, acc 1
2017-08-08T16:38:26.666390: step 3088, loss 0.0146756, acc 1
2017-08-08T16:38:26.958305: step 3089, loss 0.0103306, acc 1
2017-08-08T16:38:27.368834: step 3090, loss 0.00796289, acc 1
2017-08-08T16:38:27.720534: step 3091, loss 0.0187621, acc 0.984375
2017-08-08T16:38:27.977991: step 3092, loss 0.0136685, acc 1
2017-08-08T16:38:28.154044: step 3093, loss 0.0152779, acc 0.984375
2017-08-08T16:38:28.465987: step 3094, loss 0.0408744, acc 0.96875
2017-08-08T16:38:28.755907: step 3095, loss 0.0210233, acc 1
2017-08-08T16:38:28.992562: step 3096, loss 0.0146478, acc 1
2017-08-08T16:38:29.259062: step 3097, loss 0.0299521, acc 0.984375
2017-08-08T16:38:29.502663: step 3098, loss 0.00742136, acc 1
2017-08-08T16:38:29.941273: step 3099, loss 0.00453165, acc 1
2017-08-08T16:38:30.333163: step 3100, loss 0.00520002, acc 1

Evaluation:
2017-08-08T16:38:31.008992: step 3100, loss 1.08986, acc 0.725141

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-3100

2017-08-08T16:38:31.659808: step 3101, loss 0.0173374, acc 0.984375
2017-08-08T16:38:31.943730: step 3102, loss 0.00937314, acc 1
2017-08-08T16:38:32.236121: step 3103, loss 0.0147829, acc 1
2017-08-08T16:38:32.531810: step 3104, loss 0.030699, acc 0.984375
2017-08-08T16:38:32.921531: step 3105, loss 0.0178269, acc 1
2017-08-08T16:38:33.269948: step 3106, loss 0.0103278, acc 1
2017-08-08T16:38:33.631384: step 3107, loss 0.0192576, acc 1
2017-08-08T16:38:33.870990: step 3108, loss 0.00372451, acc 1
2017-08-08T16:38:34.131505: step 3109, loss 0.0236538, acc 0.984375
2017-08-08T16:38:34.451508: step 3110, loss 0.0220707, acc 0.984375
2017-08-08T16:38:34.661418: step 3111, loss 0.0102144, acc 1
2017-08-08T16:38:34.918928: step 3112, loss 0.00504804, acc 1
2017-08-08T16:38:35.165713: step 3113, loss 0.0157488, acc 1
2017-08-08T16:38:35.534086: step 3114, loss 0.0492055, acc 0.984375
2017-08-08T16:38:35.871313: step 3115, loss 0.00593917, acc 1
2017-08-08T16:38:36.140240: step 3116, loss 0.00997495, acc 1
2017-08-08T16:38:36.314800: step 3117, loss 0.0238082, acc 1
2017-08-08T16:38:36.697003: step 3118, loss 0.0168572, acc 1
2017-08-08T16:38:37.050583: step 3119, loss 0.00759648, acc 1
2017-08-08T16:38:37.332535: step 3120, loss 0.00634697, acc 1
2017-08-08T16:38:37.581837: step 3121, loss 0.00979711, acc 1
2017-08-08T16:38:37.925327: step 3122, loss 0.0232685, acc 0.984375
2017-08-08T16:38:38.208329: step 3123, loss 0.00351629, acc 1
2017-08-08T16:38:38.442093: step 3124, loss 0.00210087, acc 1
2017-08-08T16:38:38.621759: step 3125, loss 0.00522421, acc 1
2017-08-08T16:38:38.908531: step 3126, loss 0.0290407, acc 0.984375
2017-08-08T16:38:39.253756: step 3127, loss 0.0192781, acc 1
2017-08-08T16:38:39.465592: step 3128, loss 0.00744074, acc 1
2017-08-08T16:38:39.712484: step 3129, loss 0.0101615, acc 1
2017-08-08T16:38:40.073145: step 3130, loss 0.00124896, acc 1
2017-08-08T16:38:40.397324: step 3131, loss 0.0224335, acc 0.984375
2017-08-08T16:38:40.639381: step 3132, loss 0.0160689, acc 1
2017-08-08T16:38:40.906127: step 3133, loss 0.0231745, acc 0.984375
2017-08-08T16:38:41.108357: step 3134, loss 0.044856, acc 0.96875
2017-08-08T16:38:41.443431: step 3135, loss 0.029016, acc 0.984375
2017-08-08T16:38:41.696240: step 3136, loss 0.00985975, acc 1
2017-08-08T16:38:41.990734: step 3137, loss 0.0168675, acc 0.984375
2017-08-08T16:38:42.297310: step 3138, loss 0.00604103, acc 1
2017-08-08T16:38:42.649392: step 3139, loss 0.0258161, acc 1
2017-08-08T16:38:43.028432: step 3140, loss 0.0165532, acc 1
2017-08-08T16:38:43.352631: step 3141, loss 0.00968174, acc 1
2017-08-08T16:38:43.597088: step 3142, loss 0.00368238, acc 1
2017-08-08T16:38:43.805302: step 3143, loss 0.0157262, acc 1
2017-08-08T16:38:44.153319: step 3144, loss 0.0127157, acc 1
2017-08-08T16:38:44.360631: step 3145, loss 0.0184236, acc 0.984375
2017-08-08T16:38:44.610389: step 3146, loss 0.0209037, acc 1
2017-08-08T16:38:44.953195: step 3147, loss 0.0219546, acc 0.984375
2017-08-08T16:38:45.451470: step 3148, loss 0.0140618, acc 1
2017-08-08T16:38:45.840983: step 3149, loss 0.009029, acc 1
2017-08-08T16:38:46.091060: step 3150, loss 0.00323879, acc 1
2017-08-08T16:38:46.280439: step 3151, loss 0.0127925, acc 1
2017-08-08T16:38:46.669405: step 3152, loss 0.0198545, acc 1
2017-08-08T16:38:46.983518: step 3153, loss 0.00508965, acc 1
2017-08-08T16:38:47.267041: step 3154, loss 0.0156619, acc 0.984375
2017-08-08T16:38:47.653306: step 3155, loss 0.00795435, acc 1
2017-08-08T16:38:48.045473: step 3156, loss 0.00419909, acc 1
2017-08-08T16:38:48.448786: step 3157, loss 0.00258767, acc 1
2017-08-08T16:38:48.783551: step 3158, loss 0.00405649, acc 1
2017-08-08T16:38:49.030827: step 3159, loss 0.00552845, acc 1
2017-08-08T16:38:49.385384: step 3160, loss 0.0125363, acc 1
2017-08-08T16:38:49.806437: step 3161, loss 0.0119037, acc 1
2017-08-08T16:38:50.105460: step 3162, loss 0.00146248, acc 1
2017-08-08T16:38:50.344953: step 3163, loss 0.00399132, acc 1
2017-08-08T16:38:50.769334: step 3164, loss 0.0231562, acc 0.984375
2017-08-08T16:38:51.130043: step 3165, loss 0.0170173, acc 0.984375
2017-08-08T16:38:51.379851: step 3166, loss 0.00620028, acc 1
2017-08-08T16:38:51.588634: step 3167, loss 0.00529243, acc 1
2017-08-08T16:38:51.949818: step 3168, loss 0.00716886, acc 1
2017-08-08T16:38:52.168926: step 3169, loss 0.00677606, acc 1
2017-08-08T16:38:52.379983: step 3170, loss 0.0220098, acc 0.984375
2017-08-08T16:38:52.594290: step 3171, loss 0.00728248, acc 1
2017-08-08T16:38:52.900911: step 3172, loss 0.00612844, acc 1
2017-08-08T16:38:53.265527: step 3173, loss 0.010444, acc 1
2017-08-08T16:38:53.535508: step 3174, loss 0.0163406, acc 0.984375
2017-08-08T16:38:53.773169: step 3175, loss 0.00712521, acc 1
2017-08-08T16:38:54.139721: step 3176, loss 0.00336684, acc 1
2017-08-08T16:38:54.493830: step 3177, loss 0.00964309, acc 1
2017-08-08T16:38:54.729700: step 3178, loss 0.0052104, acc 1
2017-08-08T16:38:55.056460: step 3179, loss 0.0360431, acc 0.984375
2017-08-08T16:38:55.331058: step 3180, loss 0.0153258, acc 1
2017-08-08T16:38:55.742842: step 3181, loss 0.00883667, acc 1
2017-08-08T16:38:56.005674: step 3182, loss 0.0077551, acc 1
2017-08-08T16:38:56.437378: step 3183, loss 0.0141941, acc 1
2017-08-08T16:38:56.739331: step 3184, loss 0.00465403, acc 1
2017-08-08T16:38:57.009366: step 3185, loss 0.00159291, acc 1
2017-08-08T16:38:57.335957: step 3186, loss 0.0242419, acc 1
2017-08-08T16:38:57.528377: step 3187, loss 0.0398417, acc 0.984375
2017-08-08T16:38:57.840387: step 3188, loss 0.0223032, acc 0.984375
2017-08-08T16:38:58.133536: step 3189, loss 0.00992038, acc 1
2017-08-08T16:38:58.454875: step 3190, loss 0.00691267, acc 1
2017-08-08T16:38:58.773780: step 3191, loss 0.0163607, acc 1
2017-08-08T16:38:59.068475: step 3192, loss 0.00603202, acc 1
2017-08-08T16:38:59.507885: step 3193, loss 0.00297327, acc 1
2017-08-08T16:38:59.790197: step 3194, loss 0.0134873, acc 1
2017-08-08T16:39:00.097804: step 3195, loss 0.00456001, acc 1
2017-08-08T16:39:00.422789: step 3196, loss 0.0109383, acc 1
2017-08-08T16:39:00.701940: step 3197, loss 0.00319016, acc 1
2017-08-08T16:39:01.048679: step 3198, loss 0.00575873, acc 1
2017-08-08T16:39:01.533843: step 3199, loss 0.00609749, acc 1
2017-08-08T16:39:02.037813: step 3200, loss 0.0115127, acc 1

Evaluation:
2017-08-08T16:39:02.816016: step 3200, loss 1.15854, acc 0.719512

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-3200

2017-08-08T16:39:03.436985: step 3201, loss 0.0326746, acc 0.984375
2017-08-08T16:39:03.760311: step 3202, loss 0.0111381, acc 1
2017-08-08T16:39:04.104879: step 3203, loss 0.0064608, acc 1
2017-08-08T16:39:04.429981: step 3204, loss 0.0145304, acc 1
2017-08-08T16:39:04.844040: step 3205, loss 0.025246, acc 0.984375
2017-08-08T16:39:05.157749: step 3206, loss 0.0106143, acc 1
2017-08-08T16:39:05.548427: step 3207, loss 0.0475942, acc 0.96875
2017-08-08T16:39:05.924728: step 3208, loss 0.0203213, acc 1
2017-08-08T16:39:06.201368: step 3209, loss 0.0549515, acc 0.984375
2017-08-08T16:39:06.485421: step 3210, loss 0.0227775, acc 0.984375
2017-08-08T16:39:06.888411: step 3211, loss 0.00640569, acc 1
2017-08-08T16:39:07.125549: step 3212, loss 0.00191425, acc 1
2017-08-08T16:39:07.388755: step 3213, loss 0.0255462, acc 0.984375
2017-08-08T16:39:07.680982: step 3214, loss 0.00410948, acc 1
2017-08-08T16:39:07.966532: step 3215, loss 0.0141259, acc 1
2017-08-08T16:39:08.310827: step 3216, loss 0.00504011, acc 1
2017-08-08T16:39:08.648513: step 3217, loss 0.0313187, acc 0.984375
2017-08-08T16:39:08.912518: step 3218, loss 0.047915, acc 0.96875
2017-08-08T16:39:09.246515: step 3219, loss 0.0162279, acc 1
2017-08-08T16:39:09.554101: step 3220, loss 0.00312429, acc 1
2017-08-08T16:39:09.774622: step 3221, loss 0.0250711, acc 0.984375
2017-08-08T16:39:10.012690: step 3222, loss 0.00297117, acc 1
2017-08-08T16:39:10.229384: step 3223, loss 0.023089, acc 0.984375
2017-08-08T16:39:10.594705: step 3224, loss 0.0116444, acc 1
2017-08-08T16:39:10.978812: step 3225, loss 0.0349638, acc 0.984375
2017-08-08T16:39:11.319863: step 3226, loss 0.00726854, acc 1
2017-08-08T16:39:11.570559: step 3227, loss 0.00779286, acc 1
2017-08-08T16:39:11.812868: step 3228, loss 0.0485742, acc 0.984375
2017-08-08T16:39:12.175557: step 3229, loss 0.00721417, acc 1
2017-08-08T16:39:12.548401: step 3230, loss 0.0287677, acc 1
2017-08-08T16:39:12.824504: step 3231, loss 0.00465229, acc 1
2017-08-08T16:39:13.078923: step 3232, loss 0.00138158, acc 1
2017-08-08T16:39:13.339204: step 3233, loss 0.00258242, acc 1
2017-08-08T16:39:13.773348: step 3234, loss 0.015752, acc 1
2017-08-08T16:39:14.069825: step 3235, loss 0.0155626, acc 0.984375
2017-08-08T16:39:14.279480: step 3236, loss 0.0124561, acc 1
2017-08-08T16:39:14.515417: step 3237, loss 0.00732919, acc 1
2017-08-08T16:39:14.940493: step 3238, loss 0.00446266, acc 1
2017-08-08T16:39:15.219486: step 3239, loss 0.00818941, acc 1
2017-08-08T16:39:15.528051: step 3240, loss 0.00771482, acc 1
2017-08-08T16:39:15.830107: step 3241, loss 0.046754, acc 0.953125
2017-08-08T16:39:16.217156: step 3242, loss 0.0128108, acc 1
2017-08-08T16:39:16.511500: step 3243, loss 0.00392101, acc 1
2017-08-08T16:39:16.858176: step 3244, loss 0.015975, acc 1
2017-08-08T16:39:17.121275: step 3245, loss 0.015602, acc 1
2017-08-08T16:39:17.385909: step 3246, loss 0.0356029, acc 0.984375
2017-08-08T16:39:17.729750: step 3247, loss 0.00703518, acc 1
2017-08-08T16:39:17.991874: step 3248, loss 0.00676838, acc 1
2017-08-08T16:39:18.368395: step 3249, loss 0.00633387, acc 1
2017-08-08T16:39:18.625959: step 3250, loss 0.00696174, acc 1
2017-08-08T16:39:18.800276: step 3251, loss 0.00800054, acc 1
2017-08-08T16:39:19.114790: step 3252, loss 0.0109279, acc 1
2017-08-08T16:39:19.290145: step 3253, loss 0.017642, acc 0.984375
2017-08-08T16:39:19.508046: step 3254, loss 0.00380356, acc 1
2017-08-08T16:39:19.693756: step 3255, loss 0.00357722, acc 1
2017-08-08T16:39:19.949335: step 3256, loss 0.00175881, acc 1
2017-08-08T16:39:20.385317: step 3257, loss 0.00112131, acc 1
2017-08-08T16:39:20.741862: step 3258, loss 0.00366151, acc 1
2017-08-08T16:39:20.935303: step 3259, loss 0.00263845, acc 1
2017-08-08T16:39:21.271202: step 3260, loss 0.00828126, acc 1
2017-08-08T16:39:21.511463: step 3261, loss 0.00888841, acc 1
2017-08-08T16:39:21.710694: step 3262, loss 0.00601382, acc 1
2017-08-08T16:39:21.931648: step 3263, loss 0.00857843, acc 1
2017-08-08T16:39:22.261743: step 3264, loss 0.00923396, acc 1
2017-08-08T16:39:22.554010: step 3265, loss 0.00709386, acc 1
2017-08-08T16:39:22.996382: step 3266, loss 0.00167645, acc 1
2017-08-08T16:39:23.196784: step 3267, loss 0.0206629, acc 0.984375
2017-08-08T16:39:23.422206: step 3268, loss 0.00581193, acc 1
2017-08-08T16:39:23.785716: step 3269, loss 0.00285076, acc 1
2017-08-08T16:39:24.068288: step 3270, loss 0.0123976, acc 1
2017-08-08T16:39:24.358034: step 3271, loss 0.00419413, acc 1
2017-08-08T16:39:24.649355: step 3272, loss 0.00561311, acc 1
2017-08-08T16:39:24.917510: step 3273, loss 0.001807, acc 1
2017-08-08T16:39:25.274685: step 3274, loss 0.00810244, acc 1
2017-08-08T16:39:25.533027: step 3275, loss 0.0417475, acc 0.984375
2017-08-08T16:39:25.790439: step 3276, loss 0.00185401, acc 1
2017-08-08T16:39:26.158059: step 3277, loss 0.0154438, acc 0.984375
2017-08-08T16:39:26.531889: step 3278, loss 0.00774173, acc 1
2017-08-08T16:39:26.823050: step 3279, loss 0.0126814, acc 1
2017-08-08T16:39:27.120734: step 3280, loss 0.0180807, acc 0.984375
2017-08-08T16:39:27.468352: step 3281, loss 0.00904576, acc 1
2017-08-08T16:39:27.848304: step 3282, loss 0.00441497, acc 1
2017-08-08T16:39:28.223555: step 3283, loss 0.0192987, acc 0.984375
2017-08-08T16:39:28.587683: step 3284, loss 0.0033439, acc 1
2017-08-08T16:39:28.849712: step 3285, loss 0.0264652, acc 1
2017-08-08T16:39:29.278162: step 3286, loss 0.00241707, acc 1
2017-08-08T16:39:29.634722: step 3287, loss 0.00944982, acc 1
2017-08-08T16:39:29.922711: step 3288, loss 0.00675063, acc 1
2017-08-08T16:39:30.228081: step 3289, loss 0.00950585, acc 1
2017-08-08T16:39:30.493776: step 3290, loss 0.0123301, acc 1
2017-08-08T16:39:30.876350: step 3291, loss 0.00716293, acc 1
2017-08-08T16:39:31.305352: step 3292, loss 0.00422459, acc 1
2017-08-08T16:39:31.695508: step 3293, loss 0.0037399, acc 1
2017-08-08T16:39:31.959763: step 3294, loss 0.0105065, acc 1
2017-08-08T16:39:32.269703: step 3295, loss 0.0382319, acc 0.96875
2017-08-08T16:39:32.566625: step 3296, loss 0.00410972, acc 1
2017-08-08T16:39:32.821317: step 3297, loss 0.0155451, acc 1
2017-08-08T16:39:33.078213: step 3298, loss 0.00246945, acc 1
2017-08-08T16:39:33.269102: step 3299, loss 0.0213742, acc 0.984375
2017-08-08T16:39:33.559641: step 3300, loss 0.0173759, acc 1

Evaluation:
2017-08-08T16:39:34.169092: step 3300, loss 1.18445, acc 0.716698

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-3300

2017-08-08T16:39:34.617260: step 3301, loss 0.00945821, acc 1
2017-08-08T16:39:35.011870: step 3302, loss 0.031905, acc 0.984375
2017-08-08T16:39:35.296452: step 3303, loss 0.00227809, acc 1
2017-08-08T16:39:35.520290: step 3304, loss 0.0110705, acc 1
2017-08-08T16:39:35.817458: step 3305, loss 0.0132205, acc 1
2017-08-08T16:39:36.165314: step 3306, loss 0.0109837, acc 1
2017-08-08T16:39:36.439560: step 3307, loss 0.0122527, acc 1
2017-08-08T16:39:36.768824: step 3308, loss 0.00425812, acc 1
2017-08-08T16:39:36.983711: step 3309, loss 0.00433738, acc 1
2017-08-08T16:39:37.263437: step 3310, loss 0.00118482, acc 1
2017-08-08T16:39:37.607431: step 3311, loss 0.00384713, acc 1
2017-08-08T16:39:37.829437: step 3312, loss 0.0387267, acc 0.984375
2017-08-08T16:39:38.061160: step 3313, loss 0.0100771, acc 1
2017-08-08T16:39:38.320915: step 3314, loss 0.00475525, acc 1
2017-08-08T16:39:38.729349: step 3315, loss 0.000931679, acc 1
2017-08-08T16:39:39.054799: step 3316, loss 0.0137427, acc 1
2017-08-08T16:39:39.346759: step 3317, loss 0.00579439, acc 1
2017-08-08T16:39:39.658959: step 3318, loss 0.0452697, acc 0.984375
2017-08-08T16:39:39.999410: step 3319, loss 0.0435707, acc 0.984375
2017-08-08T16:39:40.277584: step 3320, loss 0.0372305, acc 0.984375
2017-08-08T16:39:40.581224: step 3321, loss 0.00909541, acc 1
2017-08-08T16:39:40.866900: step 3322, loss 0.000763192, acc 1
2017-08-08T16:39:41.276735: step 3323, loss 0.000931285, acc 1
2017-08-08T16:39:41.695460: step 3324, loss 0.00393105, acc 1
2017-08-08T16:39:42.045189: step 3325, loss 0.00220144, acc 1
2017-08-08T16:39:42.340862: step 3326, loss 0.00589785, acc 1
2017-08-08T16:39:42.592895: step 3327, loss 0.00824638, acc 1
2017-08-08T16:39:43.030954: step 3328, loss 0.00531134, acc 1
2017-08-08T16:39:43.355930: step 3329, loss 0.00345005, acc 1
2017-08-08T16:39:43.643726: step 3330, loss 0.00923002, acc 1
2017-08-08T16:39:43.872681: step 3331, loss 0.0149167, acc 0.984375
2017-08-08T16:39:44.284252: step 3332, loss 0.0107578, acc 1
2017-08-08T16:39:44.697380: step 3333, loss 0.00103807, acc 1
2017-08-08T16:39:45.081701: step 3334, loss 0.0433582, acc 0.984375
2017-08-08T16:39:45.376281: step 3335, loss 0.00663263, acc 1
2017-08-08T16:39:45.624219: step 3336, loss 0.00893098, acc 1
2017-08-08T16:39:46.073878: step 3337, loss 0.000926579, acc 1
2017-08-08T16:39:46.377789: step 3338, loss 0.00834262, acc 1
2017-08-08T16:39:46.629480: step 3339, loss 0.0228096, acc 0.984375
2017-08-08T16:39:46.854296: step 3340, loss 0.0616849, acc 0.984375
2017-08-08T16:39:47.298967: step 3341, loss 0.00516335, acc 1
2017-08-08T16:39:47.708701: step 3342, loss 0.014644, acc 1
2017-08-08T16:39:47.961322: step 3343, loss 0.0129444, acc 1
2017-08-08T16:39:48.155691: step 3344, loss 0.0113266, acc 1
2017-08-08T16:39:48.353402: step 3345, loss 0.00999259, acc 1
2017-08-08T16:39:48.721288: step 3346, loss 0.00830562, acc 1
2017-08-08T16:39:48.941752: step 3347, loss 0.0125735, acc 1
2017-08-08T16:39:49.232331: step 3348, loss 0.00520532, acc 1
2017-08-08T16:39:49.668370: step 3349, loss 0.0135344, acc 1
2017-08-08T16:39:50.051945: step 3350, loss 0.00154887, acc 1
2017-08-08T16:39:50.439709: step 3351, loss 0.00613816, acc 1
2017-08-08T16:39:50.687122: step 3352, loss 0.00709663, acc 1
2017-08-08T16:39:51.034623: step 3353, loss 0.000999879, acc 1
2017-08-08T16:39:51.356113: step 3354, loss 0.0294384, acc 0.984375
2017-08-08T16:39:51.666327: step 3355, loss 0.0209671, acc 1
2017-08-08T16:39:52.005395: step 3356, loss 0.0100586, acc 1
2017-08-08T16:39:52.352240: step 3357, loss 0.0180946, acc 0.984375
2017-08-08T16:39:52.806766: step 3358, loss 0.00758231, acc 1
2017-08-08T16:39:53.212240: step 3359, loss 0.00947432, acc 1
2017-08-08T16:39:53.511432: step 3360, loss 0.00668969, acc 1
2017-08-08T16:39:53.885879: step 3361, loss 0.00481225, acc 1
2017-08-08T16:39:54.210269: step 3362, loss 0.00492776, acc 1
2017-08-08T16:39:54.507934: step 3363, loss 0.00575322, acc 1
2017-08-08T16:39:54.803678: step 3364, loss 0.0100442, acc 1
2017-08-08T16:39:55.273389: step 3365, loss 0.00686527, acc 1
2017-08-08T16:39:55.616374: step 3366, loss 0.0351819, acc 0.984375
2017-08-08T16:39:55.906161: step 3367, loss 0.00401067, acc 1
2017-08-08T16:39:56.121908: step 3368, loss 0.0721956, acc 0.984375
2017-08-08T16:39:56.532892: step 3369, loss 0.00584748, acc 1
2017-08-08T16:39:56.764830: step 3370, loss 0.00447523, acc 1
2017-08-08T16:39:57.022441: step 3371, loss 0.00284137, acc 1
2017-08-08T16:39:57.327749: step 3372, loss 0.00356566, acc 1
2017-08-08T16:39:57.593731: step 3373, loss 0.00801787, acc 1
2017-08-08T16:39:58.036127: step 3374, loss 0.00832453, acc 1
2017-08-08T16:39:58.388055: step 3375, loss 0.104353, acc 0.96875
2017-08-08T16:39:58.685168: step 3376, loss 0.0135245, acc 1
2017-08-08T16:39:58.939669: step 3377, loss 0.0118745, acc 1
2017-08-08T16:39:59.399969: step 3378, loss 0.0065604, acc 1
2017-08-08T16:39:59.690392: step 3379, loss 0.00224132, acc 1
2017-08-08T16:39:59.953061: step 3380, loss 0.0352427, acc 0.984375
2017-08-08T16:40:00.232906: step 3381, loss 0.015446, acc 0.984375
2017-08-08T16:40:00.628955: step 3382, loss 0.0121577, acc 1
2017-08-08T16:40:01.024771: step 3383, loss 0.00917223, acc 1
2017-08-08T16:40:01.426571: step 3384, loss 0.0126755, acc 1
2017-08-08T16:40:01.860626: step 3385, loss 0.00431906, acc 1
2017-08-08T16:40:02.293009: step 3386, loss 0.0154402, acc 1
2017-08-08T16:40:02.753231: step 3387, loss 0.00466033, acc 1
2017-08-08T16:40:03.094247: step 3388, loss 0.0099227, acc 1
2017-08-08T16:40:03.346266: step 3389, loss 0.000863706, acc 1
2017-08-08T16:40:03.653653: step 3390, loss 0.00174041, acc 1
2017-08-08T16:40:04.093385: step 3391, loss 0.00558653, acc 1
2017-08-08T16:40:04.508151: step 3392, loss 0.0065543, acc 1
2017-08-08T16:40:04.847988: step 3393, loss 0.00284382, acc 1
2017-08-08T16:40:05.135311: step 3394, loss 0.0119355, acc 1
2017-08-08T16:40:05.363091: step 3395, loss 0.00554706, acc 1
2017-08-08T16:40:05.751516: step 3396, loss 0.00710845, acc 1
2017-08-08T16:40:06.064420: step 3397, loss 0.00837174, acc 1
2017-08-08T16:40:06.374764: step 3398, loss 0.00676921, acc 1
2017-08-08T16:40:06.663787: step 3399, loss 0.00836927, acc 1
2017-08-08T16:40:07.133370: step 3400, loss 0.00348986, acc 1

Evaluation:
2017-08-08T16:40:08.063904: step 3400, loss 1.19037, acc 0.724203

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-3400

2017-08-08T16:40:08.514768: step 3401, loss 0.00334654, acc 1
2017-08-08T16:40:08.896535: step 3402, loss 0.104469, acc 0.96875
2017-08-08T16:40:09.183264: step 3403, loss 0.0178785, acc 1
2017-08-08T16:40:09.445376: step 3404, loss 0.0240602, acc 0.984375
2017-08-08T16:40:09.667884: step 3405, loss 0.0109968, acc 1
2017-08-08T16:40:09.954151: step 3406, loss 0.00464803, acc 1
2017-08-08T16:40:10.307211: step 3407, loss 0.00744358, acc 1
2017-08-08T16:40:10.768605: step 3408, loss 0.0097018, acc 1
2017-08-08T16:40:11.053285: step 3409, loss 0.0206071, acc 0.984375
2017-08-08T16:40:11.301642: step 3410, loss 0.00223929, acc 1
2017-08-08T16:40:11.645742: step 3411, loss 0.0022054, acc 1
2017-08-08T16:40:12.013236: step 3412, loss 0.0128462, acc 1
2017-08-08T16:40:12.285540: step 3413, loss 0.00233595, acc 1
2017-08-08T16:40:12.563661: step 3414, loss 0.02569, acc 0.984375
2017-08-08T16:40:12.889377: step 3415, loss 0.00640978, acc 1
2017-08-08T16:40:13.295776: step 3416, loss 0.0299304, acc 0.984375
2017-08-08T16:40:13.589584: step 3417, loss 0.00754517, acc 1
2017-08-08T16:40:13.811176: step 3418, loss 0.0236149, acc 0.984375
2017-08-08T16:40:14.024452: step 3419, loss 0.00176533, acc 1
2017-08-08T16:40:14.357147: step 3420, loss 0.0228632, acc 0.984375
2017-08-08T16:40:14.590674: step 3421, loss 0.0116429, acc 1
2017-08-08T16:40:14.884838: step 3422, loss 0.0372409, acc 0.96875
2017-08-08T16:40:15.274163: step 3423, loss 0.0575824, acc 0.984375
2017-08-08T16:40:15.623411: step 3424, loss 0.00756797, acc 1
2017-08-08T16:40:16.069612: step 3425, loss 0.0088802, acc 1
2017-08-08T16:40:16.393669: step 3426, loss 0.00351809, acc 1
2017-08-08T16:40:16.646592: step 3427, loss 0.0257341, acc 0.984375
2017-08-08T16:40:16.973401: step 3428, loss 0.0267339, acc 0.984375
2017-08-08T16:40:17.319544: step 3429, loss 0.0042576, acc 1
2017-08-08T16:40:17.632144: step 3430, loss 0.00217887, acc 1
2017-08-08T16:40:17.933602: step 3431, loss 0.0033395, acc 1
2017-08-08T16:40:18.264245: step 3432, loss 0.00522431, acc 1
2017-08-08T16:40:18.550467: step 3433, loss 0.00219888, acc 1
2017-08-08T16:40:18.867257: step 3434, loss 0.00726763, acc 1
2017-08-08T16:40:19.192397: step 3435, loss 0.00908396, acc 1
2017-08-08T16:40:19.424252: step 3436, loss 0.0010988, acc 1
2017-08-08T16:40:19.685893: step 3437, loss 0.00449148, acc 1
2017-08-08T16:40:20.146307: step 3438, loss 0.039793, acc 0.96875
2017-08-08T16:40:20.370395: step 3439, loss 0.00656074, acc 1
2017-08-08T16:40:20.556358: step 3440, loss 0.0249577, acc 1
2017-08-08T16:40:20.773517: step 3441, loss 0.00477548, acc 1
2017-08-08T16:40:21.069364: step 3442, loss 0.0291068, acc 0.984375
2017-08-08T16:40:21.412369: step 3443, loss 0.0778679, acc 0.984375
2017-08-08T16:40:21.767435: step 3444, loss 0.00381123, acc 1
2017-08-08T16:40:22.042271: step 3445, loss 0.0471862, acc 0.984375
2017-08-08T16:40:22.351080: step 3446, loss 0.0121066, acc 1
2017-08-08T16:40:22.612960: step 3447, loss 0.00748655, acc 1
2017-08-08T16:40:22.895431: step 3448, loss 0.00753123, acc 1
2017-08-08T16:40:23.341395: step 3449, loss 0.0142815, acc 0.984375
2017-08-08T16:40:23.704730: step 3450, loss 0.049964, acc 0.966667
2017-08-08T16:40:24.067852: step 3451, loss 0.0221587, acc 0.984375
2017-08-08T16:40:24.313203: step 3452, loss 0.0108651, acc 1
2017-08-08T16:40:24.611516: step 3453, loss 0.00265492, acc 1
2017-08-08T16:40:24.921864: step 3454, loss 0.00700757, acc 1
2017-08-08T16:40:25.165525: step 3455, loss 0.00533756, acc 1
2017-08-08T16:40:25.404937: step 3456, loss 0.0154522, acc 1
2017-08-08T16:40:25.679049: step 3457, loss 0.00506548, acc 1
2017-08-08T16:40:26.061347: step 3458, loss 0.00274067, acc 1
2017-08-08T16:40:26.401410: step 3459, loss 0.000945697, acc 1
2017-08-08T16:40:26.695181: step 3460, loss 0.00506996, acc 1
2017-08-08T16:40:26.952802: step 3461, loss 0.0322436, acc 0.984375
2017-08-08T16:40:27.209593: step 3462, loss 0.00662925, acc 1
2017-08-08T16:40:27.625845: step 3463, loss 0.00363201, acc 1
2017-08-08T16:40:27.904513: step 3464, loss 0.00364173, acc 1
2017-08-08T16:40:28.271122: step 3465, loss 0.00963034, acc 1
2017-08-08T16:40:28.679588: step 3466, loss 0.00450261, acc 1
2017-08-08T16:40:29.038049: step 3467, loss 0.0062258, acc 1
2017-08-08T16:40:29.296134: step 3468, loss 0.00638042, acc 1
2017-08-08T16:40:29.522358: step 3469, loss 0.00217112, acc 1
2017-08-08T16:40:29.820267: step 3470, loss 0.00339912, acc 1
2017-08-08T16:40:30.159258: step 3471, loss 0.000824887, acc 1
2017-08-08T16:40:30.389996: step 3472, loss 0.0314052, acc 0.984375
2017-08-08T16:40:30.632982: step 3473, loss 0.0116626, acc 1
2017-08-08T16:40:30.925657: step 3474, loss 0.0042979, acc 1
2017-08-08T16:40:31.197379: step 3475, loss 0.000945281, acc 1
2017-08-08T16:40:31.485357: step 3476, loss 0.00357477, acc 1
2017-08-08T16:40:31.748694: step 3477, loss 0.0110276, acc 1
2017-08-08T16:40:32.070783: step 3478, loss 0.0479372, acc 0.984375
2017-08-08T16:40:32.281489: step 3479, loss 0.00511928, acc 1
2017-08-08T16:40:32.509043: step 3480, loss 0.00190946, acc 1
2017-08-08T16:40:32.709893: step 3481, loss 0.00240173, acc 1
2017-08-08T16:40:32.919061: step 3482, loss 0.00741444, acc 1
2017-08-08T16:40:33.226653: step 3483, loss 0.00170218, acc 1
2017-08-08T16:40:33.537351: step 3484, loss 0.00270419, acc 1
2017-08-08T16:40:33.815670: step 3485, loss 0.00275348, acc 1
2017-08-08T16:40:34.015898: step 3486, loss 0.014066, acc 1
2017-08-08T16:40:34.385275: step 3487, loss 0.00103857, acc 1
2017-08-08T16:40:34.720241: step 3488, loss 0.0174747, acc 0.984375
2017-08-08T16:40:34.954873: step 3489, loss 0.0138382, acc 1
2017-08-08T16:40:35.246834: step 3490, loss 0.0507091, acc 0.96875
2017-08-08T16:40:35.534231: step 3491, loss 0.00367285, acc 1
2017-08-08T16:40:35.996013: step 3492, loss 0.00701225, acc 1
2017-08-08T16:40:36.313808: step 3493, loss 0.00287132, acc 1
2017-08-08T16:40:36.627227: step 3494, loss 0.0016329, acc 1
2017-08-08T16:40:36.860373: step 3495, loss 0.00565508, acc 1
2017-08-08T16:40:37.076950: step 3496, loss 0.00259957, acc 1
2017-08-08T16:40:37.353678: step 3497, loss 0.00378347, acc 1
2017-08-08T16:40:37.651205: step 3498, loss 0.00186565, acc 1
2017-08-08T16:40:37.928695: step 3499, loss 0.0347127, acc 0.984375
2017-08-08T16:40:38.312465: step 3500, loss 0.000874204, acc 1

Evaluation:
2017-08-08T16:40:39.103025: step 3500, loss 1.25864, acc 0.717636

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-3500

2017-08-08T16:40:39.677020: step 3501, loss 0.0212399, acc 0.984375
2017-08-08T16:40:40.093579: step 3502, loss 0.00272125, acc 1
2017-08-08T16:40:40.386158: step 3503, loss 0.00363332, acc 1
2017-08-08T16:40:40.680669: step 3504, loss 0.00663563, acc 1
2017-08-08T16:40:41.077673: step 3505, loss 0.0141841, acc 1
2017-08-08T16:40:41.567650: step 3506, loss 0.00154056, acc 1
2017-08-08T16:40:41.912240: step 3507, loss 0.00506167, acc 1
2017-08-08T16:40:42.228684: step 3508, loss 0.0051606, acc 1
2017-08-08T16:40:42.478908: step 3509, loss 0.00312886, acc 1
2017-08-08T16:40:42.938048: step 3510, loss 0.0140511, acc 1
2017-08-08T16:40:43.217912: step 3511, loss 0.0682204, acc 0.984375
2017-08-08T16:40:43.519387: step 3512, loss 0.00128036, acc 1
2017-08-08T16:40:43.739258: step 3513, loss 0.00729485, acc 1
2017-08-08T16:40:44.081027: step 3514, loss 0.00929117, acc 1
2017-08-08T16:40:44.513966: step 3515, loss 0.00077497, acc 1
2017-08-08T16:40:44.904904: step 3516, loss 0.0289239, acc 0.984375
2017-08-08T16:40:45.166181: step 3517, loss 0.00341692, acc 1
2017-08-08T16:40:45.445733: step 3518, loss 0.000750879, acc 1
2017-08-08T16:40:45.644636: step 3519, loss 0.00648865, acc 1
2017-08-08T16:40:45.890991: step 3520, loss 0.00850368, acc 1
2017-08-08T16:40:46.156281: step 3521, loss 0.0313535, acc 0.984375
2017-08-08T16:40:46.372157: step 3522, loss 0.0108068, acc 1
2017-08-08T16:40:46.717428: step 3523, loss 0.00308732, acc 1
2017-08-08T16:40:47.021425: step 3524, loss 0.0105757, acc 1
2017-08-08T16:40:47.373544: step 3525, loss 0.0034544, acc 1
2017-08-08T16:40:47.631219: step 3526, loss 0.00236758, acc 1
2017-08-08T16:40:48.012889: step 3527, loss 0.00227564, acc 1
2017-08-08T16:40:48.262614: step 3528, loss 0.00894472, acc 1
2017-08-08T16:40:48.459164: step 3529, loss 0.0174631, acc 0.984375
2017-08-08T16:40:48.696079: step 3530, loss 0.00359218, acc 1
2017-08-08T16:40:49.127838: step 3531, loss 0.00725997, acc 1
2017-08-08T16:40:49.483194: step 3532, loss 0.0107634, acc 1
2017-08-08T16:40:49.775325: step 3533, loss 0.00439736, acc 1
2017-08-08T16:40:49.977671: step 3534, loss 0.013908, acc 1
2017-08-08T16:40:50.269325: step 3535, loss 0.00476381, acc 1
2017-08-08T16:40:50.538222: step 3536, loss 0.00595199, acc 1
2017-08-08T16:40:50.797893: step 3537, loss 0.00389848, acc 1
2017-08-08T16:40:51.020765: step 3538, loss 0.00453989, acc 1
2017-08-08T16:40:51.424726: step 3539, loss 0.00185799, acc 1
2017-08-08T16:40:51.832150: step 3540, loss 0.0105643, acc 1
2017-08-08T16:40:52.199344: step 3541, loss 0.0124094, acc 1
2017-08-08T16:40:52.428910: step 3542, loss 0.0236839, acc 0.984375
2017-08-08T16:40:52.782995: step 3543, loss 0.0801002, acc 0.984375
2017-08-08T16:40:53.098574: step 3544, loss 0.00385085, acc 1
2017-08-08T16:40:53.367761: step 3545, loss 0.00437519, acc 1
2017-08-08T16:40:53.581347: step 3546, loss 0.00928444, acc 1
2017-08-08T16:40:53.967040: step 3547, loss 0.00264954, acc 1
2017-08-08T16:40:54.310819: step 3548, loss 0.0146141, acc 1
2017-08-08T16:40:54.582676: step 3549, loss 0.0237603, acc 0.984375
2017-08-08T16:40:54.758410: step 3550, loss 0.00250082, acc 1
2017-08-08T16:40:55.126007: step 3551, loss 0.00779216, acc 1
2017-08-08T16:40:55.414740: step 3552, loss 0.00507042, acc 1
2017-08-08T16:40:55.695965: step 3553, loss 0.00487647, acc 1
2017-08-08T16:40:55.958780: step 3554, loss 0.00464261, acc 1
2017-08-08T16:40:56.231179: step 3555, loss 0.0323639, acc 0.984375
2017-08-08T16:40:56.674913: step 3556, loss 0.0205152, acc 0.984375
2017-08-08T16:40:56.999255: step 3557, loss 0.0171728, acc 1
2017-08-08T16:40:57.216311: step 3558, loss 0.00941331, acc 1
2017-08-08T16:40:57.437343: step 3559, loss 0.00375335, acc 1
2017-08-08T16:40:57.718712: step 3560, loss 0.00302581, acc 1
2017-08-08T16:40:58.031145: step 3561, loss 0.043658, acc 0.96875
2017-08-08T16:40:58.323519: step 3562, loss 0.0511836, acc 0.96875
2017-08-08T16:40:58.622862: step 3563, loss 0.00536025, acc 1
2017-08-08T16:40:59.038853: step 3564, loss 0.00786525, acc 1
2017-08-08T16:40:59.342898: step 3565, loss 0.00266153, acc 1
2017-08-08T16:40:59.606722: step 3566, loss 0.00875286, acc 1
2017-08-08T16:40:59.845147: step 3567, loss 0.0253602, acc 0.984375
2017-08-08T16:41:00.143597: step 3568, loss 0.00685112, acc 1
2017-08-08T16:41:00.459581: step 3569, loss 0.00801955, acc 1
2017-08-08T16:41:00.757926: step 3570, loss 0.0717478, acc 0.984375
2017-08-08T16:41:01.021386: step 3571, loss 0.0101836, acc 1
2017-08-08T16:41:01.405607: step 3572, loss 0.0014138, acc 1
2017-08-08T16:41:01.861369: step 3573, loss 0.00422856, acc 1
2017-08-08T16:41:02.202568: step 3574, loss 0.0066211, acc 1
2017-08-08T16:41:02.480708: step 3575, loss 0.00621229, acc 1
2017-08-08T16:41:02.863414: step 3576, loss 0.00369046, acc 1
2017-08-08T16:41:03.180686: step 3577, loss 0.00845079, acc 1
2017-08-08T16:41:03.460529: step 3578, loss 0.00177755, acc 1
2017-08-08T16:41:03.738397: step 3579, loss 0.00385226, acc 1
2017-08-08T16:41:04.101768: step 3580, loss 0.00458675, acc 1
2017-08-08T16:41:04.557799: step 3581, loss 0.00101112, acc 1
2017-08-08T16:41:04.966301: step 3582, loss 0.00149678, acc 1
2017-08-08T16:41:05.324557: step 3583, loss 0.00507628, acc 1
2017-08-08T16:41:05.579821: step 3584, loss 0.00948878, acc 1
2017-08-08T16:41:05.992337: step 3585, loss 0.04403, acc 0.984375
2017-08-08T16:41:06.253000: step 3586, loss 0.0528154, acc 0.984375
2017-08-08T16:41:06.487456: step 3587, loss 0.00589926, acc 1
2017-08-08T16:41:06.852672: step 3588, loss 0.00838099, acc 1
2017-08-08T16:41:07.248595: step 3589, loss 0.00160253, acc 1
2017-08-08T16:41:07.609644: step 3590, loss 0.00633595, acc 1
2017-08-08T16:41:07.929041: step 3591, loss 0.0130411, acc 1
2017-08-08T16:41:08.186872: step 3592, loss 0.00905184, acc 1
2017-08-08T16:41:08.607443: step 3593, loss 0.00184198, acc 1
2017-08-08T16:41:08.929119: step 3594, loss 0.0022726, acc 1
2017-08-08T16:41:09.228257: step 3595, loss 0.00243727, acc 1
2017-08-08T16:41:09.517312: step 3596, loss 0.00657309, acc 1
2017-08-08T16:41:09.808205: step 3597, loss 0.00479684, acc 1
2017-08-08T16:41:10.185644: step 3598, loss 0.0537152, acc 0.96875
2017-08-08T16:41:10.504866: step 3599, loss 0.00877236, acc 1
2017-08-08T16:41:10.798305: step 3600, loss 0.00101193, acc 1

Evaluation:
2017-08-08T16:41:11.390807: step 3600, loss 1.23755, acc 0.728893

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-3600

2017-08-08T16:41:11.752776: step 3601, loss 0.00370856, acc 1
2017-08-08T16:41:11.949357: step 3602, loss 0.00662047, acc 1
2017-08-08T16:41:12.233211: step 3603, loss 0.00452774, acc 1
2017-08-08T16:41:12.576034: step 3604, loss 0.00228767, acc 1
2017-08-08T16:41:12.878045: step 3605, loss 0.00381476, acc 1
2017-08-08T16:41:13.097025: step 3606, loss 0.00501997, acc 1
2017-08-08T16:41:13.380221: step 3607, loss 0.0513582, acc 0.984375
2017-08-08T16:41:13.685316: step 3608, loss 0.00451948, acc 1
2017-08-08T16:41:13.884150: step 3609, loss 0.00423341, acc 1
2017-08-08T16:41:14.123831: step 3610, loss 0.00695921, acc 1
2017-08-08T16:41:14.515782: step 3611, loss 0.0146478, acc 1
2017-08-08T16:41:14.767947: step 3612, loss 0.00686569, acc 1
2017-08-08T16:41:15.056760: step 3613, loss 0.00278452, acc 1
2017-08-08T16:41:15.295275: step 3614, loss 0.00254445, acc 1
2017-08-08T16:41:15.477909: step 3615, loss 0.0573918, acc 0.984375
2017-08-08T16:41:15.789765: step 3616, loss 0.00383077, acc 1
2017-08-08T16:41:16.028587: step 3617, loss 0.0046204, acc 1
2017-08-08T16:41:16.372478: step 3618, loss 0.00281876, acc 1
2017-08-08T16:41:16.627065: step 3619, loss 0.0112355, acc 1
2017-08-08T16:41:17.017320: step 3620, loss 0.0310964, acc 0.984375
2017-08-08T16:41:17.283608: step 3621, loss 0.00182854, acc 1
2017-08-08T16:41:17.562107: step 3622, loss 0.0137898, acc 1
2017-08-08T16:41:17.813560: step 3623, loss 0.114656, acc 0.96875
2017-08-08T16:41:18.130107: step 3624, loss 0.00679504, acc 1
2017-08-08T16:41:18.440864: step 3625, loss 0.0562977, acc 0.984375
2017-08-08T16:41:18.640263: step 3626, loss 0.00237629, acc 1
2017-08-08T16:41:18.853203: step 3627, loss 0.00341511, acc 1
2017-08-08T16:41:19.094762: step 3628, loss 0.00220284, acc 1
2017-08-08T16:41:19.440548: step 3629, loss 0.0027831, acc 1
2017-08-08T16:41:19.750196: step 3630, loss 0.00278239, acc 1
2017-08-08T16:41:19.965755: step 3631, loss 0.00111991, acc 1
2017-08-08T16:41:20.161319: step 3632, loss 0.00435378, acc 1
2017-08-08T16:41:20.492555: step 3633, loss 0.119374, acc 0.984375
2017-08-08T16:41:20.662674: step 3634, loss 0.00377997, acc 1
2017-08-08T16:41:20.851073: step 3635, loss 0.00920344, acc 1
2017-08-08T16:41:21.145514: step 3636, loss 0.00131243, acc 1
2017-08-08T16:41:21.422935: step 3637, loss 0.0242699, acc 0.984375
2017-08-08T16:41:21.669358: step 3638, loss 0.00486354, acc 1
2017-08-08T16:41:21.963757: step 3639, loss 0.0042784, acc 1
2017-08-08T16:41:22.213379: step 3640, loss 0.00326595, acc 1
2017-08-08T16:41:22.504523: step 3641, loss 0.0014429, acc 1
2017-08-08T16:41:22.752780: step 3642, loss 0.00164541, acc 1
2017-08-08T16:41:22.954953: step 3643, loss 0.00348502, acc 1
2017-08-08T16:41:23.181864: step 3644, loss 0.00267639, acc 1
2017-08-08T16:41:23.477394: step 3645, loss 0.0268968, acc 0.984375
2017-08-08T16:41:23.809360: step 3646, loss 0.0232733, acc 0.984375
2017-08-08T16:41:24.092896: step 3647, loss 0.0129368, acc 0.984375
2017-08-08T16:41:24.302386: step 3648, loss 0.00279109, acc 1
2017-08-08T16:41:24.594347: step 3649, loss 0.00511329, acc 1
2017-08-08T16:41:24.823056: step 3650, loss 0.000596485, acc 1
2017-08-08T16:41:25.060883: step 3651, loss 0.0102584, acc 1
2017-08-08T16:41:25.288883: step 3652, loss 0.00684475, acc 1
2017-08-08T16:41:25.499277: step 3653, loss 0.0158955, acc 1
2017-08-08T16:41:25.918874: step 3654, loss 0.0131244, acc 1
2017-08-08T16:41:26.232971: step 3655, loss 0.00169066, acc 1
2017-08-08T16:41:26.533435: step 3656, loss 0.0078565, acc 1
2017-08-08T16:41:26.738862: step 3657, loss 0.00240286, acc 1
2017-08-08T16:41:27.020831: step 3658, loss 0.00211274, acc 1
2017-08-08T16:41:27.307180: step 3659, loss 0.00432391, acc 1
2017-08-08T16:41:27.632022: step 3660, loss 0.00701547, acc 1
2017-08-08T16:41:27.930496: step 3661, loss 0.00336744, acc 1
2017-08-08T16:41:28.315289: step 3662, loss 0.031344, acc 0.984375
2017-08-08T16:41:28.698015: step 3663, loss 0.0147748, acc 1
2017-08-08T16:41:29.011944: step 3664, loss 0.00984967, acc 1
2017-08-08T16:41:29.201216: step 3665, loss 0.0103551, acc 1
2017-08-08T16:41:29.451073: step 3666, loss 0.00073245, acc 1
2017-08-08T16:41:29.824185: step 3667, loss 0.00130388, acc 1
2017-08-08T16:41:30.099235: step 3668, loss 0.00667212, acc 1
2017-08-08T16:41:30.409635: step 3669, loss 0.00853357, acc 1
2017-08-08T16:41:30.682867: step 3670, loss 0.00708741, acc 1
2017-08-08T16:41:31.120681: step 3671, loss 0.00495532, acc 1
2017-08-08T16:41:31.524368: step 3672, loss 0.00681698, acc 1
2017-08-08T16:41:31.815527: step 3673, loss 0.00188071, acc 1
2017-08-08T16:41:32.005915: step 3674, loss 0.00319115, acc 1
2017-08-08T16:41:32.226492: step 3675, loss 0.00877936, acc 1
2017-08-08T16:41:32.635563: step 3676, loss 0.0133582, acc 1
2017-08-08T16:41:32.814072: step 3677, loss 0.00213259, acc 1
2017-08-08T16:41:33.014824: step 3678, loss 0.000854013, acc 1
2017-08-08T16:41:33.356459: step 3679, loss 0.0129466, acc 0.984375
2017-08-08T16:41:33.749401: step 3680, loss 0.0261509, acc 0.984375
2017-08-08T16:41:34.181934: step 3681, loss 0.0032737, acc 1
2017-08-08T16:41:34.462935: step 3682, loss 0.0903063, acc 0.953125
2017-08-08T16:41:34.729409: step 3683, loss 0.0366307, acc 0.984375
2017-08-08T16:41:35.153643: step 3684, loss 0.00214463, acc 1
2017-08-08T16:41:35.421612: step 3685, loss 0.0138146, acc 1
2017-08-08T16:41:35.712741: step 3686, loss 0.00733477, acc 1
2017-08-08T16:41:35.961651: step 3687, loss 0.00178434, acc 1
2017-08-08T16:41:36.365366: step 3688, loss 0.00305428, acc 1
2017-08-08T16:41:36.761051: step 3689, loss 0.0302488, acc 0.984375
2017-08-08T16:41:37.125897: step 3690, loss 0.0013337, acc 1
2017-08-08T16:41:37.445965: step 3691, loss 0.00394046, acc 1
2017-08-08T16:41:37.756447: step 3692, loss 0.00770435, acc 1
2017-08-08T16:41:38.077370: step 3693, loss 0.00356, acc 1
2017-08-08T16:41:38.296422: step 3694, loss 0.00316234, acc 1
2017-08-08T16:41:38.550746: step 3695, loss 0.00857261, acc 1
2017-08-08T16:41:38.816062: step 3696, loss 0.0121274, acc 1
2017-08-08T16:41:39.208674: step 3697, loss 0.0179225, acc 0.984375
2017-08-08T16:41:39.468998: step 3698, loss 0.023547, acc 0.984375
2017-08-08T16:41:39.851982: step 3699, loss 0.0027232, acc 1
2017-08-08T16:41:40.082360: step 3700, loss 0.0294414, acc 0.984375

Evaluation:
2017-08-08T16:41:40.550932: step 3700, loss 1.27582, acc 0.726079

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-3700

2017-08-08T16:41:40.955680: step 3701, loss 0.00790293, acc 1
2017-08-08T16:41:41.168670: step 3702, loss 0.0229369, acc 0.984375
2017-08-08T16:41:41.368513: step 3703, loss 0.00403053, acc 1
2017-08-08T16:41:41.559328: step 3704, loss 0.00235554, acc 1
2017-08-08T16:41:41.899847: step 3705, loss 0.00289141, acc 1
2017-08-08T16:41:42.151787: step 3706, loss 0.000965231, acc 1
2017-08-08T16:41:42.515283: step 3707, loss 0.00666328, acc 1
2017-08-08T16:41:42.770171: step 3708, loss 0.00102958, acc 1
2017-08-08T16:41:42.974675: step 3709, loss 0.00730462, acc 1
2017-08-08T16:41:43.349365: step 3710, loss 0.015289, acc 1
2017-08-08T16:41:43.562887: step 3711, loss 0.016334, acc 1
2017-08-08T16:41:43.816518: step 3712, loss 0.00304806, acc 1
2017-08-08T16:41:44.133001: step 3713, loss 0.00245536, acc 1
2017-08-08T16:41:44.473680: step 3714, loss 0.00629152, acc 1
2017-08-08T16:41:44.796865: step 3715, loss 0.00623655, acc 1
2017-08-08T16:41:45.109263: step 3716, loss 0.00279519, acc 1
2017-08-08T16:41:45.310896: step 3717, loss 0.0054294, acc 1
2017-08-08T16:41:45.562711: step 3718, loss 0.00205487, acc 1
2017-08-08T16:41:45.901954: step 3719, loss 0.0189998, acc 1
2017-08-08T16:41:46.149434: step 3720, loss 0.00538619, acc 1
2017-08-08T16:41:46.370239: step 3721, loss 0.00104466, acc 1
2017-08-08T16:41:46.598290: step 3722, loss 0.0026883, acc 1
2017-08-08T16:41:46.981396: step 3723, loss 0.00335953, acc 1
2017-08-08T16:41:47.294790: step 3724, loss 0.00963111, acc 1
2017-08-08T16:41:47.568177: step 3725, loss 0.0107913, acc 1
2017-08-08T16:41:47.759949: step 3726, loss 0.00229572, acc 1
2017-08-08T16:41:47.959975: step 3727, loss 0.00600953, acc 1
2017-08-08T16:41:48.311870: step 3728, loss 0.00752053, acc 1
2017-08-08T16:41:48.508101: step 3729, loss 0.00200488, acc 1
2017-08-08T16:41:48.719512: step 3730, loss 0.00237227, acc 1
2017-08-08T16:41:49.028731: step 3731, loss 0.00606571, acc 1
2017-08-08T16:41:49.358179: step 3732, loss 0.0122066, acc 0.984375
2017-08-08T16:41:49.765919: step 3733, loss 0.00995564, acc 1
2017-08-08T16:41:50.035838: step 3734, loss 0.0116471, acc 1
2017-08-08T16:41:50.218165: step 3735, loss 0.00287999, acc 1
2017-08-08T16:41:50.489757: step 3736, loss 0.00364331, acc 1
2017-08-08T16:41:50.739504: step 3737, loss 0.0180628, acc 0.984375
2017-08-08T16:41:50.989881: step 3738, loss 0.00203751, acc 1
2017-08-08T16:41:51.209316: step 3739, loss 0.00481435, acc 1
2017-08-08T16:41:51.575722: step 3740, loss 0.0378846, acc 0.984375
2017-08-08T16:41:51.885342: step 3741, loss 0.0059868, acc 1
2017-08-08T16:41:52.088258: step 3742, loss 0.0062651, acc 1
2017-08-08T16:41:52.262452: step 3743, loss 0.00921117, acc 1
2017-08-08T16:41:52.529322: step 3744, loss 0.00236089, acc 1
2017-08-08T16:41:52.737997: step 3745, loss 0.00071086, acc 1
2017-08-08T16:41:52.917560: step 3746, loss 0.00213735, acc 1
2017-08-08T16:41:53.137669: step 3747, loss 0.00408037, acc 1
2017-08-08T16:41:53.426865: step 3748, loss 0.0437364, acc 0.984375
2017-08-08T16:41:53.751174: step 3749, loss 0.00122122, acc 1
2017-08-08T16:41:54.105622: step 3750, loss 0.0106252, acc 1
2017-08-08T16:41:54.323852: step 3751, loss 0.00491639, acc 1
2017-08-08T16:41:54.563524: step 3752, loss 0.00339496, acc 1
2017-08-08T16:41:54.889349: step 3753, loss 0.00606953, acc 1
2017-08-08T16:41:55.129402: step 3754, loss 0.00317719, acc 1
2017-08-08T16:41:55.382411: step 3755, loss 0.0205972, acc 0.984375
2017-08-08T16:41:55.585427: step 3756, loss 0.00230948, acc 1
2017-08-08T16:41:55.884743: step 3757, loss 0.0131106, acc 1
2017-08-08T16:41:56.197347: step 3758, loss 0.00667722, acc 1
2017-08-08T16:41:56.441567: step 3759, loss 0.00309566, acc 1
2017-08-08T16:41:56.663523: step 3760, loss 0.000633886, acc 1
2017-08-08T16:41:56.897181: step 3761, loss 0.00800597, acc 1
2017-08-08T16:41:57.276023: step 3762, loss 0.00709826, acc 1
2017-08-08T16:41:57.474241: step 3763, loss 0.000508128, acc 1
2017-08-08T16:41:57.694043: step 3764, loss 0.0182751, acc 0.984375
2017-08-08T16:41:58.088994: step 3765, loss 0.00529131, acc 1
2017-08-08T16:41:58.408281: step 3766, loss 0.00334658, acc 1
2017-08-08T16:41:58.671887: step 3767, loss 0.00757911, acc 1
2017-08-08T16:41:58.912213: step 3768, loss 0.00345714, acc 1
2017-08-08T16:41:59.115431: step 3769, loss 0.00511712, acc 1
2017-08-08T16:41:59.410898: step 3770, loss 0.00684796, acc 1
2017-08-08T16:41:59.591646: step 3771, loss 0.0312196, acc 0.984375
2017-08-08T16:41:59.843924: step 3772, loss 0.0102944, acc 1
2017-08-08T16:42:00.133228: step 3773, loss 0.00191643, acc 1
2017-08-08T16:42:00.490347: step 3774, loss 0.00545795, acc 1
2017-08-08T16:42:00.781553: step 3775, loss 0.00174362, acc 1
2017-08-08T16:42:01.011563: step 3776, loss 0.0014599, acc 1
2017-08-08T16:42:01.223950: step 3777, loss 0.00199633, acc 1
2017-08-08T16:42:01.558441: step 3778, loss 0.0170319, acc 0.984375
2017-08-08T16:42:01.890948: step 3779, loss 0.00168463, acc 1
2017-08-08T16:42:02.178945: step 3780, loss 0.00930588, acc 1
2017-08-08T16:42:02.453698: step 3781, loss 0.00513601, acc 1
2017-08-08T16:42:02.737857: step 3782, loss 0.0490393, acc 0.984375
2017-08-08T16:42:03.212174: step 3783, loss 0.00403566, acc 1
2017-08-08T16:42:03.609251: step 3784, loss 0.00354508, acc 1
2017-08-08T16:42:03.953864: step 3785, loss 0.0125801, acc 1
2017-08-08T16:42:04.207731: step 3786, loss 0.00650704, acc 1
2017-08-08T16:42:04.461530: step 3787, loss 0.00618339, acc 1
2017-08-08T16:42:04.784841: step 3788, loss 0.0414844, acc 0.984375
2017-08-08T16:42:05.016812: step 3789, loss 0.00614023, acc 1
2017-08-08T16:42:05.264608: step 3790, loss 0.00261626, acc 1
2017-08-08T16:42:05.853738: step 3791, loss 0.00161314, acc 1
2017-08-08T16:42:06.301964: step 3792, loss 0.000728816, acc 1
2017-08-08T16:42:06.630848: step 3793, loss 0.00392168, acc 1
2017-08-08T16:42:06.910670: step 3794, loss 0.00265372, acc 1
2017-08-08T16:42:07.292641: step 3795, loss 0.00214221, acc 1
2017-08-08T16:42:07.611456: step 3796, loss 0.00304239, acc 1
2017-08-08T16:42:07.882143: step 3797, loss 0.00595888, acc 1
2017-08-08T16:42:08.154560: step 3798, loss 0.00377428, acc 1
2017-08-08T16:42:08.417747: step 3799, loss 0.00268169, acc 1
2017-08-08T16:42:08.799997: step 3800, loss 0.00431774, acc 1

Evaluation:
2017-08-08T16:42:09.809012: step 3800, loss 1.36354, acc 0.719512

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-3800

2017-08-08T16:42:10.353357: step 3801, loss 0.0383599, acc 0.984375
2017-08-08T16:42:10.644432: step 3802, loss 0.0010832, acc 1
2017-08-08T16:42:10.924360: step 3803, loss 0.00178118, acc 1
2017-08-08T16:42:11.206309: step 3804, loss 0.00570379, acc 1
2017-08-08T16:42:11.571858: step 3805, loss 0.0112945, acc 1
2017-08-08T16:42:12.057374: step 3806, loss 0.00372758, acc 1
2017-08-08T16:42:12.500274: step 3807, loss 0.00434068, acc 1
2017-08-08T16:42:12.765064: step 3808, loss 0.0117442, acc 1
2017-08-08T16:42:12.965583: step 3809, loss 0.00885481, acc 1
2017-08-08T16:42:13.384597: step 3810, loss 0.0146034, acc 0.984375
2017-08-08T16:42:13.709677: step 3811, loss 0.00517239, acc 1
2017-08-08T16:42:14.001398: step 3812, loss 0.0109366, acc 1
2017-08-08T16:42:14.368163: step 3813, loss 0.00276939, acc 1
2017-08-08T16:42:14.770380: step 3814, loss 0.00208311, acc 1
2017-08-08T16:42:15.110117: step 3815, loss 0.00667665, acc 1
2017-08-08T16:42:15.447855: step 3816, loss 0.0110464, acc 1
2017-08-08T16:42:15.715585: step 3817, loss 0.0482669, acc 0.984375
2017-08-08T16:42:16.110948: step 3818, loss 0.00565132, acc 1
2017-08-08T16:42:16.467262: step 3819, loss 0.0154455, acc 1
2017-08-08T16:42:16.705354: step 3820, loss 0.00599903, acc 1
2017-08-08T16:42:17.092208: step 3821, loss 0.0109972, acc 1
2017-08-08T16:42:17.411399: step 3822, loss 0.00494824, acc 1
2017-08-08T16:42:17.652224: step 3823, loss 0.0265504, acc 0.984375
2017-08-08T16:42:17.857880: step 3824, loss 0.00216542, acc 1
2017-08-08T16:42:18.048884: step 3825, loss 0.00362073, acc 1
2017-08-08T16:42:18.457363: step 3826, loss 0.00931304, acc 1
2017-08-08T16:42:18.746470: step 3827, loss 0.0046968, acc 1
2017-08-08T16:42:18.967046: step 3828, loss 0.00613706, acc 1
2017-08-08T16:42:19.205856: step 3829, loss 0.00207201, acc 1
2017-08-08T16:42:19.533311: step 3830, loss 0.00128767, acc 1
2017-08-08T16:42:19.956004: step 3831, loss 0.000865704, acc 1
2017-08-08T16:42:20.151403: step 3832, loss 0.0107129, acc 1
2017-08-08T16:42:20.364120: step 3833, loss 0.00199342, acc 1
2017-08-08T16:42:20.644633: step 3834, loss 0.0501251, acc 0.984375
2017-08-08T16:42:21.011914: step 3835, loss 0.00056979, acc 1
2017-08-08T16:42:21.283207: step 3836, loss 0.00874571, acc 1
2017-08-08T16:42:21.562580: step 3837, loss 0.00900349, acc 1
2017-08-08T16:42:21.901565: step 3838, loss 0.00287688, acc 1
2017-08-08T16:42:22.297344: step 3839, loss 0.00091534, acc 1
2017-08-08T16:42:22.653376: step 3840, loss 0.00316201, acc 1
2017-08-08T16:42:22.956272: step 3841, loss 0.000660937, acc 1
2017-08-08T16:42:23.197497: step 3842, loss 0.00138438, acc 1
2017-08-08T16:42:23.529779: step 3843, loss 0.0018076, acc 1
2017-08-08T16:42:23.913948: step 3844, loss 0.000826221, acc 1
2017-08-08T16:42:24.205867: step 3845, loss 0.0296952, acc 0.984375
2017-08-08T16:42:24.478036: step 3846, loss 0.0129865, acc 1
2017-08-08T16:42:24.903348: step 3847, loss 0.00834695, acc 1
2017-08-08T16:42:25.335789: step 3848, loss 0.00257245, acc 1
2017-08-08T16:42:25.719715: step 3849, loss 0.0017464, acc 1
2017-08-08T16:42:25.984908: step 3850, loss 0.0115338, acc 1
2017-08-08T16:42:26.225426: step 3851, loss 0.0477916, acc 0.984375
2017-08-08T16:42:26.529600: step 3852, loss 0.0364036, acc 0.984375
2017-08-08T16:42:26.803593: step 3853, loss 0.0371547, acc 0.96875
2017-08-08T16:42:27.000184: step 3854, loss 0.0018207, acc 1
2017-08-08T16:42:27.234944: step 3855, loss 0.00533172, acc 1
2017-08-08T16:42:27.563737: step 3856, loss 0.00862376, acc 1
2017-08-08T16:42:27.935747: step 3857, loss 0.0222807, acc 0.984375
2017-08-08T16:42:28.178763: step 3858, loss 0.00266061, acc 1
2017-08-08T16:42:28.450096: step 3859, loss 0.0133657, acc 1
2017-08-08T16:42:28.793236: step 3860, loss 0.00051257, acc 1
2017-08-08T16:42:29.034735: step 3861, loss 0.0196077, acc 0.984375
2017-08-08T16:42:29.226476: step 3862, loss 0.00214436, acc 1
2017-08-08T16:42:29.484356: step 3863, loss 0.00280347, acc 1
2017-08-08T16:42:29.893394: step 3864, loss 0.0056825, acc 1
2017-08-08T16:42:30.346382: step 3865, loss 0.00287006, acc 1
2017-08-08T16:42:30.670849: step 3866, loss 0.000675898, acc 1
2017-08-08T16:42:30.993419: step 3867, loss 0.00343019, acc 1
2017-08-08T16:42:31.317379: step 3868, loss 0.00232798, acc 1
2017-08-08T16:42:31.629263: step 3869, loss 0.000973445, acc 1
2017-08-08T16:42:31.868133: step 3870, loss 0.00651878, acc 1
2017-08-08T16:42:32.158209: step 3871, loss 0.00348051, acc 1
2017-08-08T16:42:32.605670: step 3872, loss 0.0102871, acc 1
2017-08-08T16:42:33.048517: step 3873, loss 0.00122227, acc 1
2017-08-08T16:42:33.503971: step 3874, loss 0.00271946, acc 1
2017-08-08T16:42:33.776494: step 3875, loss 0.0455282, acc 0.96875
2017-08-08T16:42:34.144503: step 3876, loss 0.00117418, acc 1
2017-08-08T16:42:34.522490: step 3877, loss 0.00981954, acc 1
2017-08-08T16:42:34.779018: step 3878, loss 0.00229435, acc 1
2017-08-08T16:42:34.996303: step 3879, loss 0.0026702, acc 1
2017-08-08T16:42:35.306635: step 3880, loss 0.00201607, acc 1
2017-08-08T16:42:35.676530: step 3881, loss 0.0040881, acc 1
2017-08-08T16:42:35.885318: step 3882, loss 0.00107955, acc 1
2017-08-08T16:42:36.118911: step 3883, loss 0.00285647, acc 1
2017-08-08T16:42:36.311207: step 3884, loss 0.00104263, acc 1
2017-08-08T16:42:36.623055: step 3885, loss 0.00122752, acc 1
2017-08-08T16:42:37.030327: step 3886, loss 0.00416546, acc 1
2017-08-08T16:42:37.297150: step 3887, loss 0.00335027, acc 1
2017-08-08T16:42:37.631425: step 3888, loss 0.00161778, acc 1
2017-08-08T16:42:38.112695: step 3889, loss 0.00879653, acc 1
2017-08-08T16:42:38.541277: step 3890, loss 0.0080169, acc 1
2017-08-08T16:42:38.922817: step 3891, loss 0.0165085, acc 0.984375
2017-08-08T16:42:39.180240: step 3892, loss 0.0013263, acc 1
2017-08-08T16:42:39.423129: step 3893, loss 0.0101293, acc 1
2017-08-08T16:42:39.859950: step 3894, loss 0.0216341, acc 0.984375
2017-08-08T16:42:40.176679: step 3895, loss 0.00260802, acc 1
2017-08-08T16:42:40.500572: step 3896, loss 0.00420069, acc 1
2017-08-08T16:42:40.896334: step 3897, loss 0.0138886, acc 1
2017-08-08T16:42:41.260270: step 3898, loss 0.0013476, acc 1
2017-08-08T16:42:41.588233: step 3899, loss 0.00610695, acc 1
2017-08-08T16:42:41.848460: step 3900, loss 0.00387278, acc 1

Evaluation:
2017-08-08T16:42:42.540495: step 3900, loss 1.34048, acc 0.716698

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-3900

2017-08-08T16:42:43.046706: step 3901, loss 0.00225605, acc 1
2017-08-08T16:42:43.353599: step 3902, loss 0.000953562, acc 1
2017-08-08T16:42:43.846171: step 3903, loss 0.00670776, acc 1
2017-08-08T16:42:44.269531: step 3904, loss 0.00324239, acc 1
2017-08-08T16:42:44.609822: step 3905, loss 0.00202226, acc 1
2017-08-08T16:42:44.825666: step 3906, loss 0.00296638, acc 1
2017-08-08T16:42:45.117312: step 3907, loss 0.00499358, acc 1
2017-08-08T16:42:45.477049: step 3908, loss 0.00491848, acc 1
2017-08-08T16:42:45.761092: step 3909, loss 0.00319858, acc 1
2017-08-08T16:42:46.045289: step 3910, loss 0.0177476, acc 0.984375
2017-08-08T16:42:46.418171: step 3911, loss 0.00303443, acc 1
2017-08-08T16:42:46.746304: step 3912, loss 0.0347127, acc 0.984375
2017-08-08T16:42:47.167722: step 3913, loss 0.0141074, acc 0.984375
2017-08-08T16:42:47.526495: step 3914, loss 0.00259959, acc 1
2017-08-08T16:42:47.793490: step 3915, loss 0.00523598, acc 1
2017-08-08T16:42:48.126431: step 3916, loss 0.00382022, acc 1
2017-08-08T16:42:48.548131: step 3917, loss 0.00589095, acc 1
2017-08-08T16:42:48.779751: step 3918, loss 0.00258953, acc 1
2017-08-08T16:42:49.037499: step 3919, loss 0.0329222, acc 0.984375
2017-08-08T16:42:49.328591: step 3920, loss 0.00314691, acc 1
2017-08-08T16:42:49.803732: step 3921, loss 0.00155369, acc 1
2017-08-08T16:42:50.261959: step 3922, loss 0.000163777, acc 1
2017-08-08T16:42:50.659455: step 3923, loss 0.0379352, acc 0.984375
2017-08-08T16:42:50.972864: step 3924, loss 0.00329624, acc 1
2017-08-08T16:42:51.215568: step 3925, loss 0.0209873, acc 0.984375
2017-08-08T16:42:51.652530: step 3926, loss 0.00295872, acc 1
2017-08-08T16:42:51.950052: step 3927, loss 0.00828115, acc 1
2017-08-08T16:42:52.204325: step 3928, loss 0.00234729, acc 1
2017-08-08T16:42:52.616104: step 3929, loss 0.000849055, acc 1
2017-08-08T16:42:53.046990: step 3930, loss 0.00239547, acc 1
2017-08-08T16:42:53.326510: step 3931, loss 0.0176573, acc 0.984375
2017-08-08T16:42:53.552782: step 3932, loss 0.0494959, acc 0.96875
2017-08-08T16:42:53.757556: step 3933, loss 0.00383201, acc 1
2017-08-08T16:42:53.985486: step 3934, loss 0.00190834, acc 1
2017-08-08T16:42:54.402662: step 3935, loss 0.00300079, acc 1
2017-08-08T16:42:54.684944: step 3936, loss 0.00299075, acc 1
2017-08-08T16:42:54.972982: step 3937, loss 0.0010044, acc 1
2017-08-08T16:42:55.245013: step 3938, loss 0.00299694, acc 1
2017-08-08T16:42:55.692730: step 3939, loss 0.00144868, acc 1
2017-08-08T16:42:56.103153: step 3940, loss 0.00272659, acc 1
2017-08-08T16:42:56.431593: step 3941, loss 0.00374341, acc 1
2017-08-08T16:42:56.719327: step 3942, loss 0.000284277, acc 1
2017-08-08T16:42:57.006165: step 3943, loss 0.00282475, acc 1
2017-08-08T16:42:57.451329: step 3944, loss 0.0034094, acc 1
2017-08-08T16:42:57.715224: step 3945, loss 0.00429727, acc 1
2017-08-08T16:42:57.985698: step 3946, loss 0.00482647, acc 1
2017-08-08T16:42:58.264532: step 3947, loss 0.00997482, acc 1
2017-08-08T16:42:58.619390: step 3948, loss 0.00180851, acc 1
2017-08-08T16:42:59.131536: step 3949, loss 0.00581025, acc 1
2017-08-08T16:42:59.527966: step 3950, loss 0.00497906, acc 1
2017-08-08T16:42:59.877584: step 3951, loss 0.000501739, acc 1
2017-08-08T16:43:00.326813: step 3952, loss 0.0124264, acc 1
2017-08-08T16:43:00.565388: step 3953, loss 0.000663671, acc 1
2017-08-08T16:43:00.862779: step 3954, loss 0.00480775, acc 1
2017-08-08T16:43:01.145600: step 3955, loss 0.00522963, acc 1
2017-08-08T16:43:01.714442: step 3956, loss 0.00411057, acc 1
2017-08-08T16:43:02.236267: step 3957, loss 0.00747429, acc 1
2017-08-08T16:43:02.615350: step 3958, loss 0.00281841, acc 1
2017-08-08T16:43:02.907821: step 3959, loss 0.000605508, acc 1
2017-08-08T16:43:03.366262: step 3960, loss 0.00303602, acc 1
2017-08-08T16:43:03.640001: step 3961, loss 0.00281296, acc 1
2017-08-08T16:43:03.961842: step 3962, loss 0.00306212, acc 1
2017-08-08T16:43:04.235741: step 3963, loss 0.00856615, acc 1
2017-08-08T16:43:04.723815: step 3964, loss 0.00553637, acc 1
2017-08-08T16:43:05.076621: step 3965, loss 0.00421096, acc 1
2017-08-08T16:43:05.401173: step 3966, loss 0.00567167, acc 1
2017-08-08T16:43:05.586236: step 3967, loss 0.00111246, acc 1
2017-08-08T16:43:05.933846: step 3968, loss 0.0327405, acc 0.984375
2017-08-08T16:43:06.285461: step 3969, loss 0.00813854, acc 1
2017-08-08T16:43:06.545311: step 3970, loss 0.00147888, acc 1
2017-08-08T16:43:06.858068: step 3971, loss 0.00916761, acc 1
2017-08-08T16:43:07.181529: step 3972, loss 0.00426234, acc 1
2017-08-08T16:43:07.701500: step 3973, loss 0.00711218, acc 1
2017-08-08T16:43:08.152734: step 3974, loss 0.00307012, acc 1
2017-08-08T16:43:08.453785: step 3975, loss 0.00209869, acc 1
2017-08-08T16:43:08.702000: step 3976, loss 0.00697356, acc 1
2017-08-08T16:43:08.940427: step 3977, loss 0.00857944, acc 1
2017-08-08T16:43:09.384129: step 3978, loss 0.0111332, acc 1
2017-08-08T16:43:09.667292: step 3979, loss 0.00311979, acc 1
2017-08-08T16:43:09.967882: step 3980, loss 0.0266391, acc 0.984375
2017-08-08T16:43:10.218595: step 3981, loss 0.000902422, acc 1
2017-08-08T16:43:10.677390: step 3982, loss 0.00835069, acc 1
2017-08-08T16:43:11.035778: step 3983, loss 0.00506246, acc 1
2017-08-08T16:43:11.426461: step 3984, loss 0.00315849, acc 1
2017-08-08T16:43:11.747321: step 3985, loss 0.00233245, acc 1
2017-08-08T16:43:12.020465: step 3986, loss 0.000891206, acc 1
2017-08-08T16:43:12.499302: step 3987, loss 0.0155866, acc 0.984375
2017-08-08T16:43:12.785452: step 3988, loss 0.0153555, acc 1
2017-08-08T16:43:13.093102: step 3989, loss 0.00151838, acc 1
2017-08-08T16:43:13.385417: step 3990, loss 0.00286407, acc 1
2017-08-08T16:43:13.842361: step 3991, loss 0.0057367, acc 1
2017-08-08T16:43:14.233351: step 3992, loss 0.00173963, acc 1
2017-08-08T16:43:14.573307: step 3993, loss 0.0041339, acc 1
2017-08-08T16:43:14.781784: step 3994, loss 0.00529677, acc 1
2017-08-08T16:43:14.972682: step 3995, loss 0.0013114, acc 1
2017-08-08T16:43:15.314542: step 3996, loss 0.00946557, acc 1
2017-08-08T16:43:15.527436: step 3997, loss 0.00116628, acc 1
2017-08-08T16:43:15.713184: step 3998, loss 0.00136415, acc 1
2017-08-08T16:43:15.903736: step 3999, loss 0.0654036, acc 0.96875
2017-08-08T16:43:16.183526: step 4000, loss 0.000944364, acc 1

Evaluation:
2017-08-08T16:43:16.742342: step 4000, loss 1.34834, acc 0.725141

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-4000

2017-08-08T16:43:17.089323: step 4001, loss 0.00830045, acc 1
2017-08-08T16:43:17.382969: step 4002, loss 0.0137928, acc 0.984375
2017-08-08T16:43:17.566198: step 4003, loss 0.0271162, acc 0.984375
2017-08-08T16:43:17.733342: step 4004, loss 0.00478036, acc 1
2017-08-08T16:43:18.009314: step 4005, loss 0.00121335, acc 1
2017-08-08T16:43:18.330455: step 4006, loss 0.00505628, acc 1
2017-08-08T16:43:18.516621: step 4007, loss 0.00763651, acc 1
2017-08-08T16:43:18.703488: step 4008, loss 0.00461649, acc 1
2017-08-08T16:43:19.013348: step 4009, loss 0.00614061, acc 1
2017-08-08T16:43:19.309353: step 4010, loss 0.00346129, acc 1
2017-08-08T16:43:19.560499: step 4011, loss 0.00334614, acc 1
2017-08-08T16:43:19.924374: step 4012, loss 0.0101476, acc 1
2017-08-08T16:43:20.199338: step 4013, loss 0.00589366, acc 1
2017-08-08T16:43:20.455462: step 4014, loss 0.00343366, acc 1
2017-08-08T16:43:20.664667: step 4015, loss 0.00147963, acc 1
2017-08-08T16:43:20.963406: step 4016, loss 0.00441043, acc 1
2017-08-08T16:43:21.307732: step 4017, loss 0.00084048, acc 1
2017-08-08T16:43:21.569377: step 4018, loss 0.0127958, acc 1
2017-08-08T16:43:21.777371: step 4019, loss 0.00586525, acc 1
2017-08-08T16:43:22.082960: step 4020, loss 0.00479115, acc 1
2017-08-08T16:43:22.525583: step 4021, loss 0.00271819, acc 1
2017-08-08T16:43:22.909528: step 4022, loss 0.00180857, acc 1
2017-08-08T16:43:23.260544: step 4023, loss 0.00527967, acc 1
2017-08-08T16:43:23.556328: step 4024, loss 0.00171339, acc 1
2017-08-08T16:43:23.967732: step 4025, loss 0.0128198, acc 0.984375
2017-08-08T16:43:24.216466: step 4026, loss 0.00703047, acc 1
2017-08-08T16:43:24.471536: step 4027, loss 0.0149081, acc 1
2017-08-08T16:43:24.736017: step 4028, loss 0.0193231, acc 0.984375
2017-08-08T16:43:25.171623: step 4029, loss 0.00238351, acc 1
2017-08-08T16:43:25.640258: step 4030, loss 0.0164241, acc 1
2017-08-08T16:43:25.991558: step 4031, loss 0.00292536, acc 1
2017-08-08T16:43:26.172210: step 4032, loss 0.00173768, acc 1
2017-08-08T16:43:26.376597: step 4033, loss 0.00312807, acc 1
2017-08-08T16:43:26.761029: step 4034, loss 0.0314196, acc 0.984375
2017-08-08T16:43:27.033258: step 4035, loss 0.000965518, acc 1
2017-08-08T16:43:27.317047: step 4036, loss 0.0186539, acc 1
2017-08-08T16:43:27.548976: step 4037, loss 0.00508548, acc 1
2017-08-08T16:43:27.793852: step 4038, loss 0.012106, acc 1
2017-08-08T16:43:28.094024: step 4039, loss 0.0168058, acc 1
2017-08-08T16:43:28.481307: step 4040, loss 0.00730698, acc 1
2017-08-08T16:43:28.780635: step 4041, loss 0.0153456, acc 1
2017-08-08T16:43:29.039307: step 4042, loss 0.00328006, acc 1
2017-08-08T16:43:29.550578: step 4043, loss 0.00754363, acc 1
2017-08-08T16:43:29.824190: step 4044, loss 0.000432964, acc 1
2017-08-08T16:43:30.245536: step 4045, loss 0.013856, acc 1
2017-08-08T16:43:30.542476: step 4046, loss 0.0082243, acc 1
2017-08-08T16:43:30.903024: step 4047, loss 0.00398061, acc 1
2017-08-08T16:43:31.247394: step 4048, loss 0.0299606, acc 0.984375
2017-08-08T16:43:31.444684: step 4049, loss 0.00169953, acc 1
2017-08-08T16:43:31.802388: step 4050, loss 0.00547318, acc 1
2017-08-08T16:43:32.038381: step 4051, loss 0.00517227, acc 1
2017-08-08T16:43:32.427208: step 4052, loss 0.016909, acc 0.984375
2017-08-08T16:43:32.652384: step 4053, loss 0.0014649, acc 1
2017-08-08T16:43:32.897902: step 4054, loss 0.00463057, acc 1
2017-08-08T16:43:33.106091: step 4055, loss 0.00368239, acc 1
2017-08-08T16:43:33.433379: step 4056, loss 0.0036444, acc 1
2017-08-08T16:43:33.859106: step 4057, loss 0.00157369, acc 1
2017-08-08T16:43:34.065745: step 4058, loss 0.00118874, acc 1
2017-08-08T16:43:34.309331: step 4059, loss 0.0019566, acc 1
2017-08-08T16:43:34.636164: step 4060, loss 0.00522452, acc 1
2017-08-08T16:43:34.985546: step 4061, loss 0.000720124, acc 1
2017-08-08T16:43:35.228485: step 4062, loss 0.00697886, acc 1
2017-08-08T16:43:35.562056: step 4063, loss 0.00551381, acc 1
2017-08-08T16:43:35.884352: step 4064, loss 0.0142843, acc 0.984375
2017-08-08T16:43:36.168107: step 4065, loss 0.00213644, acc 1
2017-08-08T16:43:36.499814: step 4066, loss 0.00146732, acc 1
2017-08-08T16:43:36.774677: step 4067, loss 0.0355103, acc 0.984375
2017-08-08T16:43:36.995919: step 4068, loss 0.0292591, acc 0.984375
2017-08-08T16:43:37.379808: step 4069, loss 0.00556445, acc 1
2017-08-08T16:43:37.657130: step 4070, loss 0.000413052, acc 1
2017-08-08T16:43:38.015300: step 4071, loss 0.00620781, acc 1
2017-08-08T16:43:38.408567: step 4072, loss 0.000743398, acc 1
2017-08-08T16:43:38.761590: step 4073, loss 0.0040377, acc 1
2017-08-08T16:43:39.171618: step 4074, loss 0.00742845, acc 1
2017-08-08T16:43:39.463786: step 4075, loss 0.00742645, acc 1
2017-08-08T16:43:39.913183: step 4076, loss 0.00092125, acc 1
2017-08-08T16:43:40.162237: step 4077, loss 0.00585686, acc 1
2017-08-08T16:43:40.408642: step 4078, loss 0.00506764, acc 1
2017-08-08T16:43:40.705413: step 4079, loss 0.00454145, acc 1
2017-08-08T16:43:40.939376: step 4080, loss 0.0127532, acc 1
2017-08-08T16:43:41.191562: step 4081, loss 0.00250396, acc 1
2017-08-08T16:43:41.422962: step 4082, loss 0.0273862, acc 0.984375
2017-08-08T16:43:41.882783: step 4083, loss 0.00362539, acc 1
2017-08-08T16:43:42.219834: step 4084, loss 0.000982547, acc 1
2017-08-08T16:43:42.459293: step 4085, loss 0.00563072, acc 1
2017-08-08T16:43:42.681490: step 4086, loss 0.000742066, acc 1
2017-08-08T16:43:42.870120: step 4087, loss 0.000544548, acc 1
2017-08-08T16:43:43.131196: step 4088, loss 0.00607069, acc 1
2017-08-08T16:43:43.441332: step 4089, loss 0.00253175, acc 1
2017-08-08T16:43:43.690935: step 4090, loss 0.00636116, acc 1
2017-08-08T16:43:43.968981: step 4091, loss 0.00286135, acc 1
2017-08-08T16:43:44.359474: step 4092, loss 0.000254316, acc 1
2017-08-08T16:43:44.678422: step 4093, loss 0.00111964, acc 1
2017-08-08T16:43:44.935939: step 4094, loss 0.00141082, acc 1
2017-08-08T16:43:45.197490: step 4095, loss 0.0016856, acc 1
2017-08-08T16:43:45.434150: step 4096, loss 0.0025689, acc 1
2017-08-08T16:43:45.758248: step 4097, loss 0.000491702, acc 1
2017-08-08T16:43:45.977792: step 4098, loss 0.00198964, acc 1
2017-08-08T16:43:46.213211: step 4099, loss 0.000568377, acc 1
2017-08-08T16:43:46.507201: step 4100, loss 0.0234192, acc 0.984375

Evaluation:
2017-08-08T16:43:47.212116: step 4100, loss 1.40709, acc 0.712008

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-4100

2017-08-08T16:43:47.742248: step 4101, loss 0.00306128, acc 1
2017-08-08T16:43:47.965357: step 4102, loss 0.00505398, acc 1
2017-08-08T16:43:48.384878: step 4103, loss 0.00235918, acc 1
2017-08-08T16:43:48.635762: step 4104, loss 0.0284866, acc 0.984375
2017-08-08T16:43:48.865413: step 4105, loss 0.00433488, acc 1
2017-08-08T16:43:49.094432: step 4106, loss 0.00270581, acc 1
2017-08-08T16:43:49.486091: step 4107, loss 0.00209232, acc 1
2017-08-08T16:43:49.921500: step 4108, loss 0.00223635, acc 1
2017-08-08T16:43:50.348275: step 4109, loss 0.000745242, acc 1
2017-08-08T16:43:50.620270: step 4110, loss 0.00252708, acc 1
2017-08-08T16:43:50.909375: step 4111, loss 0.0141378, acc 1
2017-08-08T16:43:51.365666: step 4112, loss 0.00378385, acc 1
2017-08-08T16:43:51.614867: step 4113, loss 0.000752834, acc 1
2017-08-08T16:43:51.867218: step 4114, loss 0.00574422, acc 1
2017-08-08T16:43:52.182559: step 4115, loss 0.00289457, acc 1
2017-08-08T16:43:52.604401: step 4116, loss 0.0146818, acc 0.984375
2017-08-08T16:43:52.995761: step 4117, loss 0.00168069, acc 1
2017-08-08T16:43:53.324699: step 4118, loss 0.0063584, acc 1
2017-08-08T16:43:53.573249: step 4119, loss 0.0164301, acc 1
2017-08-08T16:43:53.937448: step 4120, loss 0.0311368, acc 0.984375
2017-08-08T16:43:54.153778: step 4121, loss 0.000241105, acc 1
2017-08-08T16:43:54.416896: step 4122, loss 0.00149617, acc 1
2017-08-08T16:43:54.827109: step 4123, loss 0.00293951, acc 1
2017-08-08T16:43:55.265691: step 4124, loss 0.00293327, acc 1
2017-08-08T16:43:55.560401: step 4125, loss 0.000972578, acc 1
2017-08-08T16:43:55.761373: step 4126, loss 0.011485, acc 1
2017-08-08T16:43:55.932766: step 4127, loss 0.00187403, acc 1
2017-08-08T16:43:56.301399: step 4128, loss 0.00125124, acc 1
2017-08-08T16:43:56.511945: step 4129, loss 0.00458902, acc 1
2017-08-08T16:43:56.735812: step 4130, loss 0.00207222, acc 1
2017-08-08T16:43:56.916592: step 4131, loss 0.00473591, acc 1
2017-08-08T16:43:57.262676: step 4132, loss 0.00143399, acc 1
2017-08-08T16:43:57.588291: step 4133, loss 0.0109827, acc 1
2017-08-08T16:43:57.959873: step 4134, loss 0.00622569, acc 1
2017-08-08T16:43:58.327929: step 4135, loss 0.0168203, acc 0.984375
2017-08-08T16:43:58.538799: step 4136, loss 0.0095078, acc 1
2017-08-08T16:43:58.870843: step 4137, loss 0.00360784, acc 1
2017-08-08T16:43:59.126319: step 4138, loss 0.00484689, acc 1
2017-08-08T16:43:59.368141: step 4139, loss 0.00098017, acc 1
2017-08-08T16:43:59.703910: step 4140, loss 0.000875607, acc 1
2017-08-08T16:44:00.055834: step 4141, loss 0.00204989, acc 1
2017-08-08T16:44:00.418961: step 4142, loss 0.0171209, acc 0.984375
2017-08-08T16:44:00.642211: step 4143, loss 0.0116921, acc 1
2017-08-08T16:44:00.825384: step 4144, loss 0.00240702, acc 1
2017-08-08T16:44:01.133945: step 4145, loss 0.00198603, acc 1
2017-08-08T16:44:01.381767: step 4146, loss 0.00351172, acc 1
2017-08-08T16:44:01.657225: step 4147, loss 0.0300536, acc 0.984375
2017-08-08T16:44:01.940898: step 4148, loss 0.00208837, acc 1
2017-08-08T16:44:02.233396: step 4149, loss 0.000900593, acc 1
2017-08-08T16:44:02.708074: step 4150, loss 0.00885804, acc 1
2017-08-08T16:44:03.073055: step 4151, loss 0.0106505, acc 1
2017-08-08T16:44:03.444272: step 4152, loss 0.00412153, acc 1
2017-08-08T16:44:03.665656: step 4153, loss 0.00423046, acc 1
2017-08-08T16:44:04.044256: step 4154, loss 0.00193647, acc 1
2017-08-08T16:44:04.425248: step 4155, loss 0.0368707, acc 0.984375
2017-08-08T16:44:04.704506: step 4156, loss 0.00097854, acc 1
2017-08-08T16:44:04.973295: step 4157, loss 0.00296197, acc 1
2017-08-08T16:44:05.189405: step 4158, loss 0.0037783, acc 1
2017-08-08T16:44:05.633390: step 4159, loss 0.00241284, acc 1
2017-08-08T16:44:06.016300: step 4160, loss 0.0014224, acc 1
2017-08-08T16:44:06.326048: step 4161, loss 0.00105999, acc 1
2017-08-08T16:44:06.543683: step 4162, loss 0.00587857, acc 1
2017-08-08T16:44:06.825739: step 4163, loss 0.00361787, acc 1
2017-08-08T16:44:07.031335: step 4164, loss 0.00180306, acc 1
2017-08-08T16:44:07.314564: step 4165, loss 0.00347666, acc 1
2017-08-08T16:44:07.593013: step 4166, loss 0.00457589, acc 1
2017-08-08T16:44:08.029230: step 4167, loss 0.00078938, acc 1
2017-08-08T16:44:08.453373: step 4168, loss 0.00210073, acc 1
2017-08-08T16:44:08.797655: step 4169, loss 0.0454651, acc 0.96875
2017-08-08T16:44:09.010114: step 4170, loss 0.0117198, acc 1
2017-08-08T16:44:09.273400: step 4171, loss 0.00310706, acc 1
2017-08-08T16:44:09.623335: step 4172, loss 0.0124428, acc 1
2017-08-08T16:44:09.921630: step 4173, loss 0.00852423, acc 1
2017-08-08T16:44:10.230401: step 4174, loss 0.00462109, acc 1
2017-08-08T16:44:10.516242: step 4175, loss 0.00309314, acc 1
2017-08-08T16:44:10.933853: step 4176, loss 0.00711284, acc 1
2017-08-08T16:44:11.233176: step 4177, loss 0.00127077, acc 1
2017-08-08T16:44:11.571322: step 4178, loss 0.00298478, acc 1
2017-08-08T16:44:11.842296: step 4179, loss 0.00679125, acc 1
2017-08-08T16:44:12.050697: step 4180, loss 0.0108508, acc 1
2017-08-08T16:44:12.297376: step 4181, loss 0.0106247, acc 1
2017-08-08T16:44:12.567712: step 4182, loss 0.00500419, acc 1
2017-08-08T16:44:12.790616: step 4183, loss 0.00360618, acc 1
2017-08-08T16:44:12.982958: step 4184, loss 0.00322315, acc 1
2017-08-08T16:44:13.250462: step 4185, loss 0.0344987, acc 0.984375
2017-08-08T16:44:13.608274: step 4186, loss 0.00164645, acc 1
2017-08-08T16:44:13.986632: step 4187, loss 0.0399105, acc 0.984375
2017-08-08T16:44:14.263852: step 4188, loss 0.00136517, acc 1
2017-08-08T16:44:14.564217: step 4189, loss 0.016922, acc 0.984375
2017-08-08T16:44:14.976921: step 4190, loss 0.0100381, acc 1
2017-08-08T16:44:15.258762: step 4191, loss 0.00578751, acc 1
2017-08-08T16:44:15.559610: step 4192, loss 0.00405912, acc 1
2017-08-08T16:44:15.829707: step 4193, loss 0.00498846, acc 1
2017-08-08T16:44:16.264614: step 4194, loss 0.00163755, acc 1
2017-08-08T16:44:16.641433: step 4195, loss 0.00222906, acc 1
2017-08-08T16:44:17.033004: step 4196, loss 0.0200386, acc 0.984375
2017-08-08T16:44:17.276003: step 4197, loss 0.00210162, acc 1
2017-08-08T16:44:17.514344: step 4198, loss 0.00241348, acc 1
2017-08-08T16:44:17.954834: step 4199, loss 0.000700108, acc 1
2017-08-08T16:44:18.252755: step 4200, loss 0.00134302, acc 1

Evaluation:
2017-08-08T16:44:18.994410: step 4200, loss 1.4527, acc 0.721388

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-4200

2017-08-08T16:44:19.705832: step 4201, loss 0.00651367, acc 1
2017-08-08T16:44:20.082151: step 4202, loss 0.00319445, acc 1
2017-08-08T16:44:20.355000: step 4203, loss 0.000725071, acc 1
2017-08-08T16:44:20.560288: step 4204, loss 0.00655056, acc 1
2017-08-08T16:44:20.841009: step 4205, loss 0.00217412, acc 1
2017-08-08T16:44:21.091943: step 4206, loss 0.00413606, acc 1
2017-08-08T16:44:21.345972: step 4207, loss 0.00811179, acc 1
2017-08-08T16:44:21.564145: step 4208, loss 0.000861765, acc 1
2017-08-08T16:44:21.893397: step 4209, loss 0.00443282, acc 1
2017-08-08T16:44:22.189962: step 4210, loss 0.00143347, acc 1
2017-08-08T16:44:22.452423: step 4211, loss 0.00262664, acc 1
2017-08-08T16:44:22.661542: step 4212, loss 0.00125467, acc 1
2017-08-08T16:44:22.833328: step 4213, loss 0.000483401, acc 1
2017-08-08T16:44:23.165371: step 4214, loss 0.00982124, acc 1
2017-08-08T16:44:23.400951: step 4215, loss 0.0235073, acc 0.984375
2017-08-08T16:44:23.618475: step 4216, loss 0.00199159, acc 1
2017-08-08T16:44:23.856855: step 4217, loss 0.00322055, acc 1
2017-08-08T16:44:24.293011: step 4218, loss 0.00512506, acc 1
2017-08-08T16:44:24.681117: step 4219, loss 0.00869122, acc 1
2017-08-08T16:44:25.029392: step 4220, loss 0.00996263, acc 1
2017-08-08T16:44:25.326382: step 4221, loss 0.0147486, acc 0.984375
2017-08-08T16:44:25.586621: step 4222, loss 0.000329409, acc 1
2017-08-08T16:44:26.054030: step 4223, loss 0.000868479, acc 1
2017-08-08T16:44:26.334617: step 4224, loss 0.00143119, acc 1
2017-08-08T16:44:26.650959: step 4225, loss 0.0105056, acc 1
2017-08-08T16:44:26.948339: step 4226, loss 0.0235284, acc 0.984375
2017-08-08T16:44:27.393802: step 4227, loss 0.00103373, acc 1
2017-08-08T16:44:27.789350: step 4228, loss 0.00179729, acc 1
2017-08-08T16:44:28.200247: step 4229, loss 0.00250639, acc 1
2017-08-08T16:44:28.457766: step 4230, loss 0.000541416, acc 1
2017-08-08T16:44:28.693394: step 4231, loss 0.00251606, acc 1
2017-08-08T16:44:29.120668: step 4232, loss 0.00585477, acc 1
2017-08-08T16:44:29.338711: step 4233, loss 0.00461564, acc 1
2017-08-08T16:44:29.582488: step 4234, loss 0.00600452, acc 1
2017-08-08T16:44:29.857349: step 4235, loss 0.0105661, acc 1
2017-08-08T16:44:30.265727: step 4236, loss 0.00248128, acc 1
2017-08-08T16:44:30.621374: step 4237, loss 0.00114857, acc 1
2017-08-08T16:44:30.857897: step 4238, loss 0.00224678, acc 1
2017-08-08T16:44:31.178492: step 4239, loss 0.00541307, acc 1
2017-08-08T16:44:31.532949: step 4240, loss 0.00317347, acc 1
2017-08-08T16:44:31.822886: step 4241, loss 0.0297625, acc 0.96875
2017-08-08T16:44:32.050591: step 4242, loss 0.00192802, acc 1
2017-08-08T16:44:32.331219: step 4243, loss 0.00952105, acc 1
2017-08-08T16:44:32.677988: step 4244, loss 0.00266769, acc 1
2017-08-08T16:44:33.064254: step 4245, loss 0.00489897, acc 1
2017-08-08T16:44:33.241303: step 4246, loss 0.00187041, acc 1
2017-08-08T16:44:33.521000: step 4247, loss 0.013232, acc 0.984375
2017-08-08T16:44:33.938192: step 4248, loss 0.0217338, acc 0.984375
2017-08-08T16:44:34.203318: step 4249, loss 0.0078389, acc 1
2017-08-08T16:44:34.431674: step 4250, loss 0.00688423, acc 1
2017-08-08T16:44:34.678917: step 4251, loss 0.000784282, acc 1
2017-08-08T16:44:34.967983: step 4252, loss 0.0100029, acc 1
2017-08-08T16:44:35.273375: step 4253, loss 0.00194013, acc 1
2017-08-08T16:44:35.637615: step 4254, loss 0.0169617, acc 1
2017-08-08T16:44:35.952625: step 4255, loss 0.00334046, acc 1
2017-08-08T16:44:36.161602: step 4256, loss 0.00455717, acc 1
2017-08-08T16:44:36.549569: step 4257, loss 0.00797746, acc 1
2017-08-08T16:44:36.806698: step 4258, loss 0.00292433, acc 1
2017-08-08T16:44:37.070311: step 4259, loss 0.0014344, acc 1
2017-08-08T16:44:37.348546: step 4260, loss 0.00273293, acc 1
2017-08-08T16:44:37.799116: step 4261, loss 0.00468603, acc 1
2017-08-08T16:44:38.181939: step 4262, loss 0.00267372, acc 1
2017-08-08T16:44:38.534366: step 4263, loss 0.00640758, acc 1
2017-08-08T16:44:38.730158: step 4264, loss 0.00531055, acc 1
2017-08-08T16:44:38.976034: step 4265, loss 0.00312406, acc 1
2017-08-08T16:44:39.287994: step 4266, loss 0.0346835, acc 0.984375
2017-08-08T16:44:39.530211: step 4267, loss 0.002982, acc 1
2017-08-08T16:44:39.820036: step 4268, loss 0.000947082, acc 1
2017-08-08T16:44:40.151444: step 4269, loss 0.000712975, acc 1
2017-08-08T16:44:40.552383: step 4270, loss 0.00258931, acc 1
2017-08-08T16:44:40.861568: step 4271, loss 0.00785948, acc 1
2017-08-08T16:44:41.105541: step 4272, loss 0.00175552, acc 1
2017-08-08T16:44:41.310547: step 4273, loss 0.00280199, acc 1
2017-08-08T16:44:41.588178: step 4274, loss 0.00173161, acc 1
2017-08-08T16:44:41.846052: step 4275, loss 0.0013949, acc 1
2017-08-08T16:44:42.057829: step 4276, loss 0.010884, acc 1
2017-08-08T16:44:42.304540: step 4277, loss 0.00217461, acc 1
2017-08-08T16:44:42.689362: step 4278, loss 0.00461993, acc 1
2017-08-08T16:44:43.123116: step 4279, loss 0.00374514, acc 1
2017-08-08T16:44:43.478928: step 4280, loss 0.00509212, acc 1
2017-08-08T16:44:43.731013: step 4281, loss 0.000928071, acc 1
2017-08-08T16:44:44.001215: step 4282, loss 0.00226039, acc 1
2017-08-08T16:44:44.475565: step 4283, loss 0.0294046, acc 0.984375
2017-08-08T16:44:44.727456: step 4284, loss 0.00484098, acc 1
2017-08-08T16:44:44.946945: step 4285, loss 0.000637295, acc 1
2017-08-08T16:44:45.239758: step 4286, loss 0.00745315, acc 1
2017-08-08T16:44:45.634720: step 4287, loss 0.0214934, acc 0.984375
2017-08-08T16:44:46.009323: step 4288, loss 0.000228159, acc 1
2017-08-08T16:44:46.342796: step 4289, loss 0.00177289, acc 1
2017-08-08T16:44:46.599162: step 4290, loss 0.00216158, acc 1
2017-08-08T16:44:46.980191: step 4291, loss 0.027542, acc 0.984375
2017-08-08T16:44:47.318317: step 4292, loss 0.0189685, acc 0.984375
2017-08-08T16:44:47.607970: step 4293, loss 0.00345113, acc 1
2017-08-08T16:44:47.885154: step 4294, loss 0.0121652, acc 1
2017-08-08T16:44:48.295219: step 4295, loss 0.000468112, acc 1
2017-08-08T16:44:48.697169: step 4296, loss 0.00313084, acc 1
2017-08-08T16:44:49.139217: step 4297, loss 0.00192045, acc 1
2017-08-08T16:44:49.402011: step 4298, loss 0.00819481, acc 1
2017-08-08T16:44:49.636062: step 4299, loss 0.00144057, acc 1
2017-08-08T16:44:50.017394: step 4300, loss 0.00181911, acc 1

Evaluation:
2017-08-08T16:44:50.597163: step 4300, loss 1.48065, acc 0.721388

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-4300

2017-08-08T16:44:51.055220: step 4301, loss 0.00275148, acc 1
2017-08-08T16:44:51.411755: step 4302, loss 0.00100531, acc 1
2017-08-08T16:44:51.717798: step 4303, loss 0.00778319, acc 1
2017-08-08T16:44:51.944858: step 4304, loss 0.00134873, acc 1
2017-08-08T16:44:52.229390: step 4305, loss 0.00546357, acc 1
2017-08-08T16:44:52.478084: step 4306, loss 0.00186029, acc 1
2017-08-08T16:44:52.658039: step 4307, loss 0.0069645, acc 1
2017-08-08T16:44:52.868236: step 4308, loss 0.000972921, acc 1
2017-08-08T16:44:53.103238: step 4309, loss 0.000757241, acc 1
2017-08-08T16:44:53.386859: step 4310, loss 0.00113504, acc 1
2017-08-08T16:44:53.727613: step 4311, loss 0.0711631, acc 0.96875
2017-08-08T16:44:54.063053: step 4312, loss 0.00585236, acc 1
2017-08-08T16:44:54.299098: step 4313, loss 0.00287066, acc 1
2017-08-08T16:44:54.545369: step 4314, loss 0.00145359, acc 1
2017-08-08T16:44:54.829803: step 4315, loss 0.0038058, acc 1
2017-08-08T16:44:55.024141: step 4316, loss 0.0196961, acc 0.984375
2017-08-08T16:44:55.238109: step 4317, loss 0.0178087, acc 0.984375
2017-08-08T16:44:55.468701: step 4318, loss 0.00198582, acc 1
2017-08-08T16:44:55.745377: step 4319, loss 0.00120958, acc 1
2017-08-08T16:44:56.101920: step 4320, loss 0.000736385, acc 1
2017-08-08T16:44:56.438600: step 4321, loss 0.00226505, acc 1
2017-08-08T16:44:56.638248: step 4322, loss 0.00228529, acc 1
2017-08-08T16:44:56.917647: step 4323, loss 0.00599799, acc 1
2017-08-08T16:44:57.169901: step 4324, loss 0.000905089, acc 1
2017-08-08T16:44:57.370598: step 4325, loss 0.0292401, acc 0.984375
2017-08-08T16:44:57.604402: step 4326, loss 0.00186898, acc 1
2017-08-08T16:44:57.823737: step 4327, loss 0.00369321, acc 1
2017-08-08T16:44:58.143780: step 4328, loss 0.00404721, acc 1
2017-08-08T16:44:58.437351: step 4329, loss 0.00875583, acc 1
2017-08-08T16:44:58.736489: step 4330, loss 0.00116313, acc 1
2017-08-08T16:44:58.923626: step 4331, loss 0.00175095, acc 1
2017-08-08T16:44:59.175178: step 4332, loss 0.00260758, acc 1
2017-08-08T16:44:59.469394: step 4333, loss 0.0101674, acc 1
2017-08-08T16:44:59.709798: step 4334, loss 0.0106274, acc 1
2017-08-08T16:44:59.903443: step 4335, loss 0.00113693, acc 1
2017-08-08T16:45:00.229914: step 4336, loss 0.00110594, acc 1
2017-08-08T16:45:00.677974: step 4337, loss 0.0843677, acc 0.984375
2017-08-08T16:45:01.082827: step 4338, loss 0.0194799, acc 0.984375
2017-08-08T16:45:01.384415: step 4339, loss 0.0104919, acc 1
2017-08-08T16:45:01.621615: step 4340, loss 0.00233222, acc 1
2017-08-08T16:45:01.991468: step 4341, loss 0.00183065, acc 1
2017-08-08T16:45:02.453024: step 4342, loss 0.00954615, acc 1
2017-08-08T16:45:02.745843: step 4343, loss 0.000808166, acc 1
2017-08-08T16:45:03.080595: step 4344, loss 0.00147364, acc 1
2017-08-08T16:45:03.401440: step 4345, loss 0.00116656, acc 1
2017-08-08T16:45:03.806930: step 4346, loss 0.00185444, acc 1
2017-08-08T16:45:04.308095: step 4347, loss 0.00571673, acc 1
2017-08-08T16:45:04.658781: step 4348, loss 0.0170623, acc 1
2017-08-08T16:45:04.896589: step 4349, loss 0.0154633, acc 0.984375
2017-08-08T16:45:05.207957: step 4350, loss 0.000981606, acc 1
2017-08-08T16:45:05.516308: step 4351, loss 0.00154034, acc 1
2017-08-08T16:45:05.754713: step 4352, loss 0.00131522, acc 1
2017-08-08T16:45:05.989487: step 4353, loss 0.00297088, acc 1
2017-08-08T16:45:06.336003: step 4354, loss 0.00787944, acc 1
2017-08-08T16:45:06.762816: step 4355, loss 0.000276833, acc 1
2017-08-08T16:45:07.111162: step 4356, loss 0.00150105, acc 1
2017-08-08T16:45:07.370676: step 4357, loss 0.00367664, acc 1
2017-08-08T16:45:07.625455: step 4358, loss 0.015805, acc 0.984375
2017-08-08T16:45:07.932316: step 4359, loss 0.00220248, acc 1
2017-08-08T16:45:08.384314: step 4360, loss 0.00289748, acc 1
2017-08-08T16:45:08.607707: step 4361, loss 0.004168, acc 1
2017-08-08T16:45:08.875111: step 4362, loss 0.00330722, acc 1
2017-08-08T16:45:09.223147: step 4363, loss 0.00278289, acc 1
2017-08-08T16:45:09.567079: step 4364, loss 0.000930338, acc 1
2017-08-08T16:45:09.901319: step 4365, loss 0.00124671, acc 1
2017-08-08T16:45:10.248139: step 4366, loss 0.00152726, acc 1
2017-08-08T16:45:10.469404: step 4367, loss 0.00075896, acc 1
2017-08-08T16:45:10.899747: step 4368, loss 0.000887537, acc 1
2017-08-08T16:45:11.176160: step 4369, loss 0.00123953, acc 1
2017-08-08T16:45:11.436098: step 4370, loss 0.000811231, acc 1
2017-08-08T16:45:11.709553: step 4371, loss 0.00475309, acc 1
2017-08-08T16:45:12.029349: step 4372, loss 0.00228273, acc 1
2017-08-08T16:45:12.445490: step 4373, loss 0.00251614, acc 1
2017-08-08T16:45:12.793275: step 4374, loss 0.00376918, acc 1
2017-08-08T16:45:13.064932: step 4375, loss 0.0054829, acc 1
2017-08-08T16:45:13.312709: step 4376, loss 0.00583268, acc 1
2017-08-08T16:45:13.681833: step 4377, loss 0.00856172, acc 1
2017-08-08T16:45:14.066572: step 4378, loss 0.000339568, acc 1
2017-08-08T16:45:14.361350: step 4379, loss 0.0155419, acc 0.984375
2017-08-08T16:45:14.577566: step 4380, loss 0.00150101, acc 1
2017-08-08T16:45:15.018107: step 4381, loss 0.00231632, acc 1
2017-08-08T16:45:15.398154: step 4382, loss 0.00215832, acc 1
2017-08-08T16:45:15.782263: step 4383, loss 0.000467853, acc 1
2017-08-08T16:45:16.039628: step 4384, loss 0.00193221, acc 1
2017-08-08T16:45:16.275128: step 4385, loss 0.000754976, acc 1
2017-08-08T16:45:16.585305: step 4386, loss 0.0048819, acc 1
2017-08-08T16:45:16.867323: step 4387, loss 0.0092622, acc 1
2017-08-08T16:45:17.100779: step 4388, loss 0.00298166, acc 1
2017-08-08T16:45:17.376362: step 4389, loss 0.00390423, acc 1
2017-08-08T16:45:17.789836: step 4390, loss 0.00632757, acc 1
2017-08-08T16:45:18.199993: step 4391, loss 0.000259243, acc 1
2017-08-08T16:45:18.629727: step 4392, loss 0.00278721, acc 1
2017-08-08T16:45:18.944697: step 4393, loss 0.00180759, acc 1
2017-08-08T16:45:19.193524: step 4394, loss 0.00640405, acc 1
2017-08-08T16:45:19.569389: step 4395, loss 0.00049436, acc 1
2017-08-08T16:45:19.845614: step 4396, loss 0.00185587, acc 1
2017-08-08T16:45:20.107167: step 4397, loss 0.0120014, acc 1
2017-08-08T16:45:20.347511: step 4398, loss 0.00163692, acc 1
2017-08-08T16:45:20.808759: step 4399, loss 0.0120176, acc 0.984375
2017-08-08T16:45:21.253361: step 4400, loss 0.00503568, acc 1

Evaluation:
2017-08-08T16:45:22.017032: step 4400, loss 1.48242, acc 0.727955

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-4400

2017-08-08T16:45:22.577014: step 4401, loss 0.00861876, acc 1
2017-08-08T16:45:22.856692: step 4402, loss 0.000439312, acc 1
2017-08-08T16:45:23.050906: step 4403, loss 0.00232129, acc 1
2017-08-08T16:45:23.250345: step 4404, loss 0.00280436, acc 1
2017-08-08T16:45:23.430570: step 4405, loss 0.000924877, acc 1
2017-08-08T16:45:23.808528: step 4406, loss 0.00100759, acc 1
2017-08-08T16:45:24.305307: step 4407, loss 0.0019027, acc 1
2017-08-08T16:45:24.622238: step 4408, loss 0.00319718, acc 1
2017-08-08T16:45:24.873165: step 4409, loss 0.00981639, acc 1
2017-08-08T16:45:25.193415: step 4410, loss 0.00253578, acc 1
2017-08-08T16:45:25.547015: step 4411, loss 0.000421452, acc 1
2017-08-08T16:45:25.788593: step 4412, loss 0.00104803, acc 1
2017-08-08T16:45:26.027552: step 4413, loss 0.000483693, acc 1
2017-08-08T16:45:26.263730: step 4414, loss 0.00241858, acc 1
2017-08-08T16:45:26.667106: step 4415, loss 0.000928467, acc 1
2017-08-08T16:45:27.009152: step 4416, loss 0.00297218, acc 1
2017-08-08T16:45:27.253420: step 4417, loss 0.00211371, acc 1
2017-08-08T16:45:27.525815: step 4418, loss 0.000197845, acc 1
2017-08-08T16:45:27.836350: step 4419, loss 0.00592689, acc 1
2017-08-08T16:45:28.255781: step 4420, loss 0.00389999, acc 1
2017-08-08T16:45:28.552155: step 4421, loss 0.00228887, acc 1
2017-08-08T16:45:28.784587: step 4422, loss 0.00397345, acc 1
2017-08-08T16:45:29.018007: step 4423, loss 0.000686157, acc 1
2017-08-08T16:45:29.461191: step 4424, loss 0.00505622, acc 1
2017-08-08T16:45:29.810217: step 4425, loss 0.0027166, acc 1
2017-08-08T16:45:30.079352: step 4426, loss 0.00404585, acc 1
2017-08-08T16:45:30.359934: step 4427, loss 0.00243179, acc 1
2017-08-08T16:45:30.613983: step 4428, loss 0.00121178, acc 1
2017-08-08T16:45:30.917224: step 4429, loss 0.00308346, acc 1
2017-08-08T16:45:31.113577: step 4430, loss 0.00313358, acc 1
2017-08-08T16:45:31.367564: step 4431, loss 0.000691886, acc 1
2017-08-08T16:45:31.673200: step 4432, loss 0.000834033, acc 1
2017-08-08T16:45:31.999238: step 4433, loss 0.00169499, acc 1
2017-08-08T16:45:32.207827: step 4434, loss 0.00123606, acc 1
2017-08-08T16:45:32.441410: step 4435, loss 0.00507929, acc 1
2017-08-08T16:45:32.795236: step 4436, loss 0.00332444, acc 1
2017-08-08T16:45:33.033348: step 4437, loss 0.00168104, acc 1
2017-08-08T16:45:33.233021: step 4438, loss 0.00250834, acc 1
2017-08-08T16:45:33.588613: step 4439, loss 0.00487116, acc 1
2017-08-08T16:45:33.886525: step 4440, loss 0.00287095, acc 1
2017-08-08T16:45:34.273851: step 4441, loss 0.00455214, acc 1
2017-08-08T16:45:34.559858: step 4442, loss 0.000667481, acc 1
2017-08-08T16:45:34.812325: step 4443, loss 0.000610259, acc 1
2017-08-08T16:45:35.141346: step 4444, loss 0.00148989, acc 1
2017-08-08T16:45:35.354154: step 4445, loss 0.00111235, acc 1
2017-08-08T16:45:35.605497: step 4446, loss 0.00858445, acc 1
2017-08-08T16:45:35.819977: step 4447, loss 0.00109776, acc 1
2017-08-08T16:45:36.136590: step 4448, loss 0.00865251, acc 1
2017-08-08T16:45:36.433333: step 4449, loss 0.00259694, acc 1
2017-08-08T16:45:36.651856: step 4450, loss 0.00175483, acc 1
2017-08-08T16:45:36.855146: step 4451, loss 0.000679467, acc 1
2017-08-08T16:45:37.146796: step 4452, loss 0.000447165, acc 1
2017-08-08T16:45:37.377924: step 4453, loss 0.00152806, acc 1
2017-08-08T16:45:37.601488: step 4454, loss 0.00210144, acc 1
2017-08-08T16:45:38.140055: step 4455, loss 0.000882456, acc 1
2017-08-08T16:45:38.480168: step 4456, loss 0.00352992, acc 1
2017-08-08T16:45:38.720985: step 4457, loss 0.0027429, acc 1
2017-08-08T16:45:39.125400: step 4458, loss 0.00566913, acc 1
2017-08-08T16:45:39.398401: step 4459, loss 0.00202081, acc 1
2017-08-08T16:45:39.604933: step 4460, loss 0.00360913, acc 1
2017-08-08T16:45:39.860495: step 4461, loss 0.0045474, acc 1
2017-08-08T16:45:40.157994: step 4462, loss 0.0221298, acc 0.984375
2017-08-08T16:45:40.459372: step 4463, loss 0.000529755, acc 1
2017-08-08T16:45:40.796140: step 4464, loss 0.00870344, acc 1
2017-08-08T16:45:41.018142: step 4465, loss 0.00628642, acc 1
2017-08-08T16:45:41.217292: step 4466, loss 0.000900596, acc 1
2017-08-08T16:45:41.537377: step 4467, loss 0.00818656, acc 1
2017-08-08T16:45:41.858501: step 4468, loss 0.00045355, acc 1
2017-08-08T16:45:42.156308: step 4469, loss 0.000947632, acc 1
2017-08-08T16:45:42.439591: step 4470, loss 0.00630403, acc 1
2017-08-08T16:45:42.911381: step 4471, loss 0.00251962, acc 1
2017-08-08T16:45:43.302376: step 4472, loss 0.000708091, acc 1
2017-08-08T16:45:43.544251: step 4473, loss 0.00110087, acc 1
2017-08-08T16:45:43.796584: step 4474, loss 0.00106872, acc 1
2017-08-08T16:45:44.151187: step 4475, loss 0.00710772, acc 1
2017-08-08T16:45:44.417123: step 4476, loss 0.00115359, acc 1
2017-08-08T16:45:44.680338: step 4477, loss 0.005684, acc 1
2017-08-08T16:45:44.912208: step 4478, loss 0.017321, acc 0.984375
2017-08-08T16:45:45.289649: step 4479, loss 0.00320857, acc 1
2017-08-08T16:45:45.608765: step 4480, loss 0.013474, acc 0.984375
2017-08-08T16:45:45.974171: step 4481, loss 0.00196457, acc 1
2017-08-08T16:45:46.270340: step 4482, loss 0.0313261, acc 0.984375
2017-08-08T16:45:46.515690: step 4483, loss 0.000279276, acc 1
2017-08-08T16:45:46.985597: step 4484, loss 0.00150525, acc 1
2017-08-08T16:45:47.268465: step 4485, loss 0.00124579, acc 1
2017-08-08T16:45:47.571914: step 4486, loss 0.00228522, acc 1
2017-08-08T16:45:47.875611: step 4487, loss 0.000424874, acc 1
2017-08-08T16:45:48.352351: step 4488, loss 0.000762216, acc 1
2017-08-08T16:45:48.679776: step 4489, loss 0.0331317, acc 0.984375
2017-08-08T16:45:49.037667: step 4490, loss 0.00061193, acc 1
2017-08-08T16:45:49.254247: step 4491, loss 0.00329689, acc 1
2017-08-08T16:45:49.508231: step 4492, loss 0.00544576, acc 1
2017-08-08T16:45:49.977352: step 4493, loss 0.00115217, acc 1
2017-08-08T16:45:50.277682: step 4494, loss 0.00381136, acc 1
2017-08-08T16:45:50.571356: step 4495, loss 0.00133999, acc 1
2017-08-08T16:45:50.837870: step 4496, loss 0.0418776, acc 0.96875
2017-08-08T16:45:51.269116: step 4497, loss 0.00277259, acc 1
2017-08-08T16:45:51.619275: step 4498, loss 0.00174472, acc 1
2017-08-08T16:45:52.056347: step 4499, loss 0.00571983, acc 1
2017-08-08T16:45:52.303411: step 4500, loss 0.000668824, acc 1

Evaluation:
2017-08-08T16:45:53.263000: step 4500, loss 1.48894, acc 0.728893

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-4500

2017-08-08T16:45:53.760081: step 4501, loss 0.000826514, acc 1
2017-08-08T16:45:54.013216: step 4502, loss 0.00608644, acc 1
2017-08-08T16:45:54.362112: step 4503, loss 0.000842526, acc 1
2017-08-08T16:45:54.803851: step 4504, loss 0.00813885, acc 1
2017-08-08T16:45:55.186656: step 4505, loss 0.00877587, acc 1
2017-08-08T16:45:55.427388: step 4506, loss 0.00610849, acc 1
2017-08-08T16:45:55.627957: step 4507, loss 0.00184335, acc 1
2017-08-08T16:45:55.948279: step 4508, loss 0.000337695, acc 1
2017-08-08T16:45:56.192380: step 4509, loss 0.00399538, acc 1
2017-08-08T16:45:56.489349: step 4510, loss 0.00107884, acc 1
2017-08-08T16:45:56.734937: step 4511, loss 0.00220626, acc 1
2017-08-08T16:45:57.169340: step 4512, loss 0.00213148, acc 1
2017-08-08T16:45:57.547848: step 4513, loss 0.003855, acc 1
2017-08-08T16:45:57.900387: step 4514, loss 0.0021762, acc 1
2017-08-08T16:45:58.145253: step 4515, loss 0.00433862, acc 1
2017-08-08T16:45:58.533481: step 4516, loss 0.00413542, acc 1
2017-08-08T16:45:58.801332: step 4517, loss 0.0609978, acc 0.984375
2017-08-08T16:45:59.057765: step 4518, loss 0.00267534, acc 1
2017-08-08T16:45:59.327902: step 4519, loss 0.00429641, acc 1
2017-08-08T16:45:59.708420: step 4520, loss 0.00107003, acc 1
2017-08-08T16:46:00.149357: step 4521, loss 0.000787526, acc 1
2017-08-08T16:46:00.538484: step 4522, loss 0.00243292, acc 1
2017-08-08T16:46:00.817153: step 4523, loss 0.000516363, acc 1
2017-08-08T16:46:01.067489: step 4524, loss 0.00151885, acc 1
2017-08-08T16:46:01.313433: step 4525, loss 0.0147314, acc 1
2017-08-08T16:46:01.671095: step 4526, loss 0.00632085, acc 1
2017-08-08T16:46:01.933662: step 4527, loss 0.00216011, acc 1
2017-08-08T16:46:02.236640: step 4528, loss 0.000251932, acc 1
2017-08-08T16:46:02.643219: step 4529, loss 0.00378639, acc 1
2017-08-08T16:46:03.090643: step 4530, loss 0.000810396, acc 1
2017-08-08T16:46:03.483237: step 4531, loss 0.002767, acc 1
2017-08-08T16:46:03.754010: step 4532, loss 0.00114512, acc 1
2017-08-08T16:46:04.095753: step 4533, loss 0.000931805, acc 1
2017-08-08T16:46:04.483763: step 4534, loss 0.00683198, acc 1
2017-08-08T16:46:04.722195: step 4535, loss 0.000313554, acc 1
2017-08-08T16:46:05.006749: step 4536, loss 0.000960173, acc 1
2017-08-08T16:46:05.262993: step 4537, loss 0.00142579, acc 1
2017-08-08T16:46:05.705418: step 4538, loss 0.00108837, acc 1
2017-08-08T16:46:06.097398: step 4539, loss 0.000429975, acc 1
2017-08-08T16:46:06.399383: step 4540, loss 0.000974529, acc 1
2017-08-08T16:46:06.656424: step 4541, loss 0.00270135, acc 1
2017-08-08T16:46:07.130803: step 4542, loss 0.00455267, acc 1
2017-08-08T16:46:07.398566: step 4543, loss 0.00146826, acc 1
2017-08-08T16:46:07.688562: step 4544, loss 0.00684118, acc 1
2017-08-08T16:46:08.110309: step 4545, loss 0.00474807, acc 1
2017-08-08T16:46:08.533382: step 4546, loss 0.00386679, acc 1
2017-08-08T16:46:08.861441: step 4547, loss 0.00171882, acc 1
2017-08-08T16:46:09.106454: step 4548, loss 0.00124649, acc 1
2017-08-08T16:46:09.413316: step 4549, loss 0.000934353, acc 1
2017-08-08T16:46:09.653607: step 4550, loss 0.0117645, acc 1
2017-08-08T16:46:09.893877: step 4551, loss 0.00124605, acc 1
2017-08-08T16:46:10.164481: step 4552, loss 0.00166746, acc 1
2017-08-08T16:46:10.708365: step 4553, loss 0.0056973, acc 1
2017-08-08T16:46:11.131036: step 4554, loss 0.00125867, acc 1
2017-08-08T16:46:11.434224: step 4555, loss 0.00454797, acc 1
2017-08-08T16:46:11.729897: step 4556, loss 0.0264152, acc 0.984375
2017-08-08T16:46:12.101357: step 4557, loss 0.000598544, acc 1
2017-08-08T16:46:12.436031: step 4558, loss 0.00222769, acc 1
2017-08-08T16:46:12.694770: step 4559, loss 0.00273751, acc 1
2017-08-08T16:46:12.912205: step 4560, loss 0.0181838, acc 1
2017-08-08T16:46:13.321449: step 4561, loss 0.000431507, acc 1
2017-08-08T16:46:13.736222: step 4562, loss 0.0139127, acc 1
2017-08-08T16:46:14.107639: step 4563, loss 0.0300218, acc 0.984375
2017-08-08T16:46:14.369938: step 4564, loss 0.00254726, acc 1
2017-08-08T16:46:14.609687: step 4565, loss 0.00155187, acc 1
2017-08-08T16:46:15.018806: step 4566, loss 0.0024928, acc 1
2017-08-08T16:46:15.233865: step 4567, loss 0.0264762, acc 0.984375
2017-08-08T16:46:15.480835: step 4568, loss 0.00831878, acc 1
2017-08-08T16:46:15.715955: step 4569, loss 0.0013582, acc 1
2017-08-08T16:46:16.138772: step 4570, loss 0.000897265, acc 1
2017-08-08T16:46:16.451649: step 4571, loss 0.0149631, acc 1
2017-08-08T16:46:16.739272: step 4572, loss 0.00146484, acc 1
2017-08-08T16:46:17.028298: step 4573, loss 0.00142219, acc 1
2017-08-08T16:46:17.257375: step 4574, loss 0.000447859, acc 1
2017-08-08T16:46:17.655977: step 4575, loss 0.00848873, acc 1
2017-08-08T16:46:17.921355: step 4576, loss 0.0015244, acc 1
2017-08-08T16:46:18.236159: step 4577, loss 0.00339301, acc 1
2017-08-08T16:46:18.614527: step 4578, loss 0.0116642, acc 1
2017-08-08T16:46:19.057368: step 4579, loss 0.00574676, acc 1
2017-08-08T16:46:19.505843: step 4580, loss 0.000446713, acc 1
2017-08-08T16:46:19.806961: step 4581, loss 0.00317791, acc 1
2017-08-08T16:46:20.078012: step 4582, loss 0.00466525, acc 1
2017-08-08T16:46:20.550706: step 4583, loss 0.00897042, acc 1
2017-08-08T16:46:20.836455: step 4584, loss 0.00104028, acc 1
2017-08-08T16:46:21.147200: step 4585, loss 0.0038969, acc 1
2017-08-08T16:46:21.457216: step 4586, loss 0.00405848, acc 1
2017-08-08T16:46:21.911586: step 4587, loss 0.00279087, acc 1
2017-08-08T16:46:22.325319: step 4588, loss 0.00210007, acc 1
2017-08-08T16:46:22.640875: step 4589, loss 0.00159808, acc 1
2017-08-08T16:46:22.826047: step 4590, loss 0.00654394, acc 1
2017-08-08T16:46:23.157494: step 4591, loss 0.000800033, acc 1
2017-08-08T16:46:23.429777: step 4592, loss 0.0198033, acc 0.984375
2017-08-08T16:46:23.615509: step 4593, loss 0.00241818, acc 1
2017-08-08T16:46:23.811283: step 4594, loss 0.00138003, acc 1
2017-08-08T16:46:24.128905: step 4595, loss 0.00320266, acc 1
2017-08-08T16:46:24.409371: step 4596, loss 0.0018531, acc 1
2017-08-08T16:46:24.847450: step 4597, loss 0.00140228, acc 1
2017-08-08T16:46:25.190496: step 4598, loss 0.000260717, acc 1
2017-08-08T16:46:25.421692: step 4599, loss 0.00748565, acc 1
2017-08-08T16:46:25.744351: step 4600, loss 0.00220727, acc 1

Evaluation:
2017-08-08T16:46:26.337335: step 4600, loss 1.53236, acc 0.722326

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-4600

2017-08-08T16:46:26.865373: step 4601, loss 0.0020728, acc 1
2017-08-08T16:46:27.289111: step 4602, loss 0.000292439, acc 1
2017-08-08T16:46:27.581416: step 4603, loss 0.00214836, acc 1
2017-08-08T16:46:27.796762: step 4604, loss 0.00112794, acc 1
2017-08-08T16:46:28.139544: step 4605, loss 0.00248284, acc 1
2017-08-08T16:46:28.598442: step 4606, loss 0.0054096, acc 1
2017-08-08T16:46:28.885970: step 4607, loss 0.00694316, acc 1
2017-08-08T16:46:29.378435: step 4608, loss 0.0214169, acc 0.984375
2017-08-08T16:46:29.766172: step 4609, loss 0.0108405, acc 1
2017-08-08T16:46:30.220830: step 4610, loss 0.00517269, acc 1
2017-08-08T16:46:30.572496: step 4611, loss 0.00352784, acc 1
2017-08-08T16:46:30.927045: step 4612, loss 0.0343965, acc 0.984375
2017-08-08T16:46:31.144373: step 4613, loss 0.00333888, acc 1
2017-08-08T16:46:31.510910: step 4614, loss 0.000664989, acc 1
2017-08-08T16:46:31.793100: step 4615, loss 0.00111311, acc 1
2017-08-08T16:46:32.073867: step 4616, loss 0.00146139, acc 1
2017-08-08T16:46:32.311879: step 4617, loss 0.00265473, acc 1
2017-08-08T16:46:32.748616: step 4618, loss 0.0171087, acc 0.984375
2017-08-08T16:46:33.017929: step 4619, loss 0.00098225, acc 1
2017-08-08T16:46:33.341328: step 4620, loss 0.00383227, acc 1
2017-08-08T16:46:33.616872: step 4621, loss 0.00909958, acc 1
2017-08-08T16:46:33.869749: step 4622, loss 0.00410388, acc 1
2017-08-08T16:46:34.305514: step 4623, loss 0.00903648, acc 1
2017-08-08T16:46:34.584233: step 4624, loss 0.00369288, acc 1
2017-08-08T16:46:34.897051: step 4625, loss 0.000663674, acc 1
2017-08-08T16:46:35.208426: step 4626, loss 0.00119352, acc 1
2017-08-08T16:46:35.731693: step 4627, loss 0.00173397, acc 1
2017-08-08T16:46:36.146101: step 4628, loss 0.000338523, acc 1
2017-08-08T16:46:36.481572: step 4629, loss 0.0170135, acc 0.984375
2017-08-08T16:46:36.700376: step 4630, loss 0.00375, acc 1
2017-08-08T16:46:37.051638: step 4631, loss 0.00256139, acc 1
2017-08-08T16:46:37.394947: step 4632, loss 0.00111629, acc 1
2017-08-08T16:46:37.638461: step 4633, loss 0.00616489, acc 1
2017-08-08T16:46:37.947391: step 4634, loss 0.0066006, acc 1
2017-08-08T16:46:38.276075: step 4635, loss 0.0110703, acc 0.984375
2017-08-08T16:46:38.718302: step 4636, loss 0.00260797, acc 1
2017-08-08T16:46:39.049319: step 4637, loss 0.00197305, acc 1
2017-08-08T16:46:39.338560: step 4638, loss 0.00162113, acc 1
2017-08-08T16:46:39.525706: step 4639, loss 0.00254528, acc 1
2017-08-08T16:46:39.774357: step 4640, loss 0.00255941, acc 1
2017-08-08T16:46:40.073343: step 4641, loss 0.00158636, acc 1
2017-08-08T16:46:40.262608: step 4642, loss 0.0182429, acc 0.984375
2017-08-08T16:46:40.459418: step 4643, loss 0.00753548, acc 1
2017-08-08T16:46:40.681792: step 4644, loss 0.000730571, acc 1
2017-08-08T16:46:40.992025: step 4645, loss 0.0117903, acc 1
2017-08-08T16:46:41.302873: step 4646, loss 0.00335701, acc 1
2017-08-08T16:46:41.675482: step 4647, loss 0.00419097, acc 1
2017-08-08T16:46:41.953418: step 4648, loss 0.00233718, acc 1
2017-08-08T16:46:42.241351: step 4649, loss 0.0165673, acc 0.984375
2017-08-08T16:46:42.638567: step 4650, loss 0.00626822, acc 1
2017-08-08T16:46:42.917382: step 4651, loss 0.000889004, acc 1
2017-08-08T16:46:43.202223: step 4652, loss 0.00124234, acc 1
2017-08-08T16:46:43.479423: step 4653, loss 0.000850741, acc 1
2017-08-08T16:46:43.815626: step 4654, loss 0.00176641, acc 1
2017-08-08T16:46:44.265233: step 4655, loss 0.000658082, acc 1
2017-08-08T16:46:44.632850: step 4656, loss 0.00100507, acc 1
2017-08-08T16:46:44.898249: step 4657, loss 0.0299723, acc 0.984375
2017-08-08T16:46:45.236382: step 4658, loss 0.00126137, acc 1
2017-08-08T16:46:45.609167: step 4659, loss 0.000870269, acc 1
2017-08-08T16:46:45.898090: step 4660, loss 0.00148939, acc 1
2017-08-08T16:46:46.156848: step 4661, loss 0.000731195, acc 1
2017-08-08T16:46:46.360454: step 4662, loss 0.00218869, acc 1
2017-08-08T16:46:46.665319: step 4663, loss 0.00615252, acc 1
2017-08-08T16:46:46.937320: step 4664, loss 0.00158135, acc 1
2017-08-08T16:46:47.287842: step 4665, loss 0.00578208, acc 1
2017-08-08T16:46:47.587456: step 4666, loss 0.00175393, acc 1
2017-08-08T16:46:48.160746: step 4667, loss 0.00174827, acc 1
2017-08-08T16:46:48.504175: step 4668, loss 0.00129599, acc 1
2017-08-08T16:46:48.790554: step 4669, loss 0.000896457, acc 1
2017-08-08T16:46:49.038169: step 4670, loss 0.00110447, acc 1
2017-08-08T16:46:49.337304: step 4671, loss 0.00294595, acc 1
2017-08-08T16:46:49.726949: step 4672, loss 0.00130074, acc 1
2017-08-08T16:46:50.034159: step 4673, loss 0.0006525, acc 1
2017-08-08T16:46:50.314114: step 4674, loss 0.0018274, acc 1
2017-08-08T16:46:50.541438: step 4675, loss 0.00105271, acc 1
2017-08-08T16:46:50.965449: step 4676, loss 0.0391877, acc 0.984375
2017-08-08T16:46:51.249292: step 4677, loss 0.00196657, acc 1
2017-08-08T16:46:51.471834: step 4678, loss 0.00460148, acc 1
2017-08-08T16:46:51.678239: step 4679, loss 0.000884325, acc 1
2017-08-08T16:46:52.063164: step 4680, loss 0.00125269, acc 1
2017-08-08T16:46:52.421476: step 4681, loss 0.0122916, acc 0.984375
2017-08-08T16:46:52.643089: step 4682, loss 0.00329471, acc 1
2017-08-08T16:46:52.853714: step 4683, loss 0.00793894, acc 1
2017-08-08T16:46:53.137342: step 4684, loss 0.0109605, acc 1
2017-08-08T16:46:53.442068: step 4685, loss 0.000799385, acc 1
2017-08-08T16:46:53.721688: step 4686, loss 0.00182319, acc 1
2017-08-08T16:46:54.004884: step 4687, loss 0.00119751, acc 1
2017-08-08T16:46:54.361384: step 4688, loss 0.000949866, acc 1
2017-08-08T16:46:54.710057: step 4689, loss 0.00112113, acc 1
2017-08-08T16:46:55.080925: step 4690, loss 0.00431794, acc 1
2017-08-08T16:46:55.396211: step 4691, loss 0.0220792, acc 0.984375
2017-08-08T16:46:55.612806: step 4692, loss 0.00134182, acc 1
2017-08-08T16:46:55.971235: step 4693, loss 0.0025032, acc 1
2017-08-08T16:46:56.306347: step 4694, loss 0.00254843, acc 1
2017-08-08T16:46:56.516810: step 4695, loss 0.000466061, acc 1
2017-08-08T16:46:56.733409: step 4696, loss 0.00157939, acc 1
2017-08-08T16:46:57.005434: step 4697, loss 0.00242365, acc 1
2017-08-08T16:46:57.288331: step 4698, loss 0.00493753, acc 1
2017-08-08T16:46:57.545536: step 4699, loss 0.00119485, acc 1
2017-08-08T16:46:57.824012: step 4700, loss 0.00746029, acc 1

Evaluation:
2017-08-08T16:46:58.503551: step 4700, loss 1.5457, acc 0.727017

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-4700

2017-08-08T16:46:59.018680: step 4701, loss 0.00183106, acc 1
2017-08-08T16:46:59.287195: step 4702, loss 0.00534437, acc 1
2017-08-08T16:46:59.633383: step 4703, loss 0.00880159, acc 1
2017-08-08T16:47:00.101358: step 4704, loss 0.0417464, acc 0.984375
2017-08-08T16:47:00.565631: step 4705, loss 0.00227756, acc 1
2017-08-08T16:47:00.847321: step 4706, loss 0.0524954, acc 0.96875
2017-08-08T16:47:01.067298: step 4707, loss 0.00115397, acc 1
2017-08-08T16:47:01.320722: step 4708, loss 0.00438172, acc 1
2017-08-08T16:47:01.737630: step 4709, loss 0.00615102, acc 1
2017-08-08T16:47:02.059315: step 4710, loss 0.00336889, acc 1
2017-08-08T16:47:02.456311: step 4711, loss 0.00180076, acc 1
2017-08-08T16:47:02.739392: step 4712, loss 0.0161562, acc 0.984375
2017-08-08T16:47:03.157959: step 4713, loss 0.0029769, acc 1
2017-08-08T16:47:03.617364: step 4714, loss 0.00277711, acc 1
2017-08-08T16:47:03.981608: step 4715, loss 0.0435115, acc 0.984375
2017-08-08T16:47:04.264917: step 4716, loss 0.00251058, acc 1
2017-08-08T16:47:04.524681: step 4717, loss 0.00400534, acc 1
2017-08-08T16:47:05.002666: step 4718, loss 0.00108844, acc 1
2017-08-08T16:47:05.302349: step 4719, loss 0.00261103, acc 1
2017-08-08T16:47:05.607558: step 4720, loss 0.00032176, acc 1
2017-08-08T16:47:05.893225: step 4721, loss 0.000482558, acc 1
2017-08-08T16:47:06.254833: step 4722, loss 0.000390972, acc 1
2017-08-08T16:47:06.698791: step 4723, loss 0.0048081, acc 1
2017-08-08T16:47:07.205443: step 4724, loss 0.00301185, acc 1
2017-08-08T16:47:07.435618: step 4725, loss 0.00104624, acc 1
2017-08-08T16:47:07.677189: step 4726, loss 0.00172703, acc 1
2017-08-08T16:47:08.088423: step 4727, loss 0.00340115, acc 1
2017-08-08T16:47:08.358219: step 4728, loss 0.00213583, acc 1
2017-08-08T16:47:08.636231: step 4729, loss 0.00313628, acc 1
2017-08-08T16:47:08.933495: step 4730, loss 0.000524456, acc 1
2017-08-08T16:47:09.247622: step 4731, loss 0.00128862, acc 1
2017-08-08T16:47:09.584544: step 4732, loss 0.000251147, acc 1
2017-08-08T16:47:10.023833: step 4733, loss 0.00181003, acc 1
2017-08-08T16:47:10.234254: step 4734, loss 0.0134459, acc 1
2017-08-08T16:47:10.496867: step 4735, loss 0.00115693, acc 1
2017-08-08T16:47:10.821545: step 4736, loss 0.000381241, acc 1
2017-08-08T16:47:11.128261: step 4737, loss 0.000406041, acc 1
2017-08-08T16:47:11.360349: step 4738, loss 0.00136141, acc 1
2017-08-08T16:47:11.662322: step 4739, loss 0.00138089, acc 1
2017-08-08T16:47:12.047716: step 4740, loss 0.0043451, acc 1
2017-08-08T16:47:12.419509: step 4741, loss 0.000186293, acc 1
2017-08-08T16:47:12.639886: step 4742, loss 0.00354044, acc 1
2017-08-08T16:47:12.963970: step 4743, loss 0.00361735, acc 1
2017-08-08T16:47:13.313451: step 4744, loss 0.000428664, acc 1
2017-08-08T16:47:13.576735: step 4745, loss 0.000184553, acc 1
2017-08-08T16:47:13.851457: step 4746, loss 0.00390903, acc 1
2017-08-08T16:47:14.218120: step 4747, loss 0.00118338, acc 1
2017-08-08T16:47:14.661373: step 4748, loss 0.00480139, acc 1
2017-08-08T16:47:15.001684: step 4749, loss 0.00262744, acc 1
2017-08-08T16:47:15.287100: step 4750, loss 0.0152963, acc 0.984375
2017-08-08T16:47:15.565112: step 4751, loss 0.00265475, acc 1
2017-08-08T16:47:16.054999: step 4752, loss 0.00126176, acc 1
2017-08-08T16:47:16.305490: step 4753, loss 0.00220763, acc 1
2017-08-08T16:47:16.572179: step 4754, loss 0.000404259, acc 1
2017-08-08T16:47:16.823307: step 4755, loss 0.000752106, acc 1
2017-08-08T16:47:17.299486: step 4756, loss 0.00135657, acc 1
2017-08-08T16:47:17.729377: step 4757, loss 0.00155519, acc 1
2017-08-08T16:47:18.026729: step 4758, loss 0.000786868, acc 1
2017-08-08T16:47:18.272756: step 4759, loss 0.00145054, acc 1
2017-08-08T16:47:18.584523: step 4760, loss 0.00364327, acc 1
2017-08-08T16:47:18.879938: step 4761, loss 0.00218554, acc 1
2017-08-08T16:47:19.145882: step 4762, loss 0.000958554, acc 1
2017-08-08T16:47:19.442862: step 4763, loss 0.00163946, acc 1
2017-08-08T16:47:19.891449: step 4764, loss 0.00087797, acc 1
2017-08-08T16:47:20.271590: step 4765, loss 0.00243682, acc 1
2017-08-08T16:47:20.662524: step 4766, loss 0.00489672, acc 1
2017-08-08T16:47:20.920236: step 4767, loss 0.000116016, acc 1
2017-08-08T16:47:21.155731: step 4768, loss 0.00062134, acc 1
2017-08-08T16:47:21.584525: step 4769, loss 0.000626649, acc 1
2017-08-08T16:47:21.889725: step 4770, loss 0.000794703, acc 1
2017-08-08T16:47:22.181792: step 4771, loss 0.00531546, acc 1
2017-08-08T16:47:22.388186: step 4772, loss 0.0300171, acc 0.984375
2017-08-08T16:47:22.681339: step 4773, loss 0.0323029, acc 0.984375
2017-08-08T16:47:23.080420: step 4774, loss 0.00222589, acc 1
2017-08-08T16:47:23.389186: step 4775, loss 0.000487875, acc 1
2017-08-08T16:47:23.628026: step 4776, loss 0.00130034, acc 1
2017-08-08T16:47:23.899864: step 4777, loss 0.00714492, acc 1
2017-08-08T16:47:24.356657: step 4778, loss 0.00634348, acc 1
2017-08-08T16:47:24.650201: step 4779, loss 0.00397355, acc 1
2017-08-08T16:47:24.887388: step 4780, loss 0.00185931, acc 1
2017-08-08T16:47:25.111595: step 4781, loss 0.00058869, acc 1
2017-08-08T16:47:25.479939: step 4782, loss 0.00105431, acc 1
2017-08-08T16:47:25.773305: step 4783, loss 0.00114694, acc 1
2017-08-08T16:47:26.023188: step 4784, loss 0.000819908, acc 1
2017-08-08T16:47:26.225483: step 4785, loss 0.0855701, acc 0.984375
2017-08-08T16:47:26.490728: step 4786, loss 0.00857389, acc 1
2017-08-08T16:47:26.840765: step 4787, loss 0.0040452, acc 1
2017-08-08T16:47:27.026214: step 4788, loss 0.0108381, acc 1
2017-08-08T16:47:27.263310: step 4789, loss 0.000370542, acc 1
2017-08-08T16:47:27.478090: step 4790, loss 0.000658147, acc 1
2017-08-08T16:47:27.749384: step 4791, loss 0.00152889, acc 1
2017-08-08T16:47:28.099065: step 4792, loss 0.00235406, acc 1
2017-08-08T16:47:28.402532: step 4793, loss 0.00144714, acc 1
2017-08-08T16:47:28.596024: step 4794, loss 0.000970398, acc 1
2017-08-08T16:47:28.901531: step 4795, loss 0.00168267, acc 1
2017-08-08T16:47:29.203640: step 4796, loss 0.00311444, acc 1
2017-08-08T16:47:29.421967: step 4797, loss 0.000681078, acc 1
2017-08-08T16:47:29.694058: step 4798, loss 0.00377245, acc 1
2017-08-08T16:47:30.066407: step 4799, loss 0.00574647, acc 1
2017-08-08T16:47:30.557340: step 4800, loss 0.000745499, acc 1

Evaluation:
2017-08-08T16:47:31.124064: step 4800, loss 1.58215, acc 0.724203

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-4800

2017-08-08T16:47:31.610987: step 4801, loss 0.00420029, acc 1
2017-08-08T16:47:31.872588: step 4802, loss 6.58685e-05, acc 1
2017-08-08T16:47:32.115169: step 4803, loss 0.00110892, acc 1
2017-08-08T16:47:32.436855: step 4804, loss 0.00167207, acc 1
2017-08-08T16:47:32.777513: step 4805, loss 0.000869504, acc 1
2017-08-08T16:47:33.134031: step 4806, loss 0.000609937, acc 1
2017-08-08T16:47:33.377602: step 4807, loss 0.0330841, acc 0.984375
2017-08-08T16:47:33.628168: step 4808, loss 0.000322278, acc 1
2017-08-08T16:47:34.068096: step 4809, loss 0.00379001, acc 1
2017-08-08T16:47:34.315105: step 4810, loss 0.0186495, acc 0.984375
2017-08-08T16:47:34.571864: step 4811, loss 0.00412599, acc 1
2017-08-08T16:47:34.856095: step 4812, loss 0.0201913, acc 0.984375
2017-08-08T16:47:35.284023: step 4813, loss 0.000381437, acc 1
2017-08-08T16:47:35.607242: step 4814, loss 0.0022374, acc 1
2017-08-08T16:47:35.925238: step 4815, loss 0.00269008, acc 1
2017-08-08T16:47:36.149152: step 4816, loss 0.00172354, acc 1
2017-08-08T16:47:36.548394: step 4817, loss 0.000259004, acc 1
2017-08-08T16:47:36.824457: step 4818, loss 0.000728248, acc 1
2017-08-08T16:47:37.039570: step 4819, loss 0.000677132, acc 1
2017-08-08T16:47:37.303593: step 4820, loss 0.00715763, acc 1
2017-08-08T16:47:37.777486: step 4821, loss 0.000203789, acc 1
2017-08-08T16:47:38.213851: step 4822, loss 0.00118545, acc 1
2017-08-08T16:47:38.658919: step 4823, loss 0.000317665, acc 1
2017-08-08T16:47:38.986682: step 4824, loss 0.000740946, acc 1
2017-08-08T16:47:39.272859: step 4825, loss 0.0024036, acc 1
2017-08-08T16:47:39.672612: step 4826, loss 0.000757175, acc 1
2017-08-08T16:47:39.936998: step 4827, loss 0.0168713, acc 0.984375
2017-08-08T16:47:40.242588: step 4828, loss 0.003588, acc 1
2017-08-08T16:47:40.534751: step 4829, loss 0.00726209, acc 1
2017-08-08T16:47:40.876747: step 4830, loss 0.00139165, acc 1
2017-08-08T16:47:41.140285: step 4831, loss 0.000962546, acc 1
2017-08-08T16:47:41.468579: step 4832, loss 0.00388582, acc 1
2017-08-08T16:47:41.760651: step 4833, loss 0.00418171, acc 1
2017-08-08T16:47:41.987328: step 4834, loss 0.0112167, acc 1
2017-08-08T16:47:42.328581: step 4835, loss 0.0348408, acc 0.984375
2017-08-08T16:47:42.669592: step 4836, loss 0.0103722, acc 1
2017-08-08T16:47:42.946952: step 4837, loss 0.00214676, acc 1
2017-08-08T16:47:43.160141: step 4838, loss 0.00095829, acc 1
2017-08-08T16:47:43.543539: step 4839, loss 0.00079509, acc 1
2017-08-08T16:47:43.870624: step 4840, loss 0.00361804, acc 1
2017-08-08T16:47:44.161331: step 4841, loss 0.00202815, acc 1
2017-08-08T16:47:44.433175: step 4842, loss 0.000427685, acc 1
2017-08-08T16:47:44.682989: step 4843, loss 0.00190725, acc 1
2017-08-08T16:47:45.004583: step 4844, loss 0.000489487, acc 1
2017-08-08T16:47:45.407337: step 4845, loss 0.00374907, acc 1
2017-08-08T16:47:45.676469: step 4846, loss 0.00527347, acc 1
2017-08-08T16:47:45.988888: step 4847, loss 0.000743465, acc 1
2017-08-08T16:47:46.305240: step 4848, loss 0.000981943, acc 1
2017-08-08T16:47:46.690963: step 4849, loss 0.00267676, acc 1
2017-08-08T16:47:47.095014: step 4850, loss 0.000656165, acc 1
2017-08-08T16:47:47.367791: step 4851, loss 0.000567803, acc 1
2017-08-08T16:47:47.632124: step 4852, loss 0.00172645, acc 1
2017-08-08T16:47:47.846882: step 4853, loss 0.00166754, acc 1
2017-08-08T16:47:48.190375: step 4854, loss 0.00200295, acc 1
2017-08-08T16:47:48.563505: step 4855, loss 0.00182243, acc 1
2017-08-08T16:47:48.784637: step 4856, loss 0.00275984, acc 1
2017-08-08T16:47:49.085864: step 4857, loss 0.000346033, acc 1
2017-08-08T16:47:49.372295: step 4858, loss 0.00241958, acc 1
2017-08-08T16:47:49.677379: step 4859, loss 0.00235151, acc 1
2017-08-08T16:47:50.093313: step 4860, loss 0.00568493, acc 1
2017-08-08T16:47:50.423833: step 4861, loss 0.00592431, acc 1
2017-08-08T16:47:50.670335: step 4862, loss 0.00326118, acc 1
2017-08-08T16:47:50.990565: step 4863, loss 0.00569511, acc 1
2017-08-08T16:47:51.337662: step 4864, loss 0.0113345, acc 1
2017-08-08T16:47:51.626193: step 4865, loss 0.00354145, acc 1
2017-08-08T16:47:51.876866: step 4866, loss 0.00804531, acc 1
2017-08-08T16:47:52.091650: step 4867, loss 0.00109158, acc 1
2017-08-08T16:47:52.494881: step 4868, loss 0.00520996, acc 1
2017-08-08T16:47:52.791913: step 4869, loss 0.000650191, acc 1
2017-08-08T16:47:53.050789: step 4870, loss 0.000698677, acc 1
2017-08-08T16:47:53.248236: step 4871, loss 0.000748669, acc 1
2017-08-08T16:47:53.503112: step 4872, loss 0.00206384, acc 1
2017-08-08T16:47:53.714796: step 4873, loss 0.000864305, acc 1
2017-08-08T16:47:53.968148: step 4874, loss 0.000964174, acc 1
2017-08-08T16:47:54.235557: step 4875, loss 0.000858844, acc 1
2017-08-08T16:47:54.581362: step 4876, loss 0.000622886, acc 1
2017-08-08T16:47:55.068110: step 4877, loss 0.00056579, acc 1
2017-08-08T16:47:55.409802: step 4878, loss 0.00457505, acc 1
2017-08-08T16:47:55.665609: step 4879, loss 0.00316256, acc 1
2017-08-08T16:47:55.906000: step 4880, loss 0.00192678, acc 1
2017-08-08T16:47:56.313444: step 4881, loss 0.000399989, acc 1
2017-08-08T16:47:56.596314: step 4882, loss 0.000735249, acc 1
2017-08-08T16:47:56.793732: step 4883, loss 0.00100762, acc 1
2017-08-08T16:47:57.023548: step 4884, loss 0.00388334, acc 1
2017-08-08T16:47:57.294355: step 4885, loss 0.00114098, acc 1
2017-08-08T16:47:57.590056: step 4886, loss 0.00734469, acc 1
2017-08-08T16:47:57.889462: step 4887, loss 0.00118824, acc 1
2017-08-08T16:47:58.195954: step 4888, loss 0.00461751, acc 1
2017-08-08T16:47:58.411954: step 4889, loss 0.0150357, acc 0.984375
2017-08-08T16:47:58.638858: step 4890, loss 0.00304421, acc 1
2017-08-08T16:47:59.089357: step 4891, loss 0.000443086, acc 1
2017-08-08T16:47:59.396294: step 4892, loss 0.000286424, acc 1
2017-08-08T16:47:59.672236: step 4893, loss 0.0050504, acc 1
2017-08-08T16:47:59.940253: step 4894, loss 0.000329391, acc 1
2017-08-08T16:48:00.309839: step 4895, loss 0.000477152, acc 1
2017-08-08T16:48:00.781355: step 4896, loss 0.00318227, acc 1
2017-08-08T16:48:01.171711: step 4897, loss 0.00358903, acc 1
2017-08-08T16:48:01.491810: step 4898, loss 0.0021487, acc 1
2017-08-08T16:48:01.723099: step 4899, loss 0.000848265, acc 1
2017-08-08T16:48:02.121390: step 4900, loss 0.000223403, acc 1

Evaluation:
2017-08-08T16:48:02.962580: step 4900, loss 1.61594, acc 0.718574

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-4900

2017-08-08T16:48:03.601398: step 4901, loss 0.0151519, acc 0.984375
2017-08-08T16:48:04.101951: step 4902, loss 0.00312778, acc 1
2017-08-08T16:48:04.440007: step 4903, loss 0.00204796, acc 1
2017-08-08T16:48:04.658935: step 4904, loss 0.000490615, acc 1
2017-08-08T16:48:04.989347: step 4905, loss 0.00398987, acc 1
2017-08-08T16:48:05.250521: step 4906, loss 0.00207824, acc 1
2017-08-08T16:48:05.446378: step 4907, loss 0.00250852, acc 1
2017-08-08T16:48:05.668160: step 4908, loss 0.00784978, acc 1
2017-08-08T16:48:05.959607: step 4909, loss 0.00127213, acc 1
2017-08-08T16:48:06.363112: step 4910, loss 0.000735447, acc 1
2017-08-08T16:48:06.713352: step 4911, loss 0.00103371, acc 1
2017-08-08T16:48:06.957220: step 4912, loss 0.00441787, acc 1
2017-08-08T16:48:07.155440: step 4913, loss 0.0143465, acc 1
2017-08-08T16:48:07.401320: step 4914, loss 0.000891908, acc 1
2017-08-08T16:48:07.692356: step 4915, loss 0.0012826, acc 1
2017-08-08T16:48:07.870996: step 4916, loss 0.000940959, acc 1
2017-08-08T16:48:08.097778: step 4917, loss 0.0159353, acc 0.984375
2017-08-08T16:48:08.292999: step 4918, loss 0.00294829, acc 1
2017-08-08T16:48:08.575565: step 4919, loss 0.00151603, acc 1
2017-08-08T16:48:08.835071: step 4920, loss 0.00158052, acc 1
2017-08-08T16:48:09.113272: step 4921, loss 0.0130303, acc 0.984375
2017-08-08T16:48:09.373713: step 4922, loss 0.0008634, acc 1
2017-08-08T16:48:09.572340: step 4923, loss 0.000724722, acc 1
2017-08-08T16:48:09.833321: step 4924, loss 0.00417689, acc 1
2017-08-08T16:48:10.100001: step 4925, loss 0.000941338, acc 1
2017-08-08T16:48:10.309365: step 4926, loss 0.000557603, acc 1
2017-08-08T16:48:10.741651: step 4927, loss 0.00201787, acc 1
2017-08-08T16:48:11.014466: step 4928, loss 0.00429949, acc 1
2017-08-08T16:48:11.320659: step 4929, loss 0.000301406, acc 1
2017-08-08T16:48:11.632589: step 4930, loss 0.000964507, acc 1
2017-08-08T16:48:11.869036: step 4931, loss 0.00531391, acc 1
2017-08-08T16:48:12.363613: step 4932, loss 0.00223895, acc 1
2017-08-08T16:48:12.631796: step 4933, loss 0.0022022, acc 1
2017-08-08T16:48:12.980975: step 4934, loss 0.00409378, acc 1
2017-08-08T16:48:13.358198: step 4935, loss 0.00062524, acc 1
2017-08-08T16:48:13.649021: step 4936, loss 0.000323817, acc 1
2017-08-08T16:48:13.980641: step 4937, loss 0.00293749, acc 1
2017-08-08T16:48:14.329567: step 4938, loss 0.0014512, acc 1
2017-08-08T16:48:14.746254: step 4939, loss 0.00262644, acc 1
2017-08-08T16:48:14.992166: step 4940, loss 0.00274281, acc 1
2017-08-08T16:48:15.264169: step 4941, loss 0.00214933, acc 1
2017-08-08T16:48:15.636058: step 4942, loss 0.000951893, acc 1
2017-08-08T16:48:15.918691: step 4943, loss 0.000139609, acc 1
2017-08-08T16:48:16.308837: step 4944, loss 0.00651923, acc 1
2017-08-08T16:48:16.661996: step 4945, loss 0.0010267, acc 1
2017-08-08T16:48:16.917682: step 4946, loss 0.0141407, acc 1
2017-08-08T16:48:17.326046: step 4947, loss 0.00929623, acc 1
2017-08-08T16:48:17.600556: step 4948, loss 0.00433437, acc 1
2017-08-08T16:48:17.957392: step 4949, loss 0.000647516, acc 1
2017-08-08T16:48:18.237299: step 4950, loss 0.00108939, acc 1
2017-08-08T16:48:18.445404: step 4951, loss 0.000730141, acc 1
2017-08-08T16:48:18.874684: step 4952, loss 0.000728203, acc 1
2017-08-08T16:48:19.148327: step 4953, loss 0.00119556, acc 1
2017-08-08T16:48:19.498009: step 4954, loss 0.00827505, acc 1
2017-08-08T16:48:19.869283: step 4955, loss 0.000495278, acc 1
2017-08-08T16:48:20.244652: step 4956, loss 0.0014134, acc 1
2017-08-08T16:48:20.626048: step 4957, loss 0.000772883, acc 1
2017-08-08T16:48:20.893565: step 4958, loss 0.00612878, acc 1
2017-08-08T16:48:21.316200: step 4959, loss 0.00330524, acc 1
2017-08-08T16:48:21.591123: step 4960, loss 0.000540098, acc 1
2017-08-08T16:48:21.847985: step 4961, loss 0.00163668, acc 1
2017-08-08T16:48:22.210073: step 4962, loss 0.000604235, acc 1
2017-08-08T16:48:22.393854: step 4963, loss 0.00176678, acc 1
2017-08-08T16:48:22.696979: step 4964, loss 0.000842478, acc 1
2017-08-08T16:48:22.969597: step 4965, loss 0.0013453, acc 1
2017-08-08T16:48:23.362983: step 4966, loss 0.00135356, acc 1
2017-08-08T16:48:23.797355: step 4967, loss 0.000559642, acc 1
2017-08-08T16:48:24.099467: step 4968, loss 0.0300808, acc 0.984375
2017-08-08T16:48:24.367287: step 4969, loss 0.000172067, acc 1
2017-08-08T16:48:24.579248: step 4970, loss 0.000659346, acc 1
2017-08-08T16:48:24.940715: step 4971, loss 0.00173191, acc 1
2017-08-08T16:48:25.246319: step 4972, loss 0.00605157, acc 1
2017-08-08T16:48:25.543812: step 4973, loss 0.00117396, acc 1
2017-08-08T16:48:25.840979: step 4974, loss 0.00116056, acc 1
2017-08-08T16:48:26.246696: step 4975, loss 0.00046209, acc 1
2017-08-08T16:48:26.651227: step 4976, loss 0.00244772, acc 1
2017-08-08T16:48:27.069620: step 4977, loss 0.00320093, acc 1
2017-08-08T16:48:27.303661: step 4978, loss 0.000866073, acc 1
2017-08-08T16:48:27.540296: step 4979, loss 0.000632217, acc 1
2017-08-08T16:48:27.878792: step 4980, loss 0.0016146, acc 1
2017-08-08T16:48:28.145315: step 4981, loss 0.00303853, acc 1
2017-08-08T16:48:28.397481: step 4982, loss 0.00265535, acc 1
2017-08-08T16:48:28.660898: step 4983, loss 0.00365946, acc 1
2017-08-08T16:48:28.968148: step 4984, loss 0.00173311, acc 1
2017-08-08T16:48:29.339446: step 4985, loss 0.000217599, acc 1
2017-08-08T16:48:29.646247: step 4986, loss 0.0314754, acc 0.984375
2017-08-08T16:48:29.916413: step 4987, loss 0.00109572, acc 1
2017-08-08T16:48:30.155142: step 4988, loss 0.0095668, acc 1
2017-08-08T16:48:30.509220: step 4989, loss 0.00336974, acc 1
2017-08-08T16:48:30.724300: step 4990, loss 0.00648729, acc 1
2017-08-08T16:48:30.989829: step 4991, loss 0.0188087, acc 0.984375
2017-08-08T16:48:31.225112: step 4992, loss 0.000543624, acc 1
2017-08-08T16:48:31.582396: step 4993, loss 0.0826621, acc 0.984375
2017-08-08T16:48:31.989728: step 4994, loss 0.00785119, acc 1
2017-08-08T16:48:32.384256: step 4995, loss 0.000554307, acc 1
2017-08-08T16:48:32.673252: step 4996, loss 0.00134705, acc 1
2017-08-08T16:48:32.876270: step 4997, loss 0.000479047, acc 1
2017-08-08T16:48:33.277369: step 4998, loss 0.000508801, acc 1
2017-08-08T16:48:33.581648: step 4999, loss 0.0045052, acc 1
2017-08-08T16:48:33.859043: step 5000, loss 0.000832832, acc 1

Evaluation:
2017-08-08T16:48:34.761494: step 5000, loss 1.6137, acc 0.724203

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-5000

2017-08-08T16:48:35.352462: step 5001, loss 0.00543932, acc 1
2017-08-08T16:48:35.626339: step 5002, loss 0.000761167, acc 1
2017-08-08T16:48:35.925015: step 5003, loss 0.00145998, acc 1
2017-08-08T16:48:36.371861: step 5004, loss 0.00105308, acc 1
2017-08-08T16:48:36.632712: step 5005, loss 0.000447245, acc 1
2017-08-08T16:48:36.933765: step 5006, loss 0.0106023, acc 1
2017-08-08T16:48:37.173313: step 5007, loss 0.00153908, acc 1
2017-08-08T16:48:37.499987: step 5008, loss 0.000332545, acc 1
2017-08-08T16:48:37.987800: step 5009, loss 0.0121708, acc 1
2017-08-08T16:48:38.300627: step 5010, loss 0.000259693, acc 1
2017-08-08T16:48:38.538573: step 5011, loss 0.00107675, acc 1
2017-08-08T16:48:38.741323: step 5012, loss 0.00612124, acc 1
2017-08-08T16:48:39.061715: step 5013, loss 0.0124125, acc 0.984375
2017-08-08T16:48:39.276879: step 5014, loss 0.00131219, acc 1
2017-08-08T16:48:39.492258: step 5015, loss 0.000353476, acc 1
2017-08-08T16:48:39.765054: step 5016, loss 0.00136825, acc 1
2017-08-08T16:48:40.167303: step 5017, loss 0.000351049, acc 1
2017-08-08T16:48:40.486923: step 5018, loss 0.00244549, acc 1
2017-08-08T16:48:40.782391: step 5019, loss 0.00113489, acc 1
2017-08-08T16:48:41.061655: step 5020, loss 0.00127479, acc 1
2017-08-08T16:48:41.357982: step 5021, loss 0.00110673, acc 1
2017-08-08T16:48:41.676058: step 5022, loss 0.00467036, acc 1
2017-08-08T16:48:41.910962: step 5023, loss 0.00198688, acc 1
2017-08-08T16:48:42.125805: step 5024, loss 0.00129418, acc 1
2017-08-08T16:48:42.427296: step 5025, loss 0.00722713, acc 1
2017-08-08T16:48:42.777645: step 5026, loss 0.0106143, acc 1
2017-08-08T16:48:43.173665: step 5027, loss 0.0021832, acc 1
2017-08-08T16:48:43.464457: step 5028, loss 0.00300381, acc 1
2017-08-08T16:48:43.681711: step 5029, loss 0.00119041, acc 1
2017-08-08T16:48:44.081158: step 5030, loss 0.00201655, acc 1
2017-08-08T16:48:44.291847: step 5031, loss 0.000445009, acc 1
2017-08-08T16:48:44.512956: step 5032, loss 0.00260078, acc 1
2017-08-08T16:48:44.733503: step 5033, loss 0.000966014, acc 1
2017-08-08T16:48:45.076366: step 5034, loss 0.00909569, acc 1
2017-08-08T16:48:45.376013: step 5035, loss 0.0145774, acc 1
2017-08-08T16:48:45.653208: step 5036, loss 0.00169335, acc 1
2017-08-08T16:48:45.841226: step 5037, loss 0.000240563, acc 1
2017-08-08T16:48:46.085524: step 5038, loss 0.000745643, acc 1
2017-08-08T16:48:46.424822: step 5039, loss 0.00267881, acc 1
2017-08-08T16:48:46.649729: step 5040, loss 0.00555602, acc 1
2017-08-08T16:48:46.893723: step 5041, loss 0.00226829, acc 1
2017-08-08T16:48:47.263346: step 5042, loss 0.00183446, acc 1
2017-08-08T16:48:47.609369: step 5043, loss 0.00119844, acc 1
2017-08-08T16:48:47.862910: step 5044, loss 0.0204519, acc 0.984375
2017-08-08T16:48:48.046362: step 5045, loss 0.00179429, acc 1
2017-08-08T16:48:48.227387: step 5046, loss 0.00127679, acc 1
2017-08-08T16:48:48.525693: step 5047, loss 0.000158414, acc 1
2017-08-08T16:48:48.735213: step 5048, loss 0.0012849, acc 1
2017-08-08T16:48:48.918602: step 5049, loss 0.00135326, acc 1
2017-08-08T16:48:49.101314: step 5050, loss 0.000136558, acc 1
2017-08-08T16:48:49.385050: step 5051, loss 0.00218722, acc 1
2017-08-08T16:48:49.616340: step 5052, loss 0.00297865, acc 1
2017-08-08T16:48:49.818414: step 5053, loss 0.000344325, acc 1
2017-08-08T16:48:49.981151: step 5054, loss 0.000862172, acc 1
2017-08-08T16:48:50.270903: step 5055, loss 0.0137494, acc 0.984375
2017-08-08T16:48:50.513318: step 5056, loss 0.000223115, acc 1
2017-08-08T16:48:50.729208: step 5057, loss 0.000722514, acc 1
2017-08-08T16:48:50.947020: step 5058, loss 0.000125986, acc 1
2017-08-08T16:48:51.151455: step 5059, loss 0.000847981, acc 1
2017-08-08T16:48:51.457728: step 5060, loss 0.00320715, acc 1
2017-08-08T16:48:51.739144: step 5061, loss 0.00110554, acc 1
2017-08-08T16:48:52.024253: step 5062, loss 0.000892314, acc 1
2017-08-08T16:48:52.283478: step 5063, loss 0.00166086, acc 1
2017-08-08T16:48:52.525808: step 5064, loss 0.000303591, acc 1
2017-08-08T16:48:52.914719: step 5065, loss 0.00870167, acc 1
2017-08-08T16:48:53.195846: step 5066, loss 0.0025964, acc 1
2017-08-08T16:48:53.428631: step 5067, loss 0.000510049, acc 1
2017-08-08T16:48:53.684242: step 5068, loss 0.000732444, acc 1
2017-08-08T16:48:54.131065: step 5069, loss 0.00248554, acc 1
2017-08-08T16:48:54.521229: step 5070, loss 0.00092866, acc 1
2017-08-08T16:48:54.796242: step 5071, loss 0.00245605, acc 1
2017-08-08T16:48:55.005442: step 5072, loss 0.00316609, acc 1
2017-08-08T16:48:55.323013: step 5073, loss 0.00120257, acc 1
2017-08-08T16:48:55.534722: step 5074, loss 0.000891025, acc 1
2017-08-08T16:48:55.744197: step 5075, loss 0.00262401, acc 1
2017-08-08T16:48:55.936414: step 5076, loss 0.000636215, acc 1
2017-08-08T16:48:56.337880: step 5077, loss 0.00382526, acc 1
2017-08-08T16:48:56.572891: step 5078, loss 0.00338959, acc 1
2017-08-08T16:48:56.793682: step 5079, loss 0.000693884, acc 1
2017-08-08T16:48:56.962442: step 5080, loss 0.00193699, acc 1
2017-08-08T16:48:57.149265: step 5081, loss 0.000610797, acc 1
2017-08-08T16:48:57.424596: step 5082, loss 0.0339483, acc 0.984375
2017-08-08T16:48:57.615015: step 5083, loss 0.00974945, acc 1
2017-08-08T16:48:57.781973: step 5084, loss 0.00341934, acc 1
2017-08-08T16:48:57.956997: step 5085, loss 0.00480537, acc 1
2017-08-08T16:48:58.181438: step 5086, loss 0.0040581, acc 1
2017-08-08T16:48:58.549809: step 5087, loss 0.000511592, acc 1
2017-08-08T16:48:58.901871: step 5088, loss 0.00133985, acc 1
2017-08-08T16:48:59.271388: step 5089, loss 0.00306133, acc 1
2017-08-08T16:48:59.494678: step 5090, loss 0.000363226, acc 1
2017-08-08T16:48:59.735917: step 5091, loss 0.000613423, acc 1
2017-08-08T16:49:00.074347: step 5092, loss 0.000866058, acc 1
2017-08-08T16:49:00.341828: step 5093, loss 0.00986702, acc 1
2017-08-08T16:49:00.634276: step 5094, loss 0.00205432, acc 1
2017-08-08T16:49:01.009415: step 5095, loss 0.00157491, acc 1
2017-08-08T16:49:01.429361: step 5096, loss 0.00208034, acc 1
2017-08-08T16:49:01.871726: step 5097, loss 0.000276753, acc 1
2017-08-08T16:49:02.166596: step 5098, loss 0.00449738, acc 1
2017-08-08T16:49:02.410685: step 5099, loss 0.00230971, acc 1
2017-08-08T16:49:02.831924: step 5100, loss 0.0127047, acc 1

Evaluation:
2017-08-08T16:49:03.626834: step 5100, loss 1.6329, acc 0.722326

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-5100

2017-08-08T16:49:04.183554: step 5101, loss 0.00141598, acc 1
2017-08-08T16:49:04.578692: step 5102, loss 0.00151361, acc 1
2017-08-08T16:49:05.050957: step 5103, loss 0.000988512, acc 1
2017-08-08T16:49:05.395022: step 5104, loss 0.00286677, acc 1
2017-08-08T16:49:05.628179: step 5105, loss 0.000415602, acc 1
2017-08-08T16:49:05.941020: step 5106, loss 0.000837413, acc 1
2017-08-08T16:49:06.318173: step 5107, loss 0.00245761, acc 1
2017-08-08T16:49:06.542654: step 5108, loss 0.00122572, acc 1
2017-08-08T16:49:06.806880: step 5109, loss 0.0116268, acc 1
2017-08-08T16:49:07.045441: step 5110, loss 0.000522739, acc 1
2017-08-08T16:49:07.429340: step 5111, loss 0.00940763, acc 1
2017-08-08T16:49:07.882453: step 5112, loss 0.00224155, acc 1
2017-08-08T16:49:08.204903: step 5113, loss 0.000553019, acc 1
2017-08-08T16:49:08.442626: step 5114, loss 0.0113918, acc 1
2017-08-08T16:49:08.828808: step 5115, loss 0.00140594, acc 1
2017-08-08T16:49:09.144650: step 5116, loss 0.0049791, acc 1
2017-08-08T16:49:09.407601: step 5117, loss 0.003642, acc 1
2017-08-08T16:49:09.695265: step 5118, loss 0.00163164, acc 1
2017-08-08T16:49:10.041400: step 5119, loss 0.000237106, acc 1
2017-08-08T16:49:10.362804: step 5120, loss 0.00156253, acc 1
2017-08-08T16:49:10.631505: step 5121, loss 0.0012276, acc 1
2017-08-08T16:49:10.949315: step 5122, loss 0.00133821, acc 1
2017-08-08T16:49:11.296949: step 5123, loss 0.000524687, acc 1
2017-08-08T16:49:11.687672: step 5124, loss 0.00151307, acc 1
2017-08-08T16:49:11.918915: step 5125, loss 0.000357223, acc 1
2017-08-08T16:49:12.111319: step 5126, loss 0.00040101, acc 1
2017-08-08T16:49:12.312925: step 5127, loss 0.00115252, acc 1
2017-08-08T16:49:12.585468: step 5128, loss 0.000724074, acc 1
2017-08-08T16:49:12.885319: step 5129, loss 0.0080296, acc 1
2017-08-08T16:49:13.112862: step 5130, loss 0.000798717, acc 1
2017-08-08T16:49:13.340584: step 5131, loss 0.000714091, acc 1
2017-08-08T16:49:13.524574: step 5132, loss 0.00542226, acc 1
2017-08-08T16:49:13.808526: step 5133, loss 0.000809797, acc 1
2017-08-08T16:49:14.038029: step 5134, loss 0.00317081, acc 1
2017-08-08T16:49:14.280904: step 5135, loss 0.000952175, acc 1
2017-08-08T16:49:14.553148: step 5136, loss 0.00878538, acc 1
2017-08-08T16:49:14.930333: step 5137, loss 0.000694957, acc 1
2017-08-08T16:49:15.261339: step 5138, loss 0.00579719, acc 1
2017-08-08T16:49:15.530759: step 5139, loss 0.000148593, acc 1
2017-08-08T16:49:15.760132: step 5140, loss 0.000497696, acc 1
2017-08-08T16:49:16.045312: step 5141, loss 0.00911356, acc 1
2017-08-08T16:49:16.273255: step 5142, loss 0.000302876, acc 1
2017-08-08T16:49:16.449893: step 5143, loss 0.00089338, acc 1
2017-08-08T16:49:16.643124: step 5144, loss 0.0232178, acc 0.984375
2017-08-08T16:49:16.896083: step 5145, loss 0.00192554, acc 1
2017-08-08T16:49:17.236509: step 5146, loss 0.00154705, acc 1
2017-08-08T16:49:17.540450: step 5147, loss 0.00136759, acc 1
2017-08-08T16:49:17.777362: step 5148, loss 0.00116588, acc 1
2017-08-08T16:49:17.977605: step 5149, loss 0.000215264, acc 1
2017-08-08T16:49:18.338810: step 5150, loss 0.00260006, acc 1
2017-08-08T16:49:18.602661: step 5151, loss 0.00375052, acc 1
2017-08-08T16:49:18.835033: step 5152, loss 0.000352382, acc 1
2017-08-08T16:49:19.112454: step 5153, loss 0.00101393, acc 1
2017-08-08T16:49:19.418962: step 5154, loss 0.00173278, acc 1
2017-08-08T16:49:19.692688: step 5155, loss 0.00129693, acc 1
2017-08-08T16:49:19.917052: step 5156, loss 0.000359073, acc 1
2017-08-08T16:49:20.221373: step 5157, loss 0.00697964, acc 1
2017-08-08T16:49:20.469639: step 5158, loss 0.000912395, acc 1
2017-08-08T16:49:20.663644: step 5159, loss 0.000979458, acc 1
2017-08-08T16:49:20.863783: step 5160, loss 0.00412302, acc 1
2017-08-08T16:49:21.192343: step 5161, loss 0.00147162, acc 1
2017-08-08T16:49:21.495439: step 5162, loss 0.000330348, acc 1
2017-08-08T16:49:21.873455: step 5163, loss 0.000734602, acc 1
2017-08-08T16:49:22.144044: step 5164, loss 0.000749019, acc 1
2017-08-08T16:49:22.523279: step 5165, loss 0.00128567, acc 1
2017-08-08T16:49:22.901443: step 5166, loss 0.00597871, acc 1
2017-08-08T16:49:23.167790: step 5167, loss 0.00204679, acc 1
2017-08-08T16:49:23.455347: step 5168, loss 0.001545, acc 1
2017-08-08T16:49:23.742541: step 5169, loss 0.000315071, acc 1
2017-08-08T16:49:24.221348: step 5170, loss 0.000445939, acc 1
2017-08-08T16:49:24.609042: step 5171, loss 0.00133003, acc 1
2017-08-08T16:49:24.886074: step 5172, loss 0.00130088, acc 1
2017-08-08T16:49:25.099132: step 5173, loss 0.00173792, acc 1
2017-08-08T16:49:25.369241: step 5174, loss 0.0203864, acc 0.984375
2017-08-08T16:49:25.803178: step 5175, loss 0.000534776, acc 1
2017-08-08T16:49:26.070068: step 5176, loss 0.000284383, acc 1
2017-08-08T16:49:26.341471: step 5177, loss 0.00187124, acc 1
2017-08-08T16:49:26.625368: step 5178, loss 0.000338851, acc 1
2017-08-08T16:49:26.981066: step 5179, loss 0.0026666, acc 1
2017-08-08T16:49:27.293325: step 5180, loss 0.000653151, acc 1
2017-08-08T16:49:27.507503: step 5181, loss 0.0010334, acc 1
2017-08-08T16:49:27.731315: step 5182, loss 0.00538026, acc 1
2017-08-08T16:49:28.070288: step 5183, loss 0.000793331, acc 1
2017-08-08T16:49:28.428556: step 5184, loss 0.000368633, acc 1
2017-08-08T16:49:28.710829: step 5185, loss 0.000999566, acc 1
2017-08-08T16:49:28.997472: step 5186, loss 0.000236513, acc 1
2017-08-08T16:49:29.385360: step 5187, loss 0.00285157, acc 1
2017-08-08T16:49:29.823777: step 5188, loss 0.000296175, acc 1
2017-08-08T16:49:30.240241: step 5189, loss 0.00669955, acc 1
2017-08-08T16:49:30.514889: step 5190, loss 0.00136108, acc 1
2017-08-08T16:49:30.770562: step 5191, loss 0.00107297, acc 1
2017-08-08T16:49:31.187373: step 5192, loss 0.00041695, acc 1
2017-08-08T16:49:31.479270: step 5193, loss 0.000923305, acc 1
2017-08-08T16:49:31.743207: step 5194, loss 0.0244239, acc 0.984375
2017-08-08T16:49:32.000632: step 5195, loss 0.00102399, acc 1
2017-08-08T16:49:32.389364: step 5196, loss 0.00181832, acc 1
2017-08-08T16:49:32.737362: step 5197, loss 0.000392924, acc 1
2017-08-08T16:49:33.080625: step 5198, loss 0.00109835, acc 1
2017-08-08T16:49:33.317824: step 5199, loss 0.00108233, acc 1
2017-08-08T16:49:33.561408: step 5200, loss 0.00143493, acc 1

Evaluation:
2017-08-08T16:49:34.197009: step 5200, loss 1.67548, acc 0.725141

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-5200

2017-08-08T16:49:34.658454: step 5201, loss 0.00108746, acc 1
2017-08-08T16:49:35.097389: step 5202, loss 0.00114044, acc 1
2017-08-08T16:49:35.425684: step 5203, loss 0.00161823, acc 1
2017-08-08T16:49:35.704868: step 5204, loss 0.000517723, acc 1
2017-08-08T16:49:35.978819: step 5205, loss 0.00103291, acc 1
2017-08-08T16:49:36.290897: step 5206, loss 0.00378528, acc 1
2017-08-08T16:49:36.691157: step 5207, loss 0.0020897, acc 1
2017-08-08T16:49:36.924763: step 5208, loss 0.000475347, acc 1
2017-08-08T16:49:37.221372: step 5209, loss 0.000508017, acc 1
2017-08-08T16:49:37.614486: step 5210, loss 0.00328552, acc 1
2017-08-08T16:49:37.985427: step 5211, loss 0.00145179, acc 1
2017-08-08T16:49:38.243655: step 5212, loss 0.00722851, acc 1
2017-08-08T16:49:38.493308: step 5213, loss 0.000627941, acc 1
2017-08-08T16:49:38.974619: step 5214, loss 0.00172844, acc 1
2017-08-08T16:49:39.230218: step 5215, loss 0.00041204, acc 1
2017-08-08T16:49:39.488353: step 5216, loss 0.000592636, acc 1
2017-08-08T16:49:39.722664: step 5217, loss 0.000807741, acc 1
2017-08-08T16:49:40.141396: step 5218, loss 0.000114672, acc 1
2017-08-08T16:49:40.588633: step 5219, loss 0.00482909, acc 1
2017-08-08T16:49:40.961288: step 5220, loss 0.00189438, acc 1
2017-08-08T16:49:41.211935: step 5221, loss 0.000507645, acc 1
2017-08-08T16:49:41.565639: step 5222, loss 0.000113563, acc 1
2017-08-08T16:49:41.929818: step 5223, loss 0.0180814, acc 0.984375
2017-08-08T16:49:42.236705: step 5224, loss 0.000516791, acc 1
2017-08-08T16:49:42.549372: step 5225, loss 0.0127583, acc 1
2017-08-08T16:49:42.881325: step 5226, loss 0.000347997, acc 1
2017-08-08T16:49:43.128877: step 5227, loss 0.00246687, acc 1
2017-08-08T16:49:43.343947: step 5228, loss 0.000315079, acc 1
2017-08-08T16:49:43.523619: step 5229, loss 0.000515129, acc 1
2017-08-08T16:49:43.920072: step 5230, loss 0.00251705, acc 1
2017-08-08T16:49:44.192074: step 5231, loss 0.0020742, acc 1
2017-08-08T16:49:44.438948: step 5232, loss 0.000334764, acc 1
2017-08-08T16:49:44.693492: step 5233, loss 0.00161074, acc 1
2017-08-08T16:49:44.969061: step 5234, loss 0.00396976, acc 1
2017-08-08T16:49:45.195281: step 5235, loss 0.00177042, acc 1
2017-08-08T16:49:45.388002: step 5236, loss 0.00218145, acc 1
2017-08-08T16:49:45.608425: step 5237, loss 0.000570192, acc 1
2017-08-08T16:49:45.897328: step 5238, loss 0.00245835, acc 1
2017-08-08T16:49:46.156809: step 5239, loss 0.000102906, acc 1
2017-08-08T16:49:46.364925: step 5240, loss 0.000320515, acc 1
2017-08-08T16:49:46.601892: step 5241, loss 0.00066697, acc 1
2017-08-08T16:49:46.937345: step 5242, loss 0.0096126, acc 1
2017-08-08T16:49:47.228464: step 5243, loss 8.73398e-05, acc 1
2017-08-08T16:49:47.549088: step 5244, loss 0.0084455, acc 1
2017-08-08T16:49:47.771471: step 5245, loss 0.0228506, acc 0.984375
2017-08-08T16:49:48.113457: step 5246, loss 0.00228688, acc 1
2017-08-08T16:49:48.428605: step 5247, loss 0.000421407, acc 1
2017-08-08T16:49:48.719311: step 5248, loss 0.000518275, acc 1
2017-08-08T16:49:48.984514: step 5249, loss 0.00405587, acc 1
2017-08-08T16:49:49.314452: step 5250, loss 0.000274297, acc 1
2017-08-08T16:49:49.728310: step 5251, loss 0.00841977, acc 1
2017-08-08T16:49:50.097373: step 5252, loss 0.00217399, acc 1
2017-08-08T16:49:50.370266: step 5253, loss 0.00141908, acc 1
2017-08-08T16:49:50.557384: step 5254, loss 0.00800291, acc 1
2017-08-08T16:49:50.825329: step 5255, loss 0.000524141, acc 1
2017-08-08T16:49:51.065103: step 5256, loss 0.000324361, acc 1
2017-08-08T16:49:51.239656: step 5257, loss 0.000737998, acc 1
2017-08-08T16:49:51.441644: step 5258, loss 0.000863929, acc 1
2017-08-08T16:49:51.621061: step 5259, loss 0.00271197, acc 1
2017-08-08T16:49:51.941343: step 5260, loss 0.000324288, acc 1
2017-08-08T16:49:52.349353: step 5261, loss 0.000195731, acc 1
2017-08-08T16:49:52.677463: step 5262, loss 0.0478192, acc 0.984375
2017-08-08T16:49:52.878373: step 5263, loss 0.00341409, acc 1
2017-08-08T16:49:53.075923: step 5264, loss 0.00214168, acc 1
2017-08-08T16:49:53.404642: step 5265, loss 0.000303163, acc 1
2017-08-08T16:49:53.612041: step 5266, loss 0.00153999, acc 1
2017-08-08T16:49:53.838329: step 5267, loss 0.00829763, acc 1
2017-08-08T16:49:54.118356: step 5268, loss 9.44675e-05, acc 1
2017-08-08T16:49:54.640179: step 5269, loss 0.00213488, acc 1
2017-08-08T16:49:54.977318: step 5270, loss 0.000345345, acc 1
2017-08-08T16:49:55.231317: step 5271, loss 0.000428336, acc 1
2017-08-08T16:49:55.427268: step 5272, loss 0.00226562, acc 1
2017-08-08T16:49:55.715980: step 5273, loss 0.00397286, acc 1
2017-08-08T16:49:55.945663: step 5274, loss 0.000521812, acc 1
2017-08-08T16:49:56.165104: step 5275, loss 0.000660455, acc 1
2017-08-08T16:49:56.370317: step 5276, loss 0.00331636, acc 1
2017-08-08T16:49:56.743195: step 5277, loss 0.00405552, acc 1
2017-08-08T16:49:57.048471: step 5278, loss 0.000113269, acc 1
2017-08-08T16:49:57.296308: step 5279, loss 0.000294093, acc 1
2017-08-08T16:49:57.544306: step 5280, loss 0.00143282, acc 1
2017-08-08T16:49:57.845775: step 5281, loss 0.000771208, acc 1
2017-08-08T16:49:58.251258: step 5282, loss 0.00381907, acc 1
2017-08-08T16:49:58.492335: step 5283, loss 0.00482798, acc 1
2017-08-08T16:49:58.719363: step 5284, loss 0.000506384, acc 1
2017-08-08T16:49:58.932040: step 5285, loss 0.000358762, acc 1
2017-08-08T16:49:59.309333: step 5286, loss 0.000320048, acc 1
2017-08-08T16:49:59.672852: step 5287, loss 0.00213283, acc 1
2017-08-08T16:49:59.951165: step 5288, loss 0.00206547, acc 1
2017-08-08T16:50:00.181546: step 5289, loss 0.00140122, acc 1
2017-08-08T16:50:00.521593: step 5290, loss 0.00117897, acc 1
2017-08-08T16:50:00.877332: step 5291, loss 0.000789713, acc 1
2017-08-08T16:50:01.126713: step 5292, loss 0.00203966, acc 1
2017-08-08T16:50:01.459526: step 5293, loss 0.0031347, acc 1
2017-08-08T16:50:01.694417: step 5294, loss 0.00200615, acc 1
2017-08-08T16:50:02.145498: step 5295, loss 0.0008471, acc 1
2017-08-08T16:50:02.578869: step 5296, loss 0.00100302, acc 1
2017-08-08T16:50:02.917804: step 5297, loss 0.000991585, acc 1
2017-08-08T16:50:03.196916: step 5298, loss 0.00108637, acc 1
2017-08-08T16:50:03.425239: step 5299, loss 0.0207906, acc 0.984375
2017-08-08T16:50:03.889180: step 5300, loss 0.00290054, acc 1

Evaluation:
2017-08-08T16:50:04.725681: step 5300, loss 1.69619, acc 0.723265

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-5300

2017-08-08T16:50:05.681905: step 5301, loss 0.000521269, acc 1
2017-08-08T16:50:06.025283: step 5302, loss 0.00224388, acc 1
2017-08-08T16:50:06.491550: step 5303, loss 0.0333767, acc 0.984375
2017-08-08T16:50:06.742138: step 5304, loss 0.00103021, acc 1
2017-08-08T16:50:06.997398: step 5305, loss 0.00228991, acc 1
2017-08-08T16:50:07.438236: step 5306, loss 0.000564583, acc 1
2017-08-08T16:50:07.736019: step 5307, loss 0.000252207, acc 1
2017-08-08T16:50:07.979198: step 5308, loss 0.00043766, acc 1
2017-08-08T16:50:08.332803: step 5309, loss 0.00417226, acc 1
2017-08-08T16:50:08.710432: step 5310, loss 0.000174303, acc 1
2017-08-08T16:50:09.004477: step 5311, loss 0.00234758, acc 1
2017-08-08T16:50:09.309226: step 5312, loss 0.0191237, acc 0.984375
2017-08-08T16:50:09.547012: step 5313, loss 0.00115774, acc 1
2017-08-08T16:50:09.961538: step 5314, loss 0.0144578, acc 0.984375
2017-08-08T16:50:10.255300: step 5315, loss 0.000223518, acc 1
2017-08-08T16:50:10.480899: step 5316, loss 0.0074595, acc 1
2017-08-08T16:50:10.728408: step 5317, loss 0.000563279, acc 1
2017-08-08T16:50:10.989351: step 5318, loss 0.000517537, acc 1
2017-08-08T16:50:11.438975: step 5319, loss 0.000831895, acc 1
2017-08-08T16:50:11.885388: step 5320, loss 0.000872729, acc 1
2017-08-08T16:50:12.189363: step 5321, loss 0.000809129, acc 1
2017-08-08T16:50:12.444802: step 5322, loss 0.000954966, acc 1
2017-08-08T16:50:12.849659: step 5323, loss 0.00164694, acc 1
2017-08-08T16:50:13.085434: step 5324, loss 0.00208505, acc 1
2017-08-08T16:50:13.349932: step 5325, loss 0.0010367, acc 1
2017-08-08T16:50:13.594026: step 5326, loss 0.00208708, acc 1
2017-08-08T16:50:14.091418: step 5327, loss 0.0001412, acc 1
2017-08-08T16:50:14.559942: step 5328, loss 0.00326102, acc 1
2017-08-08T16:50:14.921532: step 5329, loss 0.00315928, acc 1
2017-08-08T16:50:15.188628: step 5330, loss 0.000256914, acc 1
2017-08-08T16:50:15.612190: step 5331, loss 0.000168846, acc 1
2017-08-08T16:50:15.856462: step 5332, loss 0.000920522, acc 1
2017-08-08T16:50:16.159741: step 5333, loss 0.00409554, acc 1
2017-08-08T16:50:16.396168: step 5334, loss 0.00357861, acc 1
2017-08-08T16:50:16.827598: step 5335, loss 0.000918728, acc 1
2017-08-08T16:50:17.217642: step 5336, loss 0.00423096, acc 1
2017-08-08T16:50:17.650745: step 5337, loss 0.00102137, acc 1
2017-08-08T16:50:17.897162: step 5338, loss 0.000709344, acc 1
2017-08-08T16:50:18.144034: step 5339, loss 0.000142095, acc 1
2017-08-08T16:50:18.566284: step 5340, loss 0.002784, acc 1
2017-08-08T16:50:18.834713: step 5341, loss 0.00124112, acc 1
2017-08-08T16:50:19.072255: step 5342, loss 0.01362, acc 0.984375
2017-08-08T16:50:19.487484: step 5343, loss 0.0018384, acc 1
2017-08-08T16:50:19.938198: step 5344, loss 0.000641697, acc 1
2017-08-08T16:50:20.314826: step 5345, loss 0.000241867, acc 1
2017-08-08T16:50:20.597539: step 5346, loss 0.000265526, acc 1
2017-08-08T16:50:20.831715: step 5347, loss 0.000841827, acc 1
2017-08-08T16:50:21.238154: step 5348, loss 0.00251252, acc 1
2017-08-08T16:50:21.523946: step 5349, loss 0.0047838, acc 1
2017-08-08T16:50:21.787612: step 5350, loss 0.000554319, acc 1
2017-08-08T16:50:22.090602: step 5351, loss 0.00752459, acc 1
2017-08-08T16:50:22.445387: step 5352, loss 0.00070071, acc 1
2017-08-08T16:50:22.741379: step 5353, loss 0.00119138, acc 1
2017-08-08T16:50:23.105894: step 5354, loss 0.0122683, acc 0.984375
2017-08-08T16:50:23.341885: step 5355, loss 0.000289015, acc 1
2017-08-08T16:50:23.562215: step 5356, loss 0.0457999, acc 0.984375
2017-08-08T16:50:23.916666: step 5357, loss 0.00649516, acc 1
2017-08-08T16:50:24.201681: step 5358, loss 0.000564671, acc 1
2017-08-08T16:50:24.434777: step 5359, loss 0.00832992, acc 1
2017-08-08T16:50:24.759269: step 5360, loss 0.00463849, acc 1
2017-08-08T16:50:25.223751: step 5361, loss 0.000354249, acc 1
2017-08-08T16:50:25.605361: step 5362, loss 0.0014254, acc 1
2017-08-08T16:50:25.918728: step 5363, loss 0.00717202, acc 1
2017-08-08T16:50:26.139000: step 5364, loss 0.000382739, acc 1
2017-08-08T16:50:26.522726: step 5365, loss 0.0080259, acc 1
2017-08-08T16:50:26.875425: step 5366, loss 0.000850005, acc 1
2017-08-08T16:50:27.068095: step 5367, loss 0.00120804, acc 1
2017-08-08T16:50:27.311361: step 5368, loss 0.0357918, acc 0.984375
2017-08-08T16:50:27.626800: step 5369, loss 0.00239019, acc 1
2017-08-08T16:50:27.977566: step 5370, loss 0.000905636, acc 1
2017-08-08T16:50:28.372660: step 5371, loss 0.00146562, acc 1
2017-08-08T16:50:28.691619: step 5372, loss 0.00316464, acc 1
2017-08-08T16:50:28.978552: step 5373, loss 0.000106291, acc 1
2017-08-08T16:50:29.218051: step 5374, loss 0.00638484, acc 1
2017-08-08T16:50:29.541357: step 5375, loss 0.00299811, acc 1
2017-08-08T16:50:29.827645: step 5376, loss 0.0022355, acc 1
2017-08-08T16:50:30.011871: step 5377, loss 0.000639607, acc 1
2017-08-08T16:50:30.214111: step 5378, loss 0.00432657, acc 1
2017-08-08T16:50:30.425910: step 5379, loss 0.00060128, acc 1
2017-08-08T16:50:30.719096: step 5380, loss 0.00207514, acc 1
2017-08-08T16:50:31.050532: step 5381, loss 0.000852098, acc 1
2017-08-08T16:50:31.309589: step 5382, loss 0.0042151, acc 1
2017-08-08T16:50:31.503202: step 5383, loss 0.00299789, acc 1
2017-08-08T16:50:31.770341: step 5384, loss 0.0050067, acc 1
2017-08-08T16:50:31.988869: step 5385, loss 0.000308725, acc 1
2017-08-08T16:50:32.222255: step 5386, loss 0.0014422, acc 1
2017-08-08T16:50:32.422815: step 5387, loss 0.0004028, acc 1
2017-08-08T16:50:32.737362: step 5388, loss 0.00127445, acc 1
2017-08-08T16:50:33.017305: step 5389, loss 0.000382141, acc 1
2017-08-08T16:50:33.290058: step 5390, loss 0.025308, acc 0.984375
2017-08-08T16:50:33.472135: step 5391, loss 0.00126699, acc 1
2017-08-08T16:50:33.704945: step 5392, loss 0.000836514, acc 1
2017-08-08T16:50:34.061383: step 5393, loss 0.00117489, acc 1
2017-08-08T16:50:34.394135: step 5394, loss 0.000616612, acc 1
2017-08-08T16:50:34.591436: step 5395, loss 0.00146131, acc 1
2017-08-08T16:50:34.793386: step 5396, loss 0.00163758, acc 1
2017-08-08T16:50:35.019953: step 5397, loss 0.00439014, acc 1
2017-08-08T16:50:35.413370: step 5398, loss 0.00204276, acc 1
2017-08-08T16:50:35.696024: step 5399, loss 0.000671424, acc 1
2017-08-08T16:50:35.968052: step 5400, loss 0.000743709, acc 1

Evaluation:
2017-08-08T16:50:36.520836: step 5400, loss 1.69629, acc 0.725141

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-5400

2017-08-08T16:50:36.946832: step 5401, loss 0.000370111, acc 1
2017-08-08T16:50:37.145256: step 5402, loss 0.000879058, acc 1
2017-08-08T16:50:37.324484: step 5403, loss 0.00110145, acc 1
2017-08-08T16:50:37.651995: step 5404, loss 0.000638965, acc 1
2017-08-08T16:50:37.879525: step 5405, loss 0.00505728, acc 1
2017-08-08T16:50:38.131789: step 5406, loss 0.000212835, acc 1
2017-08-08T16:50:38.361729: step 5407, loss 0.000751346, acc 1
2017-08-08T16:50:38.662120: step 5408, loss 0.0062658, acc 1
2017-08-08T16:50:38.966145: step 5409, loss 0.000202299, acc 1
2017-08-08T16:50:39.237691: step 5410, loss 0.00175866, acc 1
2017-08-08T16:50:39.446979: step 5411, loss 0.000711134, acc 1
2017-08-08T16:50:39.712945: step 5412, loss 0.00260739, acc 1
2017-08-08T16:50:40.098725: step 5413, loss 0.000422507, acc 1
2017-08-08T16:50:40.439754: step 5414, loss 0.00850608, acc 1
2017-08-08T16:50:40.795307: step 5415, loss 0.000800235, acc 1
2017-08-08T16:50:41.048102: step 5416, loss 0.00104722, acc 1
2017-08-08T16:50:41.277026: step 5417, loss 0.000641056, acc 1
2017-08-08T16:50:41.684390: step 5418, loss 0.000664609, acc 1
2017-08-08T16:50:41.950646: step 5419, loss 0.00314727, acc 1
2017-08-08T16:50:42.211160: step 5420, loss 0.000494098, acc 1
2017-08-08T16:50:42.435966: step 5421, loss 0.000121385, acc 1
2017-08-08T16:50:42.792128: step 5422, loss 0.000985935, acc 1
2017-08-08T16:50:43.132756: step 5423, loss 0.00213491, acc 1
2017-08-08T16:50:43.456329: step 5424, loss 0.00804898, acc 1
2017-08-08T16:50:43.766585: step 5425, loss 0.00163881, acc 1
2017-08-08T16:50:44.112558: step 5426, loss 2.81959e-05, acc 1
2017-08-08T16:50:44.400651: step 5427, loss 0.000306233, acc 1
2017-08-08T16:50:44.710026: step 5428, loss 0.0252048, acc 0.984375
2017-08-08T16:50:45.098065: step 5429, loss 0.0064997, acc 1
2017-08-08T16:50:45.530668: step 5430, loss 0.0378655, acc 0.984375
2017-08-08T16:50:45.891003: step 5431, loss 0.000448783, acc 1
2017-08-08T16:50:46.115344: step 5432, loss 0.00509459, acc 1
2017-08-08T16:50:46.477608: step 5433, loss 0.00278919, acc 1
2017-08-08T16:50:46.780043: step 5434, loss 0.000980191, acc 1
2017-08-08T16:50:47.034861: step 5435, loss 0.00254911, acc 1
2017-08-08T16:50:47.264205: step 5436, loss 0.00243595, acc 1
2017-08-08T16:50:47.664067: step 5437, loss 0.00297529, acc 1
2017-08-08T16:50:48.053003: step 5438, loss 0.00447455, acc 1
2017-08-08T16:50:48.340331: step 5439, loss 0.00123904, acc 1
2017-08-08T16:50:48.557887: step 5440, loss 0.00159144, acc 1
2017-08-08T16:50:48.774850: step 5441, loss 0.0019926, acc 1
2017-08-08T16:50:49.125367: step 5442, loss 0.000185888, acc 1
2017-08-08T16:50:49.450665: step 5443, loss 0.0124632, acc 1
2017-08-08T16:50:49.681658: step 5444, loss 0.00104097, acc 1
2017-08-08T16:50:49.891450: step 5445, loss 0.00783083, acc 1
2017-08-08T16:50:50.251812: step 5446, loss 0.000849986, acc 1
2017-08-08T16:50:50.566500: step 5447, loss 0.000508116, acc 1
2017-08-08T16:50:50.863485: step 5448, loss 0.00104434, acc 1
2017-08-08T16:50:51.105041: step 5449, loss 0.00175699, acc 1
2017-08-08T16:50:51.393367: step 5450, loss 0.00767824, acc 1
2017-08-08T16:50:51.761368: step 5451, loss 0.0419559, acc 0.984375
2017-08-08T16:50:52.049707: step 5452, loss 0.00286781, acc 1
2017-08-08T16:50:52.332230: step 5453, loss 0.0026027, acc 1
2017-08-08T16:50:52.685334: step 5454, loss 0.00114673, acc 1
2017-08-08T16:50:53.077654: step 5455, loss 0.0135579, acc 1
2017-08-08T16:50:53.401851: step 5456, loss 0.0117044, acc 1
2017-08-08T16:50:53.677796: step 5457, loss 0.00426774, acc 1
2017-08-08T16:50:53.927683: step 5458, loss 0.0158705, acc 0.984375
2017-08-08T16:50:54.354666: step 5459, loss 0.00428307, acc 1
2017-08-08T16:50:54.644296: step 5460, loss 0.000612791, acc 1
2017-08-08T16:50:54.933691: step 5461, loss 0.000677776, acc 1
2017-08-08T16:50:55.311772: step 5462, loss 0.0019399, acc 1
2017-08-08T16:50:55.606514: step 5463, loss 0.00185087, acc 1
2017-08-08T16:50:55.895391: step 5464, loss 0.00348178, acc 1
2017-08-08T16:50:56.227658: step 5465, loss 0.000706928, acc 1
2017-08-08T16:50:56.471500: step 5466, loss 0.000270315, acc 1
2017-08-08T16:50:56.904155: step 5467, loss 0.0133339, acc 0.984375
2017-08-08T16:50:57.210043: step 5468, loss 0.00065291, acc 1
2017-08-08T16:50:57.465474: step 5469, loss 0.00148692, acc 1
2017-08-08T16:50:57.816625: step 5470, loss 0.000382945, acc 1
2017-08-08T16:50:58.211572: step 5471, loss 0.000827572, acc 1
2017-08-08T16:50:58.553384: step 5472, loss 0.00331376, acc 1
2017-08-08T16:50:58.786304: step 5473, loss 0.000569895, acc 1
2017-08-08T16:50:59.011626: step 5474, loss 0.00165442, acc 1
2017-08-08T16:50:59.319771: step 5475, loss 0.000365473, acc 1
2017-08-08T16:50:59.526852: step 5476, loss 0.00051676, acc 1
2017-08-08T16:50:59.767594: step 5477, loss 0.000265367, acc 1
2017-08-08T16:51:00.106202: step 5478, loss 0.000379245, acc 1
2017-08-08T16:51:00.498898: step 5479, loss 0.000236387, acc 1
2017-08-08T16:51:00.757385: step 5480, loss 0.00382075, acc 1
2017-08-08T16:51:00.932944: step 5481, loss 0.00046694, acc 1
2017-08-08T16:51:01.138024: step 5482, loss 0.00220397, acc 1
2017-08-08T16:51:01.482506: step 5483, loss 0.0148793, acc 0.984375
2017-08-08T16:51:01.748061: step 5484, loss 0.00102243, acc 1
2017-08-08T16:51:02.055230: step 5485, loss 0.000577554, acc 1
2017-08-08T16:51:02.344138: step 5486, loss 0.0152414, acc 1
2017-08-08T16:51:02.687793: step 5487, loss 0.00836173, acc 1
2017-08-08T16:51:03.063235: step 5488, loss 0.000423672, acc 1
2017-08-08T16:51:03.445043: step 5489, loss 0.000466143, acc 1
2017-08-08T16:51:03.786896: step 5490, loss 0.00471266, acc 1
2017-08-08T16:51:04.040365: step 5491, loss 0.0235031, acc 0.984375
2017-08-08T16:51:04.325497: step 5492, loss 0.00241001, acc 1
2017-08-08T16:51:04.666320: step 5493, loss 0.00240071, acc 1
2017-08-08T16:51:04.938383: step 5494, loss 0.000482277, acc 1
2017-08-08T16:51:05.220090: step 5495, loss 0.000334989, acc 1
2017-08-08T16:51:05.607874: step 5496, loss 0.00060684, acc 1
2017-08-08T16:51:06.041317: step 5497, loss 0.00144215, acc 1
2017-08-08T16:51:06.349210: step 5498, loss 0.00360487, acc 1
2017-08-08T16:51:06.690188: step 5499, loss 0.000864301, acc 1
2017-08-08T16:51:06.941527: step 5500, loss 0.000545831, acc 1

Evaluation:
2017-08-08T16:51:07.855452: step 5500, loss 1.70437, acc 0.733584

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-5500

2017-08-08T16:51:08.347940: step 5501, loss 0.00241735, acc 1
2017-08-08T16:51:08.721357: step 5502, loss 0.00880442, acc 1
2017-08-08T16:51:09.169621: step 5503, loss 0.00229077, acc 1
2017-08-08T16:51:09.539960: step 5504, loss 0.000430818, acc 1
2017-08-08T16:51:09.786949: step 5505, loss 0.00601219, acc 1
2017-08-08T16:51:10.128074: step 5506, loss 0.00200632, acc 1
2017-08-08T16:51:10.391188: step 5507, loss 0.00069987, acc 1
2017-08-08T16:51:10.652211: step 5508, loss 0.00287022, acc 1
2017-08-08T16:51:10.906362: step 5509, loss 0.00503829, acc 1
2017-08-08T16:51:11.251733: step 5510, loss 0.00368927, acc 1
2017-08-08T16:51:11.710498: step 5511, loss 0.0395379, acc 0.96875
2017-08-08T16:51:12.072791: step 5512, loss 0.00195303, acc 1
2017-08-08T16:51:12.316372: step 5513, loss 0.00425502, acc 1
2017-08-08T16:51:12.661132: step 5514, loss 0.000443853, acc 1
2017-08-08T16:51:13.092722: step 5515, loss 0.000782805, acc 1
2017-08-08T16:51:13.399729: step 5516, loss 0.00213282, acc 1
2017-08-08T16:51:13.660189: step 5517, loss 0.00135696, acc 1
2017-08-08T16:51:14.038205: step 5518, loss 0.000452465, acc 1
2017-08-08T16:51:14.437729: step 5519, loss 0.00104191, acc 1
2017-08-08T16:51:14.800983: step 5520, loss 0.00162065, acc 1
2017-08-08T16:51:15.122346: step 5521, loss 0.000455925, acc 1
2017-08-08T16:51:15.363574: step 5522, loss 0.000245822, acc 1
2017-08-08T16:51:15.687252: step 5523, loss 0.000113631, acc 1
2017-08-08T16:51:16.016414: step 5524, loss 0.000812899, acc 1
2017-08-08T16:51:16.257332: step 5525, loss 0.0287722, acc 0.984375
2017-08-08T16:51:16.533230: step 5526, loss 0.00469695, acc 1
2017-08-08T16:51:16.892312: step 5527, loss 0.00244544, acc 1
2017-08-08T16:51:17.290197: step 5528, loss 0.00056559, acc 1
2017-08-08T16:51:17.710789: step 5529, loss 0.00456915, acc 1
2017-08-08T16:51:18.026027: step 5530, loss 0.000918101, acc 1
2017-08-08T16:51:18.288592: step 5531, loss 0.000664901, acc 1
2017-08-08T16:51:18.722213: step 5532, loss 0.00209391, acc 1
2017-08-08T16:51:19.013727: step 5533, loss 0.00109934, acc 1
2017-08-08T16:51:19.286333: step 5534, loss 0.00598756, acc 1
2017-08-08T16:51:19.564333: step 5535, loss 0.0103063, acc 1
2017-08-08T16:51:19.918967: step 5536, loss 0.00159774, acc 1
2017-08-08T16:51:20.357677: step 5537, loss 0.000837073, acc 1
2017-08-08T16:51:20.753656: step 5538, loss 0.00511991, acc 1
2017-08-08T16:51:21.032810: step 5539, loss 0.000250289, acc 1
2017-08-08T16:51:21.310723: step 5540, loss 0.00713081, acc 1
2017-08-08T16:51:21.757514: step 5541, loss 0.00341343, acc 1
2017-08-08T16:51:22.084310: step 5542, loss 0.00187673, acc 1
2017-08-08T16:51:22.373566: step 5543, loss 0.0114108, acc 1
2017-08-08T16:51:22.673549: step 5544, loss 0.000948406, acc 1
2017-08-08T16:51:23.197151: step 5545, loss 0.000194478, acc 1
2017-08-08T16:51:23.591982: step 5546, loss 0.00274403, acc 1
2017-08-08T16:51:23.958539: step 5547, loss 0.000656623, acc 1
2017-08-08T16:51:24.232758: step 5548, loss 0.00356868, acc 1
2017-08-08T16:51:24.593659: step 5549, loss 0.0353641, acc 0.984375
2017-08-08T16:51:24.994468: step 5550, loss 0.00522303, acc 1
2017-08-08T16:51:25.328131: step 5551, loss 0.00294029, acc 1
2017-08-08T16:51:25.610777: step 5552, loss 0.00032933, acc 1
2017-08-08T16:51:25.962371: step 5553, loss 0.000566553, acc 1
2017-08-08T16:51:26.354773: step 5554, loss 2.76056e-05, acc 1
2017-08-08T16:51:26.652254: step 5555, loss 0.000211213, acc 1
2017-08-08T16:51:26.848309: step 5556, loss 0.000728715, acc 1
2017-08-08T16:51:27.117311: step 5557, loss 0.000447143, acc 1
2017-08-08T16:51:27.368200: step 5558, loss 0.00565961, acc 1
2017-08-08T16:51:27.619571: step 5559, loss 0.000319313, acc 1
2017-08-08T16:51:27.820120: step 5560, loss 0.000905123, acc 1
2017-08-08T16:51:28.096948: step 5561, loss 0.000455988, acc 1
2017-08-08T16:51:28.426255: step 5562, loss 0.000373411, acc 1
2017-08-08T16:51:28.647253: step 5563, loss 0.0121807, acc 0.984375
2017-08-08T16:51:28.800331: step 5564, loss 0.000477643, acc 1
2017-08-08T16:51:29.086733: step 5565, loss 0.00272325, acc 1
2017-08-08T16:51:29.296808: step 5566, loss 0.00059753, acc 1
2017-08-08T16:51:29.470439: step 5567, loss 0.000858569, acc 1
2017-08-08T16:51:29.639624: step 5568, loss 0.00545268, acc 1
2017-08-08T16:51:29.902691: step 5569, loss 0.00213946, acc 1
2017-08-08T16:51:30.153630: step 5570, loss 0.00510253, acc 1
2017-08-08T16:51:30.421601: step 5571, loss 0.0099633, acc 1
2017-08-08T16:51:30.702142: step 5572, loss 0.00147921, acc 1
2017-08-08T16:51:30.938567: step 5573, loss 0.000273832, acc 1
2017-08-08T16:51:31.349590: step 5574, loss 0.000898185, acc 1
2017-08-08T16:51:31.615039: step 5575, loss 0.000310596, acc 1
2017-08-08T16:51:31.836846: step 5576, loss 0.000159557, acc 1
2017-08-08T16:51:32.037998: step 5577, loss 0.000453489, acc 1
2017-08-08T16:51:32.368929: step 5578, loss 0.0116856, acc 1
2017-08-08T16:51:32.645330: step 5579, loss 0.001265, acc 1
2017-08-08T16:51:32.898424: step 5580, loss 0.00163492, acc 1
2017-08-08T16:51:33.094291: step 5581, loss 0.00313349, acc 1
2017-08-08T16:51:33.301234: step 5582, loss 0.00184855, acc 1
2017-08-08T16:51:33.733808: step 5583, loss 0.00314602, acc 1
2017-08-08T16:51:34.011852: step 5584, loss 0.000163719, acc 1
2017-08-08T16:51:34.298241: step 5585, loss 0.000981485, acc 1
2017-08-08T16:51:34.576780: step 5586, loss 0.000307586, acc 1
2017-08-08T16:51:34.925358: step 5587, loss 0.000451727, acc 1
2017-08-08T16:51:35.250509: step 5588, loss 8.85961e-05, acc 1
2017-08-08T16:51:35.469242: step 5589, loss 0.000767861, acc 1
2017-08-08T16:51:35.651532: step 5590, loss 0.000772385, acc 1
2017-08-08T16:51:35.893331: step 5591, loss 0.00247991, acc 1
2017-08-08T16:51:36.138263: step 5592, loss 0.00401051, acc 1
2017-08-08T16:51:36.325804: step 5593, loss 0.00104777, acc 1
2017-08-08T16:51:36.563104: step 5594, loss 0.00877659, acc 1
2017-08-08T16:51:36.838125: step 5595, loss 0.000340775, acc 1
2017-08-08T16:51:37.276885: step 5596, loss 0.000451919, acc 1
2017-08-08T16:51:37.734623: step 5597, loss 0.000631911, acc 1
2017-08-08T16:51:38.096206: step 5598, loss 0.0002782, acc 1
2017-08-08T16:51:38.265501: step 5599, loss 0.00269728, acc 1
2017-08-08T16:51:38.517847: step 5600, loss 0.000344308, acc 1

Evaluation:
2017-08-08T16:51:39.186835: step 5600, loss 1.74323, acc 0.730769

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-5600

2017-08-08T16:51:39.626607: step 5601, loss 0.0116855, acc 1
2017-08-08T16:51:39.970874: step 5602, loss 0.000569473, acc 1
2017-08-08T16:51:40.314549: step 5603, loss 0.00448243, acc 1
2017-08-08T16:51:40.627319: step 5604, loss 0.0389438, acc 0.984375
2017-08-08T16:51:40.850953: step 5605, loss 0.00121821, acc 1
2017-08-08T16:51:41.194802: step 5606, loss 0.000177706, acc 1
2017-08-08T16:51:41.406920: step 5607, loss 0.000149838, acc 1
2017-08-08T16:51:41.638370: step 5608, loss 0.00129721, acc 1
2017-08-08T16:51:41.878838: step 5609, loss 0.00300789, acc 1
2017-08-08T16:51:42.249710: step 5610, loss 0.0016836, acc 1
2017-08-08T16:51:42.596071: step 5611, loss 0.00078001, acc 1
2017-08-08T16:51:42.917735: step 5612, loss 0.0014575, acc 1
2017-08-08T16:51:43.166137: step 5613, loss 5.39165e-05, acc 1
2017-08-08T16:51:43.513793: step 5614, loss 0.0026319, acc 1
2017-08-08T16:51:43.714780: step 5615, loss 0.00106792, acc 1
2017-08-08T16:51:43.917257: step 5616, loss 0.00120631, acc 1
2017-08-08T16:51:44.208718: step 5617, loss 0.000744883, acc 1
2017-08-08T16:51:44.516981: step 5618, loss 0.00150497, acc 1
2017-08-08T16:51:44.772975: step 5619, loss 0.013413, acc 0.984375
2017-08-08T16:51:45.102471: step 5620, loss 0.118206, acc 0.96875
2017-08-08T16:51:45.410421: step 5621, loss 0.00203033, acc 1
2017-08-08T16:51:45.645136: step 5622, loss 0.000790457, acc 1
2017-08-08T16:51:45.874345: step 5623, loss 0.000159215, acc 1
2017-08-08T16:51:46.222192: step 5624, loss 0.00197052, acc 1
2017-08-08T16:51:46.507911: step 5625, loss 0.000407091, acc 1
2017-08-08T16:51:46.802234: step 5626, loss 0.00335634, acc 1
2017-08-08T16:51:47.002326: step 5627, loss 0.0140519, acc 0.984375
2017-08-08T16:51:47.219593: step 5628, loss 0.00188364, acc 1
2017-08-08T16:51:47.543546: step 5629, loss 0.029165, acc 0.984375
2017-08-08T16:51:47.757059: step 5630, loss 0.000497877, acc 1
2017-08-08T16:51:47.990766: step 5631, loss 0.0131258, acc 1
2017-08-08T16:51:48.234760: step 5632, loss 0.000167211, acc 1
2017-08-08T16:51:48.452168: step 5633, loss 0.000896757, acc 1
2017-08-08T16:51:48.684622: step 5634, loss 0.00430892, acc 1
2017-08-08T16:51:48.955636: step 5635, loss 0.000384205, acc 1
2017-08-08T16:51:49.165356: step 5636, loss 0.0012361, acc 1
2017-08-08T16:51:49.369550: step 5637, loss 0.00473914, acc 1
2017-08-08T16:51:49.731908: step 5638, loss 0.00123604, acc 1
2017-08-08T16:51:50.033607: step 5639, loss 0.000118301, acc 1
2017-08-08T16:51:50.328268: step 5640, loss 0.00465112, acc 1
2017-08-08T16:51:50.675230: step 5641, loss 0.0048069, acc 1
2017-08-08T16:51:51.043315: step 5642, loss 0.000213832, acc 1
2017-08-08T16:51:51.408284: step 5643, loss 0.00107583, acc 1
2017-08-08T16:51:51.670042: step 5644, loss 0.0012424, acc 1
2017-08-08T16:51:51.904538: step 5645, loss 0.0220719, acc 0.984375
2017-08-08T16:51:52.260422: step 5646, loss 0.0109215, acc 1
2017-08-08T16:51:52.593778: step 5647, loss 0.00101464, acc 1
2017-08-08T16:51:52.848848: step 5648, loss 0.000240432, acc 1
2017-08-08T16:51:53.069880: step 5649, loss 0.000128494, acc 1
2017-08-08T16:51:53.369381: step 5650, loss 0.0170635, acc 0.984375
2017-08-08T16:51:53.685827: step 5651, loss 0.00429787, acc 1
2017-08-08T16:51:54.035010: step 5652, loss 0.00590462, acc 1
2017-08-08T16:51:54.337943: step 5653, loss 0.000840696, acc 1
2017-08-08T16:51:54.544474: step 5654, loss 0.000810902, acc 1
2017-08-08T16:51:54.867360: step 5655, loss 0.000539347, acc 1
2017-08-08T16:51:55.114592: step 5656, loss 0.000702181, acc 1
2017-08-08T16:51:55.422122: step 5657, loss 0.0197139, acc 0.984375
2017-08-08T16:51:55.816354: step 5658, loss 0.000972318, acc 1
2017-08-08T16:51:56.229360: step 5659, loss 0.00197729, acc 1
2017-08-08T16:51:56.609318: step 5660, loss 0.000788087, acc 1
2017-08-08T16:51:56.850784: step 5661, loss 0.000319486, acc 1
2017-08-08T16:51:57.093704: step 5662, loss 0.00564823, acc 1
2017-08-08T16:51:57.502025: step 5663, loss 0.00155357, acc 1
2017-08-08T16:51:57.874989: step 5664, loss 0.00315259, acc 1
2017-08-08T16:51:58.187931: step 5665, loss 0.00112588, acc 1
2017-08-08T16:51:58.501375: step 5666, loss 0.000541885, acc 1
2017-08-08T16:51:58.905314: step 5667, loss 0.0128961, acc 1
2017-08-08T16:51:59.295227: step 5668, loss 0.00023172, acc 1
2017-08-08T16:51:59.535754: step 5669, loss 0.0113291, acc 0.984375
2017-08-08T16:51:59.811838: step 5670, loss 0.00148106, acc 1
2017-08-08T16:52:00.315637: step 5671, loss 0.000415521, acc 1
2017-08-08T16:52:00.637740: step 5672, loss 0.000495991, acc 1
2017-08-08T16:52:00.932533: step 5673, loss 0.0619314, acc 0.984375
2017-08-08T16:52:01.215851: step 5674, loss 0.00198427, acc 1
2017-08-08T16:52:01.640320: step 5675, loss 0.000562586, acc 1
2017-08-08T16:52:02.021246: step 5676, loss 0.0108731, acc 1
2017-08-08T16:52:02.487190: step 5677, loss 0.00127386, acc 1
2017-08-08T16:52:02.840242: step 5678, loss 0.000447884, acc 1
2017-08-08T16:52:03.146143: step 5679, loss 0.00226969, acc 1
2017-08-08T16:52:03.519875: step 5680, loss 0.00016863, acc 1
2017-08-08T16:52:03.883699: step 5681, loss 0.00714646, acc 1
2017-08-08T16:52:04.210604: step 5682, loss 0.00338239, acc 1
2017-08-08T16:52:04.528213: step 5683, loss 0.00365206, acc 1
2017-08-08T16:52:04.966166: step 5684, loss 0.00873658, acc 1
2017-08-08T16:52:05.413761: step 5685, loss 0.000180246, acc 1
2017-08-08T16:52:05.762440: step 5686, loss 0.00711618, acc 1
2017-08-08T16:52:06.007082: step 5687, loss 0.00133855, acc 1
2017-08-08T16:52:06.226582: step 5688, loss 0.0302122, acc 0.984375
2017-08-08T16:52:06.577779: step 5689, loss 0.000560687, acc 1
2017-08-08T16:52:06.853228: step 5690, loss 0.00125112, acc 1
2017-08-08T16:52:07.133541: step 5691, loss 0.0057699, acc 1
2017-08-08T16:52:07.361483: step 5692, loss 0.00378226, acc 1
2017-08-08T16:52:07.673466: step 5693, loss 0.00297614, acc 1
2017-08-08T16:52:08.017366: step 5694, loss 0.000488075, acc 1
2017-08-08T16:52:08.330253: step 5695, loss 0.000950297, acc 1
2017-08-08T16:52:08.525778: step 5696, loss 0.000265677, acc 1
2017-08-08T16:52:08.846460: step 5697, loss 0.0020323, acc 1
2017-08-08T16:52:09.100822: step 5698, loss 0.000518177, acc 1
2017-08-08T16:52:09.337847: step 5699, loss 0.00308124, acc 1
2017-08-08T16:52:09.602060: step 5700, loss 0.00120963, acc 1

Evaluation:
2017-08-08T16:52:10.314326: step 5700, loss 1.74141, acc 0.737336

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-5700

2017-08-08T16:52:10.755698: step 5701, loss 0.000533191, acc 1
2017-08-08T16:52:11.069415: step 5702, loss 0.0140821, acc 0.984375
2017-08-08T16:52:11.283463: step 5703, loss 0.00034426, acc 1
2017-08-08T16:52:11.509935: step 5704, loss 8.93554e-05, acc 1
2017-08-08T16:52:11.751917: step 5705, loss 0.000966309, acc 1
2017-08-08T16:52:12.169469: step 5706, loss 0.00128346, acc 1
2017-08-08T16:52:12.449786: step 5707, loss 0.00244123, acc 1
2017-08-08T16:52:12.714930: step 5708, loss 0.00534125, acc 1
2017-08-08T16:52:12.902551: step 5709, loss 3.82233e-05, acc 1
2017-08-08T16:52:13.275116: step 5710, loss 0.00303204, acc 1
2017-08-08T16:52:13.536837: step 5711, loss 0.000257967, acc 1
2017-08-08T16:52:13.772420: step 5712, loss 0.00265825, acc 1
2017-08-08T16:52:14.005429: step 5713, loss 0.000175257, acc 1
2017-08-08T16:52:14.265421: step 5714, loss 0.00196797, acc 1
2017-08-08T16:52:14.685186: step 5715, loss 0.00194504, acc 1
2017-08-08T16:52:14.983440: step 5716, loss 0.00134391, acc 1
2017-08-08T16:52:15.231774: step 5717, loss 0.00370123, acc 1
2017-08-08T16:52:15.496712: step 5718, loss 0.000173771, acc 1
2017-08-08T16:52:15.946667: step 5719, loss 0.00181002, acc 1
2017-08-08T16:52:16.221443: step 5720, loss 0.0587066, acc 0.984375
2017-08-08T16:52:16.518534: step 5721, loss 0.000490996, acc 1
2017-08-08T16:52:16.899801: step 5722, loss 0.00255486, acc 1
2017-08-08T16:52:17.314941: step 5723, loss 7.71811e-05, acc 1
2017-08-08T16:52:17.602405: step 5724, loss 0.00108267, acc 1
2017-08-08T16:52:17.879385: step 5725, loss 0.000424273, acc 1
2017-08-08T16:52:18.119372: step 5726, loss 0.00174255, acc 1
2017-08-08T16:52:18.521883: step 5727, loss 0.00678038, acc 1
2017-08-08T16:52:18.818436: step 5728, loss 0.00184925, acc 1
2017-08-08T16:52:19.112046: step 5729, loss 0.0181163, acc 0.984375
2017-08-08T16:52:19.394843: step 5730, loss 0.00214957, acc 1
2017-08-08T16:52:19.781363: step 5731, loss 0.00237757, acc 1
2017-08-08T16:52:20.261981: step 5732, loss 0.000554763, acc 1
2017-08-08T16:52:20.593502: step 5733, loss 0.000367268, acc 1
2017-08-08T16:52:20.853535: step 5734, loss 0.00577401, acc 1
2017-08-08T16:52:21.114222: step 5735, loss 0.00730486, acc 1
2017-08-08T16:52:21.449343: step 5736, loss 0.00084895, acc 1
2017-08-08T16:52:21.750361: step 5737, loss 0.00062055, acc 1
2017-08-08T16:52:22.005567: step 5738, loss 0.000144358, acc 1
2017-08-08T16:52:22.419395: step 5739, loss 0.00492251, acc 1
2017-08-08T16:52:22.753624: step 5740, loss 0.00820531, acc 1
2017-08-08T16:52:23.013316: step 5741, loss 0.00712802, acc 1
2017-08-08T16:52:23.256386: step 5742, loss 0.00743571, acc 1
2017-08-08T16:52:23.449352: step 5743, loss 0.000308934, acc 1
2017-08-08T16:52:23.723703: step 5744, loss 0.00176944, acc 1
2017-08-08T16:52:23.960102: step 5745, loss 0.00106989, acc 1
2017-08-08T16:52:24.292368: step 5746, loss 0.0160092, acc 0.984375
2017-08-08T16:52:24.658072: step 5747, loss 0.000554607, acc 1
2017-08-08T16:52:24.997382: step 5748, loss 0.00123101, acc 1
2017-08-08T16:52:25.273811: step 5749, loss 0.00372493, acc 1
2017-08-08T16:52:25.495292: step 5750, loss 0.0123802, acc 1
2017-08-08T16:52:25.689141: step 5751, loss 0.000291777, acc 1
2017-08-08T16:52:25.975117: step 5752, loss 0.00455048, acc 1
2017-08-08T16:52:26.315494: step 5753, loss 0.000483141, acc 1
2017-08-08T16:52:26.631259: step 5754, loss 0.000363086, acc 1
2017-08-08T16:52:26.932081: step 5755, loss 0.000793072, acc 1
2017-08-08T16:52:27.293438: step 5756, loss 4.89101e-05, acc 1
2017-08-08T16:52:27.654541: step 5757, loss 0.00121739, acc 1
2017-08-08T16:52:28.000469: step 5758, loss 0.00213389, acc 1
2017-08-08T16:52:28.251674: step 5759, loss 0.00131146, acc 1
2017-08-08T16:52:28.584872: step 5760, loss 0.00369404, acc 1
2017-08-08T16:52:28.850656: step 5761, loss 0.000839135, acc 1
2017-08-08T16:52:29.127971: step 5762, loss 0.00102233, acc 1
2017-08-08T16:52:29.382955: step 5763, loss 0.00073674, acc 1
2017-08-08T16:52:29.617705: step 5764, loss 0.00180246, acc 1
2017-08-08T16:52:30.060129: step 5765, loss 0.000502152, acc 1
2017-08-08T16:52:30.485201: step 5766, loss 0.000449524, acc 1
2017-08-08T16:52:30.792126: step 5767, loss 0.00132019, acc 1
2017-08-08T16:52:31.024588: step 5768, loss 0.000790938, acc 1
2017-08-08T16:52:31.266730: step 5769, loss 0.00030848, acc 1
2017-08-08T16:52:31.649394: step 5770, loss 0.00119319, acc 1
2017-08-08T16:52:31.925253: step 5771, loss 4.21084e-05, acc 1
2017-08-08T16:52:32.212701: step 5772, loss 0.00168171, acc 1
2017-08-08T16:52:32.526317: step 5773, loss 0.000621378, acc 1
2017-08-08T16:52:32.883997: step 5774, loss 0.00137242, acc 1
2017-08-08T16:52:33.256063: step 5775, loss 0.000330148, acc 1
2017-08-08T16:52:33.672166: step 5776, loss 0.000461878, acc 1
2017-08-08T16:52:33.943007: step 5777, loss 0.000153822, acc 1
2017-08-08T16:52:34.270730: step 5778, loss 0.00331367, acc 1
2017-08-08T16:52:34.555641: step 5779, loss 0.00013519, acc 1
2017-08-08T16:52:34.782450: step 5780, loss 0.00880728, acc 1
2017-08-08T16:52:34.985984: step 5781, loss 0.000430271, acc 1
2017-08-08T16:52:35.195040: step 5782, loss 0.00427211, acc 1
2017-08-08T16:52:35.510674: step 5783, loss 0.00276712, acc 1
2017-08-08T16:52:35.785292: step 5784, loss 0.0148375, acc 0.984375
2017-08-08T16:52:36.095082: step 5785, loss 0.00109015, acc 1
2017-08-08T16:52:36.294934: step 5786, loss 0.00123781, acc 1
2017-08-08T16:52:36.509852: step 5787, loss 0.00816327, acc 1
2017-08-08T16:52:36.854315: step 5788, loss 0.00211886, acc 1
2017-08-08T16:52:37.087960: step 5789, loss 0.000868873, acc 1
2017-08-08T16:52:37.313161: step 5790, loss 8.04671e-05, acc 1
2017-08-08T16:52:37.589426: step 5791, loss 0.000745311, acc 1
2017-08-08T16:52:37.844403: step 5792, loss 0.000640428, acc 1
2017-08-08T16:52:38.117481: step 5793, loss 0.0407644, acc 0.984375
2017-08-08T16:52:38.294745: step 5794, loss 0.00033367, acc 1
2017-08-08T16:52:38.477033: step 5795, loss 0.0037866, acc 1
2017-08-08T16:52:38.741478: step 5796, loss 3.36091e-05, acc 1
2017-08-08T16:52:38.981911: step 5797, loss 0.000353812, acc 1
2017-08-08T16:52:39.207395: step 5798, loss 0.000877663, acc 1
2017-08-08T16:52:39.571776: step 5799, loss 0.0019604, acc 1
2017-08-08T16:52:39.842081: step 5800, loss 0.000718377, acc 1

Evaluation:
2017-08-08T16:52:40.557572: step 5800, loss 1.88619, acc 0.711069

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-5800

2017-08-08T16:52:41.127143: step 5801, loss 0.00219819, acc 1
2017-08-08T16:52:41.318965: step 5802, loss 0.0124935, acc 0.984375
2017-08-08T16:52:41.639434: step 5803, loss 0.00176078, acc 1
2017-08-08T16:52:41.914125: step 5804, loss 0.00217197, acc 1
2017-08-08T16:52:42.275929: step 5805, loss 0.000549802, acc 1
2017-08-08T16:52:42.533208: step 5806, loss 0.0124954, acc 1
2017-08-08T16:52:42.881337: step 5807, loss 0.00041897, acc 1
2017-08-08T16:52:43.223372: step 5808, loss 0.000823831, acc 1
2017-08-08T16:52:43.473095: step 5809, loss 0.000379062, acc 1
2017-08-08T16:52:43.833239: step 5810, loss 0.0181675, acc 0.984375
2017-08-08T16:52:44.066353: step 5811, loss 0.00117539, acc 1
2017-08-08T16:52:44.467973: step 5812, loss 0.00267258, acc 1
2017-08-08T16:52:44.682478: step 5813, loss 0.0494021, acc 0.984375
2017-08-08T16:52:45.090195: step 5814, loss 0.00115724, acc 1
2017-08-08T16:52:45.415881: step 5815, loss 0.00116075, acc 1
2017-08-08T16:52:45.827417: step 5816, loss 0.000545754, acc 1
2017-08-08T16:52:46.176508: step 5817, loss 0.000546438, acc 1
2017-08-08T16:52:46.615719: step 5818, loss 0.000469056, acc 1
2017-08-08T16:52:46.922529: step 5819, loss 0.000747985, acc 1
2017-08-08T16:52:47.322709: step 5820, loss 0.00166666, acc 1
2017-08-08T16:52:47.616917: step 5821, loss 0.000415454, acc 1
2017-08-08T16:52:47.939814: step 5822, loss 0.0103667, acc 1
2017-08-08T16:52:48.280647: step 5823, loss 0.00146148, acc 1
2017-08-08T16:52:48.487378: step 5824, loss 0.000521833, acc 1
2017-08-08T16:52:48.701125: step 5825, loss 0.0152653, acc 0.984375
2017-08-08T16:52:49.051084: step 5826, loss 0.0197981, acc 1
2017-08-08T16:52:49.300718: step 5827, loss 0.000375207, acc 1
2017-08-08T16:52:49.525307: step 5828, loss 0.00775462, acc 1
2017-08-08T16:52:49.901649: step 5829, loss 0.0032979, acc 1
2017-08-08T16:52:50.194134: step 5830, loss 2.37715e-05, acc 1
2017-08-08T16:52:50.460956: step 5831, loss 0.00275652, acc 1
2017-08-08T16:52:50.656518: step 5832, loss 0.000737423, acc 1
2017-08-08T16:52:50.968545: step 5833, loss 0.000697895, acc 1
2017-08-08T16:52:51.293305: step 5834, loss 0.00316468, acc 1
2017-08-08T16:52:51.655129: step 5835, loss 0.00437175, acc 1
2017-08-08T16:52:51.859488: step 5836, loss 0.00124144, acc 1
2017-08-08T16:52:52.181246: step 5837, loss 0.00142636, acc 1
2017-08-08T16:52:52.602519: step 5838, loss 0.000648782, acc 1
2017-08-08T16:52:52.846853: step 5839, loss 0.00029157, acc 1
2017-08-08T16:52:53.109378: step 5840, loss 0.000320903, acc 1
2017-08-08T16:52:53.425360: step 5841, loss 0.000158892, acc 1
2017-08-08T16:52:53.890482: step 5842, loss 0.00973392, acc 1
2017-08-08T16:52:54.258454: step 5843, loss 9.05617e-05, acc 1
2017-08-08T16:52:54.501437: step 5844, loss 0.000185381, acc 1
2017-08-08T16:52:54.756309: step 5845, loss 0.0443721, acc 0.984375
2017-08-08T16:52:55.106042: step 5846, loss 0.000269039, acc 1
2017-08-08T16:52:55.359002: step 5847, loss 0.00230372, acc 1
2017-08-08T16:52:55.611213: step 5848, loss 0.000647718, acc 1
2017-08-08T16:52:55.970080: step 5849, loss 0.00587433, acc 1
2017-08-08T16:52:56.358685: step 5850, loss 0.00739348, acc 1
2017-08-08T16:52:56.742318: step 5851, loss 0.000191578, acc 1
2017-08-08T16:52:57.088875: step 5852, loss 0.00953162, acc 1
2017-08-08T16:52:57.381307: step 5853, loss 0.0130878, acc 0.984375
2017-08-08T16:52:57.853677: step 5854, loss 0.00125059, acc 1
2017-08-08T16:52:58.085578: step 5855, loss 0.000191398, acc 1
2017-08-08T16:52:58.306664: step 5856, loss 0.0322932, acc 0.984375
2017-08-08T16:52:58.582162: step 5857, loss 0.000124377, acc 1
2017-08-08T16:52:58.917605: step 5858, loss 0.00248935, acc 1
2017-08-08T16:52:59.304087: step 5859, loss 0.0161693, acc 0.984375
2017-08-08T16:52:59.573348: step 5860, loss 0.00413772, acc 1
2017-08-08T16:52:59.889799: step 5861, loss 0.000295917, acc 1
2017-08-08T16:53:00.126940: step 5862, loss 0.0102766, acc 1
2017-08-08T16:53:00.379631: step 5863, loss 0.000221204, acc 1
2017-08-08T16:53:00.721791: step 5864, loss 0.000146549, acc 1
2017-08-08T16:53:01.017875: step 5865, loss 0.00128877, acc 1
2017-08-08T16:53:01.257310: step 5866, loss 0.00470114, acc 1
2017-08-08T16:53:01.558660: step 5867, loss 0.00209864, acc 1
2017-08-08T16:53:01.990690: step 5868, loss 0.00315465, acc 1
2017-08-08T16:53:02.445060: step 5869, loss 0.000194213, acc 1
2017-08-08T16:53:02.774059: step 5870, loss 0.0012829, acc 1
2017-08-08T16:53:03.144451: step 5871, loss 0.00135396, acc 1
2017-08-08T16:53:03.542549: step 5872, loss 0.00138432, acc 1
2017-08-08T16:53:03.871044: step 5873, loss 0.000591398, acc 1
2017-08-08T16:53:04.191163: step 5874, loss 0.00108883, acc 1
2017-08-08T16:53:04.695380: step 5875, loss 0.000802798, acc 1
2017-08-08T16:53:05.106030: step 5876, loss 0.0388974, acc 0.984375
2017-08-08T16:53:05.414272: step 5877, loss 0.000952202, acc 1
2017-08-08T16:53:05.708053: step 5878, loss 0.00182306, acc 1
2017-08-08T16:53:05.947910: step 5879, loss 0.00113142, acc 1
2017-08-08T16:53:06.321835: step 5880, loss 0.00434956, acc 1
2017-08-08T16:53:06.519646: step 5881, loss 0.0119032, acc 0.984375
2017-08-08T16:53:06.756774: step 5882, loss 0.000980182, acc 1
2017-08-08T16:53:06.965696: step 5883, loss 0.000332428, acc 1
2017-08-08T16:53:07.271013: step 5884, loss 0.000185418, acc 1
2017-08-08T16:53:07.585379: step 5885, loss 0.00039466, acc 1
2017-08-08T16:53:07.918274: step 5886, loss 0.00214363, acc 1
2017-08-08T16:53:08.136042: step 5887, loss 0.000974194, acc 1
2017-08-08T16:53:08.474147: step 5888, loss 0.00371124, acc 1
2017-08-08T16:53:08.882231: step 5889, loss 0.00171777, acc 1
2017-08-08T16:53:09.104344: step 5890, loss 0.000384656, acc 1
2017-08-08T16:53:09.312018: step 5891, loss 0.000345036, acc 1
2017-08-08T16:53:09.541359: step 5892, loss 0.0130078, acc 0.984375
2017-08-08T16:53:09.904352: step 5893, loss 0.00164482, acc 1
2017-08-08T16:53:10.303829: step 5894, loss 0.0013001, acc 1
2017-08-08T16:53:10.680790: step 5895, loss 0.000206096, acc 1
2017-08-08T16:53:10.963907: step 5896, loss 0.000850008, acc 1
2017-08-08T16:53:11.439610: step 5897, loss 0.000608313, acc 1
2017-08-08T16:53:11.762267: step 5898, loss 0.000695911, acc 1
2017-08-08T16:53:11.991213: step 5899, loss 0.00136723, acc 1
2017-08-08T16:53:12.316342: step 5900, loss 0.020067, acc 0.984375

Evaluation:
2017-08-08T16:53:13.090447: step 5900, loss 1.79614, acc 0.734522

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-5900

2017-08-08T16:53:13.557668: step 5901, loss 0.00283663, acc 1
2017-08-08T16:53:13.827911: step 5902, loss 0.00710309, acc 1
2017-08-08T16:53:14.144458: step 5903, loss 0.000703575, acc 1
2017-08-08T16:53:14.347390: step 5904, loss 0.000170391, acc 1
2017-08-08T16:53:14.600407: step 5905, loss 0.00591341, acc 1
2017-08-08T16:53:14.895723: step 5906, loss 0.03532, acc 0.984375
2017-08-08T16:53:15.305844: step 5907, loss 0.000626701, acc 1
2017-08-08T16:53:15.660763: step 5908, loss 0.000461768, acc 1
2017-08-08T16:53:15.972313: step 5909, loss 0.000455919, acc 1
2017-08-08T16:53:16.213638: step 5910, loss 0.000185788, acc 1
2017-08-08T16:53:16.550597: step 5911, loss 0.00223548, acc 1
2017-08-08T16:53:16.763040: step 5912, loss 0.000328946, acc 1
2017-08-08T16:53:17.012800: step 5913, loss 0.000573558, acc 1
2017-08-08T16:53:17.257369: step 5914, loss 0.00016232, acc 1
2017-08-08T16:53:17.653971: step 5915, loss 0.000117101, acc 1
2017-08-08T16:53:17.960010: step 5916, loss 0.0016269, acc 1
2017-08-08T16:53:18.189415: step 5917, loss 0.00121598, acc 1
2017-08-08T16:53:18.422690: step 5918, loss 0.000299446, acc 1
2017-08-08T16:53:18.736243: step 5919, loss 0.00564453, acc 1
2017-08-08T16:53:18.925353: step 5920, loss 0.000381334, acc 1
2017-08-08T16:53:19.143581: step 5921, loss 0.00655983, acc 1
2017-08-08T16:53:19.360650: step 5922, loss 0.00243857, acc 1
2017-08-08T16:53:19.666086: step 5923, loss 0.00721716, acc 1
2017-08-08T16:53:20.021362: step 5924, loss 0.000259358, acc 1
2017-08-08T16:53:20.280463: step 5925, loss 0.000549515, acc 1
2017-08-08T16:53:20.479594: step 5926, loss 0.00248445, acc 1
2017-08-08T16:53:20.667594: step 5927, loss 0.000424883, acc 1
2017-08-08T16:53:21.025594: step 5928, loss 0.00235289, acc 1
2017-08-08T16:53:21.311669: step 5929, loss 0.00596078, acc 1
2017-08-08T16:53:21.587703: step 5930, loss 0.00094038, acc 1
2017-08-08T16:53:21.885371: step 5931, loss 0.00251153, acc 1
2017-08-08T16:53:22.325789: step 5932, loss 0.00129732, acc 1
2017-08-08T16:53:22.676023: step 5933, loss 0.000456198, acc 1
2017-08-08T16:53:23.022905: step 5934, loss 0.000957893, acc 1
2017-08-08T16:53:23.400812: step 5935, loss 0.0109926, acc 1
2017-08-08T16:53:23.622623: step 5936, loss 0.00847319, acc 1
2017-08-08T16:53:23.949394: step 5937, loss 0.000307379, acc 1
2017-08-08T16:53:24.237791: step 5938, loss 0.000416499, acc 1
2017-08-08T16:53:24.505180: step 5939, loss 0.000475874, acc 1
2017-08-08T16:53:24.729321: step 5940, loss 0.00982632, acc 1
2017-08-08T16:53:25.053309: step 5941, loss 0.00153889, acc 1
2017-08-08T16:53:25.297327: step 5942, loss 0.000512979, acc 1
2017-08-08T16:53:25.475055: step 5943, loss 0.00127633, acc 1
2017-08-08T16:53:25.688399: step 5944, loss 0.00294113, acc 1
2017-08-08T16:53:26.093364: step 5945, loss 0.00125106, acc 1
2017-08-08T16:53:26.315876: step 5946, loss 0.0038472, acc 1
2017-08-08T16:53:26.532424: step 5947, loss 0.000544449, acc 1
2017-08-08T16:53:26.733603: step 5948, loss 0.000411812, acc 1
2017-08-08T16:53:26.943571: step 5949, loss 0.00773874, acc 1
2017-08-08T16:53:27.323268: step 5950, loss 0.000193126, acc 1
2017-08-08T16:53:27.752499: step 5951, loss 0.0106536, acc 1
2017-08-08T16:53:28.151013: step 5952, loss 0.000769823, acc 1
2017-08-08T16:53:28.406646: step 5953, loss 0.0188091, acc 0.984375
2017-08-08T16:53:28.811653: step 5954, loss 0.00352331, acc 1
2017-08-08T16:53:29.173276: step 5955, loss 0.00628729, acc 1
2017-08-08T16:53:29.472225: step 5956, loss 0.000131654, acc 1
2017-08-08T16:53:29.804601: step 5957, loss 0.0138608, acc 1
2017-08-08T16:53:30.266031: step 5958, loss 0.00103773, acc 1
2017-08-08T16:53:30.729508: step 5959, loss 0.000259417, acc 1
2017-08-08T16:53:31.068903: step 5960, loss 0.000122371, acc 1
2017-08-08T16:53:31.331146: step 5961, loss 0.017555, acc 0.984375
2017-08-08T16:53:31.604845: step 5962, loss 0.00214384, acc 1
2017-08-08T16:53:31.960202: step 5963, loss 0.00191435, acc 1
2017-08-08T16:53:32.159250: step 5964, loss 0.0190656, acc 0.984375
2017-08-08T16:53:32.393980: step 5965, loss 0.00506117, acc 1
2017-08-08T16:53:32.729233: step 5966, loss 0.0051983, acc 1
2017-08-08T16:53:33.028427: step 5967, loss 0.00420047, acc 1
2017-08-08T16:53:33.314806: step 5968, loss 0.0589706, acc 0.984375
2017-08-08T16:53:33.574254: step 5969, loss 0.00658467, acc 1
2017-08-08T16:53:33.825999: step 5970, loss 0.00116827, acc 1
2017-08-08T16:53:34.241492: step 5971, loss 0.000407218, acc 1
2017-08-08T16:53:34.432043: step 5972, loss 0.00423279, acc 1
2017-08-08T16:53:34.705732: step 5973, loss 0.00828053, acc 1
2017-08-08T16:53:34.973673: step 5974, loss 0.0124351, acc 1
2017-08-08T16:53:35.299915: step 5975, loss 0.000410051, acc 1
2017-08-08T16:53:35.559657: step 5976, loss 0.00118277, acc 1
2017-08-08T16:53:35.840497: step 5977, loss 0.00257225, acc 1
2017-08-08T16:53:36.037226: step 5978, loss 0.00316218, acc 1
2017-08-08T16:53:36.350077: step 5979, loss 0.00522482, acc 1
2017-08-08T16:53:36.584358: step 5980, loss 0.000251125, acc 1
2017-08-08T16:53:36.812393: step 5981, loss 0.000507165, acc 1
2017-08-08T16:53:37.089309: step 5982, loss 0.00718479, acc 1
2017-08-08T16:53:37.453248: step 5983, loss 0.00292291, acc 1
2017-08-08T16:53:37.757317: step 5984, loss 0.000607454, acc 1
2017-08-08T16:53:37.923144: step 5985, loss 0.00902198, acc 1
2017-08-08T16:53:38.108556: step 5986, loss 0.000972037, acc 1
2017-08-08T16:53:38.417366: step 5987, loss 0.00176959, acc 1
2017-08-08T16:53:38.668339: step 5988, loss 0.00876303, acc 1
2017-08-08T16:53:38.890952: step 5989, loss 0.00869421, acc 1
2017-08-08T16:53:39.175359: step 5990, loss 0.00153003, acc 1
2017-08-08T16:53:39.512350: step 5991, loss 9.74462e-05, acc 1
2017-08-08T16:53:39.807326: step 5992, loss 0.000679396, acc 1
2017-08-08T16:53:40.048526: step 5993, loss 0.00733297, acc 1
2017-08-08T16:53:40.275257: step 5994, loss 0.000438707, acc 1
2017-08-08T16:53:40.595391: step 5995, loss 0.00474432, acc 1
2017-08-08T16:53:40.798936: step 5996, loss 0.000591769, acc 1
2017-08-08T16:53:41.033301: step 5997, loss 0.00101194, acc 1
2017-08-08T16:53:41.325129: step 5998, loss 0.000704599, acc 1
2017-08-08T16:53:41.745330: step 5999, loss 0.000926509, acc 1
2017-08-08T16:53:41.965313: step 6000, loss 0.00527646, acc 1

Evaluation:
2017-08-08T16:53:42.427107: step 6000, loss 1.81297, acc 0.73546

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-6000

2017-08-08T16:53:42.871635: step 6001, loss 0.00165587, acc 1
2017-08-08T16:53:43.095123: step 6002, loss 0.00411504, acc 1
2017-08-08T16:53:43.334340: step 6003, loss 0.000388157, acc 1
2017-08-08T16:53:43.571070: step 6004, loss 0.00335349, acc 1
2017-08-08T16:53:43.905379: step 6005, loss 0.000414165, acc 1
2017-08-08T16:53:44.353962: step 6006, loss 0.000123898, acc 1
2017-08-08T16:53:44.745505: step 6007, loss 0.000604496, acc 1
2017-08-08T16:53:45.007330: step 6008, loss 0.00042237, acc 1
2017-08-08T16:53:45.292178: step 6009, loss 0.00169765, acc 1
2017-08-08T16:53:45.568377: step 6010, loss 0.038952, acc 0.984375
2017-08-08T16:53:45.849142: step 6011, loss 0.0012831, acc 1
2017-08-08T16:53:46.025731: step 6012, loss 0.000765804, acc 1
2017-08-08T16:53:46.270893: step 6013, loss 0.00135381, acc 1
2017-08-08T16:53:46.620056: step 6014, loss 0.000154843, acc 1
2017-08-08T16:53:46.857102: step 6015, loss 0.00016661, acc 1
2017-08-08T16:53:47.090819: step 6016, loss 0.000744065, acc 1
2017-08-08T16:53:47.305723: step 6017, loss 0.000602648, acc 1
2017-08-08T16:53:47.565775: step 6018, loss 0.000263584, acc 1
2017-08-08T16:53:47.823342: step 6019, loss 0.00265411, acc 1
2017-08-08T16:53:48.020220: step 6020, loss 0.0298985, acc 0.984375
2017-08-08T16:53:48.246315: step 6021, loss 0.000394604, acc 1
2017-08-08T16:53:48.573434: step 6022, loss 0.019216, acc 0.984375
2017-08-08T16:53:49.016502: step 6023, loss 0.00103034, acc 1
2017-08-08T16:53:49.294520: step 6024, loss 0.000384298, acc 1
2017-08-08T16:53:49.514263: step 6025, loss 0.000397824, acc 1
2017-08-08T16:53:49.795366: step 6026, loss 0.000183773, acc 1
2017-08-08T16:53:50.198040: step 6027, loss 0.00258521, acc 1
2017-08-08T16:53:50.463513: step 6028, loss 0.0132578, acc 1
2017-08-08T16:53:50.743863: step 6029, loss 0.000675133, acc 1
2017-08-08T16:53:51.042679: step 6030, loss 0.00169373, acc 1
2017-08-08T16:53:51.393559: step 6031, loss 0.00302923, acc 1
2017-08-08T16:53:51.724295: step 6032, loss 0.00011317, acc 1
2017-08-08T16:53:52.005391: step 6033, loss 0.00434093, acc 1
2017-08-08T16:53:52.280532: step 6034, loss 0.000200306, acc 1
2017-08-08T16:53:52.589328: step 6035, loss 0.00125101, acc 1
2017-08-08T16:53:52.967436: step 6036, loss 0.00216972, acc 1
2017-08-08T16:53:53.265923: step 6037, loss 0.000738152, acc 1
2017-08-08T16:53:53.446048: step 6038, loss 0.104879, acc 0.984375
2017-08-08T16:53:53.641331: step 6039, loss 0.000832326, acc 1
2017-08-08T16:53:54.077341: step 6040, loss 0.00090747, acc 1
2017-08-08T16:53:54.413369: step 6041, loss 0.00187109, acc 1
2017-08-08T16:53:54.644817: step 6042, loss 0.00242965, acc 1
2017-08-08T16:53:54.849639: step 6043, loss 0.000533011, acc 1
2017-08-08T16:53:55.165375: step 6044, loss 0.000275533, acc 1
2017-08-08T16:53:55.368860: step 6045, loss 0.000880891, acc 1
2017-08-08T16:53:55.553381: step 6046, loss 8.9956e-05, acc 1
2017-08-08T16:53:55.787341: step 6047, loss 0.000199289, acc 1
2017-08-08T16:53:56.118117: step 6048, loss 0.00358371, acc 1
2017-08-08T16:53:56.405978: step 6049, loss 0.00107138, acc 1
2017-08-08T16:53:56.625302: step 6050, loss 8.74353e-05, acc 1
2017-08-08T16:53:56.797350: step 6051, loss 0.00238318, acc 1
2017-08-08T16:53:57.193636: step 6052, loss 0.00129571, acc 1
2017-08-08T16:53:57.423122: step 6053, loss 0.000449114, acc 1
2017-08-08T16:53:57.633322: step 6054, loss 0.000704149, acc 1
2017-08-08T16:53:57.897312: step 6055, loss 0.00504663, acc 1
2017-08-08T16:53:58.348202: step 6056, loss 0.000167498, acc 1
2017-08-08T16:53:58.729011: step 6057, loss 0.000119618, acc 1
2017-08-08T16:53:59.012380: step 6058, loss 0.00287937, acc 1
2017-08-08T16:53:59.336848: step 6059, loss 0.00107728, acc 1
2017-08-08T16:53:59.638932: step 6060, loss 0.00063039, acc 1
2017-08-08T16:53:59.868661: step 6061, loss 0.000372884, acc 1
2017-08-08T16:54:00.068970: step 6062, loss 0.00835724, acc 1
2017-08-08T16:54:00.389459: step 6063, loss 0.00136777, acc 1
2017-08-08T16:54:00.665317: step 6064, loss 0.00410634, acc 1
2017-08-08T16:54:00.945374: step 6065, loss 0.000440923, acc 1
2017-08-08T16:54:01.209743: step 6066, loss 0.00500899, acc 1
2017-08-08T16:54:01.425482: step 6067, loss 0.00072606, acc 1
2017-08-08T16:54:01.796508: step 6068, loss 0.000589244, acc 1
2017-08-08T16:54:02.062934: step 6069, loss 0.000218207, acc 1
2017-08-08T16:54:02.316492: step 6070, loss 0.000483991, acc 1
2017-08-08T16:54:02.629227: step 6071, loss 0.00497716, acc 1
2017-08-08T16:54:03.063807: step 6072, loss 0.00330445, acc 1
2017-08-08T16:54:03.517479: step 6073, loss 0.00247038, acc 1
2017-08-08T16:54:03.926798: step 6074, loss 0.000229294, acc 1
2017-08-08T16:54:04.202406: step 6075, loss 6.39691e-05, acc 1
2017-08-08T16:54:04.527232: step 6076, loss 0.000298565, acc 1
2017-08-08T16:54:04.986577: step 6077, loss 0.000342543, acc 1
2017-08-08T16:54:05.248831: step 6078, loss 0.00404617, acc 1
2017-08-08T16:54:05.524153: step 6079, loss 0.000106844, acc 1
2017-08-08T16:54:05.775171: step 6080, loss 7.02777e-05, acc 1
2017-08-08T16:54:06.148363: step 6081, loss 0.000190931, acc 1
2017-08-08T16:54:06.441488: step 6082, loss 0.000373438, acc 1
2017-08-08T16:54:06.699103: step 6083, loss 0.00519771, acc 1
2017-08-08T16:54:06.978466: step 6084, loss 0.00236459, acc 1
2017-08-08T16:54:07.204900: step 6085, loss 0.00168748, acc 1
2017-08-08T16:54:07.567083: step 6086, loss 0.00019003, acc 1
2017-08-08T16:54:07.767064: step 6087, loss 0.0010736, acc 1
2017-08-08T16:54:07.974039: step 6088, loss 0.000718453, acc 1
2017-08-08T16:54:08.211780: step 6089, loss 0.00784378, acc 1
2017-08-08T16:54:08.577359: step 6090, loss 0.0015128, acc 1
2017-08-08T16:54:08.967288: step 6091, loss 0.000528322, acc 1
2017-08-08T16:54:09.225669: step 6092, loss 0.00532063, acc 1
2017-08-08T16:54:09.409269: step 6093, loss 0.000252459, acc 1
2017-08-08T16:54:09.709242: step 6094, loss 0.007635, acc 1
2017-08-08T16:54:09.962933: step 6095, loss 0.00148936, acc 1
2017-08-08T16:54:10.143850: step 6096, loss 0.000649785, acc 1
2017-08-08T16:54:10.393611: step 6097, loss 0.000137845, acc 1
2017-08-08T16:54:10.691698: step 6098, loss 0.000171714, acc 1
2017-08-08T16:54:11.042466: step 6099, loss 0.0025966, acc 1
2017-08-08T16:54:11.329713: step 6100, loss 0.00101218, acc 1

Evaluation:
2017-08-08T16:54:11.895669: step 6100, loss 1.89515, acc 0.727955

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-6100

2017-08-08T16:54:12.411781: step 6101, loss 0.000855136, acc 1
2017-08-08T16:54:12.646552: step 6102, loss 0.000106687, acc 1
2017-08-08T16:54:12.858819: step 6103, loss 0.0129985, acc 1
2017-08-08T16:54:13.213352: step 6104, loss 0.00151827, acc 1
2017-08-08T16:54:13.436677: step 6105, loss 0.00054109, acc 1
2017-08-08T16:54:13.721330: step 6106, loss 0.000222604, acc 1
2017-08-08T16:54:13.936508: step 6107, loss 0.0047336, acc 1
2017-08-08T16:54:14.248521: step 6108, loss 0.0212374, acc 0.984375
2017-08-08T16:54:14.569368: step 6109, loss 0.00421756, acc 1
2017-08-08T16:54:14.837133: step 6110, loss 0.000100875, acc 1
2017-08-08T16:54:15.057380: step 6111, loss 0.00136875, acc 1
2017-08-08T16:54:15.347110: step 6112, loss 0.00418177, acc 1
2017-08-08T16:54:15.743564: step 6113, loss 0.000587204, acc 1
2017-08-08T16:54:16.129355: step 6114, loss 0.000866064, acc 1
2017-08-08T16:54:16.530843: step 6115, loss 0.000874209, acc 1
2017-08-08T16:54:16.759912: step 6116, loss 0.00476772, acc 1
2017-08-08T16:54:16.966714: step 6117, loss 0.000164251, acc 1
2017-08-08T16:54:17.344993: step 6118, loss 0.000514283, acc 1
2017-08-08T16:54:17.651713: step 6119, loss 0.0020388, acc 1
2017-08-08T16:54:17.855199: step 6120, loss 0.000155818, acc 1
2017-08-08T16:54:18.049933: step 6121, loss 0.00444153, acc 1
2017-08-08T16:54:18.309034: step 6122, loss 0.00219286, acc 1
2017-08-08T16:54:18.640888: step 6123, loss 0.00328058, acc 1
2017-08-08T16:54:18.919324: step 6124, loss 0.00295722, acc 1
2017-08-08T16:54:19.128993: step 6125, loss 0.0207584, acc 0.984375
2017-08-08T16:54:19.365360: step 6126, loss 0.00197746, acc 1
2017-08-08T16:54:19.745248: step 6127, loss 0.00122668, acc 1
2017-08-08T16:54:19.942045: step 6128, loss 0.0012029, acc 1
2017-08-08T16:54:20.162889: step 6129, loss 0.00110613, acc 1
2017-08-08T16:54:20.376833: step 6130, loss 0.00118294, acc 1
2017-08-08T16:54:20.669320: step 6131, loss 0.000638937, acc 1
2017-08-08T16:54:20.928225: step 6132, loss 0.00105619, acc 1
2017-08-08T16:54:21.138279: step 6133, loss 0.00245913, acc 1
2017-08-08T16:54:21.341785: step 6134, loss 0.00119527, acc 1
2017-08-08T16:54:21.624739: step 6135, loss 0.00172961, acc 1
2017-08-08T16:54:21.897272: step 6136, loss 0.00350008, acc 1
2017-08-08T16:54:22.114231: step 6137, loss 0.000970668, acc 1
2017-08-08T16:54:22.348765: step 6138, loss 0.00523327, acc 1
2017-08-08T16:54:22.591745: step 6139, loss 0.00113656, acc 1
2017-08-08T16:54:23.013299: step 6140, loss 0.000881705, acc 1
2017-08-08T16:54:23.383010: step 6141, loss 0.00364464, acc 1
2017-08-08T16:54:23.683404: step 6142, loss 0.00233458, acc 1
2017-08-08T16:54:23.902378: step 6143, loss 0.000922911, acc 1
2017-08-08T16:54:24.132917: step 6144, loss 0.00235732, acc 1
2017-08-08T16:54:24.645929: step 6145, loss 0.000965939, acc 1
2017-08-08T16:54:24.942741: step 6146, loss 0.000246063, acc 1
2017-08-08T16:54:25.240568: step 6147, loss 0.00103908, acc 1
2017-08-08T16:54:25.473778: step 6148, loss 0.000375629, acc 1
2017-08-08T16:54:25.805348: step 6149, loss 0.00235627, acc 1
2017-08-08T16:54:26.132975: step 6150, loss 0.0173323, acc 0.983333
2017-08-08T16:54:26.416984: step 6151, loss 0.00136943, acc 1
2017-08-08T16:54:26.651727: step 6152, loss 0.00134045, acc 1
2017-08-08T16:54:26.834530: step 6153, loss 0.00408948, acc 1
2017-08-08T16:54:27.157428: step 6154, loss 0.00116408, acc 1
2017-08-08T16:54:27.469470: step 6155, loss 0.000487114, acc 1
2017-08-08T16:54:27.709397: step 6156, loss 9.8118e-05, acc 1
2017-08-08T16:54:27.945964: step 6157, loss 0.000329223, acc 1
2017-08-08T16:54:28.279334: step 6158, loss 0.000349963, acc 1
2017-08-08T16:54:28.682934: step 6159, loss 0.000418709, acc 1
2017-08-08T16:54:29.040827: step 6160, loss 0.000113255, acc 1
2017-08-08T16:54:29.308238: step 6161, loss 0.0001839, acc 1
2017-08-08T16:54:29.497134: step 6162, loss 0.000101001, acc 1
2017-08-08T16:54:29.841359: step 6163, loss 0.000259991, acc 1
2017-08-08T16:54:30.044626: step 6164, loss 0.000852517, acc 1
2017-08-08T16:54:30.267434: step 6165, loss 0.000210786, acc 1
2017-08-08T16:54:30.509318: step 6166, loss 0.000169557, acc 1
2017-08-08T16:54:30.941362: step 6167, loss 6.82408e-05, acc 1
2017-08-08T16:54:31.369240: step 6168, loss 0.0013887, acc 1
2017-08-08T16:54:31.723688: step 6169, loss 9.70983e-05, acc 1
2017-08-08T16:54:31.962413: step 6170, loss 0.0014383, acc 1
2017-08-08T16:54:32.337356: step 6171, loss 0.00232821, acc 1
2017-08-08T16:54:32.737359: step 6172, loss 0.0093502, acc 1
2017-08-08T16:54:33.014493: step 6173, loss 0.000419001, acc 1
2017-08-08T16:54:33.278278: step 6174, loss 0.00676888, acc 1
2017-08-08T16:54:33.698500: step 6175, loss 0.00507761, acc 1
2017-08-08T16:54:34.215854: step 6176, loss 6.57703e-05, acc 1
2017-08-08T16:54:34.489668: step 6177, loss 0.000995135, acc 1
2017-08-08T16:54:34.775389: step 6178, loss 0.0343184, acc 0.96875
2017-08-08T16:54:35.068959: step 6179, loss 0.00215158, acc 1
2017-08-08T16:54:35.526985: step 6180, loss 0.000144814, acc 1
2017-08-08T16:54:35.741910: step 6181, loss 0.000169292, acc 1
2017-08-08T16:54:36.005633: step 6182, loss 0.000977856, acc 1
2017-08-08T16:54:36.269396: step 6183, loss 0.00133101, acc 1
2017-08-08T16:54:36.699521: step 6184, loss 0.0017949, acc 1
2017-08-08T16:54:37.017403: step 6185, loss 0.000842576, acc 1
2017-08-08T16:54:37.321710: step 6186, loss 0.00669011, acc 1
2017-08-08T16:54:37.539493: step 6187, loss 0.00139295, acc 1
2017-08-08T16:54:37.774002: step 6188, loss 0.00160782, acc 1
2017-08-08T16:54:38.180799: step 6189, loss 0.000658613, acc 1
2017-08-08T16:54:38.451905: step 6190, loss 0.000918333, acc 1
2017-08-08T16:54:38.752094: step 6191, loss 0.00101122, acc 1
2017-08-08T16:54:39.142108: step 6192, loss 0.0058408, acc 1
2017-08-08T16:54:39.579651: step 6193, loss 0.0011486, acc 1
2017-08-08T16:54:40.024457: step 6194, loss 0.00265715, acc 1
2017-08-08T16:54:40.256306: step 6195, loss 0.00169148, acc 1
2017-08-08T16:54:40.496546: step 6196, loss 0.00064559, acc 1
2017-08-08T16:54:40.818192: step 6197, loss 0.00386155, acc 1
2017-08-08T16:54:41.019506: step 6198, loss 0.00153328, acc 1
2017-08-08T16:54:41.272891: step 6199, loss 0.00170395, acc 1
2017-08-08T16:54:41.525948: step 6200, loss 0.00261417, acc 1

Evaluation:
2017-08-08T16:54:42.110326: step 6200, loss 1.97038, acc 0.727017

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-6200

2017-08-08T16:54:42.517348: step 6201, loss 0.00101887, acc 1
2017-08-08T16:54:42.848529: step 6202, loss 0.0377053, acc 0.984375
2017-08-08T16:54:43.061245: step 6203, loss 0.000774326, acc 1
2017-08-08T16:54:43.298437: step 6204, loss 0.00207926, acc 1
2017-08-08T16:54:43.567558: step 6205, loss 6.22679e-05, acc 1
2017-08-08T16:54:43.877184: step 6206, loss 0.00175308, acc 1
2017-08-08T16:54:44.133354: step 6207, loss 0.0447243, acc 0.984375
2017-08-08T16:54:44.432799: step 6208, loss 0.00114626, acc 1
2017-08-08T16:54:44.619207: step 6209, loss 0.00200771, acc 1
2017-08-08T16:54:44.886367: step 6210, loss 0.00929153, acc 1
2017-08-08T16:54:45.295280: step 6211, loss 0.000752829, acc 1
2017-08-08T16:54:45.565280: step 6212, loss 0.00100559, acc 1
2017-08-08T16:54:45.837964: step 6213, loss 0.000145785, acc 1
2017-08-08T16:54:46.191196: step 6214, loss 0.00730423, acc 1
2017-08-08T16:54:46.546032: step 6215, loss 0.00181811, acc 1
2017-08-08T16:54:46.993370: step 6216, loss 0.00593707, acc 1
2017-08-08T16:54:47.369346: step 6217, loss 0.0347007, acc 0.96875
2017-08-08T16:54:47.623906: step 6218, loss 0.000282557, acc 1
2017-08-08T16:54:47.819359: step 6219, loss 0.00341016, acc 1
2017-08-08T16:54:48.141098: step 6220, loss 0.000236013, acc 1
2017-08-08T16:54:48.329409: step 6221, loss 0.000545, acc 1
2017-08-08T16:54:48.526940: step 6222, loss 0.00306107, acc 1
2017-08-08T16:54:48.704913: step 6223, loss 0.00086002, acc 1
2017-08-08T16:54:49.031461: step 6224, loss 0.000438133, acc 1
2017-08-08T16:54:49.292375: step 6225, loss 0.000861028, acc 1
2017-08-08T16:54:49.611472: step 6226, loss 0.00849572, acc 1
2017-08-08T16:54:49.850821: step 6227, loss 0.000246248, acc 1
2017-08-08T16:54:50.035468: step 6228, loss 0.000922712, acc 1
2017-08-08T16:54:50.307240: step 6229, loss 0.000558802, acc 1
2017-08-08T16:54:50.495460: step 6230, loss 0.00118252, acc 1
2017-08-08T16:54:50.666890: step 6231, loss 0.000156825, acc 1
2017-08-08T16:54:50.992185: step 6232, loss 0.0212013, acc 0.984375
2017-08-08T16:54:51.341352: step 6233, loss 9.56598e-05, acc 1
2017-08-08T16:54:51.692513: step 6234, loss 0.00196931, acc 1
2017-08-08T16:54:51.937823: step 6235, loss 0.000416309, acc 1
2017-08-08T16:54:52.145337: step 6236, loss 0.000102543, acc 1
2017-08-08T16:54:52.435422: step 6237, loss 0.000480486, acc 1
2017-08-08T16:54:52.702302: step 6238, loss 0.00325825, acc 1
2017-08-08T16:54:52.981732: step 6239, loss 8.89384e-05, acc 1
2017-08-08T16:54:53.329325: step 6240, loss 0.000183745, acc 1
2017-08-08T16:54:53.784177: step 6241, loss 0.000166318, acc 1
2017-08-08T16:54:54.168555: step 6242, loss 0.000601788, acc 1
2017-08-08T16:54:54.413575: step 6243, loss 0.00315457, acc 1
2017-08-08T16:54:54.719299: step 6244, loss 0.00209065, acc 1
2017-08-08T16:54:55.093245: step 6245, loss 0.000727499, acc 1
2017-08-08T16:54:55.342024: step 6246, loss 0.000571459, acc 1
2017-08-08T16:54:55.590450: step 6247, loss 0.000754067, acc 1
2017-08-08T16:54:55.819863: step 6248, loss 0.00254867, acc 1
2017-08-08T16:54:56.237821: step 6249, loss 0.000133372, acc 1
2017-08-08T16:54:56.546180: step 6250, loss 0.000708674, acc 1
2017-08-08T16:54:56.885402: step 6251, loss 0.00159403, acc 1
2017-08-08T16:54:57.158330: step 6252, loss 0.00048727, acc 1
2017-08-08T16:54:57.379050: step 6253, loss 0.000500901, acc 1
2017-08-08T16:54:57.804615: step 6254, loss 0.000101708, acc 1
2017-08-08T16:54:58.089727: step 6255, loss 0.00170285, acc 1
2017-08-08T16:54:58.400448: step 6256, loss 0.000734652, acc 1
2017-08-08T16:54:58.693676: step 6257, loss 0.000440328, acc 1
2017-08-08T16:54:59.095778: step 6258, loss 0.000126803, acc 1
2017-08-08T16:54:59.452802: step 6259, loss 0.000368579, acc 1
2017-08-08T16:54:59.711514: step 6260, loss 0.00287423, acc 1
2017-08-08T16:54:59.925468: step 6261, loss 0.000633787, acc 1
2017-08-08T16:55:00.303788: step 6262, loss 0.000166503, acc 1
2017-08-08T16:55:00.563778: step 6263, loss 0.000859481, acc 1
2017-08-08T16:55:00.805858: step 6264, loss 0.00909173, acc 1
2017-08-08T16:55:01.081336: step 6265, loss 0.000732564, acc 1
2017-08-08T16:55:01.466964: step 6266, loss 0.000409352, acc 1
2017-08-08T16:55:01.829551: step 6267, loss 0.0173219, acc 0.984375
2017-08-08T16:55:02.209503: step 6268, loss 0.000156341, acc 1
2017-08-08T16:55:02.452137: step 6269, loss 0.00401244, acc 1
2017-08-08T16:55:02.814421: step 6270, loss 0.00148544, acc 1
2017-08-08T16:55:03.234748: step 6271, loss 0.000520344, acc 1
2017-08-08T16:55:03.565324: step 6272, loss 0.00207066, acc 1
2017-08-08T16:55:03.844528: step 6273, loss 0.00197571, acc 1
2017-08-08T16:55:04.120689: step 6274, loss 0.000379169, acc 1
2017-08-08T16:55:04.559863: step 6275, loss 0.00034304, acc 1
2017-08-08T16:55:04.950217: step 6276, loss 0.000370644, acc 1
2017-08-08T16:55:05.320669: step 6277, loss 0.00046063, acc 1
2017-08-08T16:55:05.579997: step 6278, loss 0.00129718, acc 1
2017-08-08T16:55:05.909423: step 6279, loss 0.000217193, acc 1
2017-08-08T16:55:06.101183: step 6280, loss 0.00053952, acc 1
2017-08-08T16:55:06.345308: step 6281, loss 0.00757802, acc 1
2017-08-08T16:55:06.587209: step 6282, loss 0.00441531, acc 1
2017-08-08T16:55:06.796969: step 6283, loss 0.000396027, acc 1
2017-08-08T16:55:07.115151: step 6284, loss 0.00228263, acc 1
2017-08-08T16:55:07.401319: step 6285, loss 0.000748008, acc 1
2017-08-08T16:55:07.728045: step 6286, loss 0.00161809, acc 1
2017-08-08T16:55:07.994842: step 6287, loss 0.00464873, acc 1
2017-08-08T16:55:08.250245: step 6288, loss 0.00117406, acc 1
2017-08-08T16:55:08.679421: step 6289, loss 0.000573539, acc 1
2017-08-08T16:55:08.967358: step 6290, loss 0.000698868, acc 1
2017-08-08T16:55:09.190436: step 6291, loss 0.00141413, acc 1
2017-08-08T16:55:09.498937: step 6292, loss 0.00244394, acc 1
2017-08-08T16:55:09.817257: step 6293, loss 0.00153318, acc 1
2017-08-08T16:55:10.113535: step 6294, loss 0.000475833, acc 1
2017-08-08T16:55:10.437632: step 6295, loss 0.000421617, acc 1
2017-08-08T16:55:10.684222: step 6296, loss 0.000392174, acc 1
2017-08-08T16:55:10.930678: step 6297, loss 0.000821061, acc 1
2017-08-08T16:55:11.309459: step 6298, loss 0.000988344, acc 1
2017-08-08T16:55:11.569244: step 6299, loss 0.0239751, acc 0.984375
2017-08-08T16:55:11.853267: step 6300, loss 0.00027907, acc 1

Evaluation:
2017-08-08T16:55:12.612128: step 6300, loss 1.91917, acc 0.73546

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-6300

2017-08-08T16:55:13.282854: step 6301, loss 0.00030214, acc 1
2017-08-08T16:55:13.508545: step 6302, loss 0.000664652, acc 1
2017-08-08T16:55:13.707368: step 6303, loss 0.00152197, acc 1
2017-08-08T16:55:14.120560: step 6304, loss 0.00084188, acc 1
2017-08-08T16:55:14.448075: step 6305, loss 0.000106181, acc 1
2017-08-08T16:55:14.731688: step 6306, loss 0.000188386, acc 1
2017-08-08T16:55:14.964673: step 6307, loss 0.00326545, acc 1
2017-08-08T16:55:15.354803: step 6308, loss 4.90026e-05, acc 1
2017-08-08T16:55:15.732292: step 6309, loss 0.00290424, acc 1
2017-08-08T16:55:16.051444: step 6310, loss 0.000393245, acc 1
2017-08-08T16:55:16.310460: step 6311, loss 0.00049792, acc 1
2017-08-08T16:55:16.551529: step 6312, loss 0.00138817, acc 1
2017-08-08T16:55:16.966612: step 6313, loss 0.00905779, acc 1
2017-08-08T16:55:17.259071: step 6314, loss 0.000894082, acc 1
2017-08-08T16:55:17.542679: step 6315, loss 0.0011609, acc 1
2017-08-08T16:55:17.869357: step 6316, loss 1.87368e-05, acc 1
2017-08-08T16:55:18.201309: step 6317, loss 0.000455495, acc 1
2017-08-08T16:55:18.460485: step 6318, loss 0.0019499, acc 1
2017-08-08T16:55:18.663773: step 6319, loss 4.60692e-05, acc 1
2017-08-08T16:55:18.991757: step 6320, loss 0.000579718, acc 1
2017-08-08T16:55:19.252342: step 6321, loss 9.1094e-05, acc 1
2017-08-08T16:55:19.508751: step 6322, loss 0.00135397, acc 1
2017-08-08T16:55:19.779778: step 6323, loss 0.000995344, acc 1
2017-08-08T16:55:20.161349: step 6324, loss 0.000217225, acc 1
2017-08-08T16:55:20.514712: step 6325, loss 0.000151698, acc 1
2017-08-08T16:55:20.826170: step 6326, loss 0.00369614, acc 1
2017-08-08T16:55:21.044544: step 6327, loss 0.000299105, acc 1
2017-08-08T16:55:21.380377: step 6328, loss 0.00307804, acc 1
2017-08-08T16:55:21.559548: step 6329, loss 0.000426918, acc 1
2017-08-08T16:55:21.801204: step 6330, loss 0.000223466, acc 1
2017-08-08T16:55:22.023379: step 6331, loss 0.00149783, acc 1
2017-08-08T16:55:22.315230: step 6332, loss 0.000808146, acc 1
2017-08-08T16:55:22.605905: step 6333, loss 0.00117061, acc 1
2017-08-08T16:55:22.863678: step 6334, loss 0.000629675, acc 1
2017-08-08T16:55:23.142547: step 6335, loss 7.77321e-05, acc 1
2017-08-08T16:55:23.320902: step 6336, loss 0.00160536, acc 1
2017-08-08T16:55:23.505113: step 6337, loss 0.000165426, acc 1
2017-08-08T16:55:23.811905: step 6338, loss 0.00141132, acc 1
2017-08-08T16:55:24.000374: step 6339, loss 0.00106882, acc 1
2017-08-08T16:55:24.230889: step 6340, loss 0.00277741, acc 1
2017-08-08T16:55:24.487407: step 6341, loss 0.000746303, acc 1
2017-08-08T16:55:24.779234: step 6342, loss 0.000772545, acc 1
2017-08-08T16:55:25.228302: step 6343, loss 0.000971331, acc 1
2017-08-08T16:55:25.544046: step 6344, loss 0.0146418, acc 0.984375
2017-08-08T16:55:25.803933: step 6345, loss 0.00133153, acc 1
2017-08-08T16:55:25.984466: step 6346, loss 0.0283702, acc 0.984375
2017-08-08T16:55:26.334319: step 6347, loss 0.000365389, acc 1
2017-08-08T16:55:26.565720: step 6348, loss 0.000214667, acc 1
2017-08-08T16:55:26.794173: step 6349, loss 0.000728763, acc 1
2017-08-08T16:55:27.020089: step 6350, loss 0.000626779, acc 1
2017-08-08T16:55:27.290435: step 6351, loss 0.00107245, acc 1
2017-08-08T16:55:27.574809: step 6352, loss 0.00183827, acc 1
2017-08-08T16:55:27.894053: step 6353, loss 0.000445792, acc 1
2017-08-08T16:55:28.140259: step 6354, loss 0.00067062, acc 1
2017-08-08T16:55:28.525760: step 6355, loss 0.00589597, acc 1
2017-08-08T16:55:28.742778: step 6356, loss 0.00161406, acc 1
2017-08-08T16:55:29.005567: step 6357, loss 0.00735332, acc 1
2017-08-08T16:55:29.287832: step 6358, loss 0.000317079, acc 1
2017-08-08T16:55:29.776087: step 6359, loss 8.42365e-05, acc 1
2017-08-08T16:55:30.165941: step 6360, loss 0.0127036, acc 1
2017-08-08T16:55:30.497097: step 6361, loss 0.000543766, acc 1
2017-08-08T16:55:30.767315: step 6362, loss 0.00406822, acc 1
2017-08-08T16:55:31.217367: step 6363, loss 0.000122385, acc 1
2017-08-08T16:55:31.505569: step 6364, loss 0.000901916, acc 1
2017-08-08T16:55:31.821057: step 6365, loss 0.00380291, acc 1
2017-08-08T16:55:32.245800: step 6366, loss 7.83559e-05, acc 1
2017-08-08T16:55:32.598418: step 6367, loss 0.00123815, acc 1
2017-08-08T16:55:33.020828: step 6368, loss 7.28182e-05, acc 1
2017-08-08T16:55:33.365488: step 6369, loss 0.000364513, acc 1
2017-08-08T16:55:33.582128: step 6370, loss 0.00909829, acc 1
2017-08-08T16:55:33.973573: step 6371, loss 0.00801604, acc 1
2017-08-08T16:55:34.343498: step 6372, loss 0.00133747, acc 1
2017-08-08T16:55:34.635125: step 6373, loss 1.56805e-05, acc 1
2017-08-08T16:55:34.897612: step 6374, loss 0.00160738, acc 1
2017-08-08T16:55:35.141105: step 6375, loss 0.000196084, acc 1
2017-08-08T16:55:35.535861: step 6376, loss 0.000414237, acc 1
2017-08-08T16:55:35.853676: step 6377, loss 0.00218117, acc 1
2017-08-08T16:55:36.114106: step 6378, loss 0.00102494, acc 1
2017-08-08T16:55:36.353465: step 6379, loss 0.000847111, acc 1
2017-08-08T16:55:36.688551: step 6380, loss 0.000149721, acc 1
2017-08-08T16:55:36.950793: step 6381, loss 0.000168242, acc 1
2017-08-08T16:55:37.201227: step 6382, loss 0.00209257, acc 1
2017-08-08T16:55:37.546705: step 6383, loss 0.000209884, acc 1
2017-08-08T16:55:37.936124: step 6384, loss 7.73368e-05, acc 1
2017-08-08T16:55:38.319944: step 6385, loss 0.00219295, acc 1
2017-08-08T16:55:38.636170: step 6386, loss 0.000402923, acc 1
2017-08-08T16:55:38.830377: step 6387, loss 6.8183e-05, acc 1
2017-08-08T16:55:39.162659: step 6388, loss 0.00101801, acc 1
2017-08-08T16:55:39.387320: step 6389, loss 0.000782106, acc 1
2017-08-08T16:55:39.625002: step 6390, loss 0.00101691, acc 1
2017-08-08T16:55:39.935018: step 6391, loss 0.000146958, acc 1
2017-08-08T16:55:40.287491: step 6392, loss 0.000196628, acc 1
2017-08-08T16:55:40.696447: step 6393, loss 0.00864836, acc 1
2017-08-08T16:55:40.990806: step 6394, loss 0.000655128, acc 1
2017-08-08T16:55:41.228424: step 6395, loss 0.000471707, acc 1
2017-08-08T16:55:41.515962: step 6396, loss 0.000703575, acc 1
2017-08-08T16:55:41.770029: step 6397, loss 0.00370458, acc 1
2017-08-08T16:55:41.967539: step 6398, loss 0.0105807, acc 1
2017-08-08T16:55:42.206540: step 6399, loss 0.000760285, acc 1
2017-08-08T16:55:42.570066: step 6400, loss 0.000431882, acc 1

Evaluation:
2017-08-08T16:55:43.564065: step 6400, loss 1.96944, acc 0.721388

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-6400

2017-08-08T16:55:44.071730: step 6401, loss 0.0198627, acc 0.984375
2017-08-08T16:55:44.526399: step 6402, loss 0.000667457, acc 1
2017-08-08T16:55:44.843159: step 6403, loss 0.00230574, acc 1
2017-08-08T16:55:45.103675: step 6404, loss 0.000553678, acc 1
2017-08-08T16:55:45.364467: step 6405, loss 0.000758407, acc 1
2017-08-08T16:55:45.706815: step 6406, loss 3.32409e-05, acc 1
2017-08-08T16:55:45.981551: step 6407, loss 0.0298284, acc 0.984375
2017-08-08T16:55:46.341373: step 6408, loss 0.000299197, acc 1
2017-08-08T16:55:46.680454: step 6409, loss 0.000386929, acc 1
2017-08-08T16:55:46.893515: step 6410, loss 0.00167616, acc 1
2017-08-08T16:55:47.238213: step 6411, loss 0.000195393, acc 1
2017-08-08T16:55:47.532320: step 6412, loss 0.0135122, acc 0.984375
2017-08-08T16:55:47.789566: step 6413, loss 0.00183704, acc 1
2017-08-08T16:55:48.110906: step 6414, loss 0.000618131, acc 1
2017-08-08T16:55:48.520249: step 6415, loss 0.000412442, acc 1
2017-08-08T16:55:48.862315: step 6416, loss 0.00197983, acc 1
2017-08-08T16:55:49.239944: step 6417, loss 0.000595931, acc 1
2017-08-08T16:55:49.524933: step 6418, loss 0.00142921, acc 1
2017-08-08T16:55:49.735006: step 6419, loss 0.0106492, acc 1
2017-08-08T16:55:50.130249: step 6420, loss 0.000910294, acc 1
2017-08-08T16:55:50.459769: step 6421, loss 0.00122325, acc 1
2017-08-08T16:55:50.735741: step 6422, loss 0.000194552, acc 1
2017-08-08T16:55:50.995908: step 6423, loss 0.00353694, acc 1
2017-08-08T16:55:51.409828: step 6424, loss 0.000201328, acc 1
2017-08-08T16:55:51.801389: step 6425, loss 0.000722558, acc 1
2017-08-08T16:55:52.148072: step 6426, loss 0.00176703, acc 1
2017-08-08T16:55:52.435869: step 6427, loss 0.000449011, acc 1
2017-08-08T16:55:52.688952: step 6428, loss 0.000646475, acc 1
2017-08-08T16:55:53.097174: step 6429, loss 0.000513451, acc 1
2017-08-08T16:55:53.382919: step 6430, loss 0.000546156, acc 1
2017-08-08T16:55:53.613030: step 6431, loss 9.01505e-05, acc 1
2017-08-08T16:55:53.912777: step 6432, loss 0.000283321, acc 1
2017-08-08T16:55:54.308004: step 6433, loss 0.00254633, acc 1
2017-08-08T16:55:54.709591: step 6434, loss 7.46377e-05, acc 1
2017-08-08T16:55:55.151935: step 6435, loss 0.0100971, acc 1
2017-08-08T16:55:55.492329: step 6436, loss 9.61798e-05, acc 1
2017-08-08T16:55:55.841622: step 6437, loss 0.000423615, acc 1
2017-08-08T16:55:56.244982: step 6438, loss 0.0010699, acc 1
2017-08-08T16:55:56.506134: step 6439, loss 0.000990963, acc 1
2017-08-08T16:55:56.802479: step 6440, loss 0.00342754, acc 1
2017-08-08T16:55:57.267087: step 6441, loss 0.000645948, acc 1
2017-08-08T16:55:57.686472: step 6442, loss 0.000646882, acc 1
2017-08-08T16:55:58.019929: step 6443, loss 0.00277455, acc 1
2017-08-08T16:55:58.252769: step 6444, loss 0.000653322, acc 1
2017-08-08T16:55:58.507082: step 6445, loss 0.00122117, acc 1
2017-08-08T16:55:58.995673: step 6446, loss 0.000695512, acc 1
2017-08-08T16:55:59.280354: step 6447, loss 5.80267e-05, acc 1
2017-08-08T16:55:59.545642: step 6448, loss 0.00106444, acc 1
2017-08-08T16:55:59.797471: step 6449, loss 0.000445199, acc 1
2017-08-08T16:56:00.124859: step 6450, loss 0.00064884, acc 1
2017-08-08T16:56:00.484641: step 6451, loss 8.68789e-05, acc 1
2017-08-08T16:56:00.787880: step 6452, loss 0.000143935, acc 1
2017-08-08T16:56:01.021275: step 6453, loss 0.000194277, acc 1
2017-08-08T16:56:01.245845: step 6454, loss 0.00114127, acc 1
2017-08-08T16:56:01.645225: step 6455, loss 0.000377903, acc 1
2017-08-08T16:56:01.941342: step 6456, loss 0.000230276, acc 1
2017-08-08T16:56:02.442992: step 6457, loss 6.12159e-05, acc 1
2017-08-08T16:56:02.857225: step 6458, loss 0.00107296, acc 1
2017-08-08T16:56:03.449114: step 6459, loss 1.24438e-05, acc 1
2017-08-08T16:56:03.983765: step 6460, loss 0.000148814, acc 1
2017-08-08T16:56:04.265275: step 6461, loss 8.63371e-05, acc 1
2017-08-08T16:56:04.641385: step 6462, loss 0.00692304, acc 1
2017-08-08T16:56:04.898997: step 6463, loss 0.00442888, acc 1
2017-08-08T16:56:05.357518: step 6464, loss 0.000662785, acc 1
2017-08-08T16:56:05.647564: step 6465, loss 9.61081e-05, acc 1
2017-08-08T16:56:05.965170: step 6466, loss 0.000862026, acc 1
2017-08-08T16:56:06.253637: step 6467, loss 0.00077909, acc 1
2017-08-08T16:56:06.678869: step 6468, loss 0.000196494, acc 1
2017-08-08T16:56:07.100475: step 6469, loss 7.62544e-05, acc 1
2017-08-08T16:56:07.449670: step 6470, loss 0.000119521, acc 1
2017-08-08T16:56:07.789739: step 6471, loss 0.00161952, acc 1
2017-08-08T16:56:08.033589: step 6472, loss 0.000471503, acc 1
2017-08-08T16:56:08.378779: step 6473, loss 0.00392496, acc 1
2017-08-08T16:56:08.716460: step 6474, loss 0.00150176, acc 1
2017-08-08T16:56:08.996539: step 6475, loss 0.00213667, acc 1
2017-08-08T16:56:09.290784: step 6476, loss 0.00244812, acc 1
2017-08-08T16:56:09.837701: step 6477, loss 0.00229219, acc 1
2017-08-08T16:56:10.207480: step 6478, loss 2.83615e-05, acc 1
2017-08-08T16:56:10.517548: step 6479, loss 9.2067e-05, acc 1
2017-08-08T16:56:10.728396: step 6480, loss 0.00296052, acc 1
2017-08-08T16:56:11.071489: step 6481, loss 0.000437857, acc 1
2017-08-08T16:56:11.333969: step 6482, loss 0.00105912, acc 1
2017-08-08T16:56:11.565401: step 6483, loss 0.00231205, acc 1
2017-08-08T16:56:11.806880: step 6484, loss 0.00110771, acc 1
2017-08-08T16:56:12.025096: step 6485, loss 0.000105948, acc 1
2017-08-08T16:56:12.403881: step 6486, loss 0.000202859, acc 1
2017-08-08T16:56:12.805723: step 6487, loss 0.000811341, acc 1
2017-08-08T16:56:13.132529: step 6488, loss 0.00722708, acc 1
2017-08-08T16:56:13.390963: step 6489, loss 0.00158076, acc 1
2017-08-08T16:56:13.629326: step 6490, loss 0.000130237, acc 1
2017-08-08T16:56:13.929541: step 6491, loss 0.00139377, acc 1
2017-08-08T16:56:14.161172: step 6492, loss 0.000948035, acc 1
2017-08-08T16:56:14.428662: step 6493, loss 0.00131519, acc 1
2017-08-08T16:56:14.817396: step 6494, loss 0.000776335, acc 1
2017-08-08T16:56:15.225364: step 6495, loss 0.000902155, acc 1
2017-08-08T16:56:15.513397: step 6496, loss 0.00336534, acc 1
2017-08-08T16:56:15.768119: step 6497, loss 0.000627053, acc 1
2017-08-08T16:56:15.964537: step 6498, loss 0.000209507, acc 1
2017-08-08T16:56:16.276068: step 6499, loss 0.000177844, acc 1
2017-08-08T16:56:16.537213: step 6500, loss 0.00109539, acc 1

Evaluation:
2017-08-08T16:56:17.086997: step 6500, loss 1.98769, acc 0.734522

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-6500

2017-08-08T16:56:17.649446: step 6501, loss 0.0426713, acc 0.984375
2017-08-08T16:56:17.972007: step 6502, loss 9.30009e-05, acc 1
2017-08-08T16:56:18.223517: step 6503, loss 0.000916274, acc 1
2017-08-08T16:56:18.415302: step 6504, loss 6.48855e-05, acc 1
2017-08-08T16:56:18.707021: step 6505, loss 0.000142851, acc 1
2017-08-08T16:56:18.960077: step 6506, loss 0.000123078, acc 1
2017-08-08T16:56:19.192075: step 6507, loss 0.00068083, acc 1
2017-08-08T16:56:19.410729: step 6508, loss 8.79821e-05, acc 1
2017-08-08T16:56:19.674802: step 6509, loss 0.00068462, acc 1
2017-08-08T16:56:20.087071: step 6510, loss 0.000247207, acc 1
2017-08-08T16:56:20.501444: step 6511, loss 0.00091947, acc 1
2017-08-08T16:56:20.785236: step 6512, loss 0.000660973, acc 1
2017-08-08T16:56:21.023535: step 6513, loss 0.00054164, acc 1
2017-08-08T16:56:21.269450: step 6514, loss 0.00055504, acc 1
2017-08-08T16:56:21.700809: step 6515, loss 7.4489e-05, acc 1
2017-08-08T16:56:21.988253: step 6516, loss 0.000131297, acc 1
2017-08-08T16:56:22.296393: step 6517, loss 8.68495e-05, acc 1
2017-08-08T16:56:22.687541: step 6518, loss 0.000995359, acc 1
2017-08-08T16:56:23.082001: step 6519, loss 0.000579474, acc 1
2017-08-08T16:56:23.379333: step 6520, loss 0.00147679, acc 1
2017-08-08T16:56:23.591034: step 6521, loss 0.0026216, acc 1
2017-08-08T16:56:23.848276: step 6522, loss 0.00184503, acc 1
2017-08-08T16:56:24.245397: step 6523, loss 0.0012519, acc 1
2017-08-08T16:56:24.594503: step 6524, loss 0.000606411, acc 1
2017-08-08T16:56:24.907322: step 6525, loss 0.000678846, acc 1
2017-08-08T16:56:25.179510: step 6526, loss 0.00145977, acc 1
2017-08-08T16:56:25.632141: step 6527, loss 0.000372651, acc 1
2017-08-08T16:56:26.023702: step 6528, loss 0.000392487, acc 1
2017-08-08T16:56:26.327413: step 6529, loss 0.004518, acc 1
2017-08-08T16:56:26.552499: step 6530, loss 0.000720949, acc 1
2017-08-08T16:56:26.889365: step 6531, loss 0.00125535, acc 1
2017-08-08T16:56:27.212069: step 6532, loss 0.000172702, acc 1
2017-08-08T16:56:27.492759: step 6533, loss 0.000256942, acc 1
2017-08-08T16:56:27.700668: step 6534, loss 0.00033724, acc 1
2017-08-08T16:56:28.026938: step 6535, loss 0.000645015, acc 1
2017-08-08T16:56:28.457362: step 6536, loss 0.000339921, acc 1
2017-08-08T16:56:28.800282: step 6537, loss 0.000790488, acc 1
2017-08-08T16:56:29.078632: step 6538, loss 0.000613067, acc 1
2017-08-08T16:56:29.309497: step 6539, loss 0.0013456, acc 1
2017-08-08T16:56:29.702840: step 6540, loss 0.00083764, acc 1
2017-08-08T16:56:29.982256: step 6541, loss 0.00316233, acc 1
2017-08-08T16:56:30.291510: step 6542, loss 0.000219996, acc 1
2017-08-08T16:56:30.692851: step 6543, loss 0.000674086, acc 1
2017-08-08T16:56:31.205735: step 6544, loss 0.000114539, acc 1
2017-08-08T16:56:31.587857: step 6545, loss 0.000886141, acc 1
2017-08-08T16:56:31.903241: step 6546, loss 6.10702e-05, acc 1
2017-08-08T16:56:32.289868: step 6547, loss 0.000773942, acc 1
2017-08-08T16:56:32.670270: step 6548, loss 0.000293101, acc 1
2017-08-08T16:56:32.950856: step 6549, loss 0.000442823, acc 1
2017-08-08T16:56:33.211854: step 6550, loss 0.000595776, acc 1
2017-08-08T16:56:33.514267: step 6551, loss 0.00013065, acc 1
2017-08-08T16:56:33.997399: step 6552, loss 0.000582159, acc 1
2017-08-08T16:56:34.380269: step 6553, loss 0.00152469, acc 1
2017-08-08T16:56:34.668542: step 6554, loss 0.00251517, acc 1
2017-08-08T16:56:34.906293: step 6555, loss 0.000684671, acc 1
2017-08-08T16:56:35.135367: step 6556, loss 0.00136811, acc 1
2017-08-08T16:56:35.465779: step 6557, loss 0.000187815, acc 1
2017-08-08T16:56:35.690190: step 6558, loss 0.000416972, acc 1
2017-08-08T16:56:35.954165: step 6559, loss 0.00059974, acc 1
2017-08-08T16:56:36.255505: step 6560, loss 0.005408, acc 1
2017-08-08T16:56:36.578848: step 6561, loss 0.000563968, acc 1
2017-08-08T16:56:36.915028: step 6562, loss 0.000420795, acc 1
2017-08-08T16:56:37.225715: step 6563, loss 0.00022429, acc 1
2017-08-08T16:56:37.458488: step 6564, loss 0.000181435, acc 1
2017-08-08T16:56:37.775652: step 6565, loss 0.000159478, acc 1
2017-08-08T16:56:38.103993: step 6566, loss 0.0012223, acc 1
2017-08-08T16:56:38.375468: step 6567, loss 0.00030808, acc 1
2017-08-08T16:56:38.737361: step 6568, loss 0.000475231, acc 1
2017-08-08T16:56:39.116561: step 6569, loss 0.00137003, acc 1
2017-08-08T16:56:39.547958: step 6570, loss 0.000575163, acc 1
2017-08-08T16:56:39.985434: step 6571, loss 0.00149853, acc 1
2017-08-08T16:56:40.279820: step 6572, loss 0.000114784, acc 1
2017-08-08T16:56:40.509159: step 6573, loss 0.00188991, acc 1
2017-08-08T16:56:40.905669: step 6574, loss 0.000458896, acc 1
2017-08-08T16:56:41.214315: step 6575, loss 0.000396664, acc 1
2017-08-08T16:56:41.508451: step 6576, loss 0.00172891, acc 1
2017-08-08T16:56:41.797301: step 6577, loss 0.000195326, acc 1
2017-08-08T16:56:42.183548: step 6578, loss 0.000719201, acc 1
2017-08-08T16:56:42.582671: step 6579, loss 0.000369839, acc 1
2017-08-08T16:56:43.020254: step 6580, loss 0.000649276, acc 1
2017-08-08T16:56:43.267753: step 6581, loss 5.47248e-05, acc 1
2017-08-08T16:56:43.629617: step 6582, loss 0.00230006, acc 1
2017-08-08T16:56:43.965756: step 6583, loss 0.001017, acc 1
2017-08-08T16:56:44.235810: step 6584, loss 0.000711191, acc 1
2017-08-08T16:56:44.548422: step 6585, loss 0.000982852, acc 1
2017-08-08T16:56:44.860760: step 6586, loss 0.00323161, acc 1
2017-08-08T16:56:45.322734: step 6587, loss 0.00305303, acc 1
2017-08-08T16:56:45.653065: step 6588, loss 0.000189292, acc 1
2017-08-08T16:56:45.916755: step 6589, loss 0.0015745, acc 1
2017-08-08T16:56:46.120825: step 6590, loss 0.000247194, acc 1
2017-08-08T16:56:46.357811: step 6591, loss 0.000642572, acc 1
2017-08-08T16:56:46.835112: step 6592, loss 0.00026893, acc 1
2017-08-08T16:56:47.233265: step 6593, loss 0.00863511, acc 1
2017-08-08T16:56:47.561043: step 6594, loss 0.000143976, acc 1
2017-08-08T16:56:47.846693: step 6595, loss 0.000176694, acc 1
2017-08-08T16:56:48.231021: step 6596, loss 0.00102799, acc 1
2017-08-08T16:56:48.600890: step 6597, loss 0.000133006, acc 1
2017-08-08T16:56:48.961329: step 6598, loss 0.00199094, acc 1
2017-08-08T16:56:49.311595: step 6599, loss 7.35697e-05, acc 1
2017-08-08T16:56:49.594356: step 6600, loss 0.000364511, acc 1

Evaluation:
2017-08-08T16:56:50.375374: step 6600, loss 1.98495, acc 0.730769

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-6600

2017-08-08T16:56:50.780052: step 6601, loss 5.25765e-05, acc 1
2017-08-08T16:56:51.063295: step 6602, loss 0.00021706, acc 1
2017-08-08T16:56:51.332053: step 6603, loss 0.0130078, acc 0.984375
2017-08-08T16:56:51.638435: step 6604, loss 0.000717542, acc 1
2017-08-08T16:56:51.902399: step 6605, loss 0.00045294, acc 1
2017-08-08T16:56:52.205366: step 6606, loss 0.000116029, acc 1
2017-08-08T16:56:52.542110: step 6607, loss 0.000202659, acc 1
2017-08-08T16:56:52.781997: step 6608, loss 0.000284195, acc 1
2017-08-08T16:56:53.063338: step 6609, loss 0.000757273, acc 1
2017-08-08T16:56:53.413360: step 6610, loss 0.000150989, acc 1
2017-08-08T16:56:53.886001: step 6611, loss 0.00335333, acc 1
2017-08-08T16:56:54.304115: step 6612, loss 0.000587608, acc 1
2017-08-08T16:56:54.603859: step 6613, loss 0.000377369, acc 1
2017-08-08T16:56:54.913675: step 6614, loss 0.000382261, acc 1
2017-08-08T16:56:55.410832: step 6615, loss 0.000900172, acc 1
2017-08-08T16:56:55.705723: step 6616, loss 0.0260626, acc 0.984375
2017-08-08T16:56:55.985926: step 6617, loss 0.000344472, acc 1
2017-08-08T16:56:56.230742: step 6618, loss 0.000313016, acc 1
2017-08-08T16:56:56.515404: step 6619, loss 0.000161528, acc 1
2017-08-08T16:56:56.854553: step 6620, loss 0.000414445, acc 1
2017-08-08T16:56:57.230234: step 6621, loss 0.00109885, acc 1
2017-08-08T16:56:57.573170: step 6622, loss 0.000317138, acc 1
2017-08-08T16:56:57.811445: step 6623, loss 0.00197138, acc 1
2017-08-08T16:56:58.216171: step 6624, loss 0.000562596, acc 1
2017-08-08T16:56:58.573268: step 6625, loss 0.000762485, acc 1
2017-08-08T16:56:58.849859: step 6626, loss 0.000958079, acc 1
2017-08-08T16:56:59.211554: step 6627, loss 0.000335115, acc 1
2017-08-08T16:56:59.585955: step 6628, loss 0.00203235, acc 1
2017-08-08T16:56:59.847740: step 6629, loss 0.000315244, acc 1
2017-08-08T16:57:00.128348: step 6630, loss 0.000804519, acc 1
2017-08-08T16:57:00.326902: step 6631, loss 0.00164576, acc 1
2017-08-08T16:57:00.599020: step 6632, loss 0.000100715, acc 1
2017-08-08T16:57:00.969303: step 6633, loss 0.000329438, acc 1
2017-08-08T16:57:01.214668: step 6634, loss 0.00403786, acc 1
2017-08-08T16:57:01.487450: step 6635, loss 0.0201425, acc 0.984375
2017-08-08T16:57:01.942085: step 6636, loss 0.00229908, acc 1
2017-08-08T16:57:02.380527: step 6637, loss 0.00276762, acc 1
2017-08-08T16:57:02.737030: step 6638, loss 0.000238396, acc 1
2017-08-08T16:57:03.000295: step 6639, loss 0.00112952, acc 1
2017-08-08T16:57:03.289749: step 6640, loss 0.000152443, acc 1
2017-08-08T16:57:03.662877: step 6641, loss 0.0103563, acc 1
2017-08-08T16:57:03.951476: step 6642, loss 0.000166509, acc 1
2017-08-08T16:57:04.250638: step 6643, loss 0.00695861, acc 1
2017-08-08T16:57:04.622317: step 6644, loss 0.00116313, acc 1
2017-08-08T16:57:05.072555: step 6645, loss 4.44775e-05, acc 1
2017-08-08T16:57:05.398671: step 6646, loss 0.000157722, acc 1
2017-08-08T16:57:05.712997: step 6647, loss 0.000240888, acc 1
2017-08-08T16:57:05.920312: step 6648, loss 0.000114205, acc 1
2017-08-08T16:57:06.309801: step 6649, loss 0.000722385, acc 1
2017-08-08T16:57:06.594322: step 6650, loss 0.000985726, acc 1
2017-08-08T16:57:06.817604: step 6651, loss 0.0174403, acc 0.984375
2017-08-08T16:57:07.051864: step 6652, loss 9.92496e-05, acc 1
2017-08-08T16:57:07.417198: step 6653, loss 0.000164414, acc 1
2017-08-08T16:57:07.806937: step 6654, loss 0.000754232, acc 1
2017-08-08T16:57:08.169600: step 6655, loss 2.13052e-05, acc 1
2017-08-08T16:57:08.461325: step 6656, loss 0.0045338, acc 1
2017-08-08T16:57:08.689336: step 6657, loss 0.000333565, acc 1
2017-08-08T16:57:09.036573: step 6658, loss 0.000542315, acc 1
2017-08-08T16:57:09.399446: step 6659, loss 0.0015559, acc 1
2017-08-08T16:57:09.711849: step 6660, loss 0.000308651, acc 1
2017-08-08T16:57:09.956183: step 6661, loss 6.96461e-05, acc 1
2017-08-08T16:57:10.292642: step 6662, loss 0.000130484, acc 1
2017-08-08T16:57:10.644334: step 6663, loss 0.000552343, acc 1
2017-08-08T16:57:10.885519: step 6664, loss 0.00031192, acc 1
2017-08-08T16:57:11.127362: step 6665, loss 0.02307, acc 0.984375
2017-08-08T16:57:11.358797: step 6666, loss 0.00150805, acc 1
2017-08-08T16:57:11.777707: step 6667, loss 0.000125351, acc 1
2017-08-08T16:57:12.119566: step 6668, loss 0.00170585, acc 1
2017-08-08T16:57:12.433008: step 6669, loss 4.07103e-05, acc 1
2017-08-08T16:57:12.782607: step 6670, loss 0.000795931, acc 1
2017-08-08T16:57:13.031679: step 6671, loss 0.000305874, acc 1
2017-08-08T16:57:13.328027: step 6672, loss 0.000230472, acc 1
2017-08-08T16:57:13.582470: step 6673, loss 0.0307009, acc 0.984375
2017-08-08T16:57:13.879357: step 6674, loss 0.00321222, acc 1
2017-08-08T16:57:14.202828: step 6675, loss 0.00100459, acc 1
2017-08-08T16:57:14.448264: step 6676, loss 0.0314198, acc 0.984375
2017-08-08T16:57:14.859107: step 6677, loss 0.00198163, acc 1
2017-08-08T16:57:15.226326: step 6678, loss 0.000138714, acc 1
2017-08-08T16:57:15.614902: step 6679, loss 0.000130397, acc 1
2017-08-08T16:57:15.911167: step 6680, loss 0.00196804, acc 1
2017-08-08T16:57:16.237380: step 6681, loss 4.00844e-05, acc 1
2017-08-08T16:57:16.657367: step 6682, loss 0.000210632, acc 1
2017-08-08T16:57:16.899354: step 6683, loss 0.000360258, acc 1
2017-08-08T16:57:17.213999: step 6684, loss 0.003264, acc 1
2017-08-08T16:57:17.531543: step 6685, loss 0.0003569, acc 1
2017-08-08T16:57:17.913418: step 6686, loss 0.022285, acc 0.984375
2017-08-08T16:57:18.268782: step 6687, loss 0.00103067, acc 1
2017-08-08T16:57:18.543177: step 6688, loss 0.00282309, acc 1
2017-08-08T16:57:18.889369: step 6689, loss 0.00179854, acc 1
2017-08-08T16:57:19.196640: step 6690, loss 0.000465898, acc 1
2017-08-08T16:57:19.574544: step 6691, loss 0.00309322, acc 1
2017-08-08T16:57:19.952383: step 6692, loss 0.000125589, acc 1
2017-08-08T16:57:20.226723: step 6693, loss 0.00059172, acc 1
2017-08-08T16:57:20.732975: step 6694, loss 0.00122215, acc 1
2017-08-08T16:57:20.970974: step 6695, loss 0.000128271, acc 1
2017-08-08T16:57:21.278911: step 6696, loss 0.00534761, acc 1
2017-08-08T16:57:21.607620: step 6697, loss 0.00027265, acc 1
2017-08-08T16:57:21.894414: step 6698, loss 0.013072, acc 0.984375
2017-08-08T16:57:22.304304: step 6699, loss 0.00119192, acc 1
2017-08-08T16:57:22.580347: step 6700, loss 0.000222912, acc 1

Evaluation:
2017-08-08T16:57:23.474541: step 6700, loss 2.0307, acc 0.726079

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-6700

2017-08-08T16:57:24.035908: step 6701, loss 0.00130472, acc 1
2017-08-08T16:57:24.366891: step 6702, loss 0.000196932, acc 1
2017-08-08T16:57:24.651583: step 6703, loss 0.00494498, acc 1
2017-08-08T16:57:24.949662: step 6704, loss 0.0538309, acc 0.984375
2017-08-08T16:57:25.372826: step 6705, loss 2.35475e-05, acc 1
2017-08-08T16:57:25.751879: step 6706, loss 0.000548993, acc 1
2017-08-08T16:57:26.109762: step 6707, loss 0.000368999, acc 1
2017-08-08T16:57:26.383725: step 6708, loss 0.00549189, acc 1
2017-08-08T16:57:26.645367: step 6709, loss 0.00188474, acc 1
2017-08-08T16:57:27.078537: step 6710, loss 0.00106179, acc 1
2017-08-08T16:57:27.371754: step 6711, loss 0.000405829, acc 1
2017-08-08T16:57:27.704960: step 6712, loss 0.000207684, acc 1
2017-08-08T16:57:28.104617: step 6713, loss 0.0654395, acc 0.984375
2017-08-08T16:57:28.489614: step 6714, loss 0.00302499, acc 1
2017-08-08T16:57:28.825517: step 6715, loss 0.039906, acc 0.984375
2017-08-08T16:57:29.139003: step 6716, loss 7.85503e-05, acc 1
2017-08-08T16:57:29.391804: step 6717, loss 0.00377851, acc 1
2017-08-08T16:57:29.709514: step 6718, loss 0.00473084, acc 1
2017-08-08T16:57:30.091023: step 6719, loss 0.000429611, acc 1
2017-08-08T16:57:30.368453: step 6720, loss 0.000281542, acc 1
2017-08-08T16:57:30.676922: step 6721, loss 0.000173444, acc 1
2017-08-08T16:57:31.089605: step 6722, loss 8.11992e-05, acc 1
2017-08-08T16:57:31.462688: step 6723, loss 0.000498943, acc 1
2017-08-08T16:57:31.829519: step 6724, loss 0.0407774, acc 0.984375
2017-08-08T16:57:32.144095: step 6725, loss 0.00620991, acc 1
2017-08-08T16:57:32.401339: step 6726, loss 0.00451028, acc 1
2017-08-08T16:57:32.708546: step 6727, loss 0.00253593, acc 1
2017-08-08T16:57:33.085101: step 6728, loss 0.000295408, acc 1
2017-08-08T16:57:33.336788: step 6729, loss 0.000411736, acc 1
2017-08-08T16:57:33.608325: step 6730, loss 0.00475834, acc 1
2017-08-08T16:57:33.882402: step 6731, loss 0.00159752, acc 1
2017-08-08T16:57:34.094769: step 6732, loss 0.00311927, acc 1
2017-08-08T16:57:34.576044: step 6733, loss 0.000339861, acc 1
2017-08-08T16:57:34.954670: step 6734, loss 0.00887698, acc 1
2017-08-08T16:57:35.186530: step 6735, loss 0.000351142, acc 1
2017-08-08T16:57:35.420519: step 6736, loss 0.00303499, acc 1
2017-08-08T16:57:35.761600: step 6737, loss 0.00641091, acc 1
2017-08-08T16:57:36.035619: step 6738, loss 0.00590262, acc 1
2017-08-08T16:57:36.313053: step 6739, loss 0.0382414, acc 0.984375
2017-08-08T16:57:36.683778: step 6740, loss 0.000884989, acc 1
2017-08-08T16:57:37.040287: step 6741, loss 0.0834427, acc 0.96875
2017-08-08T16:57:37.379162: step 6742, loss 0.000363293, acc 1
2017-08-08T16:57:37.635152: step 6743, loss 0.000726103, acc 1
2017-08-08T16:57:37.931636: step 6744, loss 0.00116825, acc 1
2017-08-08T16:57:38.383033: step 6745, loss 0.00108265, acc 1
2017-08-08T16:57:38.669797: step 6746, loss 0.00215128, acc 1
2017-08-08T16:57:39.020964: step 6747, loss 0.000456842, acc 1
2017-08-08T16:57:39.452883: step 6748, loss 0.00449534, acc 1
2017-08-08T16:57:39.745781: step 6749, loss 0.00700292, acc 1
2017-08-08T16:57:40.077377: step 6750, loss 0.00320823, acc 1
2017-08-08T16:57:40.324368: step 6751, loss 0.000571306, acc 1
2017-08-08T16:57:40.537380: step 6752, loss 8.3278e-05, acc 1
2017-08-08T16:57:40.825347: step 6753, loss 0.00101449, acc 1
2017-08-08T16:57:41.115904: step 6754, loss 0.00711895, acc 1
2017-08-08T16:57:41.348036: step 6755, loss 0.000864361, acc 1
2017-08-08T16:57:41.600989: step 6756, loss 0.000294605, acc 1
2017-08-08T16:57:41.865354: step 6757, loss 0.00853916, acc 1
2017-08-08T16:57:42.160671: step 6758, loss 0.00658394, acc 1
2017-08-08T16:57:42.487731: step 6759, loss 0.00049607, acc 1
2017-08-08T16:57:42.743312: step 6760, loss 0.000548296, acc 1
2017-08-08T16:57:42.902231: step 6761, loss 0.0040617, acc 1
2017-08-08T16:57:43.160888: step 6762, loss 4.84646e-05, acc 1
2017-08-08T16:57:43.470561: step 6763, loss 0.000109659, acc 1
2017-08-08T16:57:43.678881: step 6764, loss 0.000571729, acc 1
2017-08-08T16:57:43.880042: step 6765, loss 0.000424278, acc 1
2017-08-08T16:57:44.182301: step 6766, loss 0.000366443, acc 1
2017-08-08T16:57:44.624383: step 6767, loss 8.65367e-05, acc 1
2017-08-08T16:57:44.977824: step 6768, loss 0.012622, acc 0.984375
2017-08-08T16:57:45.329681: step 6769, loss 0.00128274, acc 1
2017-08-08T16:57:45.573508: step 6770, loss 0.00468074, acc 1
2017-08-08T16:57:46.075078: step 6771, loss 0.00107511, acc 1
2017-08-08T16:57:46.402380: step 6772, loss 0.000275354, acc 1
2017-08-08T16:57:46.677243: step 6773, loss 0.000296038, acc 1
2017-08-08T16:57:46.949471: step 6774, loss 0.000213777, acc 1
2017-08-08T16:57:47.240004: step 6775, loss 0.000383503, acc 1
2017-08-08T16:57:47.697710: step 6776, loss 0.000444025, acc 1
2017-08-08T16:57:48.068659: step 6777, loss 0.0514715, acc 0.96875
2017-08-08T16:57:48.321513: step 6778, loss 0.00046801, acc 1
2017-08-08T16:57:48.665367: step 6779, loss 0.00213658, acc 1
2017-08-08T16:57:48.969906: step 6780, loss 0.000310916, acc 1
2017-08-08T16:57:49.218418: step 6781, loss 0.000290995, acc 1
2017-08-08T16:57:49.450336: step 6782, loss 0.00109387, acc 1
2017-08-08T16:57:49.693360: step 6783, loss 0.000137142, acc 1
2017-08-08T16:57:50.094164: step 6784, loss 0.000496623, acc 1
2017-08-08T16:57:50.370326: step 6785, loss 0.000312392, acc 1
2017-08-08T16:57:50.627234: step 6786, loss 0.000589807, acc 1
2017-08-08T16:57:50.880294: step 6787, loss 0.000203439, acc 1
2017-08-08T16:57:51.281777: step 6788, loss 0.00064503, acc 1
2017-08-08T16:57:51.633200: step 6789, loss 0.000580441, acc 1
2017-08-08T16:57:51.895432: step 6790, loss 0.0687371, acc 0.984375
2017-08-08T16:57:52.223295: step 6791, loss 0.0012246, acc 1
2017-08-08T16:57:52.594288: step 6792, loss 0.0133351, acc 0.984375
2017-08-08T16:57:52.905308: step 6793, loss 0.000386315, acc 1
2017-08-08T16:57:53.142633: step 6794, loss 0.00101218, acc 1
2017-08-08T16:57:53.324276: step 6795, loss 0.0003888, acc 1
2017-08-08T16:57:53.604291: step 6796, loss 0.000286894, acc 1
2017-08-08T16:57:53.850236: step 6797, loss 0.00896188, acc 1
2017-08-08T16:57:54.104867: step 6798, loss 0.0691619, acc 0.984375
2017-08-08T16:57:54.513361: step 6799, loss 0.000575901, acc 1
2017-08-08T16:57:54.873980: step 6800, loss 0.00153362, acc 1

Evaluation:
2017-08-08T16:57:55.563781: step 6800, loss 2.10395, acc 0.709193

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-6800

2017-08-08T16:57:55.954653: step 6801, loss 6.36774e-05, acc 1
2017-08-08T16:57:56.176872: step 6802, loss 0.00415389, acc 1
2017-08-08T16:57:56.359830: step 6803, loss 0.00528196, acc 1
2017-08-08T16:57:56.652402: step 6804, loss 0.00622779, acc 1
2017-08-08T16:57:57.000414: step 6805, loss 0.000524589, acc 1
2017-08-08T16:57:57.286537: step 6806, loss 0.000348856, acc 1
2017-08-08T16:57:57.556524: step 6807, loss 0.000796435, acc 1
2017-08-08T16:57:57.785528: step 6808, loss 0.00829692, acc 1
2017-08-08T16:57:58.030054: step 6809, loss 0.000572185, acc 1
2017-08-08T16:57:58.303838: step 6810, loss 0.00114062, acc 1
2017-08-08T16:57:58.574798: step 6811, loss 0.0034818, acc 1
2017-08-08T16:57:58.817020: step 6812, loss 0.000229964, acc 1
2017-08-08T16:57:59.129640: step 6813, loss 0.00769263, acc 1
2017-08-08T16:57:59.542030: step 6814, loss 0.000135939, acc 1
2017-08-08T16:57:59.785530: step 6815, loss 0.00408178, acc 1
2017-08-08T16:58:00.009498: step 6816, loss 0.000824934, acc 1
2017-08-08T16:58:00.406804: step 6817, loss 0.00308477, acc 1
2017-08-08T16:58:00.756590: step 6818, loss 0.000135215, acc 1
2017-08-08T16:58:01.062295: step 6819, loss 0.000884581, acc 1
2017-08-08T16:58:01.355960: step 6820, loss 0.00337871, acc 1
2017-08-08T16:58:01.773885: step 6821, loss 0.026678, acc 0.984375
2017-08-08T16:58:02.262137: step 6822, loss 0.000338866, acc 1
2017-08-08T16:58:02.687357: step 6823, loss 0.00195863, acc 1
2017-08-08T16:58:02.978021: step 6824, loss 0.0588579, acc 0.984375
2017-08-08T16:58:03.239710: step 6825, loss 0.00122268, acc 1
2017-08-08T16:58:03.619226: step 6826, loss 0.000756929, acc 1
2017-08-08T16:58:03.865539: step 6827, loss 0.00122157, acc 1
2017-08-08T16:58:04.114271: step 6828, loss 0.000117056, acc 1
2017-08-08T16:58:04.502496: step 6829, loss 0.000107459, acc 1
2017-08-08T16:58:04.833023: step 6830, loss 0.00944654, acc 1
2017-08-08T16:58:05.069217: step 6831, loss 0.00707738, acc 1
2017-08-08T16:58:05.474163: step 6832, loss 0.070675, acc 0.984375
2017-08-08T16:58:05.742112: step 6833, loss 0.002026, acc 1
2017-08-08T16:58:06.033857: step 6834, loss 0.000667428, acc 1
2017-08-08T16:58:06.330811: step 6835, loss 0.000180558, acc 1
2017-08-08T16:58:06.754652: step 6836, loss 0.00411103, acc 1
2017-08-08T16:58:07.178776: step 6837, loss 0.00615535, acc 1
2017-08-08T16:58:07.494416: step 6838, loss 0.0599048, acc 0.984375
2017-08-08T16:58:07.847318: step 6839, loss 0.000286394, acc 1
2017-08-08T16:58:08.266620: step 6840, loss 3.06211e-05, acc 1
2017-08-08T16:58:08.548116: step 6841, loss 0.000424197, acc 1
2017-08-08T16:58:08.828870: step 6842, loss 0.000768673, acc 1
2017-08-08T16:58:09.197182: step 6843, loss 9.68581e-05, acc 1
2017-08-08T16:58:09.558303: step 6844, loss 0.00016527, acc 1
2017-08-08T16:58:09.925172: step 6845, loss 0.000567754, acc 1
2017-08-08T16:58:10.208083: step 6846, loss 0.0265375, acc 0.984375
2017-08-08T16:58:10.455716: step 6847, loss 0.00403688, acc 1
2017-08-08T16:58:10.840737: step 6848, loss 0.000579851, acc 1
2017-08-08T16:58:11.061560: step 6849, loss 0.00741273, acc 1
2017-08-08T16:58:11.292958: step 6850, loss 0.000686998, acc 1
2017-08-08T16:58:11.585701: step 6851, loss 0.00629498, acc 1
2017-08-08T16:58:11.927946: step 6852, loss 0.000203452, acc 1
2017-08-08T16:58:12.309414: step 6853, loss 0.0079839, acc 1
2017-08-08T16:58:12.584840: step 6854, loss 0.00281518, acc 1
2017-08-08T16:58:12.817366: step 6855, loss 0.00293639, acc 1
2017-08-08T16:58:13.043062: step 6856, loss 0.00457574, acc 1
2017-08-08T16:58:13.407128: step 6857, loss 0.000625299, acc 1
2017-08-08T16:58:13.704517: step 6858, loss 0.00545253, acc 1
2017-08-08T16:58:13.991362: step 6859, loss 0.0012537, acc 1
2017-08-08T16:58:14.255305: step 6860, loss 0.000307808, acc 1
2017-08-08T16:58:14.529168: step 6861, loss 0.000275758, acc 1
2017-08-08T16:58:14.929020: step 6862, loss 0.000571006, acc 1
2017-08-08T16:58:15.341405: step 6863, loss 0.0068037, acc 1
2017-08-08T16:58:15.659082: step 6864, loss 0.000554111, acc 1
2017-08-08T16:58:15.900069: step 6865, loss 0.000333162, acc 1
2017-08-08T16:58:16.098022: step 6866, loss 0.0120935, acc 1
2017-08-08T16:58:16.420068: step 6867, loss 0.0100757, acc 1
2017-08-08T16:58:16.591887: step 6868, loss 0.000583572, acc 1
2017-08-08T16:58:16.777639: step 6869, loss 7.84319e-05, acc 1
2017-08-08T16:58:17.022334: step 6870, loss 0.000766491, acc 1
2017-08-08T16:58:17.389361: step 6871, loss 0.000326109, acc 1
2017-08-08T16:58:17.766569: step 6872, loss 0.00118801, acc 1
2017-08-08T16:58:18.131461: step 6873, loss 0.0147421, acc 1
2017-08-08T16:58:18.391885: step 6874, loss 0.000114135, acc 1
2017-08-08T16:58:18.631255: step 6875, loss 0.0237536, acc 0.984375
2017-08-08T16:58:18.992671: step 6876, loss 0.00394074, acc 1
2017-08-08T16:58:19.242862: step 6877, loss 0.000829895, acc 1
2017-08-08T16:58:19.499382: step 6878, loss 0.00257176, acc 1
2017-08-08T16:58:19.755846: step 6879, loss 0.00141065, acc 1
2017-08-08T16:58:20.127503: step 6880, loss 0.000408827, acc 1
2017-08-08T16:58:20.595443: step 6881, loss 0.0156412, acc 0.984375
2017-08-08T16:58:20.920029: step 6882, loss 0.000881342, acc 1
2017-08-08T16:58:21.106829: step 6883, loss 0.000464252, acc 1
2017-08-08T16:58:21.440026: step 6884, loss 0.00116525, acc 1
2017-08-08T16:58:21.726119: step 6885, loss 0.00693123, acc 1
2017-08-08T16:58:21.918829: step 6886, loss 0.0111981, acc 1
2017-08-08T16:58:22.170566: step 6887, loss 0.0024808, acc 1
2017-08-08T16:58:22.469347: step 6888, loss 0.00332009, acc 1
2017-08-08T16:58:22.727317: step 6889, loss 0.00601022, acc 1
2017-08-08T16:58:22.919681: step 6890, loss 0.00880074, acc 1
2017-08-08T16:58:23.186791: step 6891, loss 0.00344938, acc 1
2017-08-08T16:58:23.558390: step 6892, loss 8.30671e-05, acc 1
2017-08-08T16:58:23.840119: step 6893, loss 0.000706766, acc 1
2017-08-08T16:58:24.110423: step 6894, loss 0.0346898, acc 0.984375
2017-08-08T16:58:24.379016: step 6895, loss 0.0133265, acc 0.984375
2017-08-08T16:58:24.729418: step 6896, loss 0.0037742, acc 1
2017-08-08T16:58:25.156176: step 6897, loss 0.00199155, acc 1
2017-08-08T16:58:25.434292: step 6898, loss 9.74414e-05, acc 1
2017-08-08T16:58:25.669399: step 6899, loss 0.000334865, acc 1
2017-08-08T16:58:25.997444: step 6900, loss 0.000174732, acc 1

Evaluation:
2017-08-08T16:58:26.662939: step 6900, loss 2.16447, acc 0.71576

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-6900

2017-08-08T16:58:27.165652: step 6901, loss 0.0303924, acc 0.984375
2017-08-08T16:58:27.558328: step 6902, loss 6.69888e-05, acc 1
2017-08-08T16:58:27.905260: step 6903, loss 0.0144394, acc 0.984375
2017-08-08T16:58:28.157733: step 6904, loss 0.000476865, acc 1
2017-08-08T16:58:28.375234: step 6905, loss 0.105294, acc 0.96875
2017-08-08T16:58:28.746094: step 6906, loss 4.74987e-05, acc 1
2017-08-08T16:58:29.029767: step 6907, loss 0.00100603, acc 1
2017-08-08T16:58:29.328295: step 6908, loss 0.00091911, acc 1
2017-08-08T16:58:29.604176: step 6909, loss 0.000567055, acc 1
2017-08-08T16:58:29.979835: step 6910, loss 0.000430621, acc 1
2017-08-08T16:58:30.356315: step 6911, loss 0.000129969, acc 1
2017-08-08T16:58:30.625867: step 6912, loss 0.00321758, acc 1
2017-08-08T16:58:30.819975: step 6913, loss 0.0129933, acc 1
2017-08-08T16:58:31.075752: step 6914, loss 0.00588814, acc 1
2017-08-08T16:58:31.327507: step 6915, loss 0.113675, acc 0.96875
2017-08-08T16:58:31.547680: step 6916, loss 0.0450501, acc 0.984375
2017-08-08T16:58:31.786136: step 6917, loss 0.0985634, acc 0.984375
2017-08-08T16:58:32.011555: step 6918, loss 0.00120699, acc 1
2017-08-08T16:58:32.319351: step 6919, loss 0.00357526, acc 1
2017-08-08T16:58:32.728348: step 6920, loss 0.00104975, acc 1
2017-08-08T16:58:33.078241: step 6921, loss 0.000257676, acc 1
2017-08-08T16:58:33.319716: step 6922, loss 0.00026876, acc 1
2017-08-08T16:58:33.701629: step 6923, loss 0.000385294, acc 1
2017-08-08T16:58:34.004155: step 6924, loss 0.00150037, acc 1
2017-08-08T16:58:34.283161: step 6925, loss 0.000558812, acc 1
2017-08-08T16:58:34.664064: step 6926, loss 0.00390415, acc 1
2017-08-08T16:58:35.089373: step 6927, loss 0.000362166, acc 1
2017-08-08T16:58:35.466850: step 6928, loss 0.00200622, acc 1
2017-08-08T16:58:35.872813: step 6929, loss 0.00168936, acc 1
2017-08-08T16:58:36.089806: step 6930, loss 0.00159778, acc 1
2017-08-08T16:58:36.309307: step 6931, loss 0.00066976, acc 1
2017-08-08T16:58:36.644652: step 6932, loss 0.0379214, acc 0.984375
2017-08-08T16:58:36.871477: step 6933, loss 0.00396326, acc 1
2017-08-08T16:58:37.085211: step 6934, loss 0.000937228, acc 1
2017-08-08T16:58:37.255204: step 6935, loss 0.00236804, acc 1
2017-08-08T16:58:37.555386: step 6936, loss 0.00764584, acc 1
2017-08-08T16:58:37.870792: step 6937, loss 0.00612506, acc 1
2017-08-08T16:58:38.162702: step 6938, loss 0.0223106, acc 0.984375
2017-08-08T16:58:38.414605: step 6939, loss 0.00337014, acc 1
2017-08-08T16:58:38.836558: step 6940, loss 0.000413257, acc 1
2017-08-08T16:58:39.135230: step 6941, loss 0.000548426, acc 1
2017-08-08T16:58:39.409432: step 6942, loss 0.0060381, acc 1
2017-08-08T16:58:39.666944: step 6943, loss 0.000505757, acc 1
2017-08-08T16:58:39.919531: step 6944, loss 0.000740399, acc 1
2017-08-08T16:58:40.302783: step 6945, loss 0.000262849, acc 1
2017-08-08T16:58:40.695123: step 6946, loss 0.000270483, acc 1
2017-08-08T16:58:41.036555: step 6947, loss 0.0159875, acc 0.984375
2017-08-08T16:58:41.344834: step 6948, loss 0.040668, acc 0.984375
2017-08-08T16:58:41.576334: step 6949, loss 0.000115934, acc 1
2017-08-08T16:58:42.038703: step 6950, loss 0.00325236, acc 1
2017-08-08T16:58:42.344202: step 6951, loss 0.00847389, acc 1
2017-08-08T16:58:42.601871: step 6952, loss 0.00200673, acc 1
2017-08-08T16:58:42.903687: step 6953, loss 0.000885015, acc 1
2017-08-08T16:58:43.373250: step 6954, loss 0.00132009, acc 1
2017-08-08T16:58:43.741411: step 6955, loss 0.000371217, acc 1
2017-08-08T16:58:44.081090: step 6956, loss 0.00461547, acc 1
2017-08-08T16:58:44.314592: step 6957, loss 0.000690833, acc 1
2017-08-08T16:58:44.714428: step 6958, loss 0.000910662, acc 1
2017-08-08T16:58:44.975592: step 6959, loss 0.000362787, acc 1
2017-08-08T16:58:45.208752: step 6960, loss 0.0328518, acc 0.984375
2017-08-08T16:58:45.431030: step 6961, loss 0.00100166, acc 1
2017-08-08T16:58:45.763897: step 6962, loss 0.000802687, acc 1
2017-08-08T16:58:46.035237: step 6963, loss 0.000159025, acc 1
2017-08-08T16:58:46.275735: step 6964, loss 0.000340721, acc 1
2017-08-08T16:58:46.501673: step 6965, loss 0.000696703, acc 1
2017-08-08T16:58:46.715674: step 6966, loss 0.000714914, acc 1
2017-08-08T16:58:47.026995: step 6967, loss 0.000126008, acc 1
2017-08-08T16:58:47.230905: step 6968, loss 0.0364782, acc 0.96875
2017-08-08T16:58:47.502201: step 6969, loss 0.00171539, acc 1
2017-08-08T16:58:47.783602: step 6970, loss 0.0194738, acc 0.984375
2017-08-08T16:58:48.070944: step 6971, loss 0.000162193, acc 1
2017-08-08T16:58:48.399134: step 6972, loss 0.000879467, acc 1
2017-08-08T16:58:48.632940: step 6973, loss 0.00158122, acc 1
2017-08-08T16:58:48.826570: step 6974, loss 0.0136602, acc 0.984375
2017-08-08T16:58:49.064098: step 6975, loss 0.000722555, acc 1
2017-08-08T16:58:49.278195: step 6976, loss 0.000138203, acc 1
2017-08-08T16:58:49.501707: step 6977, loss 1.48333e-05, acc 1
2017-08-08T16:58:49.683420: step 6978, loss 1.91072e-05, acc 1
2017-08-08T16:58:49.939124: step 6979, loss 4.58826e-05, acc 1
2017-08-08T16:58:50.260020: step 6980, loss 0.00129221, acc 1
2017-08-08T16:58:50.616429: step 6981, loss 0.000209787, acc 1
2017-08-08T16:58:50.799792: step 6982, loss 0.00197787, acc 1
2017-08-08T16:58:51.007071: step 6983, loss 0.00350843, acc 1
2017-08-08T16:58:51.303015: step 6984, loss 0.0419329, acc 0.96875
2017-08-08T16:58:51.581682: step 6985, loss 0.00338848, acc 1
2017-08-08T16:58:51.796230: step 6986, loss 0.0285917, acc 0.984375
2017-08-08T16:58:51.999978: step 6987, loss 0.000215689, acc 1
2017-08-08T16:58:52.258193: step 6988, loss 0.00853941, acc 1
2017-08-08T16:58:52.617191: step 6989, loss 0.000149673, acc 1
2017-08-08T16:58:52.930697: step 6990, loss 0.000471427, acc 1
2017-08-08T16:58:53.105893: step 6991, loss 0.000488274, acc 1
2017-08-08T16:58:53.328114: step 6992, loss 0.0199996, acc 0.984375
2017-08-08T16:58:53.653860: step 6993, loss 0.00122691, acc 1
2017-08-08T16:58:53.890745: step 6994, loss 0.000265611, acc 1
2017-08-08T16:58:54.166945: step 6995, loss 0.0158055, acc 0.984375
2017-08-08T16:58:54.645386: step 6996, loss 0.00114554, acc 1
2017-08-08T16:58:55.047706: step 6997, loss 0.000335317, acc 1
2017-08-08T16:58:55.366414: step 6998, loss 2.59394e-05, acc 1
2017-08-08T16:58:55.570560: step 6999, loss 0.00109603, acc 1
2017-08-08T16:58:55.851023: step 7000, loss 0.00136519, acc 1

Evaluation:
2017-08-08T16:58:56.578663: step 7000, loss 2.18186, acc 0.723265

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-7000

2017-08-08T16:58:57.001009: step 7001, loss 0.00181565, acc 1
2017-08-08T16:58:57.268759: step 7002, loss 0.00625328, acc 1
2017-08-08T16:58:57.601492: step 7003, loss 0.00198349, acc 1
2017-08-08T16:58:57.920556: step 7004, loss 0.00568759, acc 1
2017-08-08T16:58:58.133954: step 7005, loss 0.000155965, acc 1
2017-08-08T16:58:58.329811: step 7006, loss 0.0045567, acc 1
2017-08-08T16:58:58.662297: step 7007, loss 0.0118307, acc 0.984375
2017-08-08T16:58:58.903771: step 7008, loss 0.00816013, acc 1
2017-08-08T16:58:59.138180: step 7009, loss 0.000761731, acc 1
2017-08-08T16:58:59.365443: step 7010, loss 0.000503224, acc 1
2017-08-08T16:58:59.681516: step 7011, loss 0.0078661, acc 1
2017-08-08T16:58:59.977311: step 7012, loss 0.000830164, acc 1
2017-08-08T16:59:00.236750: step 7013, loss 0.0113354, acc 1
2017-08-08T16:59:00.467025: step 7014, loss 0.00593651, acc 1
2017-08-08T16:59:00.809356: step 7015, loss 0.000978092, acc 1
2017-08-08T16:59:01.008786: step 7016, loss 8.95922e-05, acc 1
2017-08-08T16:59:01.266724: step 7017, loss 0.00727639, acc 1
2017-08-08T16:59:01.519511: step 7018, loss 0.000970738, acc 1
2017-08-08T16:59:02.024214: step 7019, loss 0.000958025, acc 1
2017-08-08T16:59:02.408695: step 7020, loss 0.0015542, acc 1
2017-08-08T16:59:02.760154: step 7021, loss 0.00114251, acc 1
2017-08-08T16:59:03.002217: step 7022, loss 0.000231988, acc 1
2017-08-08T16:59:03.312093: step 7023, loss 0.000318615, acc 1
2017-08-08T16:59:03.667387: step 7024, loss 0.0028404, acc 1
2017-08-08T16:59:03.914590: step 7025, loss 0.00200101, acc 1
2017-08-08T16:59:04.160355: step 7026, loss 0.047778, acc 0.984375
2017-08-08T16:59:04.591794: step 7027, loss 0.00214513, acc 1
2017-08-08T16:59:04.828541: step 7028, loss 0.000502703, acc 1
2017-08-08T16:59:05.156682: step 7029, loss 0.000629389, acc 1
2017-08-08T16:59:05.403176: step 7030, loss 0.00245561, acc 1
2017-08-08T16:59:05.792064: step 7031, loss 6.42981e-05, acc 1
2017-08-08T16:59:06.194024: step 7032, loss 0.000801329, acc 1
2017-08-08T16:59:06.407393: step 7033, loss 0.0003817, acc 1
2017-08-08T16:59:06.614827: step 7034, loss 0.000920548, acc 1
2017-08-08T16:59:06.901264: step 7035, loss 8.66899e-05, acc 1
2017-08-08T16:59:07.156738: step 7036, loss 0.0286427, acc 0.984375
2017-08-08T16:59:07.475599: step 7037, loss 0.0181749, acc 0.984375
2017-08-08T16:59:07.704491: step 7038, loss 0.000946852, acc 1
2017-08-08T16:59:08.001188: step 7039, loss 4.74901e-05, acc 1
2017-08-08T16:59:08.336914: step 7040, loss 0.00326178, acc 1
2017-08-08T16:59:08.571211: step 7041, loss 0.000859137, acc 1
2017-08-08T16:59:08.834244: step 7042, loss 7.41867e-05, acc 1
2017-08-08T16:59:09.098840: step 7043, loss 0.0357533, acc 0.984375
2017-08-08T16:59:09.450373: step 7044, loss 0.000147082, acc 1
2017-08-08T16:59:09.845409: step 7045, loss 0.000570641, acc 1
2017-08-08T16:59:10.213265: step 7046, loss 0.0155786, acc 1
2017-08-08T16:59:10.442413: step 7047, loss 0.000657719, acc 1
2017-08-08T16:59:10.663977: step 7048, loss 0.00533168, acc 1
2017-08-08T16:59:11.000638: step 7049, loss 0.000190696, acc 1
2017-08-08T16:59:11.278708: step 7050, loss 0.000261101, acc 1
2017-08-08T16:59:11.515219: step 7051, loss 0.000152241, acc 1
2017-08-08T16:59:11.766630: step 7052, loss 0.00106067, acc 1
2017-08-08T16:59:12.053388: step 7053, loss 8.43778e-05, acc 1
2017-08-08T16:59:12.494573: step 7054, loss 0.00148759, acc 1
2017-08-08T16:59:12.789789: step 7055, loss 0.00065293, acc 1
2017-08-08T16:59:13.016941: step 7056, loss 0.0186205, acc 0.984375
2017-08-08T16:59:13.307875: step 7057, loss 0.00368148, acc 1
2017-08-08T16:59:13.555677: step 7058, loss 0.00262117, acc 1
2017-08-08T16:59:13.746640: step 7059, loss 0.00292486, acc 1
2017-08-08T16:59:14.007322: step 7060, loss 0.00223828, acc 1
2017-08-08T16:59:14.290932: step 7061, loss 0.000439035, acc 1
2017-08-08T16:59:14.558977: step 7062, loss 0.00130138, acc 1
2017-08-08T16:59:14.891933: step 7063, loss 0.000282088, acc 1
2017-08-08T16:59:15.177915: step 7064, loss 0.000424181, acc 1
2017-08-08T16:59:15.408448: step 7065, loss 4.52665e-05, acc 1
2017-08-08T16:59:15.758866: step 7066, loss 0.000173446, acc 1
2017-08-08T16:59:15.931807: step 7067, loss 0.0773538, acc 0.984375
2017-08-08T16:59:16.137941: step 7068, loss 0.00479105, acc 1
2017-08-08T16:59:16.357406: step 7069, loss 0.00112957, acc 1
2017-08-08T16:59:16.725589: step 7070, loss 0.000333793, acc 1
2017-08-08T16:59:17.081622: step 7071, loss 0.00244538, acc 1
2017-08-08T16:59:17.406569: step 7072, loss 0.000391065, acc 1
2017-08-08T16:59:17.659393: step 7073, loss 0.00110408, acc 1
2017-08-08T16:59:17.901371: step 7074, loss 0.00264312, acc 1
2017-08-08T16:59:18.210615: step 7075, loss 0.00309675, acc 1
2017-08-08T16:59:18.437295: step 7076, loss 0.00178999, acc 1
2017-08-08T16:59:18.676374: step 7077, loss 0.00103145, acc 1
2017-08-08T16:59:18.872459: step 7078, loss 0.000928382, acc 1
2017-08-08T16:59:19.194195: step 7079, loss 8.90522e-05, acc 1
2017-08-08T16:59:19.397612: step 7080, loss 0.00799537, acc 1
2017-08-08T16:59:19.609666: step 7081, loss 0.000882765, acc 1
2017-08-08T16:59:19.783497: step 7082, loss 0.00219741, acc 1
2017-08-08T16:59:20.202309: step 7083, loss 0.000257339, acc 1
2017-08-08T16:59:20.490936: step 7084, loss 0.000616305, acc 1
2017-08-08T16:59:20.767708: step 7085, loss 0.0639498, acc 0.984375
2017-08-08T16:59:21.045024: step 7086, loss 0.00136381, acc 1
2017-08-08T16:59:21.362460: step 7087, loss 0.00373237, acc 1
2017-08-08T16:59:21.689690: step 7088, loss 0.000851112, acc 1
2017-08-08T16:59:21.980439: step 7089, loss 0.0175641, acc 0.984375
2017-08-08T16:59:22.186474: step 7090, loss 0.00688251, acc 1
2017-08-08T16:59:22.542289: step 7091, loss 0.000155188, acc 1
2017-08-08T16:59:22.908191: step 7092, loss 0.000617036, acc 1
2017-08-08T16:59:23.082563: step 7093, loss 0.014798, acc 0.984375
2017-08-08T16:59:23.254724: step 7094, loss 0.000149975, acc 1
2017-08-08T16:59:23.496057: step 7095, loss 0.00615143, acc 1
2017-08-08T16:59:23.888109: step 7096, loss 0.0124413, acc 1
2017-08-08T16:59:24.216781: step 7097, loss 0.000308468, acc 1
2017-08-08T16:59:24.463209: step 7098, loss 0.00199838, acc 1
2017-08-08T16:59:24.753477: step 7099, loss 0.00077231, acc 1
2017-08-08T16:59:24.980050: step 7100, loss 0.0136591, acc 0.984375

Evaluation:
2017-08-08T16:59:25.528677: step 7100, loss 2.24276, acc 0.725141

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-7100

2017-08-08T16:59:26.089561: step 7101, loss 0.00489501, acc 1
2017-08-08T16:59:26.357826: step 7102, loss 0.000779348, acc 1
2017-08-08T16:59:26.565002: step 7103, loss 0.00115755, acc 1
2017-08-08T16:59:26.745469: step 7104, loss 0.00136664, acc 1
2017-08-08T16:59:27.029296: step 7105, loss 0.0113087, acc 1
2017-08-08T16:59:27.261485: step 7106, loss 0.00225181, acc 1
2017-08-08T16:59:27.523662: step 7107, loss 0.000244668, acc 1
2017-08-08T16:59:27.785356: step 7108, loss 0.000670395, acc 1
2017-08-08T16:59:28.058147: step 7109, loss 0.0118353, acc 0.984375
2017-08-08T16:59:28.338942: step 7110, loss 7.26686e-05, acc 1
2017-08-08T16:59:28.545763: step 7111, loss 0.000385341, acc 1
2017-08-08T16:59:28.793648: step 7112, loss 0.00557862, acc 1
2017-08-08T16:59:29.118856: step 7113, loss 0.000493526, acc 1
2017-08-08T16:59:29.388095: step 7114, loss 0.000123555, acc 1
2017-08-08T16:59:29.643923: step 7115, loss 0.059137, acc 0.984375
2017-08-08T16:59:29.908164: step 7116, loss 0.00567892, acc 1
2017-08-08T16:59:30.263504: step 7117, loss 0.000840144, acc 1
2017-08-08T16:59:30.593716: step 7118, loss 0.000164149, acc 1
2017-08-08T16:59:30.865225: step 7119, loss 9.71744e-05, acc 1
2017-08-08T16:59:31.054858: step 7120, loss 0.0176648, acc 0.984375
2017-08-08T16:59:31.401232: step 7121, loss 0.0061681, acc 1
2017-08-08T16:59:31.684097: step 7122, loss 0.000433749, acc 1
2017-08-08T16:59:31.932175: step 7123, loss 0.00238314, acc 1
2017-08-08T16:59:32.195195: step 7124, loss 0.0021622, acc 1
2017-08-08T16:59:32.456035: step 7125, loss 0.000137107, acc 1
2017-08-08T16:59:32.769944: step 7126, loss 0.00073936, acc 1
2017-08-08T16:59:33.033366: step 7127, loss 0.00153393, acc 1
2017-08-08T16:59:33.384221: step 7128, loss 0.00156714, acc 1
2017-08-08T16:59:33.627076: step 7129, loss 0.000403296, acc 1
2017-08-08T16:59:34.144239: step 7130, loss 0.000967129, acc 1
2017-08-08T16:59:34.407494: step 7131, loss 0.00132386, acc 1
2017-08-08T16:59:34.669143: step 7132, loss 0.00037638, acc 1
2017-08-08T16:59:34.944935: step 7133, loss 0.0018578, acc 1
2017-08-08T16:59:35.370091: step 7134, loss 0.006775, acc 1
2017-08-08T16:59:35.779450: step 7135, loss 0.00106197, acc 1
2017-08-08T16:59:36.090247: step 7136, loss 0.00262034, acc 1
2017-08-08T16:59:36.293049: step 7137, loss 0.00172852, acc 1
2017-08-08T16:59:36.524879: step 7138, loss 0.000475212, acc 1
2017-08-08T16:59:36.832649: step 7139, loss 0.00029446, acc 1
2017-08-08T16:59:37.103923: step 7140, loss 0.0371686, acc 0.984375
2017-08-08T16:59:37.289000: step 7141, loss 0.00190201, acc 1
2017-08-08T16:59:37.541729: step 7142, loss 0.0051089, acc 1
2017-08-08T16:59:37.816355: step 7143, loss 0.000571763, acc 1
2017-08-08T16:59:38.109552: step 7144, loss 0.00432346, acc 1
2017-08-08T16:59:38.383360: step 7145, loss 9.06728e-05, acc 1
2017-08-08T16:59:38.635103: step 7146, loss 0.00736225, acc 1
2017-08-08T16:59:38.904325: step 7147, loss 0.00075324, acc 1
2017-08-08T16:59:39.339404: step 7148, loss 0.00122616, acc 1
2017-08-08T16:59:39.542204: step 7149, loss 0.00691639, acc 1
2017-08-08T16:59:39.740236: step 7150, loss 0.00693788, acc 1
2017-08-08T16:59:40.049463: step 7151, loss 0.000245374, acc 1
2017-08-08T16:59:40.428932: step 7152, loss 1.16773e-05, acc 1
2017-08-08T16:59:40.869976: step 7153, loss 5.37224e-05, acc 1
2017-08-08T16:59:41.202303: step 7154, loss 4.67389e-05, acc 1
2017-08-08T16:59:41.449946: step 7155, loss 0.00094236, acc 1
2017-08-08T16:59:41.732796: step 7156, loss 0.000379788, acc 1
2017-08-08T16:59:42.175202: step 7157, loss 4.21375e-05, acc 1
2017-08-08T16:59:42.439958: step 7158, loss 0.000162453, acc 1
2017-08-08T16:59:42.735518: step 7159, loss 0.00684714, acc 1
2017-08-08T16:59:42.945356: step 7160, loss 0.000295634, acc 1
2017-08-08T16:59:43.286774: step 7161, loss 0.0020177, acc 1
2017-08-08T16:59:43.702531: step 7162, loss 0.0081005, acc 1
2017-08-08T16:59:44.063264: step 7163, loss 0.00116212, acc 1
2017-08-08T16:59:44.298831: step 7164, loss 0.022182, acc 0.984375
2017-08-08T16:59:44.598379: step 7165, loss 9.02207e-05, acc 1
2017-08-08T16:59:44.919399: step 7166, loss 0.000324169, acc 1
2017-08-08T16:59:45.234957: step 7167, loss 0.000261624, acc 1
2017-08-08T16:59:45.538964: step 7168, loss 0.000810021, acc 1
2017-08-08T16:59:45.927650: step 7169, loss 0.000282103, acc 1
2017-08-08T16:59:46.340112: step 7170, loss 0.00165411, acc 1
2017-08-08T16:59:46.766566: step 7171, loss 8.26706e-05, acc 1
2017-08-08T16:59:47.033193: step 7172, loss 0.00113898, acc 1
2017-08-08T16:59:47.497147: step 7173, loss 0.00122758, acc 1
2017-08-08T16:59:47.736028: step 7174, loss 0.00859493, acc 1
2017-08-08T16:59:47.956226: step 7175, loss 0.000414015, acc 1
2017-08-08T16:59:48.191174: step 7176, loss 0.000331585, acc 1
2017-08-08T16:59:48.493025: step 7177, loss 0.00399082, acc 1
2017-08-08T16:59:48.890030: step 7178, loss 0.000500455, acc 1
2017-08-08T16:59:49.201838: step 7179, loss 0.000301777, acc 1
2017-08-08T16:59:49.443190: step 7180, loss 0.000804207, acc 1
2017-08-08T16:59:49.661899: step 7181, loss 0.00380704, acc 1
2017-08-08T16:59:50.069329: step 7182, loss 0.000318756, acc 1
2017-08-08T16:59:50.284597: step 7183, loss 0.0219637, acc 0.984375
2017-08-08T16:59:50.537246: step 7184, loss 0.000370668, acc 1
2017-08-08T16:59:50.781566: step 7185, loss 0.00129296, acc 1
2017-08-08T16:59:51.241901: step 7186, loss 0.0170374, acc 1
2017-08-08T16:59:51.624527: step 7187, loss 0.00284139, acc 1
2017-08-08T16:59:51.856263: step 7188, loss 0.000505295, acc 1
2017-08-08T16:59:52.105325: step 7189, loss 0.00402515, acc 1
2017-08-08T16:59:52.431252: step 7190, loss 0.000122716, acc 1
2017-08-08T16:59:52.658318: step 7191, loss 5.55029e-05, acc 1
2017-08-08T16:59:52.897454: step 7192, loss 0.00096419, acc 1
2017-08-08T16:59:53.208802: step 7193, loss 0.00552895, acc 1
2017-08-08T16:59:53.518990: step 7194, loss 0.00947029, acc 1
2017-08-08T16:59:53.778749: step 7195, loss 8.45772e-05, acc 1
2017-08-08T16:59:53.998723: step 7196, loss 0.000370272, acc 1
2017-08-08T16:59:54.177370: step 7197, loss 0.0186958, acc 1
2017-08-08T16:59:54.434374: step 7198, loss 0.000241112, acc 1
2017-08-08T16:59:54.687972: step 7199, loss 0.00100489, acc 1
2017-08-08T16:59:54.924957: step 7200, loss 0.00176752, acc 1

Evaluation:
2017-08-08T16:59:55.537492: step 7200, loss 2.2986, acc 0.727017

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-7200

2017-08-08T16:59:56.252120: step 7201, loss 0.00349611, acc 1
2017-08-08T16:59:56.586801: step 7202, loss 0.000372606, acc 1
2017-08-08T16:59:56.889717: step 7203, loss 0.0156705, acc 0.984375
2017-08-08T16:59:57.312981: step 7204, loss 0.000441345, acc 1
2017-08-08T16:59:57.640888: step 7205, loss 0.00320995, acc 1
2017-08-08T16:59:57.915175: step 7206, loss 0.00354359, acc 1
2017-08-08T16:59:58.145449: step 7207, loss 0.00095216, acc 1
2017-08-08T16:59:58.491587: step 7208, loss 4.12025e-05, acc 1
2017-08-08T16:59:58.791525: step 7209, loss 0.00266947, acc 1
2017-08-08T16:59:59.050234: step 7210, loss 0.0159648, acc 0.984375
2017-08-08T16:59:59.245654: step 7211, loss 0.00549876, acc 1
2017-08-08T16:59:59.476309: step 7212, loss 0.000476346, acc 1
2017-08-08T16:59:59.774078: step 7213, loss 9.96654e-05, acc 1
2017-08-08T16:59:59.971585: step 7214, loss 0.00216352, acc 1
2017-08-08T17:00:00.191067: step 7215, loss 0.0187994, acc 0.984375
2017-08-08T17:00:00.513652: step 7216, loss 0.00074562, acc 1
2017-08-08T17:00:00.880013: step 7217, loss 0.00290118, acc 1
2017-08-08T17:00:01.240546: step 7218, loss 0.00029867, acc 1
2017-08-08T17:00:01.577231: step 7219, loss 0.000698623, acc 1
2017-08-08T17:00:01.847593: step 7220, loss 0.000359974, acc 1
2017-08-08T17:00:02.528388: step 7221, loss 0.0224111, acc 0.984375
2017-08-08T17:00:03.002918: step 7222, loss 0.000933125, acc 1
2017-08-08T17:00:03.277659: step 7223, loss 0.0136793, acc 1
2017-08-08T17:00:03.592251: step 7224, loss 0.000211155, acc 1
2017-08-08T17:00:03.926525: step 7225, loss 0.0105681, acc 1
2017-08-08T17:00:04.306966: step 7226, loss 0.00352947, acc 1
2017-08-08T17:00:04.605755: step 7227, loss 6.08844e-05, acc 1
2017-08-08T17:00:05.004695: step 7228, loss 0.000175609, acc 1
2017-08-08T17:00:05.210710: step 7229, loss 0.0120452, acc 1
2017-08-08T17:00:05.603154: step 7230, loss 0.000414393, acc 1
2017-08-08T17:00:05.825331: step 7231, loss 0.000270823, acc 1
2017-08-08T17:00:06.065390: step 7232, loss 0.00114464, acc 1
2017-08-08T17:00:06.329421: step 7233, loss 0.00205317, acc 1
2017-08-08T17:00:06.692291: step 7234, loss 0.00101059, acc 1
2017-08-08T17:00:07.080130: step 7235, loss 0.0045709, acc 1
2017-08-08T17:00:07.458616: step 7236, loss 0.000512654, acc 1
2017-08-08T17:00:07.707727: step 7237, loss 0.00247079, acc 1
2017-08-08T17:00:07.942320: step 7238, loss 0.015197, acc 0.984375
2017-08-08T17:00:08.232127: step 7239, loss 0.000273645, acc 1
2017-08-08T17:00:08.449413: step 7240, loss 0.00225851, acc 1
2017-08-08T17:00:08.716641: step 7241, loss 0.00086096, acc 1
2017-08-08T17:00:09.098303: step 7242, loss 0.00351706, acc 1
2017-08-08T17:00:09.500960: step 7243, loss 0.000216202, acc 1
2017-08-08T17:00:09.874803: step 7244, loss 9.39878e-05, acc 1
2017-08-08T17:00:10.216431: step 7245, loss 0.00231386, acc 1
2017-08-08T17:00:10.464802: step 7246, loss 0.000408488, acc 1
2017-08-08T17:00:10.826254: step 7247, loss 0.00190314, acc 1
2017-08-08T17:00:11.192753: step 7248, loss 1.87186e-05, acc 1
2017-08-08T17:00:11.490572: step 7249, loss 1.42093e-05, acc 1
2017-08-08T17:00:11.824314: step 7250, loss 0.00239924, acc 1
2017-08-08T17:00:12.229446: step 7251, loss 0.000208441, acc 1
2017-08-08T17:00:12.583221: step 7252, loss 0.0105453, acc 1
2017-08-08T17:00:13.042311: step 7253, loss 0.000925046, acc 1
2017-08-08T17:00:13.343117: step 7254, loss 0.00145801, acc 1
2017-08-08T17:00:13.616553: step 7255, loss 0.00113677, acc 1
2017-08-08T17:00:14.069124: step 7256, loss 0.00172704, acc 1
2017-08-08T17:00:14.344782: step 7257, loss 0.0160089, acc 0.984375
2017-08-08T17:00:14.642104: step 7258, loss 4.90189e-05, acc 1
2017-08-08T17:00:14.956329: step 7259, loss 0.00188048, acc 1
2017-08-08T17:00:15.364971: step 7260, loss 0.000118836, acc 1
2017-08-08T17:00:15.763687: step 7261, loss 0.000319244, acc 1
2017-08-08T17:00:16.176609: step 7262, loss 0.00387068, acc 1
2017-08-08T17:00:16.462864: step 7263, loss 0.027111, acc 0.984375
2017-08-08T17:00:16.766219: step 7264, loss 0.000290652, acc 1
2017-08-08T17:00:17.216049: step 7265, loss 0.000513732, acc 1
2017-08-08T17:00:17.553902: step 7266, loss 0.000677874, acc 1
2017-08-08T17:00:17.884364: step 7267, loss 0.00352691, acc 1
2017-08-08T17:00:18.322965: step 7268, loss 0.000295253, acc 1
2017-08-08T17:00:18.812867: step 7269, loss 0.000103794, acc 1
2017-08-08T17:00:19.227534: step 7270, loss 0.000340612, acc 1
2017-08-08T17:00:19.543099: step 7271, loss 0.00115221, acc 1
2017-08-08T17:00:19.816051: step 7272, loss 0.0190294, acc 0.984375
2017-08-08T17:00:20.229516: step 7273, loss 2.14745e-05, acc 1
2017-08-08T17:00:20.536880: step 7274, loss 0.00232524, acc 1
2017-08-08T17:00:20.763075: step 7275, loss 0.00013939, acc 1
2017-08-08T17:00:21.026706: step 7276, loss 0.0401046, acc 0.984375
2017-08-08T17:00:21.355933: step 7277, loss 0.000890712, acc 1
2017-08-08T17:00:21.772650: step 7278, loss 0.00198173, acc 1
2017-08-08T17:00:22.204202: step 7279, loss 0.0186155, acc 0.984375
2017-08-08T17:00:22.533378: step 7280, loss 0.00607101, acc 1
2017-08-08T17:00:22.858365: step 7281, loss 0.00720973, acc 1
2017-08-08T17:00:23.252097: step 7282, loss 0.000802999, acc 1
2017-08-08T17:00:23.474300: step 7283, loss 0.000893767, acc 1
2017-08-08T17:00:23.748953: step 7284, loss 0.00105853, acc 1
2017-08-08T17:00:24.001342: step 7285, loss 4.46027e-05, acc 1
2017-08-08T17:00:24.264729: step 7286, loss 0.00119897, acc 1
2017-08-08T17:00:24.607827: step 7287, loss 0.00138392, acc 1
2017-08-08T17:00:24.883315: step 7288, loss 0.00122884, acc 1
2017-08-08T17:00:25.079823: step 7289, loss 0.0072991, acc 1
2017-08-08T17:00:25.258767: step 7290, loss 0.000369923, acc 1
2017-08-08T17:00:25.525374: step 7291, loss 0.0279279, acc 0.984375
2017-08-08T17:00:25.856505: step 7292, loss 4.24867e-05, acc 1
2017-08-08T17:00:26.164929: step 7293, loss 0.0177168, acc 0.984375
2017-08-08T17:00:26.465719: step 7294, loss 0.0592764, acc 0.984375
2017-08-08T17:00:26.824767: step 7295, loss 0.00627548, acc 1
2017-08-08T17:00:27.104453: step 7296, loss 0.000604641, acc 1
2017-08-08T17:00:27.437370: step 7297, loss 0.0206928, acc 0.984375
2017-08-08T17:00:27.661899: step 7298, loss 0.0069743, acc 1
2017-08-08T17:00:27.856597: step 7299, loss 0.0513029, acc 0.96875
2017-08-08T17:00:28.192827: step 7300, loss 0.000222294, acc 1

Evaluation:
2017-08-08T17:00:28.849192: step 7300, loss 2.34459, acc 0.722326

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-7300

2017-08-08T17:00:29.258147: step 7301, loss 0.00721012, acc 1
2017-08-08T17:00:29.639534: step 7302, loss 0.000419021, acc 1
2017-08-08T17:00:29.929172: step 7303, loss 5.5758e-05, acc 1
2017-08-08T17:00:30.276679: step 7304, loss 0.000145998, acc 1
2017-08-08T17:00:30.524637: step 7305, loss 0.000153882, acc 1
2017-08-08T17:00:30.857369: step 7306, loss 0.000490242, acc 1
2017-08-08T17:00:31.224513: step 7307, loss 0.00231415, acc 1
2017-08-08T17:00:31.450047: step 7308, loss 0.000654756, acc 1
2017-08-08T17:00:31.680400: step 7309, loss 0.00138332, acc 1
2017-08-08T17:00:31.893396: step 7310, loss 0.00230392, acc 1
2017-08-08T17:00:32.258867: step 7311, loss 0.00237472, acc 1
2017-08-08T17:00:32.570426: step 7312, loss 0.00238374, acc 1
2017-08-08T17:00:32.761642: step 7313, loss 0.00210998, acc 1
2017-08-08T17:00:32.990941: step 7314, loss 0.00025592, acc 1
2017-08-08T17:00:33.237707: step 7315, loss 0.0096355, acc 1
2017-08-08T17:00:33.484386: step 7316, loss 0.00260773, acc 1
2017-08-08T17:00:33.675134: step 7317, loss 0.00552641, acc 1
2017-08-08T17:00:33.921448: step 7318, loss 0.00380398, acc 1
2017-08-08T17:00:34.242764: step 7319, loss 0.00213846, acc 1
2017-08-08T17:00:34.625384: step 7320, loss 0.0344206, acc 0.984375
2017-08-08T17:00:34.963208: step 7321, loss 0.0014823, acc 1
2017-08-08T17:00:35.218090: step 7322, loss 0.00013856, acc 1
2017-08-08T17:00:35.509191: step 7323, loss 0.000195396, acc 1
2017-08-08T17:00:35.880236: step 7324, loss 0.000561416, acc 1
2017-08-08T17:00:36.108848: step 7325, loss 0.0011441, acc 1
2017-08-08T17:00:36.402924: step 7326, loss 0.000687418, acc 1
2017-08-08T17:00:36.713746: step 7327, loss 0.000655594, acc 1
2017-08-08T17:00:37.022645: step 7328, loss 0.00262841, acc 1
2017-08-08T17:00:37.342630: step 7329, loss 0.00199439, acc 1
2017-08-08T17:00:37.595902: step 7330, loss 0.00321934, acc 1
2017-08-08T17:00:37.888813: step 7331, loss 0.000879572, acc 1
2017-08-08T17:00:38.121678: step 7332, loss 0.00635896, acc 1
2017-08-08T17:00:38.396820: step 7333, loss 0.00447022, acc 1
2017-08-08T17:00:38.750028: step 7334, loss 0.000401956, acc 1
2017-08-08T17:00:39.146746: step 7335, loss 0.000112158, acc 1
2017-08-08T17:00:39.547432: step 7336, loss 0.000563108, acc 1
2017-08-08T17:00:39.877634: step 7337, loss 0.0138193, acc 0.984375
2017-08-08T17:00:40.115198: step 7338, loss 0.000212743, acc 1
2017-08-08T17:00:40.381373: step 7339, loss 0.0269078, acc 0.984375
2017-08-08T17:00:40.802700: step 7340, loss 0.000713145, acc 1
2017-08-08T17:00:41.088962: step 7341, loss 0.0065597, acc 1
2017-08-08T17:00:41.362945: step 7342, loss 0.00133814, acc 1
2017-08-08T17:00:41.670402: step 7343, loss 0.000517755, acc 1
2017-08-08T17:00:42.123916: step 7344, loss 0.00502229, acc 1
2017-08-08T17:00:42.491236: step 7345, loss 0.00139351, acc 1
2017-08-08T17:00:42.864007: step 7346, loss 0.0192158, acc 0.984375
2017-08-08T17:00:43.136255: step 7347, loss 1.57477e-05, acc 1
2017-08-08T17:00:43.373329: step 7348, loss 0.00114133, acc 1
2017-08-08T17:00:43.751717: step 7349, loss 0.0246262, acc 0.984375
2017-08-08T17:00:44.021355: step 7350, loss 0.00141185, acc 1
2017-08-08T17:00:44.272451: step 7351, loss 0.00049103, acc 1
2017-08-08T17:00:44.501311: step 7352, loss 0.00011812, acc 1
2017-08-08T17:00:44.802991: step 7353, loss 0.000206567, acc 1
2017-08-08T17:00:45.173682: step 7354, loss 0.000111631, acc 1
2017-08-08T17:00:45.448124: step 7355, loss 0.000649974, acc 1
2017-08-08T17:00:45.718591: step 7356, loss 0.003393, acc 1
2017-08-08T17:00:46.078826: step 7357, loss 0.00479863, acc 1
2017-08-08T17:00:46.377709: step 7358, loss 0.00177752, acc 1
2017-08-08T17:00:46.669573: step 7359, loss 0.00483046, acc 1
2017-08-08T17:00:47.069821: step 7360, loss 0.000426, acc 1
2017-08-08T17:00:47.541471: step 7361, loss 0.00291064, acc 1
2017-08-08T17:00:47.925850: step 7362, loss 0.000214289, acc 1
2017-08-08T17:00:48.272729: step 7363, loss 0.00197405, acc 1
2017-08-08T17:00:48.519146: step 7364, loss 0.0214963, acc 0.984375
2017-08-08T17:00:48.984883: step 7365, loss 0.0200774, acc 0.984375
2017-08-08T17:00:49.255848: step 7366, loss 7.98499e-05, acc 1
2017-08-08T17:00:49.533117: step 7367, loss 0.0125166, acc 0.984375
2017-08-08T17:00:49.799908: step 7368, loss 0.00593899, acc 1
2017-08-08T17:00:50.317487: step 7369, loss 0.000650857, acc 1
2017-08-08T17:00:50.699942: step 7370, loss 0.000249078, acc 1
2017-08-08T17:00:51.047805: step 7371, loss 0.00061411, acc 1
2017-08-08T17:00:51.292947: step 7372, loss 0.000828662, acc 1
2017-08-08T17:00:51.668078: step 7373, loss 6.87546e-05, acc 1
2017-08-08T17:00:51.947473: step 7374, loss 0.00426544, acc 1
2017-08-08T17:00:52.221742: step 7375, loss 0.000587597, acc 1
2017-08-08T17:00:52.514879: step 7376, loss 0.024134, acc 0.984375
2017-08-08T17:00:52.971691: step 7377, loss 0.000501691, acc 1
2017-08-08T17:00:53.370053: step 7378, loss 0.000926139, acc 1
2017-08-08T17:00:53.642188: step 7379, loss 0.00747179, acc 1
2017-08-08T17:00:53.853342: step 7380, loss 0.000121272, acc 1
2017-08-08T17:00:54.164384: step 7381, loss 0.0317967, acc 0.984375
2017-08-08T17:00:54.534546: step 7382, loss 0.000374591, acc 1
2017-08-08T17:00:54.819041: step 7383, loss 0.0108696, acc 1
2017-08-08T17:00:55.092074: step 7384, loss 0.00909676, acc 1
2017-08-08T17:00:55.439472: step 7385, loss 0.00050851, acc 1
2017-08-08T17:00:55.788550: step 7386, loss 0.000996074, acc 1
2017-08-08T17:00:56.063882: step 7387, loss 0.000369725, acc 1
2017-08-08T17:00:56.305666: step 7388, loss 0.00151598, acc 1
2017-08-08T17:00:56.559524: step 7389, loss 0.008519, acc 1
2017-08-08T17:00:56.932637: step 7390, loss 0.00116645, acc 1
2017-08-08T17:00:57.260735: step 7391, loss 0.000219098, acc 1
2017-08-08T17:00:57.533376: step 7392, loss 0.00159587, acc 1
2017-08-08T17:00:57.867644: step 7393, loss 0.00204849, acc 1
2017-08-08T17:00:58.214974: step 7394, loss 0.0051404, acc 1
2017-08-08T17:00:58.437584: step 7395, loss 0.000896324, acc 1
2017-08-08T17:00:58.639561: step 7396, loss 0.0194817, acc 0.984375
2017-08-08T17:00:58.849377: step 7397, loss 0.0198536, acc 0.984375
2017-08-08T17:00:59.223250: step 7398, loss 0.0593832, acc 0.984375
2017-08-08T17:00:59.450585: step 7399, loss 0.00106234, acc 1
2017-08-08T17:00:59.665130: step 7400, loss 0.000778427, acc 1

Evaluation:
2017-08-08T17:01:00.383657: step 7400, loss 2.42595, acc 0.722326

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-7400

2017-08-08T17:01:01.028569: step 7401, loss 0.00116807, acc 1
2017-08-08T17:01:01.257321: step 7402, loss 0.000533602, acc 1
2017-08-08T17:01:01.592772: step 7403, loss 0.00840659, acc 1
2017-08-08T17:01:02.064256: step 7404, loss 0.00376139, acc 1
2017-08-08T17:01:02.378362: step 7405, loss 0.000308632, acc 1
2017-08-08T17:01:02.639953: step 7406, loss 0.000182386, acc 1
2017-08-08T17:01:02.948788: step 7407, loss 0.00578418, acc 1
2017-08-08T17:01:03.324906: step 7408, loss 7.16716e-05, acc 1
2017-08-08T17:01:03.765379: step 7409, loss 2.66706e-05, acc 1
2017-08-08T17:01:04.113376: step 7410, loss 0.00040508, acc 1
2017-08-08T17:01:04.474052: step 7411, loss 0.000780292, acc 1
2017-08-08T17:01:04.702567: step 7412, loss 0.000555725, acc 1
2017-08-08T17:01:04.934011: step 7413, loss 0.00913117, acc 1
2017-08-08T17:01:05.283193: step 7414, loss 0.00334965, acc 1
2017-08-08T17:01:05.489337: step 7415, loss 0.100596, acc 0.984375
2017-08-08T17:01:05.712601: step 7416, loss 0.0215097, acc 0.984375
2017-08-08T17:01:06.014270: step 7417, loss 0.00143314, acc 1
2017-08-08T17:01:06.459125: step 7418, loss 0.000320383, acc 1
2017-08-08T17:01:06.879234: step 7419, loss 0.000795072, acc 1
2017-08-08T17:01:07.248536: step 7420, loss 2.79491e-05, acc 1
2017-08-08T17:01:07.467016: step 7421, loss 0.000239646, acc 1
2017-08-08T17:01:07.660534: step 7422, loss 0.00406402, acc 1
2017-08-08T17:01:08.118744: step 7423, loss 0.000956743, acc 1
2017-08-08T17:01:08.334221: step 7424, loss 0.0427546, acc 0.984375
2017-08-08T17:01:08.540355: step 7425, loss 0.00452451, acc 1
2017-08-08T17:01:08.770474: step 7426, loss 0.000865157, acc 1
2017-08-08T17:01:09.201310: step 7427, loss 0.00154896, acc 1
2017-08-08T17:01:09.643171: step 7428, loss 0.0154964, acc 0.984375
2017-08-08T17:01:09.918852: step 7429, loss 0.000931557, acc 1
2017-08-08T17:01:10.145246: step 7430, loss 0.000222669, acc 1
2017-08-08T17:01:10.630540: step 7431, loss 0.000850264, acc 1
2017-08-08T17:01:10.924842: step 7432, loss 0.000256083, acc 1
2017-08-08T17:01:11.220374: step 7433, loss 0.00351809, acc 1
2017-08-08T17:01:11.495079: step 7434, loss 0.000379358, acc 1
2017-08-08T17:01:11.936530: step 7435, loss 0.000550842, acc 1
2017-08-08T17:01:12.234160: step 7436, loss 0.00156901, acc 1
2017-08-08T17:01:12.545760: step 7437, loss 0.000914947, acc 1
2017-08-08T17:01:12.776817: step 7438, loss 0.000341959, acc 1
2017-08-08T17:01:13.020185: step 7439, loss 0.0821898, acc 0.96875
2017-08-08T17:01:13.391934: step 7440, loss 0.0356996, acc 0.984375
2017-08-08T17:01:13.681462: step 7441, loss 0.00173704, acc 1
2017-08-08T17:01:13.991091: step 7442, loss 0.000428847, acc 1
2017-08-08T17:01:14.412547: step 7443, loss 0.00114947, acc 1
2017-08-08T17:01:14.853558: step 7444, loss 0.0484871, acc 0.984375
2017-08-08T17:01:15.293838: step 7445, loss 0.0084692, acc 1
2017-08-08T17:01:15.619718: step 7446, loss 0.000849961, acc 1
2017-08-08T17:01:15.905371: step 7447, loss 0.000241553, acc 1
2017-08-08T17:01:16.321365: step 7448, loss 0.000110791, acc 1
2017-08-08T17:01:16.596982: step 7449, loss 0.000157395, acc 1
2017-08-08T17:01:16.866811: step 7450, loss 0.00253425, acc 1
2017-08-08T17:01:17.217810: step 7451, loss 0.000241699, acc 1
2017-08-08T17:01:17.620707: step 7452, loss 0.00185537, acc 1
2017-08-08T17:01:18.022500: step 7453, loss 2.00434e-05, acc 1
2017-08-08T17:01:18.417961: step 7454, loss 0.00164131, acc 1
2017-08-08T17:01:18.695319: step 7455, loss 0.000646224, acc 1
2017-08-08T17:01:18.930019: step 7456, loss 0.00525538, acc 1
2017-08-08T17:01:19.432213: step 7457, loss 0.00279933, acc 1
2017-08-08T17:01:19.690563: step 7458, loss 0.00434282, acc 1
2017-08-08T17:01:19.944390: step 7459, loss 0.00199675, acc 1
2017-08-08T17:01:20.279447: step 7460, loss 0.00144915, acc 1
2017-08-08T17:01:20.868488: step 7461, loss 0.00373077, acc 1
2017-08-08T17:01:21.333433: step 7462, loss 0.00874485, acc 1
2017-08-08T17:01:21.675263: step 7463, loss 0.00211794, acc 1
2017-08-08T17:01:21.880928: step 7464, loss 0.00155945, acc 1
2017-08-08T17:01:22.115843: step 7465, loss 6.17927e-05, acc 1
2017-08-08T17:01:22.484017: step 7466, loss 0.000106885, acc 1
2017-08-08T17:01:22.760528: step 7467, loss 0.000175006, acc 1
2017-08-08T17:01:23.043479: step 7468, loss 0.00217992, acc 1
2017-08-08T17:01:23.337446: step 7469, loss 0.00677145, acc 1
2017-08-08T17:01:23.610628: step 7470, loss 0.00435293, acc 1
2017-08-08T17:01:23.912192: step 7471, loss 0.00013503, acc 1
2017-08-08T17:01:24.237119: step 7472, loss 0.000193358, acc 1
2017-08-08T17:01:24.483619: step 7473, loss 0.000152154, acc 1
2017-08-08T17:01:24.968770: step 7474, loss 6.74488e-05, acc 1
2017-08-08T17:01:25.244800: step 7475, loss 0.00143315, acc 1
2017-08-08T17:01:25.426659: step 7476, loss 0.000170149, acc 1
2017-08-08T17:01:25.621343: step 7477, loss 0.0042551, acc 1
2017-08-08T17:01:25.919252: step 7478, loss 0.00471479, acc 1
2017-08-08T17:01:26.380426: step 7479, loss 0.000407948, acc 1
2017-08-08T17:01:26.716042: step 7480, loss 0.000320985, acc 1
2017-08-08T17:01:26.965928: step 7481, loss 0.00351012, acc 1
2017-08-08T17:01:27.472641: step 7482, loss 0.000813564, acc 1
2017-08-08T17:01:27.690918: step 7483, loss 0.0024091, acc 1
2017-08-08T17:01:27.949919: step 7484, loss 0.000406308, acc 1
2017-08-08T17:01:28.221646: step 7485, loss 0.0110781, acc 0.984375
2017-08-08T17:01:28.653368: step 7486, loss 0.0457528, acc 0.984375
2017-08-08T17:01:29.023102: step 7487, loss 0.00123594, acc 1
2017-08-08T17:01:29.454766: step 7488, loss 0.00245448, acc 1
2017-08-08T17:01:29.808928: step 7489, loss 0.000284091, acc 1
2017-08-08T17:01:30.077379: step 7490, loss 0.019963, acc 0.984375
2017-08-08T17:01:30.553035: step 7491, loss 0.000407054, acc 1
2017-08-08T17:01:30.839808: step 7492, loss 0.00287292, acc 1
2017-08-08T17:01:31.135429: step 7493, loss 0.00584854, acc 1
2017-08-08T17:01:31.411353: step 7494, loss 0.0312119, acc 0.984375
2017-08-08T17:01:31.742779: step 7495, loss 0.00273304, acc 1
2017-08-08T17:01:32.019456: step 7496, loss 0.000138224, acc 1
2017-08-08T17:01:32.305385: step 7497, loss 7.08817e-05, acc 1
2017-08-08T17:01:32.495021: step 7498, loss 0.00111441, acc 1
2017-08-08T17:01:32.737548: step 7499, loss 0.000119048, acc 1
2017-08-08T17:01:33.133209: step 7500, loss 0.000835104, acc 1

Evaluation:
2017-08-08T17:01:33.765707: step 7500, loss 2.65916, acc 0.711069

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-7500

2017-08-08T17:01:34.376398: step 7501, loss 0.000558048, acc 1
2017-08-08T17:01:34.789393: step 7502, loss 0.014985, acc 0.984375
2017-08-08T17:01:35.028508: step 7503, loss 0.176005, acc 0.984375
2017-08-08T17:01:35.243081: step 7504, loss 0.0198854, acc 0.984375
2017-08-08T17:01:35.617941: step 7505, loss 0.0029154, acc 1
2017-08-08T17:01:35.871988: step 7506, loss 0.000493774, acc 1
2017-08-08T17:01:36.101364: step 7507, loss 0.000184782, acc 1
2017-08-08T17:01:36.329421: step 7508, loss 0.000892053, acc 1
2017-08-08T17:01:36.645413: step 7509, loss 0.00149243, acc 1
2017-08-08T17:01:36.985303: step 7510, loss 0.000168161, acc 1
2017-08-08T17:01:37.305267: step 7511, loss 0.042162, acc 0.984375
2017-08-08T17:01:37.564136: step 7512, loss 0.0046581, acc 1
2017-08-08T17:01:37.768388: step 7513, loss 0.00060571, acc 1
2017-08-08T17:01:38.202857: step 7514, loss 2.86615e-05, acc 1
2017-08-08T17:01:38.455560: step 7515, loss 0.000732185, acc 1
2017-08-08T17:01:38.712830: step 7516, loss 0.000515936, acc 1
2017-08-08T17:01:39.209764: step 7517, loss 0.011029, acc 1
2017-08-08T17:01:39.577351: step 7518, loss 0.000991205, acc 1
2017-08-08T17:01:39.894363: step 7519, loss 0.00184536, acc 1
2017-08-08T17:01:40.116806: step 7520, loss 0.00396759, acc 1
2017-08-08T17:01:40.473498: step 7521, loss 0.000234361, acc 1
2017-08-08T17:01:40.656599: step 7522, loss 0.00940047, acc 1
2017-08-08T17:01:40.918561: step 7523, loss 0.000748147, acc 1
2017-08-08T17:01:41.129939: step 7524, loss 0.00266981, acc 1
2017-08-08T17:01:41.481262: step 7525, loss 0.000299229, acc 1
2017-08-08T17:01:41.849320: step 7526, loss 0.0140455, acc 1
2017-08-08T17:01:42.087355: step 7527, loss 0.00197111, acc 1
2017-08-08T17:01:42.270303: step 7528, loss 0.00110297, acc 1
2017-08-08T17:01:42.537306: step 7529, loss 0.000802792, acc 1
2017-08-08T17:01:42.798818: step 7530, loss 0.00171561, acc 1
2017-08-08T17:01:43.062464: step 7531, loss 0.00124526, acc 1
2017-08-08T17:01:43.412424: step 7532, loss 0.00172513, acc 1
2017-08-08T17:01:43.606414: step 7533, loss 0.00580686, acc 1
2017-08-08T17:01:44.037614: step 7534, loss 2.85041e-05, acc 1
2017-08-08T17:01:44.339240: step 7535, loss 0.000293369, acc 1
2017-08-08T17:01:44.719988: step 7536, loss 0.0101731, acc 1
2017-08-08T17:01:45.098348: step 7537, loss 0.000636677, acc 1
2017-08-08T17:01:45.346837: step 7538, loss 0.000202254, acc 1
2017-08-08T17:01:45.623614: step 7539, loss 0.0208949, acc 0.984375
2017-08-08T17:01:45.796500: step 7540, loss 0.0114224, acc 1
2017-08-08T17:01:46.096041: step 7541, loss 0.000300719, acc 1
2017-08-08T17:01:46.353923: step 7542, loss 0.000143738, acc 1
2017-08-08T17:01:46.656176: step 7543, loss 0.000724727, acc 1
2017-08-08T17:01:46.950023: step 7544, loss 0.000119423, acc 1
2017-08-08T17:01:47.288351: step 7545, loss 0.0170285, acc 0.984375
2017-08-08T17:01:47.524796: step 7546, loss 0.00681475, acc 1
2017-08-08T17:01:47.821258: step 7547, loss 0.00179857, acc 1
2017-08-08T17:01:48.089149: step 7548, loss 0.00179511, acc 1
2017-08-08T17:01:48.300594: step 7549, loss 0.000115968, acc 1
2017-08-08T17:01:48.619534: step 7550, loss 0.000147111, acc 1
2017-08-08T17:01:48.829204: step 7551, loss 0.000206425, acc 1
2017-08-08T17:01:49.177323: step 7552, loss 0.00185599, acc 1
2017-08-08T17:01:49.410955: step 7553, loss 0.000177588, acc 1
2017-08-08T17:01:49.663579: step 7554, loss 0.000848835, acc 1
2017-08-08T17:01:49.947742: step 7555, loss 4.29132e-05, acc 1
2017-08-08T17:01:50.202467: step 7556, loss 8.89647e-05, acc 1
2017-08-08T17:01:50.550465: step 7557, loss 0.00149437, acc 1
2017-08-08T17:01:50.804020: step 7558, loss 0.00134805, acc 1
2017-08-08T17:01:51.200811: step 7559, loss 0.00401156, acc 1
2017-08-08T17:01:51.472822: step 7560, loss 0.00125349, acc 1
2017-08-08T17:01:51.886397: step 7561, loss 0.000265291, acc 1
2017-08-08T17:01:52.220648: step 7562, loss 0.00809531, acc 1
2017-08-08T17:01:52.450615: step 7563, loss 6.40589e-05, acc 1
2017-08-08T17:01:52.873957: step 7564, loss 0.00276662, acc 1
2017-08-08T17:01:53.102742: step 7565, loss 0.00327621, acc 1
2017-08-08T17:01:53.381936: step 7566, loss 0.00142157, acc 1
2017-08-08T17:01:53.666645: step 7567, loss 0.00135807, acc 1
2017-08-08T17:01:54.056688: step 7568, loss 0.000738781, acc 1
2017-08-08T17:01:54.468755: step 7569, loss 0.00752251, acc 1
2017-08-08T17:01:54.840261: step 7570, loss 0.077781, acc 0.984375
2017-08-08T17:01:55.126552: step 7571, loss 0.000519647, acc 1
2017-08-08T17:01:55.428714: step 7572, loss 0.0001935, acc 1
2017-08-08T17:01:55.838111: step 7573, loss 0.000342479, acc 1
2017-08-08T17:01:56.104175: step 7574, loss 0.000548392, acc 1
2017-08-08T17:01:56.355529: step 7575, loss 0.000796717, acc 1
2017-08-08T17:01:56.617474: step 7576, loss 0.00015301, acc 1
2017-08-08T17:01:56.949592: step 7577, loss 6.7419e-05, acc 1
2017-08-08T17:01:57.185950: step 7578, loss 0.0040545, acc 1
2017-08-08T17:01:57.564478: step 7579, loss 0.00866177, acc 1
2017-08-08T17:01:57.851231: step 7580, loss 0.000126679, acc 1
2017-08-08T17:01:58.063553: step 7581, loss 0.00150172, acc 1
2017-08-08T17:01:58.331662: step 7582, loss 0.000361519, acc 1
2017-08-08T17:01:58.527500: step 7583, loss 0.000305353, acc 1
2017-08-08T17:01:58.746107: step 7584, loss 0.00124481, acc 1
2017-08-08T17:01:58.970624: step 7585, loss 0.00103322, acc 1
2017-08-08T17:01:59.312448: step 7586, loss 0.0015139, acc 1
2017-08-08T17:01:59.611833: step 7587, loss 0.00153663, acc 1
2017-08-08T17:01:59.897267: step 7588, loss 0.000539297, acc 1
2017-08-08T17:02:00.088967: step 7589, loss 0.000684528, acc 1
2017-08-08T17:02:00.295266: step 7590, loss 0.000132679, acc 1
2017-08-08T17:02:00.667310: step 7591, loss 0.00155877, acc 1
2017-08-08T17:02:00.857472: step 7592, loss 0.000217486, acc 1
2017-08-08T17:02:01.078280: step 7593, loss 0.00152691, acc 1
2017-08-08T17:02:01.353355: step 7594, loss 0.0244755, acc 0.984375
2017-08-08T17:02:01.631467: step 7595, loss 0.000737237, acc 1
2017-08-08T17:02:01.999190: step 7596, loss 0.000525716, acc 1
2017-08-08T17:02:02.369489: step 7597, loss 0.000949405, acc 1
2017-08-08T17:02:02.606398: step 7598, loss 0.00205956, acc 1
2017-08-08T17:02:02.844771: step 7599, loss 0.000267435, acc 1
2017-08-08T17:02:03.212198: step 7600, loss 0.00012053, acc 1

Evaluation:
2017-08-08T17:02:03.930017: step 7600, loss 2.52769, acc 0.716698

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-7600

2017-08-08T17:02:04.509415: step 7601, loss 0.000774481, acc 1
2017-08-08T17:02:04.873278: step 7602, loss 0.000853426, acc 1
2017-08-08T17:02:05.312164: step 7603, loss 0.000697583, acc 1
2017-08-08T17:02:05.643805: step 7604, loss 0.00110854, acc 1
2017-08-08T17:02:05.857165: step 7605, loss 0.0118968, acc 0.984375
2017-08-08T17:02:06.216296: step 7606, loss 7.01156e-05, acc 1
2017-08-08T17:02:06.588205: step 7607, loss 0.000841061, acc 1
2017-08-08T17:02:06.884427: step 7608, loss 0.00351087, acc 1
2017-08-08T17:02:07.082062: step 7609, loss 0.000834925, acc 1
2017-08-08T17:02:07.373828: step 7610, loss 0.000201709, acc 1
2017-08-08T17:02:07.588978: step 7611, loss 0.000135959, acc 1
2017-08-08T17:02:07.821259: step 7612, loss 0.000781849, acc 1
2017-08-08T17:02:08.034098: step 7613, loss 0.00326793, acc 1
2017-08-08T17:02:08.197001: step 7614, loss 0.000734734, acc 1
2017-08-08T17:02:08.395228: step 7615, loss 0.000273881, acc 1
2017-08-08T17:02:08.681875: step 7616, loss 0.00295182, acc 1
2017-08-08T17:02:08.892369: step 7617, loss 0.000335187, acc 1
2017-08-08T17:02:09.121573: step 7618, loss 0.0170352, acc 1
2017-08-08T17:02:09.407860: step 7619, loss 0.00104611, acc 1
2017-08-08T17:02:09.783628: step 7620, loss 0.000201239, acc 1
2017-08-08T17:02:10.147516: step 7621, loss 0.00017045, acc 1
2017-08-08T17:02:10.507132: step 7622, loss 0.00524118, acc 1
2017-08-08T17:02:10.825430: step 7623, loss 0.00379576, acc 1
2017-08-08T17:02:11.121368: step 7624, loss 5.61281e-05, acc 1
2017-08-08T17:02:11.402989: step 7625, loss 0.00529263, acc 1
2017-08-08T17:02:11.586659: step 7626, loss 0.0193037, acc 0.984375
2017-08-08T17:02:11.792159: step 7627, loss 0.0024551, acc 1
2017-08-08T17:02:12.045310: step 7628, loss 0.00116005, acc 1
2017-08-08T17:02:12.357361: step 7629, loss 6.40372e-05, acc 1
2017-08-08T17:02:12.686202: step 7630, loss 8.63732e-05, acc 1
2017-08-08T17:02:12.890632: step 7631, loss 0.00225108, acc 1
2017-08-08T17:02:13.081750: step 7632, loss 0.00175817, acc 1
2017-08-08T17:02:13.408135: step 7633, loss 0.00886444, acc 1
2017-08-08T17:02:13.618414: step 7634, loss 0.0149574, acc 0.984375
2017-08-08T17:02:13.804738: step 7635, loss 0.00370857, acc 1
2017-08-08T17:02:14.051413: step 7636, loss 0.000301596, acc 1
2017-08-08T17:02:14.311745: step 7637, loss 0.00999795, acc 1
2017-08-08T17:02:14.603925: step 7638, loss 0.000190653, acc 1
2017-08-08T17:02:14.921759: step 7639, loss 0.000616263, acc 1
2017-08-08T17:02:15.186823: step 7640, loss 0.00211034, acc 1
2017-08-08T17:02:15.409813: step 7641, loss 0.000209638, acc 1
2017-08-08T17:02:15.725361: step 7642, loss 3.13858e-05, acc 1
2017-08-08T17:02:15.969892: step 7643, loss 9.92807e-05, acc 1
2017-08-08T17:02:16.226209: step 7644, loss 0.00235175, acc 1
2017-08-08T17:02:16.493797: step 7645, loss 0.000437949, acc 1
2017-08-08T17:02:16.869329: step 7646, loss 5.2074e-05, acc 1
2017-08-08T17:02:17.157683: step 7647, loss 0.0103667, acc 1
2017-08-08T17:02:17.474082: step 7648, loss 0.00190223, acc 1
2017-08-08T17:02:17.689195: step 7649, loss 0.000559011, acc 1
2017-08-08T17:02:18.029670: step 7650, loss 0.00103899, acc 1
2017-08-08T17:02:18.252589: step 7651, loss 0.00025467, acc 1
2017-08-08T17:02:18.479845: step 7652, loss 1.27019e-05, acc 1
2017-08-08T17:02:18.793603: step 7653, loss 0.00527422, acc 1
2017-08-08T17:02:19.074217: step 7654, loss 0.00104476, acc 1
2017-08-08T17:02:19.374308: step 7655, loss 0.00211427, acc 1
2017-08-08T17:02:19.620137: step 7656, loss 0.000154671, acc 1
2017-08-08T17:02:19.784309: step 7657, loss 0.000336265, acc 1
2017-08-08T17:02:20.122636: step 7658, loss 0.00106878, acc 1
2017-08-08T17:02:20.314693: step 7659, loss 0.00151614, acc 1
2017-08-08T17:02:20.489187: step 7660, loss 0.000140047, acc 1
2017-08-08T17:02:20.717320: step 7661, loss 0.000157104, acc 1
2017-08-08T17:02:21.029031: step 7662, loss 8.00912e-05, acc 1
2017-08-08T17:02:21.373813: step 7663, loss 0.000354552, acc 1
2017-08-08T17:02:21.535071: step 7664, loss 0.000522646, acc 1
2017-08-08T17:02:21.752334: step 7665, loss 0.00124176, acc 1
2017-08-08T17:02:22.095441: step 7666, loss 0.000346187, acc 1
2017-08-08T17:02:22.286754: step 7667, loss 0.000826308, acc 1
2017-08-08T17:02:22.481855: step 7668, loss 0.000302856, acc 1
2017-08-08T17:02:22.677789: step 7669, loss 0.0146873, acc 0.984375
2017-08-08T17:02:23.029340: step 7670, loss 0.000261149, acc 1
2017-08-08T17:02:23.289312: step 7671, loss 0.000659951, acc 1
2017-08-08T17:02:23.517336: step 7672, loss 0.000527997, acc 1
2017-08-08T17:02:23.746598: step 7673, loss 0.000491335, acc 1
2017-08-08T17:02:23.943159: step 7674, loss 9.43937e-05, acc 1
2017-08-08T17:02:24.237975: step 7675, loss 0.000204602, acc 1
2017-08-08T17:02:24.447560: step 7676, loss 0.00457996, acc 1
2017-08-08T17:02:24.644220: step 7677, loss 8.21648e-05, acc 1
2017-08-08T17:02:24.865448: step 7678, loss 0.0112495, acc 1
2017-08-08T17:02:25.181105: step 7679, loss 4.31294e-05, acc 1
2017-08-08T17:02:25.471911: step 7680, loss 0.00572181, acc 1
2017-08-08T17:02:25.733715: step 7681, loss 0.000235047, acc 1
2017-08-08T17:02:25.975267: step 7682, loss 0.000113888, acc 1
2017-08-08T17:02:26.304500: step 7683, loss 0.00605233, acc 1
2017-08-08T17:02:26.532042: step 7684, loss 0.000146589, acc 1
2017-08-08T17:02:26.731175: step 7685, loss 0.000286027, acc 1
2017-08-08T17:02:26.942203: step 7686, loss 8.82621e-05, acc 1
2017-08-08T17:02:27.213311: step 7687, loss 9.36498e-05, acc 1
2017-08-08T17:02:27.520188: step 7688, loss 0.000266179, acc 1
2017-08-08T17:02:27.805772: step 7689, loss 0.0152098, acc 0.984375
2017-08-08T17:02:28.035437: step 7690, loss 0.0011026, acc 1
2017-08-08T17:02:28.250900: step 7691, loss 5.5065e-05, acc 1
2017-08-08T17:02:28.626125: step 7692, loss 0.000413283, acc 1
2017-08-08T17:02:28.853183: step 7693, loss 0.00116049, acc 1
2017-08-08T17:02:29.076507: step 7694, loss 0.00176057, acc 1
2017-08-08T17:02:29.412438: step 7695, loss 0.000268278, acc 1
2017-08-08T17:02:29.800036: step 7696, loss 0.00237586, acc 1
2017-08-08T17:02:30.170740: step 7697, loss 0.00078769, acc 1
2017-08-08T17:02:30.406870: step 7698, loss 0.000505813, acc 1
2017-08-08T17:02:30.580592: step 7699, loss 0.000398694, acc 1
2017-08-08T17:02:30.865406: step 7700, loss 4.82724e-05, acc 1

Evaluation:
2017-08-08T17:02:31.429649: step 7700, loss 2.53488, acc 0.728893

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-7700

2017-08-08T17:02:31.897372: step 7701, loss 0.000411345, acc 1
2017-08-08T17:02:32.213378: step 7702, loss 0.000170035, acc 1
2017-08-08T17:02:32.456099: step 7703, loss 0.00160974, acc 1
2017-08-08T17:02:32.793488: step 7704, loss 8.78035e-05, acc 1
2017-08-08T17:02:33.064959: step 7705, loss 0.00353771, acc 1
2017-08-08T17:02:33.486375: step 7706, loss 0.000435191, acc 1
2017-08-08T17:02:33.798623: step 7707, loss 0.000335492, acc 1
2017-08-08T17:02:34.111265: step 7708, loss 0.00118775, acc 1
2017-08-08T17:02:34.466826: step 7709, loss 0.000972919, acc 1
2017-08-08T17:02:34.861495: step 7710, loss 0.000196715, acc 1
2017-08-08T17:02:35.155774: step 7711, loss 0.000660612, acc 1
2017-08-08T17:02:35.497284: step 7712, loss 0.00394692, acc 1
2017-08-08T17:02:35.731158: step 7713, loss 0.00182611, acc 1
2017-08-08T17:02:35.970058: step 7714, loss 0.000482822, acc 1
2017-08-08T17:02:36.235923: step 7715, loss 0.00118912, acc 1
2017-08-08T17:02:36.576095: step 7716, loss 0.0163441, acc 1
2017-08-08T17:02:36.805507: step 7717, loss 0.00101136, acc 1
2017-08-08T17:02:37.081035: step 7718, loss 8.22208e-05, acc 1
2017-08-08T17:02:37.325356: step 7719, loss 0.00170799, acc 1
2017-08-08T17:02:37.639742: step 7720, loss 5.38645e-05, acc 1
2017-08-08T17:02:37.920696: step 7721, loss 0.00388604, acc 1
2017-08-08T17:02:38.176224: step 7722, loss 0.00285623, acc 1
2017-08-08T17:02:38.370105: step 7723, loss 0.000177671, acc 1
2017-08-08T17:02:38.763164: step 7724, loss 0.00127902, acc 1
2017-08-08T17:02:38.936282: step 7725, loss 0.00072824, acc 1
2017-08-08T17:02:39.183696: step 7726, loss 0.0551297, acc 0.96875
2017-08-08T17:02:39.465481: step 7727, loss 5.71872e-05, acc 1
2017-08-08T17:02:39.713313: step 7728, loss 0.000812281, acc 1
2017-08-08T17:02:39.945860: step 7729, loss 0.000772268, acc 1
2017-08-08T17:02:40.122175: step 7730, loss 0.00654148, acc 1
2017-08-08T17:02:40.318506: step 7731, loss 0.000208208, acc 1
2017-08-08T17:02:40.587529: step 7732, loss 0.0703294, acc 0.984375
2017-08-08T17:02:40.792273: step 7733, loss 0.00220987, acc 1
2017-08-08T17:02:41.023546: step 7734, loss 2.20206e-05, acc 1
2017-08-08T17:02:41.201490: step 7735, loss 0.00014015, acc 1
2017-08-08T17:02:41.485299: step 7736, loss 0.0080275, acc 1
2017-08-08T17:02:41.741335: step 7737, loss 1.52093e-05, acc 1
2017-08-08T17:02:41.955291: step 7738, loss 0.000145966, acc 1
2017-08-08T17:02:42.113114: step 7739, loss 0.000165233, acc 1
2017-08-08T17:02:42.302444: step 7740, loss 0.00293078, acc 1
2017-08-08T17:02:42.559439: step 7741, loss 0.0186025, acc 0.984375
2017-08-08T17:02:42.755948: step 7742, loss 0.00168325, acc 1
2017-08-08T17:02:42.979821: step 7743, loss 0.000992998, acc 1
2017-08-08T17:02:43.188578: step 7744, loss 0.000219372, acc 1
2017-08-08T17:02:43.577362: step 7745, loss 0.0133626, acc 0.984375
2017-08-08T17:02:44.002834: step 7746, loss 0.000162107, acc 1
2017-08-08T17:02:44.363514: step 7747, loss 0.00945465, acc 1
2017-08-08T17:02:44.605783: step 7748, loss 0.000470188, acc 1
2017-08-08T17:02:44.870239: step 7749, loss 0.000781906, acc 1
2017-08-08T17:02:45.188453: step 7750, loss 0.000359268, acc 1
2017-08-08T17:02:45.476857: step 7751, loss 0.00307466, acc 1
2017-08-08T17:02:45.727142: step 7752, loss 0.000478142, acc 1
2017-08-08T17:02:46.068705: step 7753, loss 0.0183084, acc 0.984375
2017-08-08T17:02:46.414631: step 7754, loss 0.000690883, acc 1
2017-08-08T17:02:46.828256: step 7755, loss 0.000554559, acc 1
2017-08-08T17:02:47.204033: step 7756, loss 0.00100968, acc 1
2017-08-08T17:02:47.474179: step 7757, loss 0.00138486, acc 1
2017-08-08T17:02:47.744817: step 7758, loss 7.20782e-05, acc 1
2017-08-08T17:02:48.153787: step 7759, loss 0.000441587, acc 1
2017-08-08T17:02:48.346048: step 7760, loss 0.000955801, acc 1
2017-08-08T17:02:48.607562: step 7761, loss 0.000218367, acc 1
2017-08-08T17:02:48.887267: step 7762, loss 2.63282e-05, acc 1
2017-08-08T17:02:49.292859: step 7763, loss 0.00437039, acc 1
2017-08-08T17:02:49.672159: step 7764, loss 0.000739875, acc 1
2017-08-08T17:02:50.076471: step 7765, loss 9.05665e-05, acc 1
2017-08-08T17:02:50.390468: step 7766, loss 0.0073253, acc 1
2017-08-08T17:02:50.650829: step 7767, loss 0.000195856, acc 1
2017-08-08T17:02:51.028187: step 7768, loss 1.95773e-05, acc 1
2017-08-08T17:02:51.356348: step 7769, loss 0.000189295, acc 1
2017-08-08T17:02:51.646637: step 7770, loss 2.61195e-05, acc 1
2017-08-08T17:02:51.936248: step 7771, loss 0.000267537, acc 1
2017-08-08T17:02:52.323684: step 7772, loss 0.00270393, acc 1
2017-08-08T17:02:52.651873: step 7773, loss 0.000196493, acc 1
2017-08-08T17:02:53.002475: step 7774, loss 0.00104135, acc 1
2017-08-08T17:02:53.213965: step 7775, loss 0.00146221, acc 1
2017-08-08T17:02:53.458858: step 7776, loss 0.00044469, acc 1
2017-08-08T17:02:53.957392: step 7777, loss 6.24793e-05, acc 1
2017-08-08T17:02:54.251802: step 7778, loss 6.56367e-05, acc 1
2017-08-08T17:02:54.648456: step 7779, loss 0.00302513, acc 1
2017-08-08T17:02:55.057904: step 7780, loss 1.70296e-05, acc 1
2017-08-08T17:02:55.445378: step 7781, loss 0.000299424, acc 1
2017-08-08T17:02:55.819540: step 7782, loss 1.34695e-05, acc 1
2017-08-08T17:02:56.088300: step 7783, loss 0.00064743, acc 1
2017-08-08T17:02:56.449927: step 7784, loss 2.74509e-05, acc 1
2017-08-08T17:02:56.744765: step 7785, loss 0.0021555, acc 1
2017-08-08T17:02:56.982861: step 7786, loss 0.00106442, acc 1
2017-08-08T17:02:57.277277: step 7787, loss 3.48424e-05, acc 1
2017-08-08T17:02:57.705000: step 7788, loss 0.00988794, acc 1
2017-08-08T17:02:58.160014: step 7789, loss 2.19481e-05, acc 1
2017-08-08T17:02:58.523714: step 7790, loss 0.0150062, acc 0.984375
2017-08-08T17:02:58.818995: step 7791, loss 0.000979726, acc 1
2017-08-08T17:02:59.109362: step 7792, loss 0.000121555, acc 1
2017-08-08T17:02:59.531237: step 7793, loss 0.00394092, acc 1
2017-08-08T17:02:59.802272: step 7794, loss 9.74621e-05, acc 1
2017-08-08T17:03:00.091146: step 7795, loss 0.000240614, acc 1
2017-08-08T17:03:00.383100: step 7796, loss 0.0041721, acc 1
2017-08-08T17:03:00.821366: step 7797, loss 5.00461e-05, acc 1
2017-08-08T17:03:01.158269: step 7798, loss 0.000949091, acc 1
2017-08-08T17:03:01.668639: step 7799, loss 5.32014e-05, acc 1
2017-08-08T17:03:01.929749: step 7800, loss 0.000455394, acc 1

Evaluation:
2017-08-08T17:03:02.636978: step 7800, loss 2.55421, acc 0.719512

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-7800

2017-08-08T17:03:03.270129: step 7801, loss 8.60071e-05, acc 1
2017-08-08T17:03:03.617293: step 7802, loss 1.75509e-05, acc 1
2017-08-08T17:03:03.905360: step 7803, loss 0.00084728, acc 1
2017-08-08T17:03:04.375522: step 7804, loss 0.000329504, acc 1
2017-08-08T17:03:04.770607: step 7805, loss 0.000134063, acc 1
2017-08-08T17:03:05.075853: step 7806, loss 8.5157e-05, acc 1
2017-08-08T17:03:05.316433: step 7807, loss 0.00038818, acc 1
2017-08-08T17:03:05.579827: step 7808, loss 3.82692e-05, acc 1
2017-08-08T17:03:05.948287: step 7809, loss 0.000107918, acc 1
2017-08-08T17:03:06.247233: step 7810, loss 0.000224319, acc 1
2017-08-08T17:03:06.522114: step 7811, loss 7.58684e-05, acc 1
2017-08-08T17:03:06.775957: step 7812, loss 6.93591e-05, acc 1
2017-08-08T17:03:07.176458: step 7813, loss 7.60029e-05, acc 1
2017-08-08T17:03:07.473647: step 7814, loss 0.000366526, acc 1
2017-08-08T17:03:07.765521: step 7815, loss 0.00145598, acc 1
2017-08-08T17:03:08.005461: step 7816, loss 2.21181e-05, acc 1
2017-08-08T17:03:08.206443: step 7817, loss 0.00372978, acc 1
2017-08-08T17:03:08.562733: step 7818, loss 0.00064688, acc 1
2017-08-08T17:03:08.772290: step 7819, loss 0.000336286, acc 1
2017-08-08T17:03:09.019827: step 7820, loss 0.0011251, acc 1
2017-08-08T17:03:09.231103: step 7821, loss 0.00294411, acc 1
2017-08-08T17:03:09.531013: step 7822, loss 6.23265e-05, acc 1
2017-08-08T17:03:09.853307: step 7823, loss 7.9562e-05, acc 1
2017-08-08T17:03:10.138700: step 7824, loss 8.1662e-05, acc 1
2017-08-08T17:03:10.334842: step 7825, loss 0.000275596, acc 1
2017-08-08T17:03:10.654517: step 7826, loss 0.000384773, acc 1
2017-08-08T17:03:10.995482: step 7827, loss 0.0158355, acc 0.984375
2017-08-08T17:03:11.298209: step 7828, loss 0.000160719, acc 1
2017-08-08T17:03:11.612240: step 7829, loss 3.90248e-05, acc 1
2017-08-08T17:03:11.987254: step 7830, loss 0.00018597, acc 1
2017-08-08T17:03:12.400704: step 7831, loss 0.000423289, acc 1
2017-08-08T17:03:12.755078: step 7832, loss 0.000193282, acc 1
2017-08-08T17:03:13.008071: step 7833, loss 1.3374e-05, acc 1
2017-08-08T17:03:13.369391: step 7834, loss 0.000549496, acc 1
2017-08-08T17:03:13.737859: step 7835, loss 5.67712e-05, acc 1
2017-08-08T17:03:14.050255: step 7836, loss 0.000914258, acc 1
2017-08-08T17:03:14.325796: step 7837, loss 6.90538e-05, acc 1
2017-08-08T17:03:14.592496: step 7838, loss 0.000217858, acc 1
2017-08-08T17:03:15.004374: step 7839, loss 2.97944e-05, acc 1
2017-08-08T17:03:15.308984: step 7840, loss 0.000249743, acc 1
2017-08-08T17:03:15.586635: step 7841, loss 6.33298e-05, acc 1
2017-08-08T17:03:15.817620: step 7842, loss 0.000114885, acc 1
2017-08-08T17:03:16.185356: step 7843, loss 0.000173235, acc 1
2017-08-08T17:03:16.494961: step 7844, loss 9.94155e-06, acc 1
2017-08-08T17:03:16.732902: step 7845, loss 0.000881136, acc 1
2017-08-08T17:03:16.983654: step 7846, loss 0.000134662, acc 1
2017-08-08T17:03:17.239037: step 7847, loss 6.3363e-05, acc 1
2017-08-08T17:03:17.517443: step 7848, loss 0.000491064, acc 1
2017-08-08T17:03:17.889739: step 7849, loss 0.000209557, acc 1
2017-08-08T17:03:18.220127: step 7850, loss 0.000209153, acc 1
2017-08-08T17:03:18.451600: step 7851, loss 0.000203697, acc 1
2017-08-08T17:03:18.789959: step 7852, loss 9.50332e-05, acc 1
2017-08-08T17:03:19.067690: step 7853, loss 2.78541e-05, acc 1
2017-08-08T17:03:19.323371: step 7854, loss 0.000564333, acc 1
2017-08-08T17:03:19.573277: step 7855, loss 0.000266034, acc 1
2017-08-08T17:03:19.909374: step 7856, loss 0.00126578, acc 1
2017-08-08T17:03:20.357387: step 7857, loss 0.000394404, acc 1
2017-08-08T17:03:20.705518: step 7858, loss 0.00569831, acc 1
2017-08-08T17:03:20.965587: step 7859, loss 0.000395524, acc 1
2017-08-08T17:03:21.189522: step 7860, loss 0.00407972, acc 1
2017-08-08T17:03:21.506964: step 7861, loss 2.10222e-05, acc 1
2017-08-08T17:03:21.763886: step 7862, loss 0.0362521, acc 0.984375
2017-08-08T17:03:21.937553: step 7863, loss 4.64449e-05, acc 1
2017-08-08T17:03:22.129754: step 7864, loss 0.000416112, acc 1
2017-08-08T17:03:22.388536: step 7865, loss 0.000400714, acc 1
2017-08-08T17:03:22.639622: step 7866, loss 0.0321364, acc 0.984375
2017-08-08T17:03:22.855545: step 7867, loss 0.0167897, acc 0.984375
2017-08-08T17:03:23.008787: step 7868, loss 5.78322e-05, acc 1
2017-08-08T17:03:23.209366: step 7869, loss 0.000340868, acc 1
2017-08-08T17:03:23.498303: step 7870, loss 6.57643e-06, acc 1
2017-08-08T17:03:23.679389: step 7871, loss 0.00324464, acc 1
2017-08-08T17:03:23.842910: step 7872, loss 8.35097e-05, acc 1
2017-08-08T17:03:24.036629: step 7873, loss 0.000202478, acc 1
2017-08-08T17:03:24.407724: step 7874, loss 0.000563302, acc 1
2017-08-08T17:03:24.731511: step 7875, loss 0.00364898, acc 1
2017-08-08T17:03:24.983213: step 7876, loss 0.0010331, acc 1
2017-08-08T17:03:25.219086: step 7877, loss 6.90746e-06, acc 1
2017-08-08T17:03:25.513369: step 7878, loss 3.79633e-05, acc 1
2017-08-08T17:03:25.890670: step 7879, loss 0.00108062, acc 1
2017-08-08T17:03:26.102703: step 7880, loss 0.000218167, acc 1
2017-08-08T17:03:26.356986: step 7881, loss 0.0522512, acc 0.984375
2017-08-08T17:03:26.679379: step 7882, loss 0.000142602, acc 1
2017-08-08T17:03:26.924129: step 7883, loss 0.000424791, acc 1
2017-08-08T17:03:27.281616: step 7884, loss 0.000302284, acc 1
2017-08-08T17:03:27.523585: step 7885, loss 0.0329232, acc 0.96875
2017-08-08T17:03:27.726849: step 7886, loss 0.0383145, acc 0.984375
2017-08-08T17:03:28.006043: step 7887, loss 0.000296181, acc 1
2017-08-08T17:03:28.319035: step 7888, loss 2.60985e-05, acc 1
2017-08-08T17:03:28.609406: step 7889, loss 0.00270297, acc 1
2017-08-08T17:03:28.918530: step 7890, loss 0.0267062, acc 0.984375
2017-08-08T17:03:29.203293: step 7891, loss 0.0137614, acc 0.984375
2017-08-08T17:03:29.565280: step 7892, loss 0.000287191, acc 1
2017-08-08T17:03:29.871904: step 7893, loss 0.00524533, acc 1
2017-08-08T17:03:30.126438: step 7894, loss 0.000635186, acc 1
2017-08-08T17:03:30.503661: step 7895, loss 0.00490269, acc 1
2017-08-08T17:03:30.785276: step 7896, loss 0.00275766, acc 1
2017-08-08T17:03:31.033613: step 7897, loss 0.000199368, acc 1
2017-08-08T17:03:31.293283: step 7898, loss 0.00308777, acc 1
2017-08-08T17:03:31.609400: step 7899, loss 0.000462406, acc 1
2017-08-08T17:03:31.946265: step 7900, loss 0.00988694, acc 1

Evaluation:
2017-08-08T17:03:32.586284: step 7900, loss 2.73096, acc 0.718574

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-7900

2017-08-08T17:03:33.081566: step 7901, loss 0.000645649, acc 1
2017-08-08T17:03:33.290867: step 7902, loss 0.0212017, acc 1
2017-08-08T17:03:33.498990: step 7903, loss 0.122533, acc 0.984375
2017-08-08T17:03:33.691392: step 7904, loss 0.00244574, acc 1
2017-08-08T17:03:34.002768: step 7905, loss 6.90918e-05, acc 1
2017-08-08T17:03:34.376727: step 7906, loss 0.000636494, acc 1
2017-08-08T17:03:34.676849: step 7907, loss 0.000104136, acc 1
2017-08-08T17:03:34.890931: step 7908, loss 1.39179e-05, acc 1
2017-08-08T17:03:35.102104: step 7909, loss 7.74467e-05, acc 1
2017-08-08T17:03:35.550934: step 7910, loss 0.000358811, acc 1
2017-08-08T17:03:35.848399: step 7911, loss 0.000830343, acc 1
2017-08-08T17:03:36.080803: step 7912, loss 3.46812e-06, acc 1
2017-08-08T17:03:36.345362: step 7913, loss 0.00536905, acc 1
2017-08-08T17:03:36.641180: step 7914, loss 0.016438, acc 0.984375
2017-08-08T17:03:36.929370: step 7915, loss 0.00149872, acc 1
2017-08-08T17:03:37.181994: step 7916, loss 0.00109461, acc 1
2017-08-08T17:03:37.440271: step 7917, loss 0.000138299, acc 1
2017-08-08T17:03:37.816784: step 7918, loss 0.000896791, acc 1
2017-08-08T17:03:38.042057: step 7919, loss 0.00580532, acc 1
2017-08-08T17:03:38.271097: step 7920, loss 0.00136347, acc 1
2017-08-08T17:03:38.575914: step 7921, loss 2.12562e-05, acc 1
2017-08-08T17:03:38.941389: step 7922, loss 0.0170149, acc 0.984375
2017-08-08T17:03:39.302366: step 7923, loss 7.56559e-05, acc 1
2017-08-08T17:03:39.631631: step 7924, loss 0.0133401, acc 0.984375
2017-08-08T17:03:39.858071: step 7925, loss 0.00063429, acc 1
2017-08-08T17:03:40.061131: step 7926, loss 0.00118659, acc 1
2017-08-08T17:03:40.382051: step 7927, loss 0.000456488, acc 1
2017-08-08T17:03:40.602263: step 7928, loss 0.000224475, acc 1
2017-08-08T17:03:40.849040: step 7929, loss 0.00400041, acc 1
2017-08-08T17:03:41.102626: step 7930, loss 1.1112e-05, acc 1
2017-08-08T17:03:41.465379: step 7931, loss 0.00477871, acc 1
2017-08-08T17:03:41.880886: step 7932, loss 0.00230811, acc 1
2017-08-08T17:03:42.241715: step 7933, loss 0.000266933, acc 1
2017-08-08T17:03:42.553408: step 7934, loss 0.00687618, acc 1
2017-08-08T17:03:42.799031: step 7935, loss 1.47693e-05, acc 1
2017-08-08T17:03:43.217478: step 7936, loss 0.00422726, acc 1
2017-08-08T17:03:43.569589: step 7937, loss 0.000402007, acc 1
2017-08-08T17:03:43.853980: step 7938, loss 0.000513446, acc 1
2017-08-08T17:03:44.129493: step 7939, loss 0.00208959, acc 1
2017-08-08T17:03:44.617337: step 7940, loss 0.00185667, acc 1
2017-08-08T17:03:44.890744: step 7941, loss 0.00323427, acc 1
2017-08-08T17:03:45.153910: step 7942, loss 0.000244995, acc 1
2017-08-08T17:03:45.375505: step 7943, loss 6.25784e-06, acc 1
2017-08-08T17:03:45.615995: step 7944, loss 5.56284e-05, acc 1
2017-08-08T17:03:45.981716: step 7945, loss 0.00960276, acc 1
2017-08-08T17:03:46.245490: step 7946, loss 0.00327592, acc 1
2017-08-08T17:03:46.463057: step 7947, loss 0.00128114, acc 1
2017-08-08T17:03:46.694817: step 7948, loss 0.0120322, acc 1
2017-08-08T17:03:46.974848: step 7949, loss 2.69894e-05, acc 1
2017-08-08T17:03:47.242666: step 7950, loss 0.0048776, acc 1
2017-08-08T17:03:47.566252: step 7951, loss 0.00521676, acc 1
2017-08-08T17:03:47.816646: step 7952, loss 0.00231661, acc 1
2017-08-08T17:03:48.078652: step 7953, loss 0.000167008, acc 1
2017-08-08T17:03:48.398549: step 7954, loss 0.000410851, acc 1
2017-08-08T17:03:48.638967: step 7955, loss 0.000131248, acc 1
2017-08-08T17:03:48.898546: step 7956, loss 0.000161283, acc 1
2017-08-08T17:03:49.215731: step 7957, loss 0.000197642, acc 1
2017-08-08T17:03:49.684648: step 7958, loss 0.00996646, acc 1
2017-08-08T17:03:50.082822: step 7959, loss 0.00728434, acc 1
2017-08-08T17:03:50.398568: step 7960, loss 0.00647471, acc 1
2017-08-08T17:03:50.660687: step 7961, loss 0.000765313, acc 1
2017-08-08T17:03:51.005163: step 7962, loss 0.000262409, acc 1
2017-08-08T17:03:51.203507: step 7963, loss 0.00014931, acc 1
2017-08-08T17:03:51.405212: step 7964, loss 0.00038399, acc 1
2017-08-08T17:03:51.611185: step 7965, loss 0.00103582, acc 1
2017-08-08T17:03:51.932550: step 7966, loss 0.000347435, acc 1
2017-08-08T17:03:52.219816: step 7967, loss 0.000469245, acc 1
2017-08-08T17:03:52.551172: step 7968, loss 0.000715848, acc 1
2017-08-08T17:03:52.759793: step 7969, loss 4.6734e-05, acc 1
2017-08-08T17:03:53.020370: step 7970, loss 5.41155e-05, acc 1
2017-08-08T17:03:53.401318: step 7971, loss 0.000246787, acc 1
2017-08-08T17:03:53.698177: step 7972, loss 0.000835096, acc 1
2017-08-08T17:03:53.931337: step 7973, loss 0.000219966, acc 1
2017-08-08T17:03:54.232265: step 7974, loss 1.11488e-05, acc 1
2017-08-08T17:03:54.656230: step 7975, loss 0.00497479, acc 1
2017-08-08T17:03:55.033762: step 7976, loss 0.000602851, acc 1
2017-08-08T17:03:55.325160: step 7977, loss 1.41476e-05, acc 1
2017-08-08T17:03:55.566493: step 7978, loss 0.0116523, acc 1
2017-08-08T17:03:55.834000: step 7979, loss 0.000145975, acc 1
2017-08-08T17:03:56.305296: step 7980, loss 0.00119135, acc 1
2017-08-08T17:03:56.586979: step 7981, loss 0.000266762, acc 1
2017-08-08T17:03:56.811902: step 7982, loss 0.000509176, acc 1
2017-08-08T17:03:57.073410: step 7983, loss 5.22486e-05, acc 1
2017-08-08T17:03:57.439219: step 7984, loss 0.000190163, acc 1
2017-08-08T17:03:57.756104: step 7985, loss 0.000399144, acc 1
2017-08-08T17:03:57.947414: step 7986, loss 0.000210529, acc 1
2017-08-08T17:03:58.121344: step 7987, loss 1.41861e-05, acc 1
2017-08-08T17:03:58.433825: step 7988, loss 0.000443288, acc 1
2017-08-08T17:03:58.613572: step 7989, loss 1.47965e-05, acc 1
2017-08-08T17:03:58.851013: step 7990, loss 0.0225519, acc 0.984375
2017-08-08T17:03:59.078913: step 7991, loss 0.0026487, acc 1
2017-08-08T17:03:59.534198: step 7992, loss 1.79382e-05, acc 1
2017-08-08T17:03:59.911196: step 7993, loss 0.000699828, acc 1
2017-08-08T17:04:00.189546: step 7994, loss 6.4639e-05, acc 1
2017-08-08T17:04:00.404124: step 7995, loss 0.000432663, acc 1
2017-08-08T17:04:00.796635: step 7996, loss 0.000340933, acc 1
2017-08-08T17:04:01.026211: step 7997, loss 0.000884291, acc 1
2017-08-08T17:04:01.321186: step 7998, loss 0.000335263, acc 1
2017-08-08T17:04:01.601174: step 7999, loss 1.15359e-05, acc 1
2017-08-08T17:04:02.001971: step 8000, loss 0.0023879, acc 1

Evaluation:
2017-08-08T17:04:02.981398: step 8000, loss 2.61783, acc 0.721388

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-8000

2017-08-08T17:04:03.484316: step 8001, loss 0.000387259, acc 1
2017-08-08T17:04:03.830381: step 8002, loss 7.65142e-06, acc 1
2017-08-08T17:04:04.128677: step 8003, loss 0.00457911, acc 1
2017-08-08T17:04:04.509455: step 8004, loss 0.000840834, acc 1
2017-08-08T17:04:04.915538: step 8005, loss 0.00204377, acc 1
2017-08-08T17:04:05.186242: step 8006, loss 0.00180262, acc 1
2017-08-08T17:04:05.518782: step 8007, loss 0.025333, acc 0.984375
2017-08-08T17:04:05.896500: step 8008, loss 0.00584682, acc 1
2017-08-08T17:04:06.214744: step 8009, loss 1.80746e-05, acc 1
2017-08-08T17:04:06.460046: step 8010, loss 0.00993704, acc 1
2017-08-08T17:04:06.764201: step 8011, loss 0.000616052, acc 1
2017-08-08T17:04:07.164306: step 8012, loss 0.000366759, acc 1
2017-08-08T17:04:07.530826: step 8013, loss 0.000723456, acc 1
2017-08-08T17:04:07.764776: step 8014, loss 7.42481e-05, acc 1
2017-08-08T17:04:07.954072: step 8015, loss 0.00119127, acc 1
2017-08-08T17:04:08.318805: step 8016, loss 0.0143881, acc 0.984375
2017-08-08T17:04:08.656363: step 8017, loss 0.00117772, acc 1
2017-08-08T17:04:08.949362: step 8018, loss 7.80695e-05, acc 1
2017-08-08T17:04:09.240692: step 8019, loss 0.000154965, acc 1
2017-08-08T17:04:09.609347: step 8020, loss 0.00029706, acc 1
2017-08-08T17:04:10.043991: step 8021, loss 0.000394131, acc 1
2017-08-08T17:04:10.460156: step 8022, loss 0.00696132, acc 1
2017-08-08T17:04:10.802282: step 8023, loss 0.010582, acc 1
2017-08-08T17:04:11.050985: step 8024, loss 5.24162e-05, acc 1
2017-08-08T17:04:11.393390: step 8025, loss 0.00374701, acc 1
2017-08-08T17:04:11.763356: step 8026, loss 0.000277115, acc 1
2017-08-08T17:04:12.042128: step 8027, loss 0.000732812, acc 1
2017-08-08T17:04:12.299161: step 8028, loss 6.09974e-05, acc 1
2017-08-08T17:04:12.661608: step 8029, loss 0.00112077, acc 1
2017-08-08T17:04:13.007237: step 8030, loss 0.000163918, acc 1
2017-08-08T17:04:13.378457: step 8031, loss 4.13011e-05, acc 1
2017-08-08T17:04:13.667469: step 8032, loss 5.08103e-06, acc 1
2017-08-08T17:04:13.968292: step 8033, loss 0.0062109, acc 1
2017-08-08T17:04:14.416585: step 8034, loss 0.000199684, acc 1
2017-08-08T17:04:14.695284: step 8035, loss 0.00134042, acc 1
2017-08-08T17:04:14.985474: step 8036, loss 0.000467072, acc 1
2017-08-08T17:04:15.305034: step 8037, loss 0.000291421, acc 1
2017-08-08T17:04:15.639276: step 8038, loss 0.00035223, acc 1
2017-08-08T17:04:15.998512: step 8039, loss 0.00284225, acc 1
2017-08-08T17:04:16.365391: step 8040, loss 0.000736554, acc 1
2017-08-08T17:04:16.660373: step 8041, loss 0.000197674, acc 1
2017-08-08T17:04:16.897567: step 8042, loss 0.0271648, acc 0.96875
2017-08-08T17:04:17.297371: step 8043, loss 0.00674119, acc 1
2017-08-08T17:04:17.577123: step 8044, loss 0.000834041, acc 1
2017-08-08T17:04:17.862223: step 8045, loss 2.81341e-05, acc 1
2017-08-08T17:04:18.135876: step 8046, loss 0.00014568, acc 1
2017-08-08T17:04:18.526216: step 8047, loss 0.000794914, acc 1
2017-08-08T17:04:18.950802: step 8048, loss 0.00134644, acc 1
2017-08-08T17:04:19.308340: step 8049, loss 0.00638603, acc 1
2017-08-08T17:04:19.589708: step 8050, loss 0.000124798, acc 1
2017-08-08T17:04:19.889360: step 8051, loss 0.00022017, acc 1
2017-08-08T17:04:20.172138: step 8052, loss 0.000469997, acc 1
2017-08-08T17:04:20.421884: step 8053, loss 0.00299454, acc 1
2017-08-08T17:04:20.613040: step 8054, loss 0.00510422, acc 1
2017-08-08T17:04:20.780052: step 8055, loss 0.000316681, acc 1
2017-08-08T17:04:21.082627: step 8056, loss 0.000223811, acc 1
2017-08-08T17:04:21.459880: step 8057, loss 0.00215513, acc 1
2017-08-08T17:04:21.694942: step 8058, loss 0.0100005, acc 1
2017-08-08T17:04:21.963821: step 8059, loss 0.000153115, acc 1
2017-08-08T17:04:22.321140: step 8060, loss 0.00150103, acc 1
2017-08-08T17:04:22.684626: step 8061, loss 0.00348637, acc 1
2017-08-08T17:04:23.021735: step 8062, loss 0.000732727, acc 1
2017-08-08T17:04:23.244687: step 8063, loss 0.00136144, acc 1
2017-08-08T17:04:23.565438: step 8064, loss 0.000191704, acc 1
2017-08-08T17:04:23.874589: step 8065, loss 0.000449785, acc 1
2017-08-08T17:04:24.165879: step 8066, loss 5.76703e-05, acc 1
2017-08-08T17:04:24.406459: step 8067, loss 0.00215082, acc 1
2017-08-08T17:04:24.620215: step 8068, loss 1.23538e-05, acc 1
2017-08-08T17:04:25.001576: step 8069, loss 6.16183e-05, acc 1
2017-08-08T17:04:25.352440: step 8070, loss 0.000994995, acc 1
2017-08-08T17:04:25.591219: step 8071, loss 0.000414356, acc 1
2017-08-08T17:04:25.886857: step 8072, loss 1.90234e-05, acc 1
2017-08-08T17:04:26.263417: step 8073, loss 0.000318855, acc 1
2017-08-08T17:04:26.554233: step 8074, loss 0.000122045, acc 1
2017-08-08T17:04:26.773301: step 8075, loss 0.000542554, acc 1
2017-08-08T17:04:26.955780: step 8076, loss 8.95645e-05, acc 1
2017-08-08T17:04:27.131638: step 8077, loss 6.87946e-05, acc 1
2017-08-08T17:04:27.406554: step 8078, loss 3.75104e-05, acc 1
2017-08-08T17:04:27.599655: step 8079, loss 0.000245834, acc 1
2017-08-08T17:04:27.864936: step 8080, loss 0.0357658, acc 0.984375
2017-08-08T17:04:28.097371: step 8081, loss 2.13816e-05, acc 1
2017-08-08T17:04:28.401375: step 8082, loss 0.00419187, acc 1
2017-08-08T17:04:28.739781: step 8083, loss 0.00168986, acc 1
2017-08-08T17:04:29.045830: step 8084, loss 0.000955378, acc 1
2017-08-08T17:04:29.284513: step 8085, loss 0.000235047, acc 1
2017-08-08T17:04:29.533590: step 8086, loss 0.00075803, acc 1
2017-08-08T17:04:29.904921: step 8087, loss 4.69819e-05, acc 1
2017-08-08T17:04:30.135891: step 8088, loss 0.00232297, acc 1
2017-08-08T17:04:30.393342: step 8089, loss 0.000237185, acc 1
2017-08-08T17:04:30.715571: step 8090, loss 0.00104667, acc 1
2017-08-08T17:04:31.146433: step 8091, loss 0.000165345, acc 1
2017-08-08T17:04:31.537350: step 8092, loss 0.000182804, acc 1
2017-08-08T17:04:31.841701: step 8093, loss 0.00354291, acc 1
2017-08-08T17:04:32.182499: step 8094, loss 0.00279195, acc 1
2017-08-08T17:04:32.519886: step 8095, loss 7.66433e-05, acc 1
2017-08-08T17:04:32.726180: step 8096, loss 0.00036424, acc 1
2017-08-08T17:04:32.980690: step 8097, loss 8.13349e-06, acc 1
2017-08-08T17:04:33.265357: step 8098, loss 0.000977785, acc 1
2017-08-08T17:04:33.613382: step 8099, loss 0.000566319, acc 1
2017-08-08T17:04:33.883730: step 8100, loss 5.52138e-05, acc 1

Evaluation:
2017-08-08T17:04:34.331681: step 8100, loss 2.67492, acc 0.719512

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-8100

2017-08-08T17:04:34.736188: step 8101, loss 0.000672583, acc 1
2017-08-08T17:04:35.017094: step 8102, loss 0.000124258, acc 1
2017-08-08T17:04:35.338740: step 8103, loss 0.000286583, acc 1
2017-08-08T17:04:35.713927: step 8104, loss 0.000268331, acc 1
2017-08-08T17:04:36.047820: step 8105, loss 0.000427251, acc 1
2017-08-08T17:04:36.351475: step 8106, loss 0.00019698, acc 1
2017-08-08T17:04:36.632624: step 8107, loss 3.25398e-05, acc 1
2017-08-08T17:04:36.846367: step 8108, loss 0.000189505, acc 1
2017-08-08T17:04:37.181052: step 8109, loss 3.60938e-05, acc 1
2017-08-08T17:04:37.428633: step 8110, loss 2.33066e-05, acc 1
2017-08-08T17:04:37.663019: step 8111, loss 2.10983e-05, acc 1
2017-08-08T17:04:37.923124: step 8112, loss 4.00833e-05, acc 1
2017-08-08T17:04:38.203436: step 8113, loss 0.00529373, acc 1
2017-08-08T17:04:38.524027: step 8114, loss 0.000210667, acc 1
2017-08-08T17:04:38.821391: step 8115, loss 2.36567e-05, acc 1
2017-08-08T17:04:39.008474: step 8116, loss 1.88103e-05, acc 1
2017-08-08T17:04:39.195645: step 8117, loss 0.000477226, acc 1
2017-08-08T17:04:39.542389: step 8118, loss 0.000296522, acc 1
2017-08-08T17:04:39.756514: step 8119, loss 0.00217218, acc 1
2017-08-08T17:04:39.992018: step 8120, loss 0.00205304, acc 1
2017-08-08T17:04:40.201406: step 8121, loss 7.20741e-05, acc 1
2017-08-08T17:04:40.537418: step 8122, loss 0.0225591, acc 0.984375
2017-08-08T17:04:40.873312: step 8123, loss 8.91492e-05, acc 1
2017-08-08T17:04:41.129700: step 8124, loss 0.000153545, acc 1
2017-08-08T17:04:41.352580: step 8125, loss 0.00162124, acc 1
2017-08-08T17:04:41.670830: step 8126, loss 7.73135e-05, acc 1
2017-08-08T17:04:41.896063: step 8127, loss 0.0010293, acc 1
2017-08-08T17:04:42.124801: step 8128, loss 6.64387e-05, acc 1
2017-08-08T17:04:42.353022: step 8129, loss 0.000155902, acc 1
2017-08-08T17:04:42.587865: step 8130, loss 0.000728526, acc 1
2017-08-08T17:04:43.046049: step 8131, loss 0.00224374, acc 1
2017-08-08T17:04:43.473456: step 8132, loss 6.16596e-05, acc 1
2017-08-08T17:04:43.700665: step 8133, loss 0.00043126, acc 1
2017-08-08T17:04:43.887957: step 8134, loss 0.000938508, acc 1
2017-08-08T17:04:44.151609: step 8135, loss 0.000322419, acc 1
2017-08-08T17:04:44.391083: step 8136, loss 0.00014202, acc 1
2017-08-08T17:04:44.599532: step 8137, loss 5.37602e-05, acc 1
2017-08-08T17:04:44.812836: step 8138, loss 0.000282349, acc 1
2017-08-08T17:04:45.235621: step 8139, loss 0.00316235, acc 1
2017-08-08T17:04:45.575059: step 8140, loss 7.16521e-05, acc 1
2017-08-08T17:04:45.878997: step 8141, loss 0.00548163, acc 1
2017-08-08T17:04:46.139691: step 8142, loss 2.36833e-05, acc 1
2017-08-08T17:04:46.413753: step 8143, loss 0.000121425, acc 1
2017-08-08T17:04:46.783956: step 8144, loss 0.0022258, acc 1
2017-08-08T17:04:47.033907: step 8145, loss 0.00392661, acc 1
2017-08-08T17:04:47.266498: step 8146, loss 0.000361903, acc 1
2017-08-08T17:04:47.434055: step 8147, loss 0.000239657, acc 1
2017-08-08T17:04:47.922796: step 8148, loss 0.000462276, acc 1
2017-08-08T17:04:48.233413: step 8149, loss 0.000442719, acc 1
2017-08-08T17:04:48.521480: step 8150, loss 4.74958e-06, acc 1
2017-08-08T17:04:48.796291: step 8151, loss 0.00205213, acc 1
2017-08-08T17:04:49.211748: step 8152, loss 0.0266088, acc 0.984375
2017-08-08T17:04:49.454768: step 8153, loss 0.00246652, acc 1
2017-08-08T17:04:49.664902: step 8154, loss 3.18284e-05, acc 1
2017-08-08T17:04:49.857331: step 8155, loss 0.00400456, acc 1
2017-08-08T17:04:50.201881: step 8156, loss 0.000183183, acc 1
2017-08-08T17:04:50.629360: step 8157, loss 0.000167066, acc 1
2017-08-08T17:04:50.968985: step 8158, loss 0.00239549, acc 1
2017-08-08T17:04:51.166943: step 8159, loss 0.00320789, acc 1
2017-08-08T17:04:51.479335: step 8160, loss 2.84661e-05, acc 1
2017-08-08T17:04:51.733313: step 8161, loss 0.0126425, acc 0.984375
2017-08-08T17:04:52.030447: step 8162, loss 0.000325839, acc 1
2017-08-08T17:04:52.322006: step 8163, loss 0.000826446, acc 1
2017-08-08T17:04:52.739590: step 8164, loss 0.024372, acc 0.984375
2017-08-08T17:04:53.135069: step 8165, loss 0.000160766, acc 1
2017-08-08T17:04:53.562433: step 8166, loss 0.00181445, acc 1
2017-08-08T17:04:53.823203: step 8167, loss 5.12354e-05, acc 1
2017-08-08T17:04:54.051019: step 8168, loss 0.00324853, acc 1
2017-08-08T17:04:54.471689: step 8169, loss 0.000662687, acc 1
2017-08-08T17:04:54.782717: step 8170, loss 6.12871e-05, acc 1
2017-08-08T17:04:55.057782: step 8171, loss 0.00413728, acc 1
2017-08-08T17:04:55.371174: step 8172, loss 0.0089928, acc 1
2017-08-08T17:04:55.720726: step 8173, loss 4.44385e-06, acc 1
2017-08-08T17:04:56.149158: step 8174, loss 0.000319405, acc 1
2017-08-08T17:04:56.475261: step 8175, loss 0.00153902, acc 1
2017-08-08T17:04:56.768334: step 8176, loss 4.30737e-05, acc 1
2017-08-08T17:04:56.986257: step 8177, loss 0.0277404, acc 0.984375
2017-08-08T17:04:57.198436: step 8178, loss 0.00168724, acc 1
2017-08-08T17:04:57.521591: step 8179, loss 0.00398054, acc 1
2017-08-08T17:04:57.817147: step 8180, loss 0.00965815, acc 1
2017-08-08T17:04:58.131043: step 8181, loss 0.0232299, acc 0.984375
2017-08-08T17:04:58.459659: step 8182, loss 0.00120077, acc 1
2017-08-08T17:04:58.804455: step 8183, loss 6.75237e-05, acc 1
2017-08-08T17:04:59.166218: step 8184, loss 0.000531663, acc 1
2017-08-08T17:04:59.405277: step 8185, loss 0.000289729, acc 1
2017-08-08T17:04:59.776616: step 8186, loss 0.00122792, acc 1
2017-08-08T17:05:00.028519: step 8187, loss 0.000213415, acc 1
2017-08-08T17:05:00.255643: step 8188, loss 0.000233383, acc 1
2017-08-08T17:05:00.468736: step 8189, loss 0.0104409, acc 1
2017-08-08T17:05:00.781289: step 8190, loss 0.000241247, acc 1
2017-08-08T17:05:01.158972: step 8191, loss 4.16485e-05, acc 1
2017-08-08T17:05:01.380144: step 8192, loss 5.01942e-05, acc 1
2017-08-08T17:05:01.848735: step 8193, loss 0.000112865, acc 1
2017-08-08T17:05:02.129324: step 8194, loss 4.38102e-05, acc 1
2017-08-08T17:05:02.465203: step 8195, loss 0.00046105, acc 1
2017-08-08T17:05:02.923034: step 8196, loss 0.000158115, acc 1
2017-08-08T17:05:03.181360: step 8197, loss 1.02621e-05, acc 1
2017-08-08T17:05:03.420110: step 8198, loss 0.000238007, acc 1
2017-08-08T17:05:03.683311: step 8199, loss 0.000141763, acc 1
2017-08-08T17:05:04.097615: step 8200, loss 7.85139e-05, acc 1

Evaluation:
2017-08-08T17:05:05.076298: step 8200, loss 2.63382, acc 0.718574

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-8200

2017-08-08T17:05:05.468800: step 8201, loss 2.46053e-06, acc 1
2017-08-08T17:05:05.798197: step 8202, loss 0.00018913, acc 1
2017-08-08T17:05:06.050752: step 8203, loss 0.000105159, acc 1
2017-08-08T17:05:06.233787: step 8204, loss 0.000286586, acc 1
2017-08-08T17:05:06.472095: step 8205, loss 0.00187119, acc 1
2017-08-08T17:05:06.767981: step 8206, loss 0.000149285, acc 1
2017-08-08T17:05:07.157580: step 8207, loss 0.000488479, acc 1
2017-08-08T17:05:07.524493: step 8208, loss 0.00041659, acc 1
2017-08-08T17:05:07.873958: step 8209, loss 0.00142105, acc 1
2017-08-08T17:05:08.224386: step 8210, loss 1.67681e-05, acc 1
2017-08-08T17:05:08.485845: step 8211, loss 0.0021154, acc 1
2017-08-08T17:05:08.973975: step 8212, loss 0.00234003, acc 1
2017-08-08T17:05:09.209883: step 8213, loss 0.000650313, acc 1
2017-08-08T17:05:09.468603: step 8214, loss 0.000154501, acc 1
2017-08-08T17:05:09.761690: step 8215, loss 0.000421128, acc 1
2017-08-08T17:05:10.112518: step 8216, loss 0.0015946, acc 1
2017-08-08T17:05:10.435883: step 8217, loss 7.69467e-05, acc 1
2017-08-08T17:05:10.761651: step 8218, loss 0.000139373, acc 1
2017-08-08T17:05:11.041098: step 8219, loss 0.000173011, acc 1
2017-08-08T17:05:11.345383: step 8220, loss 0.000236567, acc 1
2017-08-08T17:05:11.757386: step 8221, loss 0.000863083, acc 1
2017-08-08T17:05:12.012646: step 8222, loss 0.00077961, acc 1
2017-08-08T17:05:12.323524: step 8223, loss 0.003393, acc 1
2017-08-08T17:05:12.596656: step 8224, loss 0.000997422, acc 1
2017-08-08T17:05:13.072341: step 8225, loss 8.72317e-05, acc 1
2017-08-08T17:05:13.502044: step 8226, loss 0.000173247, acc 1
2017-08-08T17:05:13.869374: step 8227, loss 0.000425325, acc 1
2017-08-08T17:05:14.133103: step 8228, loss 0.000355504, acc 1
2017-08-08T17:05:14.465272: step 8229, loss 8.46725e-05, acc 1
2017-08-08T17:05:14.851784: step 8230, loss 0.000387212, acc 1
2017-08-08T17:05:15.130663: step 8231, loss 0.00171405, acc 1
2017-08-08T17:05:15.380610: step 8232, loss 0.00243719, acc 1
2017-08-08T17:05:15.721062: step 8233, loss 0.000676269, acc 1
2017-08-08T17:05:16.117819: step 8234, loss 0.000700298, acc 1
2017-08-08T17:05:16.449000: step 8235, loss 3.29304e-06, acc 1
2017-08-08T17:05:16.808306: step 8236, loss 0.000357444, acc 1
2017-08-08T17:05:17.047909: step 8237, loss 0.00107675, acc 1
2017-08-08T17:05:17.448222: step 8238, loss 0.0018845, acc 1
2017-08-08T17:05:17.670919: step 8239, loss 1.94265e-05, acc 1
2017-08-08T17:05:17.952783: step 8240, loss 0.00024104, acc 1
2017-08-08T17:05:18.235052: step 8241, loss 0.000104058, acc 1
2017-08-08T17:05:18.577694: step 8242, loss 9.40699e-06, acc 1
2017-08-08T17:05:18.994061: step 8243, loss 1.94018e-05, acc 1
2017-08-08T17:05:19.344416: step 8244, loss 0.000878084, acc 1
2017-08-08T17:05:19.614401: step 8245, loss 3.14443e-05, acc 1
2017-08-08T17:05:19.823720: step 8246, loss 0.000536963, acc 1
2017-08-08T17:05:20.113639: step 8247, loss 8.62233e-05, acc 1
2017-08-08T17:05:20.477865: step 8248, loss 7.98674e-05, acc 1
2017-08-08T17:05:20.728801: step 8249, loss 0.0969421, acc 0.984375
2017-08-08T17:05:21.025635: step 8250, loss 0.000158601, acc 1
2017-08-08T17:05:21.339279: step 8251, loss 0.000312953, acc 1
2017-08-08T17:05:21.682283: step 8252, loss 0.00163453, acc 1
2017-08-08T17:05:22.114012: step 8253, loss 0.000152389, acc 1
2017-08-08T17:05:22.346508: step 8254, loss 0.0499484, acc 0.984375
2017-08-08T17:05:22.585765: step 8255, loss 0.000522386, acc 1
2017-08-08T17:05:22.874780: step 8256, loss 0.000389471, acc 1
2017-08-08T17:05:23.296427: step 8257, loss 0.000439287, acc 1
2017-08-08T17:05:23.564571: step 8258, loss 0.000201544, acc 1
2017-08-08T17:05:23.847177: step 8259, loss 0.000212854, acc 1
2017-08-08T17:05:24.145285: step 8260, loss 0.000287609, acc 1
2017-08-08T17:05:24.523066: step 8261, loss 0.000409988, acc 1
2017-08-08T17:05:24.869745: step 8262, loss 3.02116e-06, acc 1
2017-08-08T17:05:25.117542: step 8263, loss 0.00213486, acc 1
2017-08-08T17:05:25.311678: step 8264, loss 0.000453696, acc 1
2017-08-08T17:05:25.680531: step 8265, loss 7.74453e-06, acc 1
2017-08-08T17:05:25.993179: step 8266, loss 0.000958073, acc 1
2017-08-08T17:05:26.237530: step 8267, loss 4.82349e-05, acc 1
2017-08-08T17:05:26.481327: step 8268, loss 2.86468e-06, acc 1
2017-08-08T17:05:26.847544: step 8269, loss 0.00024157, acc 1
2017-08-08T17:05:27.287285: step 8270, loss 1.27307e-05, acc 1
2017-08-08T17:05:27.601668: step 8271, loss 0.000155779, acc 1
2017-08-08T17:05:27.789988: step 8272, loss 5.17127e-05, acc 1
2017-08-08T17:05:28.010407: step 8273, loss 7.59817e-05, acc 1
2017-08-08T17:05:28.354047: step 8274, loss 0.0145615, acc 0.984375
2017-08-08T17:05:28.554117: step 8275, loss 0.000494818, acc 1
2017-08-08T17:05:28.791151: step 8276, loss 0.00105756, acc 1
2017-08-08T17:05:29.102564: step 8277, loss 5.17185e-05, acc 1
2017-08-08T17:05:29.545361: step 8278, loss 0.000658966, acc 1
2017-08-08T17:05:30.017428: step 8279, loss 2.67303e-05, acc 1
2017-08-08T17:05:30.287221: step 8280, loss 1.25745e-05, acc 1
2017-08-08T17:05:30.515666: step 8281, loss 0.000175988, acc 1
2017-08-08T17:05:30.739360: step 8282, loss 0.00269165, acc 1
2017-08-08T17:05:31.047172: step 8283, loss 0.000314664, acc 1
2017-08-08T17:05:31.292210: step 8284, loss 0.000868319, acc 1
2017-08-08T17:05:31.509013: step 8285, loss 0.000257411, acc 1
2017-08-08T17:05:31.861370: step 8286, loss 0.00203941, acc 1
2017-08-08T17:05:32.209130: step 8287, loss 2.51916e-05, acc 1
2017-08-08T17:05:32.639635: step 8288, loss 0.00081154, acc 1
2017-08-08T17:05:32.987755: step 8289, loss 5.52019e-05, acc 1
2017-08-08T17:05:33.240045: step 8290, loss 8.54772e-05, acc 1
2017-08-08T17:05:33.637251: step 8291, loss 0.000462292, acc 1
2017-08-08T17:05:33.936630: step 8292, loss 0.000354876, acc 1
2017-08-08T17:05:34.234405: step 8293, loss 5.41504e-05, acc 1
2017-08-08T17:05:34.539045: step 8294, loss 0.000194986, acc 1
2017-08-08T17:05:34.906022: step 8295, loss 0.000332235, acc 1
2017-08-08T17:05:35.253703: step 8296, loss 0.00846186, acc 1
2017-08-08T17:05:35.671799: step 8297, loss 0.00101622, acc 1
2017-08-08T17:05:35.931020: step 8298, loss 0.0012951, acc 1
2017-08-08T17:05:36.196250: step 8299, loss 0.000403971, acc 1
2017-08-08T17:05:36.630657: step 8300, loss 0.00583062, acc 1

Evaluation:
2017-08-08T17:05:37.079992: step 8300, loss 2.7001, acc 0.71576

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-8300

2017-08-08T17:05:37.677973: step 8301, loss 0.000136545, acc 1
2017-08-08T17:05:37.959155: step 8302, loss 0.00103113, acc 1
2017-08-08T17:05:38.317916: step 8303, loss 7.12351e-05, acc 1
2017-08-08T17:05:38.593643: step 8304, loss 8.13594e-05, acc 1
2017-08-08T17:05:38.901158: step 8305, loss 1.88035e-05, acc 1
2017-08-08T17:05:39.193812: step 8306, loss 0.000126885, acc 1
2017-08-08T17:05:39.393035: step 8307, loss 1.50845e-05, acc 1
2017-08-08T17:05:39.609284: step 8308, loss 0.00481399, acc 1
2017-08-08T17:05:39.896985: step 8309, loss 9.49851e-05, acc 1
2017-08-08T17:05:40.270867: step 8310, loss 0.000341167, acc 1
2017-08-08T17:05:40.607495: step 8311, loss 0.00112633, acc 1
2017-08-08T17:05:40.931309: step 8312, loss 0.000527587, acc 1
2017-08-08T17:05:41.149752: step 8313, loss 0.000100279, acc 1
2017-08-08T17:05:41.423058: step 8314, loss 1.49195e-06, acc 1
2017-08-08T17:05:41.883427: step 8315, loss 7.84866e-05, acc 1
2017-08-08T17:05:42.176167: step 8316, loss 8.40869e-05, acc 1
2017-08-08T17:05:42.449667: step 8317, loss 0.00105124, acc 1
2017-08-08T17:05:42.734370: step 8318, loss 0.000332933, acc 1
2017-08-08T17:05:43.193909: step 8319, loss 7.83687e-05, acc 1
2017-08-08T17:05:43.587144: step 8320, loss 0.0017799, acc 1
2017-08-08T17:05:43.954482: step 8321, loss 2.72961e-05, acc 1
2017-08-08T17:05:44.230641: step 8322, loss 0.000100548, acc 1
2017-08-08T17:05:44.605351: step 8323, loss 2.08496e-05, acc 1
2017-08-08T17:05:44.888208: step 8324, loss 0.00268493, acc 1
2017-08-08T17:05:45.143630: step 8325, loss 9.91117e-05, acc 1
2017-08-08T17:05:45.386066: step 8326, loss 0.00016658, acc 1
2017-08-08T17:05:45.676731: step 8327, loss 0.000712239, acc 1
2017-08-08T17:05:46.028512: step 8328, loss 0.000252641, acc 1
2017-08-08T17:05:46.402322: step 8329, loss 0.00471768, acc 1
2017-08-08T17:05:46.719232: step 8330, loss 0.00729339, acc 1
2017-08-08T17:05:46.966971: step 8331, loss 0.00413242, acc 1
2017-08-08T17:05:47.370684: step 8332, loss 5.54401e-05, acc 1
2017-08-08T17:05:47.696767: step 8333, loss 0.00040538, acc 1
2017-08-08T17:05:47.980665: step 8334, loss 0.000194977, acc 1
2017-08-08T17:05:48.272766: step 8335, loss 0.00553965, acc 1
2017-08-08T17:05:48.723593: step 8336, loss 6.36589e-06, acc 1
2017-08-08T17:05:49.198458: step 8337, loss 0.000132525, acc 1
2017-08-08T17:05:49.547669: step 8338, loss 6.73629e-05, acc 1
2017-08-08T17:05:49.791440: step 8339, loss 2.79209e-05, acc 1
2017-08-08T17:05:50.178370: step 8340, loss 0.000556048, acc 1
2017-08-08T17:05:50.463058: step 8341, loss 8.59768e-05, acc 1
2017-08-08T17:05:50.656182: step 8342, loss 0.00154448, acc 1
2017-08-08T17:05:50.837813: step 8343, loss 9.50736e-06, acc 1
2017-08-08T17:05:51.118980: step 8344, loss 4.72712e-06, acc 1
2017-08-08T17:05:51.401601: step 8345, loss 7.0342e-05, acc 1
2017-08-08T17:05:51.685381: step 8346, loss 0.000656809, acc 1
2017-08-08T17:05:52.027902: step 8347, loss 1.71094e-05, acc 1
2017-08-08T17:05:52.254690: step 8348, loss 8.11513e-05, acc 1
2017-08-08T17:05:52.552158: step 8349, loss 0.000228431, acc 1
2017-08-08T17:05:52.837349: step 8350, loss 9.00889e-05, acc 1
2017-08-08T17:05:53.096895: step 8351, loss 0.00499522, acc 1
2017-08-08T17:05:53.366495: step 8352, loss 0.000179006, acc 1
2017-08-08T17:05:53.916405: step 8353, loss 0.000337024, acc 1
2017-08-08T17:05:54.340787: step 8354, loss 7.09504e-05, acc 1
2017-08-08T17:05:54.671440: step 8355, loss 3.11185e-05, acc 1
2017-08-08T17:05:54.908478: step 8356, loss 0.000963819, acc 1
2017-08-08T17:05:55.382184: step 8357, loss 0.00136161, acc 1
2017-08-08T17:05:55.710543: step 8358, loss 0.000627832, acc 1
2017-08-08T17:05:55.970505: step 8359, loss 0.000385847, acc 1
2017-08-08T17:05:56.195681: step 8360, loss 0.00706844, acc 1
2017-08-08T17:05:56.537607: step 8361, loss 0.000320361, acc 1
2017-08-08T17:05:56.852707: step 8362, loss 1.28508e-05, acc 1
2017-08-08T17:05:57.192045: step 8363, loss 0.000139277, acc 1
2017-08-08T17:05:57.488070: step 8364, loss 0.000203421, acc 1
2017-08-08T17:05:57.706653: step 8365, loss 0.000671649, acc 1
2017-08-08T17:05:58.108504: step 8366, loss 0.00013403, acc 1
2017-08-08T17:05:58.465296: step 8367, loss 0.000408029, acc 1
2017-08-08T17:05:58.758953: step 8368, loss 1.60143e-05, acc 1
2017-08-08T17:05:59.017384: step 8369, loss 1.17744e-05, acc 1
2017-08-08T17:05:59.433208: step 8370, loss 0.000462545, acc 1
2017-08-08T17:05:59.849948: step 8371, loss 0.000164463, acc 1
2017-08-08T17:06:00.213751: step 8372, loss 0.000555997, acc 1
2017-08-08T17:06:00.478947: step 8373, loss 0.00437672, acc 1
2017-08-08T17:06:00.733391: step 8374, loss 7.59334e-05, acc 1
2017-08-08T17:06:01.171843: step 8375, loss 0.000197722, acc 1
2017-08-08T17:06:01.454997: step 8376, loss 0.000153176, acc 1
2017-08-08T17:06:01.769778: step 8377, loss 0.000739112, acc 1
2017-08-08T17:06:02.134823: step 8378, loss 1.24483e-05, acc 1
2017-08-08T17:06:02.517461: step 8379, loss 9.86143e-05, acc 1
2017-08-08T17:06:02.972245: step 8380, loss 0.000152101, acc 1
2017-08-08T17:06:03.380633: step 8381, loss 0.0196753, acc 0.984375
2017-08-08T17:06:03.708292: step 8382, loss 0.00134489, acc 1
2017-08-08T17:06:03.919790: step 8383, loss 0.00106439, acc 1
2017-08-08T17:06:04.351399: step 8384, loss 0.000462931, acc 1
2017-08-08T17:06:04.744131: step 8385, loss 0.000114899, acc 1
2017-08-08T17:06:05.078830: step 8386, loss 0.0133351, acc 0.984375
2017-08-08T17:06:05.365432: step 8387, loss 0.000100297, acc 1
2017-08-08T17:06:05.665202: step 8388, loss 1.40865e-05, acc 1
2017-08-08T17:06:06.085296: step 8389, loss 0.00154733, acc 1
2017-08-08T17:06:06.513905: step 8390, loss 0.00215802, acc 1
2017-08-08T17:06:06.820316: step 8391, loss 0.00346524, acc 1
2017-08-08T17:06:07.085309: step 8392, loss 8.91245e-05, acc 1
2017-08-08T17:06:07.481928: step 8393, loss 0.000823869, acc 1
2017-08-08T17:06:07.781490: step 8394, loss 0.000106576, acc 1
2017-08-08T17:06:08.083236: step 8395, loss 0.000315869, acc 1
2017-08-08T17:06:08.364272: step 8396, loss 0.000856978, acc 1
2017-08-08T17:06:08.786486: step 8397, loss 6.66739e-05, acc 1
2017-08-08T17:06:09.122395: step 8398, loss 6.07346e-05, acc 1
2017-08-08T17:06:09.521967: step 8399, loss 1.16972e-06, acc 1
2017-08-08T17:06:09.829215: step 8400, loss 0.000799336, acc 1

Evaluation:
2017-08-08T17:06:10.463756: step 8400, loss 2.66656, acc 0.724203

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-8400

2017-08-08T17:06:11.137590: step 8401, loss 3.31745e-05, acc 1
2017-08-08T17:06:11.411391: step 8402, loss 0.000248805, acc 1
2017-08-08T17:06:11.719038: step 8403, loss 0.000220117, acc 1
2017-08-08T17:06:12.116985: step 8404, loss 0.00258983, acc 1
2017-08-08T17:06:12.563399: step 8405, loss 4.31187e-06, acc 1
2017-08-08T17:06:12.864696: step 8406, loss 0.00112776, acc 1
2017-08-08T17:06:13.099640: step 8407, loss 6.09758e-06, acc 1
2017-08-08T17:06:13.405371: step 8408, loss 0.000354194, acc 1
2017-08-08T17:06:13.650697: step 8409, loss 0.000349489, acc 1
2017-08-08T17:06:13.867294: step 8410, loss 0.000709285, acc 1
2017-08-08T17:06:14.252611: step 8411, loss 3.80351e-05, acc 1
2017-08-08T17:06:14.520262: step 8412, loss 0.000104328, acc 1
2017-08-08T17:06:14.865394: step 8413, loss 1.99206e-05, acc 1
2017-08-08T17:06:15.169251: step 8414, loss 0.000455304, acc 1
2017-08-08T17:06:15.463597: step 8415, loss 0.00164031, acc 1
2017-08-08T17:06:15.830901: step 8416, loss 0.00018696, acc 1
2017-08-08T17:06:16.015274: step 8417, loss 0.000104059, acc 1
2017-08-08T17:06:16.315434: step 8418, loss 8.50412e-05, acc 1
2017-08-08T17:06:16.546276: step 8419, loss 0.000160663, acc 1
2017-08-08T17:06:16.873428: step 8420, loss 0.000254663, acc 1
2017-08-08T17:06:17.086043: step 8421, loss 0.000211521, acc 1
2017-08-08T17:06:17.431146: step 8422, loss 0.000410129, acc 1
2017-08-08T17:06:17.708585: step 8423, loss 0.00598182, acc 1
2017-08-08T17:06:17.920512: step 8424, loss 0.00248579, acc 1
2017-08-08T17:06:18.290709: step 8425, loss 8.17456e-05, acc 1
2017-08-08T17:06:18.580681: step 8426, loss 6.35369e-05, acc 1
2017-08-08T17:06:18.801362: step 8427, loss 6.47072e-05, acc 1
2017-08-08T17:06:19.154209: step 8428, loss 7.29556e-06, acc 1
2017-08-08T17:06:19.404054: step 8429, loss 0.000367381, acc 1
2017-08-08T17:06:19.799158: step 8430, loss 0.000621863, acc 1
2017-08-08T17:06:20.072159: step 8431, loss 2.72595e-05, acc 1
2017-08-08T17:06:20.477550: step 8432, loss 4.37395e-05, acc 1
2017-08-08T17:06:20.830152: step 8433, loss 0.00014294, acc 1
2017-08-08T17:06:21.205415: step 8434, loss 0.000307939, acc 1
2017-08-08T17:06:21.600763: step 8435, loss 0.000232542, acc 1
2017-08-08T17:06:21.897149: step 8436, loss 0.000211979, acc 1
2017-08-08T17:06:22.216122: step 8437, loss 0.0064011, acc 1
2017-08-08T17:06:22.412864: step 8438, loss 0.00132978, acc 1
2017-08-08T17:06:22.794895: step 8439, loss 0.000202084, acc 1
2017-08-08T17:06:23.093550: step 8440, loss 0.000348764, acc 1
2017-08-08T17:06:23.528935: step 8441, loss 0.000335106, acc 1
2017-08-08T17:06:23.817206: step 8442, loss 0.000165192, acc 1
2017-08-08T17:06:24.099833: step 8443, loss 8.98873e-06, acc 1
2017-08-08T17:06:24.453403: step 8444, loss 0.000313547, acc 1
2017-08-08T17:06:24.798677: step 8445, loss 0.000534591, acc 1
2017-08-08T17:06:25.012987: step 8446, loss 0.00281391, acc 1
2017-08-08T17:06:25.249513: step 8447, loss 6.18782e-05, acc 1
2017-08-08T17:06:25.505363: step 8448, loss 0.000452716, acc 1
2017-08-08T17:06:25.793301: step 8449, loss 0.000119642, acc 1
2017-08-08T17:06:26.067464: step 8450, loss 1.90268e-05, acc 1
2017-08-08T17:06:26.300982: step 8451, loss 0.00137236, acc 1
2017-08-08T17:06:26.501951: step 8452, loss 4.35759e-05, acc 1
2017-08-08T17:06:26.725371: step 8453, loss 5.79166e-05, acc 1
2017-08-08T17:06:27.018541: step 8454, loss 1.57458e-05, acc 1
2017-08-08T17:06:27.215944: step 8455, loss 0.000292874, acc 1
2017-08-08T17:06:27.446209: step 8456, loss 6.94485e-05, acc 1
2017-08-08T17:06:27.692541: step 8457, loss 0.0188327, acc 0.984375
2017-08-08T17:06:27.992421: step 8458, loss 0.000127225, acc 1
2017-08-08T17:06:28.293399: step 8459, loss 0.000242954, acc 1
2017-08-08T17:06:28.739915: step 8460, loss 0.000145258, acc 1
2017-08-08T17:06:29.084008: step 8461, loss 0.000386723, acc 1
2017-08-08T17:06:29.328826: step 8462, loss 0.000175135, acc 1
2017-08-08T17:06:29.623103: step 8463, loss 0.000343259, acc 1
2017-08-08T17:06:29.861674: step 8464, loss 0.000102361, acc 1
2017-08-08T17:06:30.084363: step 8465, loss 0.0257162, acc 0.984375
2017-08-08T17:06:30.369871: step 8466, loss 7.44505e-05, acc 1
2017-08-08T17:06:30.764109: step 8467, loss 8.2989e-05, acc 1
2017-08-08T17:06:31.170903: step 8468, loss 1.26467e-05, acc 1
2017-08-08T17:06:31.571064: step 8469, loss 0.00115542, acc 1
2017-08-08T17:06:31.865906: step 8470, loss 0.00102787, acc 1
2017-08-08T17:06:32.156989: step 8471, loss 1.49751e-06, acc 1
2017-08-08T17:06:32.491198: step 8472, loss 0.000660656, acc 1
2017-08-08T17:06:32.724550: step 8473, loss 0.00562815, acc 1
2017-08-08T17:06:32.945620: step 8474, loss 0.000872129, acc 1
2017-08-08T17:06:33.315099: step 8475, loss 0.0029221, acc 1
2017-08-08T17:06:33.708291: step 8476, loss 3.40528e-05, acc 1
2017-08-08T17:06:33.961641: step 8477, loss 2.68673e-05, acc 1
2017-08-08T17:06:34.179805: step 8478, loss 0.000719018, acc 1
2017-08-08T17:06:34.475526: step 8479, loss 8.53713e-05, acc 1
2017-08-08T17:06:34.722112: step 8480, loss 0.000175626, acc 1
2017-08-08T17:06:34.940478: step 8481, loss 0.0224514, acc 0.984375
2017-08-08T17:06:35.161704: step 8482, loss 1.0201e-05, acc 1
2017-08-08T17:06:35.454044: step 8483, loss 0.000145801, acc 1
2017-08-08T17:06:35.798080: step 8484, loss 0.00638668, acc 1
2017-08-08T17:06:36.051886: step 8485, loss 0.000569433, acc 1
2017-08-08T17:06:36.256040: step 8486, loss 0.000455859, acc 1
2017-08-08T17:06:36.464633: step 8487, loss 0.0251602, acc 0.984375
2017-08-08T17:06:36.809339: step 8488, loss 1.27089e-05, acc 1
2017-08-08T17:06:37.024295: step 8489, loss 0.000365483, acc 1
2017-08-08T17:06:37.237307: step 8490, loss 0.000730967, acc 1
2017-08-08T17:06:37.407447: step 8491, loss 0.00012688, acc 1
2017-08-08T17:06:37.686777: step 8492, loss 0.00172969, acc 1
2017-08-08T17:06:37.965194: step 8493, loss 0.000191832, acc 1
2017-08-08T17:06:38.313293: step 8494, loss 0.000795223, acc 1
2017-08-08T17:06:38.585380: step 8495, loss 0.00175091, acc 1
2017-08-08T17:06:38.830665: step 8496, loss 0.000741295, acc 1
2017-08-08T17:06:39.265834: step 8497, loss 8.57485e-05, acc 1
2017-08-08T17:06:39.561784: step 8498, loss 9.58405e-05, acc 1
2017-08-08T17:06:39.798749: step 8499, loss 0.000492039, acc 1
2017-08-08T17:06:40.073730: step 8500, loss 9.51498e-05, acc 1

Evaluation:
2017-08-08T17:06:40.804745: step 8500, loss 2.72599, acc 0.714822

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-8500

2017-08-08T17:06:41.512797: step 8501, loss 0.000115904, acc 1
2017-08-08T17:06:41.812686: step 8502, loss 7.76128e-06, acc 1
2017-08-08T17:06:42.055554: step 8503, loss 5.61244e-05, acc 1
2017-08-08T17:06:42.453338: step 8504, loss 2.91276e-05, acc 1
2017-08-08T17:06:42.712507: step 8505, loss 0.000486215, acc 1
2017-08-08T17:06:42.917580: step 8506, loss 0.0189312, acc 0.984375
2017-08-08T17:06:43.127854: step 8507, loss 0.00141155, acc 1
2017-08-08T17:06:43.449554: step 8508, loss 0.000141154, acc 1
2017-08-08T17:06:43.806758: step 8509, loss 0.00265791, acc 1
2017-08-08T17:06:44.055804: step 8510, loss 4.1498e-05, acc 1
2017-08-08T17:06:44.234437: step 8511, loss 0.00015846, acc 1
2017-08-08T17:06:44.481947: step 8512, loss 0.000518182, acc 1
2017-08-08T17:06:44.802546: step 8513, loss 0.000727933, acc 1
2017-08-08T17:06:45.059899: step 8514, loss 6.27142e-05, acc 1
2017-08-08T17:06:45.284466: step 8515, loss 4.64117e-05, acc 1
2017-08-08T17:06:45.694341: step 8516, loss 0.000256176, acc 1
2017-08-08T17:06:46.076772: step 8517, loss 3.95259e-05, acc 1
2017-08-08T17:06:46.481380: step 8518, loss 0.00104112, acc 1
2017-08-08T17:06:46.759619: step 8519, loss 0.000666844, acc 1
2017-08-08T17:06:47.013772: step 8520, loss 2.06063e-05, acc 1
2017-08-08T17:06:47.489208: step 8521, loss 0.00083571, acc 1
2017-08-08T17:06:47.812519: step 8522, loss 9.80043e-06, acc 1
2017-08-08T17:06:48.124713: step 8523, loss 4.44199e-05, acc 1
2017-08-08T17:06:48.479724: step 8524, loss 0.000137161, acc 1
2017-08-08T17:06:48.910450: step 8525, loss 0.000460009, acc 1
2017-08-08T17:06:49.339515: step 8526, loss 0.00011203, acc 1
2017-08-08T17:06:49.622867: step 8527, loss 0.000572776, acc 1
2017-08-08T17:06:49.906360: step 8528, loss 5.33778e-05, acc 1
2017-08-08T17:06:50.286931: step 8529, loss 0.000303626, acc 1
2017-08-08T17:06:50.586455: step 8530, loss 0.0396587, acc 0.984375
2017-08-08T17:06:50.889127: step 8531, loss 0.000204751, acc 1
2017-08-08T17:06:51.141067: step 8532, loss 7.64413e-05, acc 1
2017-08-08T17:06:51.545363: step 8533, loss 5.86136e-05, acc 1
2017-08-08T17:06:51.928687: step 8534, loss 0.000206093, acc 1
2017-08-08T17:06:52.188215: step 8535, loss 2.59686e-05, acc 1
2017-08-08T17:06:52.402036: step 8536, loss 0.000543666, acc 1
2017-08-08T17:06:52.687653: step 8537, loss 0.000397535, acc 1
2017-08-08T17:06:52.893685: step 8538, loss 0.00149994, acc 1
2017-08-08T17:06:53.112002: step 8539, loss 0.000887656, acc 1
2017-08-08T17:06:53.408854: step 8540, loss 0.000383525, acc 1
2017-08-08T17:06:53.748675: step 8541, loss 0.00657883, acc 1
2017-08-08T17:06:54.049057: step 8542, loss 7.89484e-05, acc 1
2017-08-08T17:06:54.295371: step 8543, loss 0.000288546, acc 1
2017-08-08T17:06:54.498026: step 8544, loss 0.0314197, acc 0.984375
2017-08-08T17:06:54.712442: step 8545, loss 3.76795e-06, acc 1
2017-08-08T17:06:54.939481: step 8546, loss 6.16455e-05, acc 1
2017-08-08T17:06:55.185377: step 8547, loss 0.000199134, acc 1
2017-08-08T17:06:55.458367: step 8548, loss 1.60834e-05, acc 1
2017-08-08T17:06:55.786178: step 8549, loss 8.35901e-05, acc 1
2017-08-08T17:06:56.116867: step 8550, loss 0.00058587, acc 1
2017-08-08T17:06:56.363114: step 8551, loss 0.0145027, acc 0.984375
2017-08-08T17:06:56.598001: step 8552, loss 9.66453e-05, acc 1
2017-08-08T17:06:56.901563: step 8553, loss 7.07698e-05, acc 1
2017-08-08T17:06:57.198754: step 8554, loss 0.00159716, acc 1
2017-08-08T17:06:57.482942: step 8555, loss 0.00190052, acc 1
2017-08-08T17:06:57.725365: step 8556, loss 0.000820532, acc 1
2017-08-08T17:06:57.991919: step 8557, loss 0.0101164, acc 1
2017-08-08T17:06:58.331433: step 8558, loss 0.00014684, acc 1
2017-08-08T17:06:58.591051: step 8559, loss 3.83471e-05, acc 1
2017-08-08T17:06:58.959182: step 8560, loss 0.000343482, acc 1
2017-08-08T17:06:59.235228: step 8561, loss 0.000177272, acc 1
2017-08-08T17:06:59.652815: step 8562, loss 5.73243e-05, acc 1
2017-08-08T17:06:59.927783: step 8563, loss 0.000305778, acc 1
2017-08-08T17:07:00.179940: step 8564, loss 0.000290109, acc 1
2017-08-08T17:07:00.413385: step 8565, loss 2.22026e-05, acc 1
2017-08-08T17:07:00.741794: step 8566, loss 0.0020059, acc 1
2017-08-08T17:07:01.093421: step 8567, loss 0.000822008, acc 1
2017-08-08T17:07:01.475880: step 8568, loss 2.57466e-05, acc 1
2017-08-08T17:07:01.740008: step 8569, loss 0.000131177, acc 1
2017-08-08T17:07:02.080665: step 8570, loss 1.10015e-05, acc 1
2017-08-08T17:07:02.599968: step 8571, loss 0.000941208, acc 1
2017-08-08T17:07:02.911758: step 8572, loss 0.000577389, acc 1
2017-08-08T17:07:03.267977: step 8573, loss 0.000107506, acc 1
2017-08-08T17:07:03.651888: step 8574, loss 0.000489929, acc 1
2017-08-08T17:07:04.091055: step 8575, loss 0.0189926, acc 1
2017-08-08T17:07:04.465371: step 8576, loss 0.000715472, acc 1
2017-08-08T17:07:04.753256: step 8577, loss 0.000354541, acc 1
2017-08-08T17:07:05.039600: step 8578, loss 7.61642e-05, acc 1
2017-08-08T17:07:05.497439: step 8579, loss 0.000516342, acc 1
2017-08-08T17:07:05.760053: step 8580, loss 0.000123865, acc 1
2017-08-08T17:07:06.037945: step 8581, loss 0.00126792, acc 1
2017-08-08T17:07:06.385834: step 8582, loss 0.000413367, acc 1
2017-08-08T17:07:06.809877: step 8583, loss 0.000147467, acc 1
2017-08-08T17:07:07.254735: step 8584, loss 7.51337e-05, acc 1
2017-08-08T17:07:07.551221: step 8585, loss 5.34982e-05, acc 1
2017-08-08T17:07:07.900138: step 8586, loss 2.06307e-05, acc 1
2017-08-08T17:07:08.187158: step 8587, loss 0.0281047, acc 0.984375
2017-08-08T17:07:08.365325: step 8588, loss 9.50823e-05, acc 1
2017-08-08T17:07:08.578407: step 8589, loss 0.00100457, acc 1
2017-08-08T17:07:08.912247: step 8590, loss 5.24039e-05, acc 1
2017-08-08T17:07:09.201309: step 8591, loss 0.000101399, acc 1
2017-08-08T17:07:09.541327: step 8592, loss 0.00100503, acc 1
2017-08-08T17:07:09.787510: step 8593, loss 0.00013023, acc 1
2017-08-08T17:07:09.992467: step 8594, loss 0.0227148, acc 0.984375
2017-08-08T17:07:10.528006: step 8595, loss 9.16821e-05, acc 1
2017-08-08T17:07:10.779981: step 8596, loss 1.81207e-05, acc 1
2017-08-08T17:07:11.022860: step 8597, loss 0.0379042, acc 0.984375
2017-08-08T17:07:11.351504: step 8598, loss 0.0100502, acc 1
2017-08-08T17:07:11.761433: step 8599, loss 0.00024088, acc 1
2017-08-08T17:07:12.185366: step 8600, loss 0.000631963, acc 1

Evaluation:
2017-08-08T17:07:12.881355: step 8600, loss 2.77706, acc 0.722326

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-8600

2017-08-08T17:07:13.556402: step 8601, loss 0.0106303, acc 1
2017-08-08T17:07:13.855631: step 8602, loss 0.000212328, acc 1
2017-08-08T17:07:14.133363: step 8603, loss 5.29007e-05, acc 1
2017-08-08T17:07:14.423984: step 8604, loss 6.99263e-05, acc 1
2017-08-08T17:07:14.822242: step 8605, loss 0.000516001, acc 1
2017-08-08T17:07:15.206524: step 8606, loss 7.7114e-05, acc 1
2017-08-08T17:07:15.550172: step 8607, loss 0.00095551, acc 1
2017-08-08T17:07:15.802672: step 8608, loss 0.00780693, acc 1
2017-08-08T17:07:16.155702: step 8609, loss 0.00159855, acc 1
2017-08-08T17:07:16.504203: step 8610, loss 0.0031747, acc 1
2017-08-08T17:07:16.801948: step 8611, loss 0.000388376, acc 1
2017-08-08T17:07:17.006216: step 8612, loss 2.43568e-05, acc 1
2017-08-08T17:07:17.310096: step 8613, loss 2.47146e-05, acc 1
2017-08-08T17:07:17.582424: step 8614, loss 3.08165e-05, acc 1
2017-08-08T17:07:17.912935: step 8615, loss 7.47892e-05, acc 1
2017-08-08T17:07:18.208828: step 8616, loss 0.000104899, acc 1
2017-08-08T17:07:18.498604: step 8617, loss 0.000686155, acc 1
2017-08-08T17:07:18.893811: step 8618, loss 0.000860785, acc 1
2017-08-08T17:07:19.212561: step 8619, loss 0.00110286, acc 1
2017-08-08T17:07:19.449804: step 8620, loss 0.000122932, acc 1
2017-08-08T17:07:19.633170: step 8621, loss 0.000244986, acc 1
2017-08-08T17:07:19.959419: step 8622, loss 0.000764149, acc 1
2017-08-08T17:07:20.377399: step 8623, loss 9.13138e-05, acc 1
2017-08-08T17:07:20.738559: step 8624, loss 0.000130815, acc 1
2017-08-08T17:07:21.005360: step 8625, loss 0.000199831, acc 1
2017-08-08T17:07:21.392383: step 8626, loss 0.000226193, acc 1
2017-08-08T17:07:21.592475: step 8627, loss 4.97757e-05, acc 1
2017-08-08T17:07:21.849886: step 8628, loss 0.000353575, acc 1
2017-08-08T17:07:22.116320: step 8629, loss 0.000938295, acc 1
2017-08-08T17:07:22.466896: step 8630, loss 8.85074e-05, acc 1
2017-08-08T17:07:22.855183: step 8631, loss 0.00011434, acc 1
2017-08-08T17:07:23.118653: step 8632, loss 0.00123184, acc 1
2017-08-08T17:07:23.339573: step 8633, loss 0.0406112, acc 0.984375
2017-08-08T17:07:23.674813: step 8634, loss 0.000466688, acc 1
2017-08-08T17:07:23.913879: step 8635, loss 8.58168e-05, acc 1
2017-08-08T17:07:24.152212: step 8636, loss 0.000209952, acc 1
2017-08-08T17:07:24.484969: step 8637, loss 0.000138689, acc 1
2017-08-08T17:07:24.857419: step 8638, loss 8.34125e-05, acc 1
2017-08-08T17:07:25.323736: step 8639, loss 0.00117345, acc 1
2017-08-08T17:07:25.634171: step 8640, loss 0.000280634, acc 1
2017-08-08T17:07:25.889064: step 8641, loss 0.000639441, acc 1
2017-08-08T17:07:26.201358: step 8642, loss 0.00194245, acc 1
2017-08-08T17:07:26.437349: step 8643, loss 3.30882e-05, acc 1
2017-08-08T17:07:26.703765: step 8644, loss 0.000123876, acc 1
2017-08-08T17:07:26.998447: step 8645, loss 0.000110051, acc 1
2017-08-08T17:07:27.449390: step 8646, loss 5.72798e-05, acc 1
2017-08-08T17:07:27.880620: step 8647, loss 0.000453286, acc 1
2017-08-08T17:07:28.255829: step 8648, loss 0.000102068, acc 1
2017-08-08T17:07:28.494213: step 8649, loss 0.00729907, acc 1
2017-08-08T17:07:28.825214: step 8650, loss 0.000142226, acc 1
2017-08-08T17:07:29.111178: step 8651, loss 0.000214914, acc 1
2017-08-08T17:07:29.322996: step 8652, loss 0.0019762, acc 1
2017-08-08T17:07:29.547155: step 8653, loss 1.32164e-05, acc 1
2017-08-08T17:07:29.770827: step 8654, loss 0.000600598, acc 1
2017-08-08T17:07:30.108446: step 8655, loss 0.00729088, acc 1
2017-08-08T17:07:30.423294: step 8656, loss 3.87228e-05, acc 1
2017-08-08T17:07:30.733022: step 8657, loss 0.000249073, acc 1
2017-08-08T17:07:31.004927: step 8658, loss 0.000333358, acc 1
2017-08-08T17:07:31.365356: step 8659, loss 0.000225937, acc 1
2017-08-08T17:07:31.667968: step 8660, loss 0.000222791, acc 1
2017-08-08T17:07:31.949535: step 8661, loss 8.32163e-05, acc 1
2017-08-08T17:07:32.214282: step 8662, loss 0.000845684, acc 1
2017-08-08T17:07:32.498144: step 8663, loss 7.58787e-05, acc 1
2017-08-08T17:07:32.877929: step 8664, loss 1.30362e-05, acc 1
2017-08-08T17:07:33.225740: step 8665, loss 0.00020419, acc 1
2017-08-08T17:07:33.532307: step 8666, loss 0.000665514, acc 1
2017-08-08T17:07:33.746742: step 8667, loss 0.000825161, acc 1
2017-08-08T17:07:34.085222: step 8668, loss 0.000116479, acc 1
2017-08-08T17:07:34.406174: step 8669, loss 0.00012004, acc 1
2017-08-08T17:07:34.638960: step 8670, loss 0.00175036, acc 1
2017-08-08T17:07:34.878434: step 8671, loss 4.13865e-06, acc 1
2017-08-08T17:07:35.262410: step 8672, loss 0.00108432, acc 1
2017-08-08T17:07:35.706138: step 8673, loss 0.0139414, acc 0.984375
2017-08-08T17:07:36.109848: step 8674, loss 1.17744e-05, acc 1
2017-08-08T17:07:36.409184: step 8675, loss 5.35468e-05, acc 1
2017-08-08T17:07:36.636661: step 8676, loss 0.00334777, acc 1
2017-08-08T17:07:36.860845: step 8677, loss 8.09988e-06, acc 1
2017-08-08T17:07:37.153724: step 8678, loss 5.80364e-05, acc 1
2017-08-08T17:07:37.459947: step 8679, loss 0.000178778, acc 1
2017-08-08T17:07:37.689313: step 8680, loss 0.000237764, acc 1
2017-08-08T17:07:38.087043: step 8681, loss 0.000108025, acc 1
2017-08-08T17:07:38.399168: step 8682, loss 8.04001e-05, acc 1
2017-08-08T17:07:38.602123: step 8683, loss 0.000257981, acc 1
2017-08-08T17:07:38.815121: step 8684, loss 0.00491113, acc 1
2017-08-08T17:07:39.153211: step 8685, loss 0.00022264, acc 1
2017-08-08T17:07:39.467543: step 8686, loss 0.00143449, acc 1
2017-08-08T17:07:39.726430: step 8687, loss 0.000299461, acc 1
2017-08-08T17:07:40.003893: step 8688, loss 0.000448651, acc 1
2017-08-08T17:07:40.261354: step 8689, loss 0.000667973, acc 1
2017-08-08T17:07:40.619280: step 8690, loss 0.000232729, acc 1
2017-08-08T17:07:40.885476: step 8691, loss 0.000101658, acc 1
2017-08-08T17:07:41.154885: step 8692, loss 0.00157998, acc 1
2017-08-08T17:07:41.352229: step 8693, loss 0.000203495, acc 1
2017-08-08T17:07:41.634813: step 8694, loss 1.4071e-05, acc 1
2017-08-08T17:07:41.847745: step 8695, loss 0.000128908, acc 1
2017-08-08T17:07:42.070338: step 8696, loss 0.000119742, acc 1
2017-08-08T17:07:42.340365: step 8697, loss 4.08444e-05, acc 1
2017-08-08T17:07:42.693375: step 8698, loss 0.000107309, acc 1
2017-08-08T17:07:42.932993: step 8699, loss 2.15094e-05, acc 1
2017-08-08T17:07:43.168473: step 8700, loss 6.42778e-05, acc 1

Evaluation:
2017-08-08T17:07:43.550889: step 8700, loss 2.81009, acc 0.721388

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-8700

2017-08-08T17:07:43.975266: step 8701, loss 0.000372859, acc 1
2017-08-08T17:07:44.297398: step 8702, loss 0.000259615, acc 1
2017-08-08T17:07:44.513656: step 8703, loss 0.0353089, acc 0.984375
2017-08-08T17:07:44.769834: step 8704, loss 0.000924295, acc 1
2017-08-08T17:07:45.039628: step 8705, loss 0.000196032, acc 1
2017-08-08T17:07:45.353431: step 8706, loss 0.0013598, acc 1
2017-08-08T17:07:45.734539: step 8707, loss 0.00213262, acc 1
2017-08-08T17:07:46.210329: step 8708, loss 0.00297102, acc 1
2017-08-08T17:07:46.483107: step 8709, loss 4.0007e-05, acc 1
2017-08-08T17:07:46.707540: step 8710, loss 0.000240359, acc 1
2017-08-08T17:07:46.998107: step 8711, loss 0.000428218, acc 1
2017-08-08T17:07:47.235959: step 8712, loss 0.000324386, acc 1
2017-08-08T17:07:47.457369: step 8713, loss 0.000291043, acc 1
2017-08-08T17:07:47.737875: step 8714, loss 0.00135581, acc 1
2017-08-08T17:07:48.087471: step 8715, loss 5.45627e-05, acc 1
2017-08-08T17:07:48.405799: step 8716, loss 2.29113e-05, acc 1
2017-08-08T17:07:48.630006: step 8717, loss 0.0473967, acc 0.984375
2017-08-08T17:07:48.797480: step 8718, loss 0.000104427, acc 1
2017-08-08T17:07:49.129329: step 8719, loss 0.000376287, acc 1
2017-08-08T17:07:49.322125: step 8720, loss 0.000761072, acc 1
2017-08-08T17:07:49.516417: step 8721, loss 0.000319878, acc 1
2017-08-08T17:07:49.801962: step 8722, loss 0.0187567, acc 0.984375
2017-08-08T17:07:50.131182: step 8723, loss 0.00903979, acc 1
2017-08-08T17:07:50.437942: step 8724, loss 0.000756895, acc 1
2017-08-08T17:07:50.622758: step 8725, loss 1.75332e-05, acc 1
2017-08-08T17:07:50.852328: step 8726, loss 0.00150699, acc 1
2017-08-08T17:07:51.150683: step 8727, loss 0.000110496, acc 1
2017-08-08T17:07:51.345017: step 8728, loss 1.13547e-05, acc 1
2017-08-08T17:07:51.605869: step 8729, loss 0.000748998, acc 1
2017-08-08T17:07:52.024540: step 8730, loss 0.00305952, acc 1
2017-08-08T17:07:52.481854: step 8731, loss 0.000404777, acc 1
2017-08-08T17:07:52.831074: step 8732, loss 1.47445e-05, acc 1
2017-08-08T17:07:53.138239: step 8733, loss 0.00164229, acc 1
2017-08-08T17:07:53.445368: step 8734, loss 2.43935e-05, acc 1
2017-08-08T17:07:53.888696: step 8735, loss 0.0723233, acc 0.984375
2017-08-08T17:07:54.179218: step 8736, loss 3.04032e-05, acc 1
2017-08-08T17:07:54.490132: step 8737, loss 5.98302e-05, acc 1
2017-08-08T17:07:54.715242: step 8738, loss 5.0245e-05, acc 1
2017-08-08T17:07:55.183409: step 8739, loss 4.60562e-05, acc 1
2017-08-08T17:07:55.553009: step 8740, loss 0.00045116, acc 1
2017-08-08T17:07:55.908176: step 8741, loss 0.0015753, acc 1
2017-08-08T17:07:56.151746: step 8742, loss 3.45318e-06, acc 1
2017-08-08T17:07:56.523888: step 8743, loss 0.000205451, acc 1
2017-08-08T17:07:56.818887: step 8744, loss 0.00120476, acc 1
2017-08-08T17:07:57.061300: step 8745, loss 0.00148809, acc 1
2017-08-08T17:07:57.325256: step 8746, loss 0.000392858, acc 1
2017-08-08T17:07:57.607403: step 8747, loss 0.0308567, acc 0.984375
2017-08-08T17:07:57.969288: step 8748, loss 0.0152369, acc 0.984375
2017-08-08T17:07:58.345632: step 8749, loss 4.20533e-05, acc 1
2017-08-08T17:07:58.754358: step 8750, loss 0.000443259, acc 1
2017-08-08T17:07:59.080090: step 8751, loss 0.00978856, acc 1
2017-08-08T17:07:59.337400: step 8752, loss 0.0137103, acc 0.984375
2017-08-08T17:07:59.714427: step 8753, loss 0.000355602, acc 1
2017-08-08T17:07:59.928074: step 8754, loss 6.20195e-05, acc 1
2017-08-08T17:08:00.178937: step 8755, loss 4.44788e-05, acc 1
2017-08-08T17:08:00.455166: step 8756, loss 0.000125388, acc 1
2017-08-08T17:08:00.913480: step 8757, loss 0.000524585, acc 1
2017-08-08T17:08:01.333785: step 8758, loss 0.00031081, acc 1
2017-08-08T17:08:01.719269: step 8759, loss 0.0020127, acc 1
2017-08-08T17:08:02.016870: step 8760, loss 0.00401465, acc 1
2017-08-08T17:08:02.491625: step 8761, loss 0.000137302, acc 1
2017-08-08T17:08:02.848659: step 8762, loss 0.00081858, acc 1
2017-08-08T17:08:03.167721: step 8763, loss 0.000386771, acc 1
2017-08-08T17:08:03.421322: step 8764, loss 0.00557204, acc 1
2017-08-08T17:08:03.810422: step 8765, loss 0.000692239, acc 1
2017-08-08T17:08:04.219846: step 8766, loss 0.0529972, acc 0.984375
2017-08-08T17:08:04.648988: step 8767, loss 0.0152324, acc 0.984375
2017-08-08T17:08:04.927652: step 8768, loss 0.0395261, acc 0.984375
2017-08-08T17:08:05.169392: step 8769, loss 0.000213374, acc 1
2017-08-08T17:08:05.568042: step 8770, loss 0.000232442, acc 1
2017-08-08T17:08:05.826457: step 8771, loss 0.000232855, acc 1
2017-08-08T17:08:06.121237: step 8772, loss 2.69947e-05, acc 1
2017-08-08T17:08:06.385719: step 8773, loss 0.0014067, acc 1
2017-08-08T17:08:06.713282: step 8774, loss 0.000127404, acc 1
2017-08-08T17:08:07.061362: step 8775, loss 0.00259663, acc 1
2017-08-08T17:08:07.291034: step 8776, loss 0.0338224, acc 0.984375
2017-08-08T17:08:07.517378: step 8777, loss 2.32586e-05, acc 1
2017-08-08T17:08:07.865026: step 8778, loss 0.00044032, acc 1
2017-08-08T17:08:08.048445: step 8779, loss 6.87406e-05, acc 1
2017-08-08T17:08:08.221734: step 8780, loss 0.000475896, acc 1
2017-08-08T17:08:08.401194: step 8781, loss 0.015619, acc 0.984375
2017-08-08T17:08:08.687018: step 8782, loss 0.0212275, acc 0.984375
2017-08-08T17:08:08.998978: step 8783, loss 0.00657134, acc 1
2017-08-08T17:08:09.290891: step 8784, loss 0.007451, acc 1
2017-08-08T17:08:09.497449: step 8785, loss 0.000368972, acc 1
2017-08-08T17:08:09.817346: step 8786, loss 1.58328e-05, acc 1
2017-08-08T17:08:10.053491: step 8787, loss 0.000894526, acc 1
2017-08-08T17:08:10.306003: step 8788, loss 0.00156157, acc 1
2017-08-08T17:08:10.551168: step 8789, loss 8.94661e-05, acc 1
2017-08-08T17:08:10.809675: step 8790, loss 0.000432869, acc 1
2017-08-08T17:08:11.161626: step 8791, loss 5.90443e-05, acc 1
2017-08-08T17:08:11.517832: step 8792, loss 0.00172514, acc 1
2017-08-08T17:08:11.833267: step 8793, loss 0.000854177, acc 1
2017-08-08T17:08:12.142298: step 8794, loss 5.78889e-06, acc 1
2017-08-08T17:08:12.455741: step 8795, loss 0.000219124, acc 1
2017-08-08T17:08:12.855914: step 8796, loss 0.000294976, acc 1
2017-08-08T17:08:13.114281: step 8797, loss 0.000348374, acc 1
2017-08-08T17:08:13.378265: step 8798, loss 0.00113602, acc 1
2017-08-08T17:08:13.610995: step 8799, loss 0.00025132, acc 1
2017-08-08T17:08:14.057681: step 8800, loss 2.00863e-05, acc 1

Evaluation:
2017-08-08T17:08:15.039469: step 8800, loss 2.87459, acc 0.724203

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-8800

2017-08-08T17:08:15.502227: step 8801, loss 0.000587257, acc 1
2017-08-08T17:08:15.889385: step 8802, loss 0.00261993, acc 1
2017-08-08T17:08:16.152099: step 8803, loss 0.000738482, acc 1
2017-08-08T17:08:16.421744: step 8804, loss 0.00186192, acc 1
2017-08-08T17:08:16.720659: step 8805, loss 0.000116011, acc 1
2017-08-08T17:08:17.065453: step 8806, loss 5.37698e-05, acc 1
2017-08-08T17:08:17.424803: step 8807, loss 0.0084904, acc 1
2017-08-08T17:08:17.785671: step 8808, loss 8.72602e-05, acc 1
2017-08-08T17:08:18.101907: step 8809, loss 9.1783e-05, acc 1
2017-08-08T17:08:18.336412: step 8810, loss 1.87217e-05, acc 1
2017-08-08T17:08:18.711321: step 8811, loss 0.0564072, acc 0.984375
2017-08-08T17:08:18.966845: step 8812, loss 2.3117e-05, acc 1
2017-08-08T17:08:19.215189: step 8813, loss 0.000430448, acc 1
2017-08-08T17:08:19.505441: step 8814, loss 9.92182e-05, acc 1
2017-08-08T17:08:19.860944: step 8815, loss 6.44364e-05, acc 1
2017-08-08T17:08:20.171459: step 8816, loss 0.000270231, acc 1
2017-08-08T17:08:20.445889: step 8817, loss 0.00125809, acc 1
2017-08-08T17:08:20.676108: step 8818, loss 0.00170362, acc 1
2017-08-08T17:08:21.045000: step 8819, loss 2.51528e-05, acc 1
2017-08-08T17:08:21.393002: step 8820, loss 0.000212701, acc 1
2017-08-08T17:08:21.660849: step 8821, loss 0.0002491, acc 1
2017-08-08T17:08:21.977939: step 8822, loss 0.000431697, acc 1
2017-08-08T17:08:22.300099: step 8823, loss 0.00170108, acc 1
2017-08-08T17:08:22.598013: step 8824, loss 0.1245, acc 0.984375
2017-08-08T17:08:22.883646: step 8825, loss 0.000184256, acc 1
2017-08-08T17:08:23.158539: step 8826, loss 0.00360532, acc 1
2017-08-08T17:08:23.367506: step 8827, loss 1.62876e-05, acc 1
2017-08-08T17:08:23.643470: step 8828, loss 0.000293208, acc 1
2017-08-08T17:08:23.893215: step 8829, loss 0.00051389, acc 1
2017-08-08T17:08:24.114036: step 8830, loss 6.85076e-05, acc 1
2017-08-08T17:08:24.351428: step 8831, loss 6.5888e-05, acc 1
2017-08-08T17:08:24.756055: step 8832, loss 3.39228e-05, acc 1
2017-08-08T17:08:25.181749: step 8833, loss 0.000373912, acc 1
2017-08-08T17:08:25.380109: step 8834, loss 8.49284e-05, acc 1
2017-08-08T17:08:25.617106: step 8835, loss 0.000425482, acc 1
2017-08-08T17:08:25.933378: step 8836, loss 0.000209289, acc 1
2017-08-08T17:08:26.205326: step 8837, loss 0.000233423, acc 1
2017-08-08T17:08:26.401446: step 8838, loss 0.000134577, acc 1
2017-08-08T17:08:26.617403: step 8839, loss 0.00104822, acc 1
2017-08-08T17:08:26.987132: step 8840, loss 3.26487e-05, acc 1
2017-08-08T17:08:27.297831: step 8841, loss 0.038324, acc 0.984375
2017-08-08T17:08:27.563128: step 8842, loss 1.5244e-05, acc 1
2017-08-08T17:08:27.842489: step 8843, loss 0.00295223, acc 1
2017-08-08T17:08:28.165313: step 8844, loss 2.95784e-06, acc 1
2017-08-08T17:08:28.363790: step 8845, loss 5.93067e-05, acc 1
2017-08-08T17:08:28.545978: step 8846, loss 0.000105185, acc 1
2017-08-08T17:08:28.777446: step 8847, loss 0.0014235, acc 1
2017-08-08T17:08:29.041338: step 8848, loss 0.06483, acc 0.984375
2017-08-08T17:08:29.425888: step 8849, loss 4.138e-05, acc 1
2017-08-08T17:08:29.727673: step 8850, loss 1.35353e-05, acc 1
2017-08-08T17:08:29.954911: step 8851, loss 5.71975e-06, acc 1
2017-08-08T17:08:30.199788: step 8852, loss 9.66707e-07, acc 1
2017-08-08T17:08:30.588404: step 8853, loss 0.000187123, acc 1
2017-08-08T17:08:30.771413: step 8854, loss 0.000754255, acc 1
2017-08-08T17:08:30.977399: step 8855, loss 4.2187e-05, acc 1
2017-08-08T17:08:31.229474: step 8856, loss 8.80142e-05, acc 1
2017-08-08T17:08:31.629483: step 8857, loss 1.82997e-05, acc 1
2017-08-08T17:08:31.998722: step 8858, loss 5.75715e-06, acc 1
2017-08-08T17:08:32.215572: step 8859, loss 2.12399e-05, acc 1
2017-08-08T17:08:32.450132: step 8860, loss 0.000159507, acc 1
2017-08-08T17:08:32.865660: step 8861, loss 6.91818e-05, acc 1
2017-08-08T17:08:33.170397: step 8862, loss 0.000269679, acc 1
2017-08-08T17:08:33.375975: step 8863, loss 4.47881e-05, acc 1
2017-08-08T17:08:33.670167: step 8864, loss 0.000904659, acc 1
2017-08-08T17:08:33.981540: step 8865, loss 0.0001213, acc 1
2017-08-08T17:08:34.335385: step 8866, loss 0.00368219, acc 1
2017-08-08T17:08:34.605889: step 8867, loss 2.85698e-05, acc 1
2017-08-08T17:08:34.872086: step 8868, loss 0.000806036, acc 1
2017-08-08T17:08:35.125309: step 8869, loss 0.000909157, acc 1
2017-08-08T17:08:35.410328: step 8870, loss 0.000767674, acc 1
2017-08-08T17:08:35.607679: step 8871, loss 0.000918156, acc 1
2017-08-08T17:08:35.830150: step 8872, loss 0.00416097, acc 1
2017-08-08T17:08:36.076289: step 8873, loss 3.50587e-05, acc 1
2017-08-08T17:08:36.500011: step 8874, loss 0.000724017, acc 1
2017-08-08T17:08:36.838619: step 8875, loss 2.75293e-06, acc 1
2017-08-08T17:08:37.211741: step 8876, loss 0.000381766, acc 1
2017-08-08T17:08:37.551163: step 8877, loss 0.000112748, acc 1
2017-08-08T17:08:37.907904: step 8878, loss 0.000366956, acc 1
2017-08-08T17:08:38.126538: step 8879, loss 5.16218e-05, acc 1
2017-08-08T17:08:38.332986: step 8880, loss 0.000723987, acc 1
2017-08-08T17:08:38.531442: step 8881, loss 1.06965e-05, acc 1
2017-08-08T17:08:38.933396: step 8882, loss 6.08702e-05, acc 1
2017-08-08T17:08:39.363038: step 8883, loss 0.00321235, acc 1
2017-08-08T17:08:39.647731: step 8884, loss 0.000234705, acc 1
2017-08-08T17:08:39.857063: step 8885, loss 0.000151077, acc 1
2017-08-08T17:08:40.133386: step 8886, loss 5.16502e-05, acc 1
2017-08-08T17:08:40.421329: step 8887, loss 0.000122192, acc 1
2017-08-08T17:08:40.608697: step 8888, loss 0.00108466, acc 1
2017-08-08T17:08:40.846916: step 8889, loss 0.00746907, acc 1
2017-08-08T17:08:41.171624: step 8890, loss 0.00510045, acc 1
2017-08-08T17:08:41.562527: step 8891, loss 0.000273641, acc 1
2017-08-08T17:08:41.852302: step 8892, loss 0.000117099, acc 1
2017-08-08T17:08:42.074768: step 8893, loss 0.00243524, acc 1
2017-08-08T17:08:42.261168: step 8894, loss 0.000927795, acc 1
2017-08-08T17:08:42.616259: step 8895, loss 0.00067018, acc 1
2017-08-08T17:08:42.843141: step 8896, loss 6.79525e-05, acc 1
2017-08-08T17:08:43.041755: step 8897, loss 6.4087e-05, acc 1
2017-08-08T17:08:43.295283: step 8898, loss 0.00296942, acc 1
2017-08-08T17:08:43.665564: step 8899, loss 0.000193409, acc 1
2017-08-08T17:08:44.097366: step 8900, loss 0.00407648, acc 1

Evaluation:
2017-08-08T17:08:44.828742: step 8900, loss 2.90115, acc 0.724203

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-8900

2017-08-08T17:08:45.412466: step 8901, loss 0.000981619, acc 1
2017-08-08T17:08:45.616623: step 8902, loss 1.26501e-05, acc 1
2017-08-08T17:08:45.820991: step 8903, loss 0.000733039, acc 1
2017-08-08T17:08:46.075363: step 8904, loss 7.18339e-05, acc 1
2017-08-08T17:08:46.381924: step 8905, loss 0.000282484, acc 1
2017-08-08T17:08:46.618896: step 8906, loss 0.000239888, acc 1
2017-08-08T17:08:46.865667: step 8907, loss 0.00058719, acc 1
2017-08-08T17:08:47.087281: step 8908, loss 3.82178e-05, acc 1
2017-08-08T17:08:47.373343: step 8909, loss 0.000534829, acc 1
2017-08-08T17:08:47.557134: step 8910, loss 3.85758e-05, acc 1
2017-08-08T17:08:47.756657: step 8911, loss 0.000110932, acc 1
2017-08-08T17:08:47.924685: step 8912, loss 0.00230617, acc 1
2017-08-08T17:08:48.234217: step 8913, loss 0.00148454, acc 1
2017-08-08T17:08:48.625359: step 8914, loss 0.00143058, acc 1
2017-08-08T17:08:49.002922: step 8915, loss 0.000103583, acc 1
2017-08-08T17:08:49.253916: step 8916, loss 0.000495908, acc 1
2017-08-08T17:08:49.591037: step 8917, loss 3.42358e-05, acc 1
2017-08-08T17:08:50.010059: step 8918, loss 0.000419758, acc 1
2017-08-08T17:08:50.371879: step 8919, loss 0.000403385, acc 1
2017-08-08T17:08:50.653801: step 8920, loss 0.000131567, acc 1
2017-08-08T17:08:51.009402: step 8921, loss 0.00011756, acc 1
2017-08-08T17:08:51.389203: step 8922, loss 0.000131535, acc 1
2017-08-08T17:08:51.785528: step 8923, loss 0.000198931, acc 1
2017-08-08T17:08:52.132985: step 8924, loss 1.68659e-05, acc 1
2017-08-08T17:08:52.388873: step 8925, loss 4.30186e-05, acc 1
2017-08-08T17:08:52.813918: step 8926, loss 3.00644e-05, acc 1
2017-08-08T17:08:53.124858: step 8927, loss 4.48586e-05, acc 1
2017-08-08T17:08:53.383457: step 8928, loss 0.0120959, acc 0.984375
2017-08-08T17:08:53.688659: step 8929, loss 7.39324e-05, acc 1
2017-08-08T17:08:53.961383: step 8930, loss 0.000194087, acc 1
2017-08-08T17:08:54.381488: step 8931, loss 0.018413, acc 0.984375
2017-08-08T17:08:54.653388: step 8932, loss 1.3411e-05, acc 1
2017-08-08T17:08:54.986376: step 8933, loss 0.00012443, acc 1
2017-08-08T17:08:55.240009: step 8934, loss 0.000901067, acc 1
2017-08-08T17:08:55.500228: step 8935, loss 7.24716e-06, acc 1
2017-08-08T17:08:55.848741: step 8936, loss 0.000537316, acc 1
2017-08-08T17:08:56.095927: step 8937, loss 9.89198e-05, acc 1
2017-08-08T17:08:56.351261: step 8938, loss 0.000689906, acc 1
2017-08-08T17:08:56.649291: step 8939, loss 0.000408551, acc 1
2017-08-08T17:08:57.092772: step 8940, loss 6.37305e-05, acc 1
2017-08-08T17:08:57.421939: step 8941, loss 0.00106164, acc 1
2017-08-08T17:08:57.691534: step 8942, loss 0.000121331, acc 1
2017-08-08T17:08:57.886691: step 8943, loss 3.39578e-05, acc 1
2017-08-08T17:08:58.190346: step 8944, loss 0.000246098, acc 1
2017-08-08T17:08:58.417452: step 8945, loss 0.0641826, acc 0.984375
2017-08-08T17:08:58.632378: step 8946, loss 0.000135431, acc 1
2017-08-08T17:08:58.905435: step 8947, loss 0.0183149, acc 0.984375
2017-08-08T17:08:59.458167: step 8948, loss 0.000297251, acc 1
2017-08-08T17:08:59.818565: step 8949, loss 3.28013e-05, acc 1
2017-08-08T17:09:00.132444: step 8950, loss 0.000378793, acc 1
2017-08-08T17:09:00.443329: step 8951, loss 0.000149495, acc 1
2017-08-08T17:09:00.719992: step 8952, loss 0.00210174, acc 1
2017-08-08T17:09:00.968552: step 8953, loss 0.000401714, acc 1
2017-08-08T17:09:01.271234: step 8954, loss 0.00136807, acc 1
2017-08-08T17:09:01.668202: step 8955, loss 0.000170088, acc 1
2017-08-08T17:09:02.306553: step 8956, loss 0.000192874, acc 1
2017-08-08T17:09:02.732076: step 8957, loss 8.5314e-05, acc 1
2017-08-08T17:09:03.111652: step 8958, loss 0.000169329, acc 1
2017-08-08T17:09:03.399205: step 8959, loss 0.000142965, acc 1
2017-08-08T17:09:03.797572: step 8960, loss 0.0140892, acc 0.984375
2017-08-08T17:09:04.220054: step 8961, loss 0.00665672, acc 1
2017-08-08T17:09:04.550286: step 8962, loss 0.0815504, acc 0.984375
2017-08-08T17:09:04.879315: step 8963, loss 7.05654e-05, acc 1
2017-08-08T17:09:05.317404: step 8964, loss 0.00686661, acc 1
2017-08-08T17:09:05.712185: step 8965, loss 9.88631e-05, acc 1
2017-08-08T17:09:06.100343: step 8966, loss 0.0381694, acc 0.984375
2017-08-08T17:09:06.340724: step 8967, loss 0.000644435, acc 1
2017-08-08T17:09:06.592851: step 8968, loss 1.42916e-05, acc 1
2017-08-08T17:09:06.982483: step 8969, loss 6.79012e-05, acc 1
2017-08-08T17:09:07.235580: step 8970, loss 9.6858e-05, acc 1
2017-08-08T17:09:07.460307: step 8971, loss 0.000439623, acc 1
2017-08-08T17:09:07.752588: step 8972, loss 0.00200275, acc 1
2017-08-08T17:09:08.198819: step 8973, loss 0.00209013, acc 1
2017-08-08T17:09:08.569891: step 8974, loss 0.00411205, acc 1
2017-08-08T17:09:09.064954: step 8975, loss 0.000169391, acc 1
2017-08-08T17:09:09.324371: step 8976, loss 0.0524562, acc 0.984375
2017-08-08T17:09:09.721363: step 8977, loss 0.000844856, acc 1
2017-08-08T17:09:09.969714: step 8978, loss 0.10588, acc 0.984375
2017-08-08T17:09:10.207365: step 8979, loss 0.00121192, acc 1
2017-08-08T17:09:10.498295: step 8980, loss 0.0028816, acc 1
2017-08-08T17:09:10.977405: step 8981, loss 9.53082e-05, acc 1
2017-08-08T17:09:11.416538: step 8982, loss 0.000197823, acc 1
2017-08-08T17:09:11.783267: step 8983, loss 0.000447906, acc 1
2017-08-08T17:09:12.047407: step 8984, loss 5.44202e-06, acc 1
2017-08-08T17:09:12.437380: step 8985, loss 0.000519391, acc 1
2017-08-08T17:09:12.786083: step 8986, loss 0.000115497, acc 1
2017-08-08T17:09:13.028431: step 8987, loss 0.00083425, acc 1
2017-08-08T17:09:13.249305: step 8988, loss 0.00369221, acc 1
2017-08-08T17:09:13.661363: step 8989, loss 8.60655e-06, acc 1
2017-08-08T17:09:14.051429: step 8990, loss 3.92429e-05, acc 1
2017-08-08T17:09:14.408422: step 8991, loss 1.82325e-05, acc 1
2017-08-08T17:09:14.669278: step 8992, loss 0.00254441, acc 1
2017-08-08T17:09:14.998621: step 8993, loss 0.0082755, acc 1
2017-08-08T17:09:15.422420: step 8994, loss 5.48498e-06, acc 1
2017-08-08T17:09:15.724367: step 8995, loss 0.0214283, acc 0.984375
2017-08-08T17:09:16.000955: step 8996, loss 0.000370234, acc 1
2017-08-08T17:09:16.274537: step 8997, loss 0.000398921, acc 1
2017-08-08T17:09:16.701318: step 8998, loss 0.00222166, acc 1
2017-08-08T17:09:17.026316: step 8999, loss 0.116969, acc 0.984375
2017-08-08T17:09:17.215765: step 9000, loss 0.000375909, acc 1

Evaluation:
2017-08-08T17:09:17.673028: step 9000, loss 3.12056, acc 0.719512

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-9000

2017-08-08T17:09:18.127839: step 9001, loss 2.18067e-05, acc 1
2017-08-08T17:09:18.372473: step 9002, loss 0.0032204, acc 1
2017-08-08T17:09:18.573948: step 9003, loss 0.0401341, acc 0.984375
2017-08-08T17:09:18.958013: step 9004, loss 0.000355739, acc 1
2017-08-08T17:09:19.243752: step 9005, loss 0.00104735, acc 1
2017-08-08T17:09:19.568228: step 9006, loss 0.000539826, acc 1
2017-08-08T17:09:19.746602: step 9007, loss 0.000338185, acc 1
2017-08-08T17:09:20.106475: step 9008, loss 1.45002e-05, acc 1
2017-08-08T17:09:20.331188: step 9009, loss 7.33103e-05, acc 1
2017-08-08T17:09:20.539151: step 9010, loss 0.00332314, acc 1
2017-08-08T17:09:20.745443: step 9011, loss 0.000321507, acc 1
2017-08-08T17:09:21.108210: step 9012, loss 5.2039e-05, acc 1
2017-08-08T17:09:21.382216: step 9013, loss 0.000633719, acc 1
2017-08-08T17:09:21.645701: step 9014, loss 0.000342391, acc 1
2017-08-08T17:09:21.849944: step 9015, loss 0.0093205, acc 1
2017-08-08T17:09:22.253808: step 9016, loss 0.000839627, acc 1
2017-08-08T17:09:22.614528: step 9017, loss 0.000613292, acc 1
2017-08-08T17:09:22.894988: step 9018, loss 0.00359055, acc 1
2017-08-08T17:09:23.133474: step 9019, loss 7.54129e-06, acc 1
2017-08-08T17:09:23.413453: step 9020, loss 0.000136647, acc 1
2017-08-08T17:09:23.720478: step 9021, loss 6.59739e-05, acc 1
2017-08-08T17:09:24.061065: step 9022, loss 0.00128474, acc 1
2017-08-08T17:09:24.352343: step 9023, loss 0.00371491, acc 1
2017-08-08T17:09:24.653375: step 9024, loss 0.0042221, acc 1
2017-08-08T17:09:24.829028: step 9025, loss 0.128616, acc 0.984375
2017-08-08T17:09:25.094356: step 9026, loss 0.000597285, acc 1
2017-08-08T17:09:25.380424: step 9027, loss 0.000919374, acc 1
2017-08-08T17:09:25.600665: step 9028, loss 0.000229267, acc 1
2017-08-08T17:09:25.826034: step 9029, loss 0.000251138, acc 1
2017-08-08T17:09:26.195270: step 9030, loss 0.00054444, acc 1
2017-08-08T17:09:26.534814: step 9031, loss 0.000197275, acc 1
2017-08-08T17:09:26.812748: step 9032, loss 0.000498784, acc 1
2017-08-08T17:09:27.135824: step 9033, loss 0.0293987, acc 0.984375
2017-08-08T17:09:27.396220: step 9034, loss 0.0023972, acc 1
2017-08-08T17:09:27.811644: step 9035, loss 0.000921215, acc 1
2017-08-08T17:09:28.052905: step 9036, loss 3.00255e-06, acc 1
2017-08-08T17:09:28.274104: step 9037, loss 0.0841129, acc 0.984375
2017-08-08T17:09:28.491476: step 9038, loss 0.00160402, acc 1
2017-08-08T17:09:28.745367: step 9039, loss 0.000150029, acc 1
2017-08-08T17:09:29.162536: step 9040, loss 0.000254188, acc 1
2017-08-08T17:09:29.578111: step 9041, loss 4.39555e-05, acc 1
2017-08-08T17:09:29.854447: step 9042, loss 0.000145306, acc 1
2017-08-08T17:09:30.069032: step 9043, loss 0.000281066, acc 1
2017-08-08T17:09:30.292475: step 9044, loss 4.44304e-05, acc 1
2017-08-08T17:09:30.651298: step 9045, loss 0.0024863, acc 1
2017-08-08T17:09:30.847987: step 9046, loss 7.94181e-06, acc 1
2017-08-08T17:09:31.060216: step 9047, loss 0.00104022, acc 1
2017-08-08T17:09:31.410612: step 9048, loss 9.17542e-05, acc 1
2017-08-08T17:09:31.797515: step 9049, loss 0.00105069, acc 1
2017-08-08T17:09:32.188199: step 9050, loss 0.000152505, acc 1
2017-08-08T17:09:32.543331: step 9051, loss 3.5705e-05, acc 1
2017-08-08T17:09:32.789263: step 9052, loss 0.000193533, acc 1
2017-08-08T17:09:33.144118: step 9053, loss 0.0246352, acc 0.984375
2017-08-08T17:09:33.483675: step 9054, loss 0.000284992, acc 1
2017-08-08T17:09:33.714878: step 9055, loss 0.000268588, acc 1
2017-08-08T17:09:33.950565: step 9056, loss 0.000993336, acc 1
2017-08-08T17:09:34.341367: step 9057, loss 0.000569891, acc 1
2017-08-08T17:09:34.783853: step 9058, loss 1.13493e-05, acc 1
2017-08-08T17:09:35.067799: step 9059, loss 2.44213e-05, acc 1
2017-08-08T17:09:35.298719: step 9060, loss 0.00116753, acc 1
2017-08-08T17:09:35.673371: step 9061, loss 0.000863049, acc 1
2017-08-08T17:09:35.921398: step 9062, loss 0.00105688, acc 1
2017-08-08T17:09:36.120630: step 9063, loss 0.00271757, acc 1
2017-08-08T17:09:36.331127: step 9064, loss 0.0849504, acc 0.984375
2017-08-08T17:09:36.606030: step 9065, loss 4.28981e-05, acc 1
2017-08-08T17:09:37.052231: step 9066, loss 3.16917e-05, acc 1
2017-08-08T17:09:37.421256: step 9067, loss 2.88425e-05, acc 1
2017-08-08T17:09:37.723753: step 9068, loss 3.28983e-05, acc 1
2017-08-08T17:09:37.925001: step 9069, loss 0.00569615, acc 1
2017-08-08T17:09:38.235605: step 9070, loss 6.08493e-05, acc 1
2017-08-08T17:09:38.470350: step 9071, loss 0.000649619, acc 1
2017-08-08T17:09:38.645212: step 9072, loss 2.92231e-05, acc 1
2017-08-08T17:09:38.881647: step 9073, loss 0.000232728, acc 1
2017-08-08T17:09:39.268126: step 9074, loss 0.000232029, acc 1
2017-08-08T17:09:39.688197: step 9075, loss 0.0033829, acc 1
2017-08-08T17:09:40.101928: step 9076, loss 0.0225056, acc 0.984375
2017-08-08T17:09:40.310094: step 9077, loss 0.000570773, acc 1
2017-08-08T17:09:40.496547: step 9078, loss 3.90188e-05, acc 1
2017-08-08T17:09:40.910556: step 9079, loss 0.00763508, acc 1
2017-08-08T17:09:41.115325: step 9080, loss 8.50795e-05, acc 1
2017-08-08T17:09:41.324273: step 9081, loss 3.72246e-05, acc 1
2017-08-08T17:09:41.512007: step 9082, loss 0.0308392, acc 0.984375
2017-08-08T17:09:41.845962: step 9083, loss 0.000326676, acc 1
2017-08-08T17:09:42.124537: step 9084, loss 0.000267567, acc 1
2017-08-08T17:09:42.371705: step 9085, loss 0.00235476, acc 1
2017-08-08T17:09:42.600648: step 9086, loss 4.87055e-06, acc 1
2017-08-08T17:09:42.857344: step 9087, loss 5.3022e-05, acc 1
2017-08-08T17:09:43.139700: step 9088, loss 0.0120945, acc 0.984375
2017-08-08T17:09:43.327015: step 9089, loss 0.000288898, acc 1
2017-08-08T17:09:43.496532: step 9090, loss 0.00171211, acc 1
2017-08-08T17:09:43.765336: step 9091, loss 0.0129686, acc 0.984375
2017-08-08T17:09:44.153340: step 9092, loss 0.000216777, acc 1
2017-08-08T17:09:44.415083: step 9093, loss 0.00354353, acc 1
2017-08-08T17:09:44.612492: step 9094, loss 0.000196189, acc 1
2017-08-08T17:09:44.896437: step 9095, loss 0.00317184, acc 1
2017-08-08T17:09:45.234109: step 9096, loss 0.000104933, acc 1
2017-08-08T17:09:45.494125: step 9097, loss 0.0667209, acc 0.984375
2017-08-08T17:09:45.780057: step 9098, loss 9.46553e-05, acc 1
2017-08-08T17:09:46.193415: step 9099, loss 0.000375541, acc 1
2017-08-08T17:09:46.577350: step 9100, loss 2.83314e-05, acc 1

Evaluation:
2017-08-08T17:09:47.244990: step 9100, loss 2.93399, acc 0.72045

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-9100

2017-08-08T17:09:47.709376: step 9101, loss 0.00839669, acc 1
2017-08-08T17:09:48.065042: step 9102, loss 0.0131767, acc 1
2017-08-08T17:09:48.276593: step 9103, loss 0.000468337, acc 1
2017-08-08T17:09:48.519556: step 9104, loss 0.000371075, acc 1
2017-08-08T17:09:48.791876: step 9105, loss 0.000179174, acc 1
2017-08-08T17:09:49.228812: step 9106, loss 0.000585255, acc 1
2017-08-08T17:09:49.667126: step 9107, loss 0.000451527, acc 1
2017-08-08T17:09:49.943883: step 9108, loss 0.000951769, acc 1
2017-08-08T17:09:50.211149: step 9109, loss 0.0162229, acc 0.984375
2017-08-08T17:09:50.478479: step 9110, loss 0.0497426, acc 0.984375
2017-08-08T17:09:50.813800: step 9111, loss 0.114186, acc 0.984375
2017-08-08T17:09:51.024918: step 9112, loss 0.000117907, acc 1
2017-08-08T17:09:51.204659: step 9113, loss 0.0013143, acc 1
2017-08-08T17:09:51.541339: step 9114, loss 0.00996341, acc 1
2017-08-08T17:09:51.841383: step 9115, loss 0.000850887, acc 1
2017-08-08T17:09:52.113153: step 9116, loss 2.4158e-06, acc 1
2017-08-08T17:09:52.315831: step 9117, loss 0.0765992, acc 0.984375
2017-08-08T17:09:52.630244: step 9118, loss 0.00022471, acc 1
2017-08-08T17:09:52.818079: step 9119, loss 0.00403247, acc 1
2017-08-08T17:09:52.995173: step 9120, loss 3.65542e-05, acc 1
2017-08-08T17:09:53.266568: step 9121, loss 0.000344336, acc 1
2017-08-08T17:09:53.673684: step 9122, loss 9.35953e-05, acc 1
2017-08-08T17:09:53.989373: step 9123, loss 2.83604e-05, acc 1
2017-08-08T17:09:54.271375: step 9124, loss 0.000166071, acc 1
2017-08-08T17:09:54.497360: step 9125, loss 0.000348123, acc 1
2017-08-08T17:09:54.817634: step 9126, loss 0.000135841, acc 1
2017-08-08T17:09:55.087619: step 9127, loss 0.00836508, acc 1
2017-08-08T17:09:55.289864: step 9128, loss 7.19568e-05, acc 1
2017-08-08T17:09:55.497239: step 9129, loss 0.000282063, acc 1
2017-08-08T17:09:55.812439: step 9130, loss 0.00135921, acc 1
2017-08-08T17:09:56.122901: step 9131, loss 4.99016e-05, acc 1
2017-08-08T17:09:56.444767: step 9132, loss 0.000102512, acc 1
2017-08-08T17:09:56.646089: step 9133, loss 0.00157319, acc 1
2017-08-08T17:09:56.876839: step 9134, loss 5.1437e-05, acc 1
2017-08-08T17:09:57.207394: step 9135, loss 0.00617079, acc 1
2017-08-08T17:09:57.447914: step 9136, loss 4.66587e-05, acc 1
2017-08-08T17:09:57.717481: step 9137, loss 0.00192205, acc 1
2017-08-08T17:09:57.979980: step 9138, loss 0.000468743, acc 1
2017-08-08T17:09:58.391303: step 9139, loss 0.000474459, acc 1
2017-08-08T17:09:58.681396: step 9140, loss 0.000345507, acc 1
2017-08-08T17:09:59.081512: step 9141, loss 0.00116593, acc 1
2017-08-08T17:09:59.358435: step 9142, loss 0.032149, acc 0.984375
2017-08-08T17:09:59.647832: step 9143, loss 0.000131264, acc 1
2017-08-08T17:09:59.925583: step 9144, loss 0.000237564, acc 1
2017-08-08T17:10:00.165281: step 9145, loss 0.00167336, acc 1
2017-08-08T17:10:00.459251: step 9146, loss 9.28472e-05, acc 1
2017-08-08T17:10:00.820175: step 9147, loss 2.78476e-05, acc 1
2017-08-08T17:10:01.141378: step 9148, loss 1.8422e-05, acc 1
2017-08-08T17:10:01.537364: step 9149, loss 0.00176567, acc 1
2017-08-08T17:10:01.881964: step 9150, loss 0.000927391, acc 1
2017-08-08T17:10:02.272788: step 9151, loss 0.000712039, acc 1
2017-08-08T17:10:02.718302: step 9152, loss 0.00117763, acc 1
2017-08-08T17:10:03.040277: step 9153, loss 0.0171635, acc 0.984375
2017-08-08T17:10:03.328616: step 9154, loss 3.09359e-05, acc 1
2017-08-08T17:10:03.608070: step 9155, loss 4.02638e-05, acc 1
2017-08-08T17:10:03.961431: step 9156, loss 0.00390221, acc 1
2017-08-08T17:10:04.469165: step 9157, loss 0.000402543, acc 1
2017-08-08T17:10:04.892963: step 9158, loss 0.00282269, acc 1
2017-08-08T17:10:05.164850: step 9159, loss 0.000632874, acc 1
2017-08-08T17:10:05.412794: step 9160, loss 0.00213738, acc 1
2017-08-08T17:10:05.831294: step 9161, loss 0.000145626, acc 1
2017-08-08T17:10:06.095969: step 9162, loss 0.00120344, acc 1
2017-08-08T17:10:06.395449: step 9163, loss 8.38319e-06, acc 1
2017-08-08T17:10:06.664861: step 9164, loss 0.000100034, acc 1
2017-08-08T17:10:07.056088: step 9165, loss 0.012081, acc 0.984375
2017-08-08T17:10:07.466431: step 9166, loss 0.00711741, acc 1
2017-08-08T17:10:07.769383: step 9167, loss 0.00054095, acc 1
2017-08-08T17:10:08.082172: step 9168, loss 0.00088622, acc 1
2017-08-08T17:10:08.381920: step 9169, loss 1.79639e-05, acc 1
2017-08-08T17:10:08.839188: step 9170, loss 0.000227697, acc 1
2017-08-08T17:10:09.094044: step 9171, loss 0.000205711, acc 1
2017-08-08T17:10:09.410666: step 9172, loss 0.00222295, acc 1
2017-08-08T17:10:09.663468: step 9173, loss 8.2646e-05, acc 1
2017-08-08T17:10:09.987539: step 9174, loss 0.00147887, acc 1
2017-08-08T17:10:10.437315: step 9175, loss 0.000120337, acc 1
2017-08-08T17:10:10.814648: step 9176, loss 0.00354261, acc 1
2017-08-08T17:10:11.084424: step 9177, loss 0.000534272, acc 1
2017-08-08T17:10:11.330843: step 9178, loss 0.000254926, acc 1
2017-08-08T17:10:11.806994: step 9179, loss 0.000296514, acc 1
2017-08-08T17:10:12.095975: step 9180, loss 0.000934978, acc 1
2017-08-08T17:10:12.343847: step 9181, loss 4.33798e-05, acc 1
2017-08-08T17:10:12.623521: step 9182, loss 0.000663392, acc 1
2017-08-08T17:10:13.023956: step 9183, loss 1.38828e-05, acc 1
2017-08-08T17:10:13.426308: step 9184, loss 6.36863e-05, acc 1
2017-08-08T17:10:13.753242: step 9185, loss 9.45349e-05, acc 1
2017-08-08T17:10:14.018298: step 9186, loss 0.00115856, acc 1
2017-08-08T17:10:14.293406: step 9187, loss 0.0413013, acc 0.984375
2017-08-08T17:10:14.640447: step 9188, loss 0.000217966, acc 1
2017-08-08T17:10:14.940369: step 9189, loss 0.00256179, acc 1
2017-08-08T17:10:15.313576: step 9190, loss 9.21892e-05, acc 1
2017-08-08T17:10:15.646516: step 9191, loss 2.8639e-05, acc 1
2017-08-08T17:10:15.907084: step 9192, loss 0.000247622, acc 1
2017-08-08T17:10:16.291462: step 9193, loss 0.000104171, acc 1
2017-08-08T17:10:16.548897: step 9194, loss 0.0185688, acc 0.984375
2017-08-08T17:10:16.821539: step 9195, loss 0.00183833, acc 1
2017-08-08T17:10:17.097357: step 9196, loss 0.000188614, acc 1
2017-08-08T17:10:17.516637: step 9197, loss 0.00215375, acc 1
2017-08-08T17:10:17.897179: step 9198, loss 0.000554963, acc 1
2017-08-08T17:10:18.294406: step 9199, loss 0.000112707, acc 1
2017-08-08T17:10:18.511616: step 9200, loss 0.000250211, acc 1

Evaluation:
2017-08-08T17:10:19.396600: step 9200, loss 3.07393, acc 0.718574

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-9200

2017-08-08T17:10:19.886537: step 9201, loss 0.000202748, acc 1
2017-08-08T17:10:20.297549: step 9202, loss 1.25187e-05, acc 1
2017-08-08T17:10:20.602619: step 9203, loss 0.0391247, acc 0.984375
2017-08-08T17:10:20.920695: step 9204, loss 0.0403705, acc 0.984375
2017-08-08T17:10:21.189839: step 9205, loss 0.00131014, acc 1
2017-08-08T17:10:21.485460: step 9206, loss 0.000126076, acc 1
2017-08-08T17:10:21.854836: step 9207, loss 0.00048733, acc 1
2017-08-08T17:10:22.127379: step 9208, loss 1.73834e-05, acc 1
2017-08-08T17:10:22.333415: step 9209, loss 0.0034442, acc 1
2017-08-08T17:10:22.587549: step 9210, loss 7.25735e-05, acc 1
2017-08-08T17:10:22.949033: step 9211, loss 0.000949349, acc 1
2017-08-08T17:10:23.293338: step 9212, loss 0.0135716, acc 0.984375
2017-08-08T17:10:23.625515: step 9213, loss 0.019031, acc 0.984375
2017-08-08T17:10:23.949773: step 9214, loss 0.000957555, acc 1
2017-08-08T17:10:24.189189: step 9215, loss 0.00101277, acc 1
2017-08-08T17:10:24.652550: step 9216, loss 0.000508079, acc 1
2017-08-08T17:10:24.959943: step 9217, loss 0.00188067, acc 1
2017-08-08T17:10:25.275848: step 9218, loss 0.00203865, acc 1
2017-08-08T17:10:25.597909: step 9219, loss 1.69847e-05, acc 1
2017-08-08T17:10:25.954175: step 9220, loss 8.20065e-06, acc 1
2017-08-08T17:10:26.312876: step 9221, loss 1.19664e-05, acc 1
2017-08-08T17:10:26.706394: step 9222, loss 0.000314425, acc 1
2017-08-08T17:10:27.006851: step 9223, loss 1.03949e-05, acc 1
2017-08-08T17:10:27.282046: step 9224, loss 0.000132228, acc 1
2017-08-08T17:10:27.679500: step 9225, loss 0.00423111, acc 1
2017-08-08T17:10:27.976474: step 9226, loss 0.00465012, acc 1
2017-08-08T17:10:28.236013: step 9227, loss 6.85873e-05, acc 1
2017-08-08T17:10:28.505291: step 9228, loss 0.00368331, acc 1
2017-08-08T17:10:28.733341: step 9229, loss 0.00754358, acc 1
2017-08-08T17:10:29.069334: step 9230, loss 2.9731e-05, acc 1
2017-08-08T17:10:29.374826: step 9231, loss 0.000986885, acc 1
2017-08-08T17:10:29.563002: step 9232, loss 0.000245868, acc 1
2017-08-08T17:10:29.797281: step 9233, loss 0.00127193, acc 1
2017-08-08T17:10:30.091144: step 9234, loss 0.000143997, acc 1
2017-08-08T17:10:30.340589: step 9235, loss 0.00402337, acc 1
2017-08-08T17:10:30.626943: step 9236, loss 0.00225994, acc 1
2017-08-08T17:10:30.866923: step 9237, loss 1.20906e-05, acc 1
2017-08-08T17:10:31.248552: step 9238, loss 0.00130842, acc 1
2017-08-08T17:10:31.622646: step 9239, loss 0.00103702, acc 1
2017-08-08T17:10:31.896175: step 9240, loss 0.000294018, acc 1
2017-08-08T17:10:32.115694: step 9241, loss 0.000121556, acc 1
2017-08-08T17:10:32.461413: step 9242, loss 0.000570615, acc 1
2017-08-08T17:10:32.711365: step 9243, loss 0.00582768, acc 1
2017-08-08T17:10:32.997564: step 9244, loss 7.12744e-05, acc 1
2017-08-08T17:10:33.347114: step 9245, loss 0.00132419, acc 1
2017-08-08T17:10:33.687716: step 9246, loss 0.00104013, acc 1
2017-08-08T17:10:34.005644: step 9247, loss 0.00013986, acc 1
2017-08-08T17:10:34.208138: step 9248, loss 0.000681594, acc 1
2017-08-08T17:10:34.483941: step 9249, loss 0.000524839, acc 1
2017-08-08T17:10:34.740323: step 9250, loss 0.000133099, acc 1
2017-08-08T17:10:35.023180: step 9251, loss 0.000343493, acc 1
2017-08-08T17:10:35.256112: step 9252, loss 0.000102011, acc 1
2017-08-08T17:10:35.654637: step 9253, loss 5.55128e-05, acc 1
2017-08-08T17:10:36.023478: step 9254, loss 1.80997e-05, acc 1
2017-08-08T17:10:36.333494: step 9255, loss 4.63654e-05, acc 1
2017-08-08T17:10:36.596258: step 9256, loss 0.000323652, acc 1
2017-08-08T17:10:36.870212: step 9257, loss 0.00344647, acc 1
2017-08-08T17:10:37.307274: step 9258, loss 3.78115e-07, acc 1
2017-08-08T17:10:37.553618: step 9259, loss 0.000805029, acc 1
2017-08-08T17:10:37.752863: step 9260, loss 6.93669e-05, acc 1
2017-08-08T17:10:37.994143: step 9261, loss 0.00112769, acc 1
2017-08-08T17:10:38.279058: step 9262, loss 3.43485e-05, acc 1
2017-08-08T17:10:38.632237: step 9263, loss 0.0110939, acc 0.984375
2017-08-08T17:10:38.968359: step 9264, loss 0.000416641, acc 1
2017-08-08T17:10:39.198717: step 9265, loss 8.8035e-05, acc 1
2017-08-08T17:10:39.381902: step 9266, loss 0.000246598, acc 1
2017-08-08T17:10:39.751364: step 9267, loss 0.0077321, acc 1
2017-08-08T17:10:40.058057: step 9268, loss 0.00059005, acc 1
2017-08-08T17:10:40.312755: step 9269, loss 8.82747e-06, acc 1
2017-08-08T17:10:40.592222: step 9270, loss 0.000307219, acc 1
2017-08-08T17:10:40.872292: step 9271, loss 0.000610055, acc 1
2017-08-08T17:10:41.120900: step 9272, loss 3.24111e-05, acc 1
2017-08-08T17:10:41.432007: step 9273, loss 0.000571295, acc 1
2017-08-08T17:10:41.646701: step 9274, loss 0.00942708, acc 1
2017-08-08T17:10:41.990673: step 9275, loss 1.63607e-05, acc 1
2017-08-08T17:10:42.253120: step 9276, loss 0.000372361, acc 1
2017-08-08T17:10:42.591142: step 9277, loss 0.00784157, acc 1
2017-08-08T17:10:42.823126: step 9278, loss 0.000174391, acc 1
2017-08-08T17:10:43.117730: step 9279, loss 0.00207099, acc 1
2017-08-08T17:10:43.465783: step 9280, loss 0.000133364, acc 1
2017-08-08T17:10:43.759657: step 9281, loss 4.47934e-05, acc 1
2017-08-08T17:10:44.060575: step 9282, loss 8.34261e-05, acc 1
2017-08-08T17:10:44.284834: step 9283, loss 0.00101865, acc 1
2017-08-08T17:10:44.581363: step 9284, loss 0.000369459, acc 1
2017-08-08T17:10:44.791695: step 9285, loss 0.00133877, acc 1
2017-08-08T17:10:45.142881: step 9286, loss 0.000529457, acc 1
2017-08-08T17:10:45.360239: step 9287, loss 0.000596802, acc 1
2017-08-08T17:10:45.608024: step 9288, loss 0.00118199, acc 1
2017-08-08T17:10:45.881953: step 9289, loss 0.00329483, acc 1
2017-08-08T17:10:46.122169: step 9290, loss 0.000811787, acc 1
2017-08-08T17:10:46.555418: step 9291, loss 0.000244638, acc 1
2017-08-08T17:10:46.840721: step 9292, loss 6.55785e-06, acc 1
2017-08-08T17:10:47.232448: step 9293, loss 0.00183952, acc 1
2017-08-08T17:10:47.553863: step 9294, loss 5.18929e-05, acc 1
2017-08-08T17:10:47.894550: step 9295, loss 5.81237e-05, acc 1
2017-08-08T17:10:48.103650: step 9296, loss 2.22329e-05, acc 1
2017-08-08T17:10:48.304550: step 9297, loss 0.000678933, acc 1
2017-08-08T17:10:48.608757: step 9298, loss 0.000444301, acc 1
2017-08-08T17:10:48.852066: step 9299, loss 4.98886e-05, acc 1
2017-08-08T17:10:49.184216: step 9300, loss 0.000139952, acc 1

Evaluation:
2017-08-08T17:10:49.722225: step 9300, loss 3.05222, acc 0.721388

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-9300

2017-08-08T17:10:50.241661: step 9301, loss 0.00321078, acc 1
2017-08-08T17:10:50.495740: step 9302, loss 7.91699e-05, acc 1
2017-08-08T17:10:50.748164: step 9303, loss 0.000357974, acc 1
2017-08-08T17:10:51.017388: step 9304, loss 0.00085031, acc 1
2017-08-08T17:10:51.301709: step 9305, loss 9.75363e-05, acc 1
2017-08-08T17:10:51.592332: step 9306, loss 0.000542482, acc 1
2017-08-08T17:10:51.887056: step 9307, loss 0.000255101, acc 1
2017-08-08T17:10:52.156287: step 9308, loss 0.00155702, acc 1
2017-08-08T17:10:52.393455: step 9309, loss 4.44097e-05, acc 1
2017-08-08T17:10:52.602059: step 9310, loss 0.000108705, acc 1
2017-08-08T17:10:52.869857: step 9311, loss 0.000201726, acc 1
2017-08-08T17:10:53.124288: step 9312, loss 0.0483144, acc 0.984375
2017-08-08T17:10:53.457352: step 9313, loss 0.000119921, acc 1
2017-08-08T17:10:53.731475: step 9314, loss 0.000104193, acc 1
2017-08-08T17:10:53.994789: step 9315, loss 0.0449267, acc 0.984375
2017-08-08T17:10:54.326892: step 9316, loss 0.000851683, acc 1
2017-08-08T17:10:54.727373: step 9317, loss 0.000756229, acc 1
2017-08-08T17:10:54.994858: step 9318, loss 0.00126014, acc 1
2017-08-08T17:10:55.239558: step 9319, loss 8.08544e-05, acc 1
2017-08-08T17:10:55.594358: step 9320, loss 1.00811e-05, acc 1
2017-08-08T17:10:56.059472: step 9321, loss 6.74e-05, acc 1
2017-08-08T17:10:56.454549: step 9322, loss 0.000481265, acc 1
2017-08-08T17:10:56.750627: step 9323, loss 0.00147907, acc 1
2017-08-08T17:10:57.011994: step 9324, loss 5.64201e-05, acc 1
2017-08-08T17:10:57.302919: step 9325, loss 3.90509e-05, acc 1
2017-08-08T17:10:57.605349: step 9326, loss 8.15873e-05, acc 1
2017-08-08T17:10:57.807280: step 9327, loss 7.17269e-06, acc 1
2017-08-08T17:10:58.048972: step 9328, loss 0.000193131, acc 1
2017-08-08T17:10:58.299517: step 9329, loss 4.68086e-05, acc 1
2017-08-08T17:10:58.639802: step 9330, loss 0.000764655, acc 1
2017-08-08T17:10:58.957331: step 9331, loss 0.000490715, acc 1
2017-08-08T17:10:59.197341: step 9332, loss 0.0006849, acc 1
2017-08-08T17:10:59.417717: step 9333, loss 0.00684921, acc 1
2017-08-08T17:10:59.824680: step 9334, loss 0.000235162, acc 1
2017-08-08T17:11:00.065013: step 9335, loss 0.000345682, acc 1
2017-08-08T17:11:00.272840: step 9336, loss 0.000556964, acc 1
2017-08-08T17:11:00.487658: step 9337, loss 2.34871e-06, acc 1
2017-08-08T17:11:00.824772: step 9338, loss 0.00248748, acc 1
2017-08-08T17:11:01.205491: step 9339, loss 1.01114e-05, acc 1
2017-08-08T17:11:01.535138: step 9340, loss 0.000329304, acc 1
2017-08-08T17:11:01.789466: step 9341, loss 0.114577, acc 0.984375
2017-08-08T17:11:02.128166: step 9342, loss 5.25711e-05, acc 1
2017-08-08T17:11:02.545857: step 9343, loss 0.000729292, acc 1
2017-08-08T17:11:02.882781: step 9344, loss 0.000232064, acc 1
2017-08-08T17:11:03.125385: step 9345, loss 0.00115446, acc 1
2017-08-08T17:11:03.614597: step 9346, loss 3.81923e-05, acc 1
2017-08-08T17:11:04.043589: step 9347, loss 0.000207177, acc 1
2017-08-08T17:11:04.390613: step 9348, loss 0.00381816, acc 1
2017-08-08T17:11:04.644022: step 9349, loss 0.000358135, acc 1
2017-08-08T17:11:04.989412: step 9350, loss 0.000637786, acc 1
2017-08-08T17:11:05.274851: step 9351, loss 0.000575259, acc 1
2017-08-08T17:11:05.494673: step 9352, loss 0.00094229, acc 1
2017-08-08T17:11:05.756409: step 9353, loss 0.000634319, acc 1
2017-08-08T17:11:06.033590: step 9354, loss 0.000398038, acc 1
2017-08-08T17:11:06.506725: step 9355, loss 0.000165417, acc 1
2017-08-08T17:11:06.862896: step 9356, loss 0.000512215, acc 1
2017-08-08T17:11:07.252372: step 9357, loss 0.000227229, acc 1
2017-08-08T17:11:07.491337: step 9358, loss 0.00536233, acc 1
2017-08-08T17:11:07.721050: step 9359, loss 3.11533e-05, acc 1
2017-08-08T17:11:08.075352: step 9360, loss 0.00191682, acc 1
2017-08-08T17:11:08.289167: step 9361, loss 0.000143306, acc 1
2017-08-08T17:11:08.589690: step 9362, loss 0.000146371, acc 1
2017-08-08T17:11:08.850885: step 9363, loss 1.91457e-05, acc 1
2017-08-08T17:11:09.283523: step 9364, loss 0.00142343, acc 1
2017-08-08T17:11:09.710157: step 9365, loss 0.000176177, acc 1
2017-08-08T17:11:10.008960: step 9366, loss 0.00076742, acc 1
2017-08-08T17:11:10.227489: step 9367, loss 0.00018169, acc 1
2017-08-08T17:11:10.533024: step 9368, loss 0.000194207, acc 1
2017-08-08T17:11:10.836030: step 9369, loss 2.29194e-05, acc 1
2017-08-08T17:11:11.061206: step 9370, loss 0.00827509, acc 1
2017-08-08T17:11:11.255452: step 9371, loss 0.000105705, acc 1
2017-08-08T17:11:11.501159: step 9372, loss 0.00198869, acc 1
2017-08-08T17:11:11.941365: step 9373, loss 0.000638846, acc 1
2017-08-08T17:11:12.301362: step 9374, loss 0.000484676, acc 1
2017-08-08T17:11:12.570813: step 9375, loss 2.91214e-05, acc 1
2017-08-08T17:11:12.774369: step 9376, loss 0.000269992, acc 1
2017-08-08T17:11:13.136904: step 9377, loss 0.00164626, acc 1
2017-08-08T17:11:13.345264: step 9378, loss 6.62154e-05, acc 1
2017-08-08T17:11:13.607112: step 9379, loss 0.000123375, acc 1
2017-08-08T17:11:14.045064: step 9380, loss 0.0035518, acc 1
2017-08-08T17:11:14.353359: step 9381, loss 0.000527811, acc 1
2017-08-08T17:11:14.695180: step 9382, loss 3.20209e-05, acc 1
2017-08-08T17:11:14.880689: step 9383, loss 4.31496e-05, acc 1
2017-08-08T17:11:15.121416: step 9384, loss 2.73607e-05, acc 1
2017-08-08T17:11:15.434843: step 9385, loss 9.37519e-05, acc 1
2017-08-08T17:11:15.689328: step 9386, loss 0.000621512, acc 1
2017-08-08T17:11:15.941377: step 9387, loss 0.0678751, acc 0.984375
2017-08-08T17:11:16.299346: step 9388, loss 2.28469e-05, acc 1
2017-08-08T17:11:16.670928: step 9389, loss 5.20624e-05, acc 1
2017-08-08T17:11:16.906764: step 9390, loss 0.000902444, acc 1
2017-08-08T17:11:17.079028: step 9391, loss 4.16757e-05, acc 1
2017-08-08T17:11:17.405366: step 9392, loss 1.68507e-05, acc 1
2017-08-08T17:11:17.736811: step 9393, loss 0.000131106, acc 1
2017-08-08T17:11:18.024575: step 9394, loss 0.0138542, acc 0.984375
2017-08-08T17:11:18.313019: step 9395, loss 6.35074e-05, acc 1
2017-08-08T17:11:18.657921: step 9396, loss 0.00047745, acc 1
2017-08-08T17:11:19.006025: step 9397, loss 6.12749e-05, acc 1
2017-08-08T17:11:19.256142: step 9398, loss 0.000162948, acc 1
2017-08-08T17:11:19.457390: step 9399, loss 0.00389931, acc 1
2017-08-08T17:11:19.793403: step 9400, loss 0.00763771, acc 1

Evaluation:
2017-08-08T17:11:20.330877: step 9400, loss 3.2965, acc 0.714822

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-9400

2017-08-08T17:11:20.840412: step 9401, loss 0.0012636, acc 1
2017-08-08T17:11:21.191116: step 9402, loss 0.0065416, acc 1
2017-08-08T17:11:21.432751: step 9403, loss 0.000157909, acc 1
2017-08-08T17:11:21.654260: step 9404, loss 0.00468851, acc 1
2017-08-08T17:11:21.900398: step 9405, loss 0.000242377, acc 1
2017-08-08T17:11:22.104794: step 9406, loss 0.000117269, acc 1
2017-08-08T17:11:22.277765: step 9407, loss 2.10661e-06, acc 1
2017-08-08T17:11:22.473256: step 9408, loss 0.016513, acc 0.984375
2017-08-08T17:11:22.710157: step 9409, loss 1.34649e-05, acc 1
2017-08-08T17:11:23.067599: step 9410, loss 0.000405429, acc 1
2017-08-08T17:11:23.467124: step 9411, loss 4.91478e-05, acc 1
2017-08-08T17:11:23.780457: step 9412, loss 0.0547813, acc 0.984375
2017-08-08T17:11:24.010577: step 9413, loss 4.01095e-05, acc 1
2017-08-08T17:11:24.261630: step 9414, loss 0.000254178, acc 1
2017-08-08T17:11:24.758323: step 9415, loss 3.87034e-05, acc 1
2017-08-08T17:11:25.046045: step 9416, loss 0.000103583, acc 1
2017-08-08T17:11:25.256106: step 9417, loss 0.00391892, acc 1
2017-08-08T17:11:25.468510: step 9418, loss 0.000213401, acc 1
2017-08-08T17:11:25.885362: step 9419, loss 9.416e-06, acc 1
2017-08-08T17:11:26.255137: step 9420, loss 4.5084e-05, acc 1
2017-08-08T17:11:26.446254: step 9421, loss 0.000188138, acc 1
2017-08-08T17:11:26.642559: step 9422, loss 5.78767e-05, acc 1
2017-08-08T17:11:26.925342: step 9423, loss 0.000179242, acc 1
2017-08-08T17:11:27.172426: step 9424, loss 0.000340494, acc 1
2017-08-08T17:11:27.352544: step 9425, loss 0.00327696, acc 1
2017-08-08T17:11:27.533290: step 9426, loss 0.000555323, acc 1
2017-08-08T17:11:27.850185: step 9427, loss 0.00015067, acc 1
2017-08-08T17:11:28.244365: step 9428, loss 0.0135228, acc 0.984375
2017-08-08T17:11:28.580966: step 9429, loss 0.0230108, acc 0.984375
2017-08-08T17:11:28.834297: step 9430, loss 0.000542544, acc 1
2017-08-08T17:11:29.272744: step 9431, loss 0.000318603, acc 1
2017-08-08T17:11:29.537384: step 9432, loss 0.00015557, acc 1
2017-08-08T17:11:29.848221: step 9433, loss 1.41708e-05, acc 1
2017-08-08T17:11:30.111545: step 9434, loss 2.51079e-06, acc 1
2017-08-08T17:11:30.409502: step 9435, loss 0.00875188, acc 1
2017-08-08T17:11:30.721441: step 9436, loss 0.000152185, acc 1
2017-08-08T17:11:31.016362: step 9437, loss 4.97384e-05, acc 1
2017-08-08T17:11:31.256549: step 9438, loss 0.000656412, acc 1
2017-08-08T17:11:31.570809: step 9439, loss 0.00400399, acc 1
2017-08-08T17:11:31.986058: step 9440, loss 0.000114138, acc 1
2017-08-08T17:11:32.228788: step 9441, loss 0.000260521, acc 1
2017-08-08T17:11:32.505996: step 9442, loss 2.14436e-05, acc 1
2017-08-08T17:11:32.792376: step 9443, loss 0.0573131, acc 0.984375
2017-08-08T17:11:33.125382: step 9444, loss 0.000511876, acc 1
2017-08-08T17:11:33.437940: step 9445, loss 0.000325517, acc 1
2017-08-08T17:11:33.640673: step 9446, loss 0.000127218, acc 1
2017-08-08T17:11:33.937579: step 9447, loss 1.37557e-05, acc 1
2017-08-08T17:11:34.213732: step 9448, loss 5.79081e-05, acc 1
2017-08-08T17:11:34.466824: step 9449, loss 0.0016369, acc 1
2017-08-08T17:11:34.759928: step 9450, loss 0.0052811, acc 1
2017-08-08T17:11:35.031325: step 9451, loss 8.18347e-05, acc 1
2017-08-08T17:11:35.468244: step 9452, loss 0.000103489, acc 1
2017-08-08T17:11:35.863894: step 9453, loss 0.000364397, acc 1
2017-08-08T17:11:36.181556: step 9454, loss 0.000117037, acc 1
2017-08-08T17:11:36.486723: step 9455, loss 0.00012183, acc 1
2017-08-08T17:11:36.787459: step 9456, loss 3.57991e-06, acc 1
2017-08-08T17:11:36.999541: step 9457, loss 5.4691e-05, acc 1
2017-08-08T17:11:37.235849: step 9458, loss 2.17123e-05, acc 1
2017-08-08T17:11:37.457968: step 9459, loss 0.000128191, acc 1
2017-08-08T17:11:37.753095: step 9460, loss 0.000346839, acc 1
2017-08-08T17:11:38.002679: step 9461, loss 0.0627052, acc 0.984375
2017-08-08T17:11:38.285872: step 9462, loss 0.00203498, acc 1
2017-08-08T17:11:38.519958: step 9463, loss 0.000193891, acc 1
2017-08-08T17:11:38.772387: step 9464, loss 5.04972e-05, acc 1
2017-08-08T17:11:39.057592: step 9465, loss 6.27253e-05, acc 1
2017-08-08T17:11:39.319642: step 9466, loss 0.000225462, acc 1
2017-08-08T17:11:39.597391: step 9467, loss 7.08118e-06, acc 1
2017-08-08T17:11:39.907055: step 9468, loss 2.29239e-05, acc 1
2017-08-08T17:11:40.349865: step 9469, loss 0.000184123, acc 1
2017-08-08T17:11:40.837382: step 9470, loss 0.0013068, acc 1
2017-08-08T17:11:41.142079: step 9471, loss 7.07523e-05, acc 1
2017-08-08T17:11:41.385633: step 9472, loss 0.000348462, acc 1
2017-08-08T17:11:41.704508: step 9473, loss 0.000571971, acc 1
2017-08-08T17:11:41.944838: step 9474, loss 4.27467e-05, acc 1
2017-08-08T17:11:42.162602: step 9475, loss 0.000136697, acc 1
2017-08-08T17:11:42.360541: step 9476, loss 6.90211e-05, acc 1
2017-08-08T17:11:42.725404: step 9477, loss 0.000119367, acc 1
2017-08-08T17:11:43.022416: step 9478, loss 0.00123632, acc 1
2017-08-08T17:11:43.262404: step 9479, loss 0.000822549, acc 1
2017-08-08T17:11:43.472073: step 9480, loss 0.00552146, acc 1
2017-08-08T17:11:43.712805: step 9481, loss 0.000100338, acc 1
2017-08-08T17:11:44.043539: step 9482, loss 0.00216837, acc 1
2017-08-08T17:11:44.234163: step 9483, loss 0.012804, acc 0.984375
2017-08-08T17:11:44.447576: step 9484, loss 0.00436382, acc 1
2017-08-08T17:11:44.641614: step 9485, loss 3.24917e-05, acc 1
2017-08-08T17:11:44.883959: step 9486, loss 0.000460628, acc 1
2017-08-08T17:11:45.109191: step 9487, loss 0.000871883, acc 1
2017-08-08T17:11:45.349207: step 9488, loss 4.97673e-06, acc 1
2017-08-08T17:11:45.508725: step 9489, loss 0.0167947, acc 0.984375
2017-08-08T17:11:45.751539: step 9490, loss 0.00371505, acc 1
2017-08-08T17:11:45.956265: step 9491, loss 9.35663e-05, acc 1
2017-08-08T17:11:46.127053: step 9492, loss 6.18908e-05, acc 1
2017-08-08T17:11:46.320721: step 9493, loss 2.69633e-05, acc 1
2017-08-08T17:11:46.567861: step 9494, loss 2.46386e-05, acc 1
2017-08-08T17:11:46.823299: step 9495, loss 0.000450891, acc 1
2017-08-08T17:11:47.102699: step 9496, loss 0.00140481, acc 1
2017-08-08T17:11:47.326258: step 9497, loss 0.000493038, acc 1
2017-08-08T17:11:47.536179: step 9498, loss 4.14963e-06, acc 1
2017-08-08T17:11:47.808932: step 9499, loss 5.14283e-05, acc 1
2017-08-08T17:11:47.997378: step 9500, loss 0.0166801, acc 0.984375

Evaluation:
2017-08-08T17:11:48.565746: step 9500, loss 3.17632, acc 0.722326

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-9500

2017-08-08T17:11:49.220812: step 9501, loss 0.000543822, acc 1
2017-08-08T17:11:49.562885: step 9502, loss 0.000216944, acc 1
2017-08-08T17:11:49.812592: step 9503, loss 0.000258517, acc 1
2017-08-08T17:11:50.056309: step 9504, loss 0.000197496, acc 1
2017-08-08T17:11:50.478647: step 9505, loss 4.20389e-05, acc 1
2017-08-08T17:11:50.802719: step 9506, loss 0.000914608, acc 1
2017-08-08T17:11:51.098165: step 9507, loss 0.000191307, acc 1
2017-08-08T17:11:51.464582: step 9508, loss 0.000363267, acc 1
2017-08-08T17:11:51.881405: step 9509, loss 8.45087e-05, acc 1
2017-08-08T17:11:52.104493: step 9510, loss 0.0137418, acc 0.984375
2017-08-08T17:11:52.424293: step 9511, loss 0.000171176, acc 1
2017-08-08T17:11:52.617783: step 9512, loss 0.00647304, acc 1
2017-08-08T17:11:52.873283: step 9513, loss 2.24071e-05, acc 1
2017-08-08T17:11:53.133008: step 9514, loss 0.000200045, acc 1
2017-08-08T17:11:53.317315: step 9515, loss 2.61174e-05, acc 1
2017-08-08T17:11:53.533308: step 9516, loss 0.00734761, acc 1
2017-08-08T17:11:53.792189: step 9517, loss 0.00217486, acc 1
2017-08-08T17:11:54.045401: step 9518, loss 0.000306312, acc 1
2017-08-08T17:11:54.308276: step 9519, loss 0.042067, acc 0.984375
2017-08-08T17:11:54.573273: step 9520, loss 0.000396183, acc 1
2017-08-08T17:11:54.925051: step 9521, loss 0.000178307, acc 1
2017-08-08T17:11:55.114102: step 9522, loss 0.000764048, acc 1
2017-08-08T17:11:55.371653: step 9523, loss 0.000664395, acc 1
2017-08-08T17:11:55.617441: step 9524, loss 5.96177e-05, acc 1
2017-08-08T17:11:56.004755: step 9525, loss 0.0148506, acc 0.984375
2017-08-08T17:11:56.461375: step 9526, loss 0.00114709, acc 1
2017-08-08T17:11:56.827816: step 9527, loss 0.00587703, acc 1
2017-08-08T17:11:57.032047: step 9528, loss 6.46256e-05, acc 1
2017-08-08T17:11:57.347246: step 9529, loss 1.19572e-05, acc 1
2017-08-08T17:11:57.665841: step 9530, loss 0.000394688, acc 1
2017-08-08T17:11:57.859303: step 9531, loss 1.44401e-05, acc 1
2017-08-08T17:11:58.129331: step 9532, loss 0.000384732, acc 1
2017-08-08T17:11:58.517964: step 9533, loss 0.000444059, acc 1
2017-08-08T17:11:59.044219: step 9534, loss 0.000219459, acc 1
2017-08-08T17:11:59.425370: step 9535, loss 4.13473e-05, acc 1
2017-08-08T17:11:59.767632: step 9536, loss 0.000525393, acc 1
2017-08-08T17:12:00.043094: step 9537, loss 0.000403408, acc 1
2017-08-08T17:12:00.375844: step 9538, loss 0.0212693, acc 1
2017-08-08T17:12:00.732445: step 9539, loss 1.81035e-05, acc 1
2017-08-08T17:12:01.037463: step 9540, loss 0.000963, acc 1
2017-08-08T17:12:01.301554: step 9541, loss 0.000729495, acc 1
2017-08-08T17:12:01.636372: step 9542, loss 0.000991957, acc 1
2017-08-08T17:12:02.135680: step 9543, loss 0.00124687, acc 1
2017-08-08T17:12:02.503060: step 9544, loss 0.0293255, acc 0.984375
2017-08-08T17:12:02.805280: step 9545, loss 0.000832033, acc 1
2017-08-08T17:12:03.066302: step 9546, loss 0.000460759, acc 1
2017-08-08T17:12:03.512521: step 9547, loss 5.36028e-05, acc 1
2017-08-08T17:12:03.873252: step 9548, loss 0.000176588, acc 1
2017-08-08T17:12:04.198677: step 9549, loss 0.057993, acc 0.984375
2017-08-08T17:12:04.448551: step 9550, loss 0.000192044, acc 1
2017-08-08T17:12:04.781495: step 9551, loss 0.000451492, acc 1
2017-08-08T17:12:05.147769: step 9552, loss 0.000539421, acc 1
2017-08-08T17:12:05.496353: step 9553, loss 0.00102968, acc 1
2017-08-08T17:12:05.770706: step 9554, loss 0.000135718, acc 1
2017-08-08T17:12:05.997912: step 9555, loss 0.000715321, acc 1
2017-08-08T17:12:06.318145: step 9556, loss 0.000993939, acc 1
2017-08-08T17:12:06.688163: step 9557, loss 0.0443266, acc 0.984375
2017-08-08T17:12:06.928308: step 9558, loss 0.000180142, acc 1
2017-08-08T17:12:07.167632: step 9559, loss 0.00218602, acc 1
2017-08-08T17:12:07.577684: step 9560, loss 0.000160406, acc 1
2017-08-08T17:12:07.981734: step 9561, loss 9.27575e-07, acc 1
2017-08-08T17:12:08.270192: step 9562, loss 0.000676282, acc 1
2017-08-08T17:12:08.508712: step 9563, loss 0.0150923, acc 0.984375
2017-08-08T17:12:08.752593: step 9564, loss 0.00262344, acc 1
2017-08-08T17:12:09.101687: step 9565, loss 0.00109733, acc 1
2017-08-08T17:12:09.339642: step 9566, loss 0.000259897, acc 1
2017-08-08T17:12:09.583855: step 9567, loss 0.000325896, acc 1
2017-08-08T17:12:09.867476: step 9568, loss 0.000125585, acc 1
2017-08-08T17:12:10.287284: step 9569, loss 0.000477705, acc 1
2017-08-08T17:12:10.696389: step 9570, loss 0.000829512, acc 1
2017-08-08T17:12:11.082252: step 9571, loss 0.00376289, acc 1
2017-08-08T17:12:11.366377: step 9572, loss 7.35831e-06, acc 1
2017-08-08T17:12:11.620473: step 9573, loss 0.000978987, acc 1
2017-08-08T17:12:12.108559: step 9574, loss 0.00233185, acc 1
2017-08-08T17:12:12.374107: step 9575, loss 0.0431799, acc 0.984375
2017-08-08T17:12:12.669723: step 9576, loss 0.00269069, acc 1
2017-08-08T17:12:12.884997: step 9577, loss 0.00175353, acc 1
2017-08-08T17:12:13.235704: step 9578, loss 0.00978659, acc 1
2017-08-08T17:12:13.542750: step 9579, loss 0.000237572, acc 1
2017-08-08T17:12:13.864421: step 9580, loss 0.00041473, acc 1
2017-08-08T17:12:14.061731: step 9581, loss 0.00263393, acc 1
2017-08-08T17:12:14.233410: step 9582, loss 0.00195087, acc 1
2017-08-08T17:12:14.588537: step 9583, loss 0.00845758, acc 1
2017-08-08T17:12:14.824603: step 9584, loss 2.01196e-05, acc 1
2017-08-08T17:12:15.112639: step 9585, loss 0.00212246, acc 1
2017-08-08T17:12:15.511272: step 9586, loss 0.000562624, acc 1
2017-08-08T17:12:15.917121: step 9587, loss 0.00230773, acc 1
2017-08-08T17:12:16.281895: step 9588, loss 1.13714e-05, acc 1
2017-08-08T17:12:16.628762: step 9589, loss 0.000148507, acc 1
2017-08-08T17:12:16.875970: step 9590, loss 1.29026e-05, acc 1
2017-08-08T17:12:17.155903: step 9591, loss 4.59302e-06, acc 1
2017-08-08T17:12:17.413885: step 9592, loss 0.000662057, acc 1
2017-08-08T17:12:17.606358: step 9593, loss 0.00648147, acc 1
2017-08-08T17:12:17.847589: step 9594, loss 5.31019e-05, acc 1
2017-08-08T17:12:18.119960: step 9595, loss 8.23575e-05, acc 1
2017-08-08T17:12:18.398225: step 9596, loss 2.82435e-05, acc 1
2017-08-08T17:12:18.757381: step 9597, loss 8.12627e-05, acc 1
2017-08-08T17:12:19.005349: step 9598, loss 5.28704e-05, acc 1
2017-08-08T17:12:19.239893: step 9599, loss 0.000486044, acc 1
2017-08-08T17:12:19.409525: step 9600, loss 0.00406927, acc 1

Evaluation:
2017-08-08T17:12:20.023492: step 9600, loss 3.20972, acc 0.719512

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-9600

2017-08-08T17:12:20.415880: step 9601, loss 2.6338e-05, acc 1
2017-08-08T17:12:20.649374: step 9602, loss 0.000151703, acc 1
2017-08-08T17:12:20.955760: step 9603, loss 0.023624, acc 0.984375
2017-08-08T17:12:21.250331: step 9604, loss 1.46422e-05, acc 1
2017-08-08T17:12:21.553843: step 9605, loss 4.61889e-05, acc 1
2017-08-08T17:12:21.769796: step 9606, loss 3.0018e-05, acc 1
2017-08-08T17:12:22.113341: step 9607, loss 0.000153707, acc 1
2017-08-08T17:12:22.307681: step 9608, loss 0.000266677, acc 1
2017-08-08T17:12:22.500273: step 9609, loss 0.00100244, acc 1
2017-08-08T17:12:22.687340: step 9610, loss 0.000685912, acc 1
2017-08-08T17:12:22.916032: step 9611, loss 0.000257246, acc 1
2017-08-08T17:12:23.245501: step 9612, loss 0.000245045, acc 1
2017-08-08T17:12:23.545428: step 9613, loss 4.81265e-06, acc 1
2017-08-08T17:12:23.769639: step 9614, loss 0.000313448, acc 1
2017-08-08T17:12:24.027943: step 9615, loss 0.000548076, acc 1
2017-08-08T17:12:24.445333: step 9616, loss 3.66565e-05, acc 1
2017-08-08T17:12:24.735950: step 9617, loss 1.8955e-05, acc 1
2017-08-08T17:12:25.037779: step 9618, loss 0.0125606, acc 0.984375
2017-08-08T17:12:25.419399: step 9619, loss 7.8115e-05, acc 1
2017-08-08T17:12:25.845956: step 9620, loss 1.83153e-05, acc 1
2017-08-08T17:12:26.196197: step 9621, loss 0.000204059, acc 1
2017-08-08T17:12:26.499879: step 9622, loss 0.00115325, acc 1
2017-08-08T17:12:26.670275: step 9623, loss 0.000310769, acc 1
2017-08-08T17:12:26.957438: step 9624, loss 0.00520673, acc 1
2017-08-08T17:12:27.169320: step 9625, loss 0.000607163, acc 1
2017-08-08T17:12:27.347280: step 9626, loss 0.00105224, acc 1
2017-08-08T17:12:27.533972: step 9627, loss 0.000151089, acc 1
2017-08-08T17:12:27.854433: step 9628, loss 0.000223574, acc 1
2017-08-08T17:12:28.327181: step 9629, loss 1.02444e-06, acc 1
2017-08-08T17:12:28.678719: step 9630, loss 0.0012098, acc 1
2017-08-08T17:12:28.844122: step 9631, loss 0.00880461, acc 1
2017-08-08T17:12:29.034824: step 9632, loss 9.5313e-06, acc 1
2017-08-08T17:12:29.412778: step 9633, loss 2.25989e-05, acc 1
2017-08-08T17:12:29.694345: step 9634, loss 0.0940354, acc 0.96875
2017-08-08T17:12:30.019352: step 9635, loss 0.000226217, acc 1
2017-08-08T17:12:30.253446: step 9636, loss 0.000268862, acc 1
2017-08-08T17:12:30.587453: step 9637, loss 0.00115522, acc 1
2017-08-08T17:12:30.983335: step 9638, loss 5.46008e-05, acc 1
2017-08-08T17:12:31.315552: step 9639, loss 9.98347e-05, acc 1
2017-08-08T17:12:31.529184: step 9640, loss 0.000719855, acc 1
2017-08-08T17:12:31.797334: step 9641, loss 0.000799163, acc 1
2017-08-08T17:12:32.129588: step 9642, loss 0.0019366, acc 1
2017-08-08T17:12:32.336862: step 9643, loss 0.00781155, acc 1
2017-08-08T17:12:32.535903: step 9644, loss 0.00192356, acc 1
2017-08-08T17:12:32.813887: step 9645, loss 0.0510989, acc 0.96875
2017-08-08T17:12:33.191749: step 9646, loss 7.25077e-05, acc 1
2017-08-08T17:12:33.524616: step 9647, loss 0.000350566, acc 1
2017-08-08T17:12:33.839607: step 9648, loss 0.000305381, acc 1
2017-08-08T17:12:34.042680: step 9649, loss 3.6088e-05, acc 1
2017-08-08T17:12:34.324423: step 9650, loss 0.000185274, acc 1
2017-08-08T17:12:34.700425: step 9651, loss 0.00085565, acc 1
2017-08-08T17:12:34.996500: step 9652, loss 0.021819, acc 0.984375
2017-08-08T17:12:35.249193: step 9653, loss 0.000152737, acc 1
2017-08-08T17:12:35.572198: step 9654, loss 0.000231328, acc 1
2017-08-08T17:12:36.038195: step 9655, loss 0.000981328, acc 1
2017-08-08T17:12:36.427397: step 9656, loss 9.77392e-06, acc 1
2017-08-08T17:12:36.781217: step 9657, loss 1.51477e-05, acc 1
2017-08-08T17:12:37.035569: step 9658, loss 0.00608541, acc 1
2017-08-08T17:12:37.305974: step 9659, loss 8.84858e-05, acc 1
2017-08-08T17:12:37.657501: step 9660, loss 0.000108943, acc 1
2017-08-08T17:12:37.940484: step 9661, loss 0.0336627, acc 0.984375
2017-08-08T17:12:38.228700: step 9662, loss 9.5778e-05, acc 1
2017-08-08T17:12:38.495521: step 9663, loss 0.00158306, acc 1
2017-08-08T17:12:38.873598: step 9664, loss 9.94099e-05, acc 1
2017-08-08T17:12:39.217042: step 9665, loss 0.00067134, acc 1
2017-08-08T17:12:39.634153: step 9666, loss 1.38202e-05, acc 1
2017-08-08T17:12:39.953330: step 9667, loss 0.00289854, acc 1
2017-08-08T17:12:40.255714: step 9668, loss 6.77092e-05, acc 1
2017-08-08T17:12:40.693039: step 9669, loss 0.0011256, acc 1
2017-08-08T17:12:40.972495: step 9670, loss 0.000100093, acc 1
2017-08-08T17:12:41.232452: step 9671, loss 0.000191347, acc 1
2017-08-08T17:12:41.512683: step 9672, loss 4.53389e-05, acc 1
2017-08-08T17:12:41.896132: step 9673, loss 0.000201908, acc 1
2017-08-08T17:12:42.215456: step 9674, loss 1.9387e-05, acc 1
2017-08-08T17:12:42.603432: step 9675, loss 2.47161e-06, acc 1
2017-08-08T17:12:42.881172: step 9676, loss 0.0483045, acc 0.984375
2017-08-08T17:12:43.216051: step 9677, loss 7.40972e-05, acc 1
2017-08-08T17:12:43.447017: step 9678, loss 2.9466e-06, acc 1
2017-08-08T17:12:43.667026: step 9679, loss 0.00194235, acc 1
2017-08-08T17:12:43.927073: step 9680, loss 0.000481422, acc 1
2017-08-08T17:12:44.193339: step 9681, loss 1.34217e-05, acc 1
2017-08-08T17:12:44.569159: step 9682, loss 7.83184e-05, acc 1
2017-08-08T17:12:44.996003: step 9683, loss 0.00142362, acc 1
2017-08-08T17:12:45.361153: step 9684, loss 0.000138655, acc 1
2017-08-08T17:12:45.571226: step 9685, loss 6.41904e-05, acc 1
2017-08-08T17:12:45.770337: step 9686, loss 0.000543324, acc 1
2017-08-08T17:12:46.198643: step 9687, loss 0.000531464, acc 1
2017-08-08T17:12:46.484259: step 9688, loss 0.001163, acc 1
2017-08-08T17:12:46.775737: step 9689, loss 4.22216e-05, acc 1
2017-08-08T17:12:47.139567: step 9690, loss 0.0515977, acc 0.984375
2017-08-08T17:12:47.494348: step 9691, loss 0.000579879, acc 1
2017-08-08T17:12:47.765738: step 9692, loss 0.000169422, acc 1
2017-08-08T17:12:47.999306: step 9693, loss 7.60566e-05, acc 1
2017-08-08T17:12:48.210369: step 9694, loss 0.0360729, acc 0.984375
2017-08-08T17:12:48.553401: step 9695, loss 0.00116084, acc 1
2017-08-08T17:12:48.743859: step 9696, loss 1.6812e-05, acc 1
2017-08-08T17:12:48.946158: step 9697, loss 0.00020409, acc 1
2017-08-08T17:12:49.151309: step 9698, loss 4.74645e-05, acc 1
2017-08-08T17:12:49.448871: step 9699, loss 0.000804245, acc 1
2017-08-08T17:12:49.734866: step 9700, loss 0.000200327, acc 1

Evaluation:
2017-08-08T17:12:50.330201: step 9700, loss 3.24554, acc 0.726079

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-9700

2017-08-08T17:12:50.805365: step 9701, loss 0.00100911, acc 1
2017-08-08T17:12:51.055479: step 9702, loss 6.37336e-05, acc 1
2017-08-08T17:12:51.253609: step 9703, loss 0.000342743, acc 1
2017-08-08T17:12:51.479620: step 9704, loss 0.0307439, acc 0.984375
2017-08-08T17:12:51.800914: step 9705, loss 0.00226265, acc 1
2017-08-08T17:12:52.076115: step 9706, loss 0.000133306, acc 1
2017-08-08T17:12:52.395988: step 9707, loss 7.89767e-05, acc 1
2017-08-08T17:12:52.594044: step 9708, loss 0.00265965, acc 1
2017-08-08T17:12:52.777396: step 9709, loss 3.4504e-05, acc 1
2017-08-08T17:12:53.062427: step 9710, loss 5.1089e-05, acc 1
2017-08-08T17:12:53.257787: step 9711, loss 0.00324794, acc 1
2017-08-08T17:12:53.494832: step 9712, loss 0.000229498, acc 1
2017-08-08T17:12:53.721199: step 9713, loss 9.1279e-05, acc 1
2017-08-08T17:12:54.067883: step 9714, loss 0.00698083, acc 1
2017-08-08T17:12:54.385367: step 9715, loss 0.00078002, acc 1
2017-08-08T17:12:54.652322: step 9716, loss 0.000217572, acc 1
2017-08-08T17:12:54.840784: step 9717, loss 0.0001241, acc 1
2017-08-08T17:12:55.187294: step 9718, loss 1.58323e-06, acc 1
2017-08-08T17:12:55.398463: step 9719, loss 0.00041024, acc 1
2017-08-08T17:12:55.603747: step 9720, loss 0.000836703, acc 1
2017-08-08T17:12:55.955311: step 9721, loss 6.58505e-06, acc 1
2017-08-08T17:12:56.302005: step 9722, loss 0.00937812, acc 1
2017-08-08T17:12:56.573387: step 9723, loss 0.00385574, acc 1
2017-08-08T17:12:56.859412: step 9724, loss 7.83728e-05, acc 1
2017-08-08T17:12:57.057480: step 9725, loss 5.74737e-06, acc 1
2017-08-08T17:12:57.337498: step 9726, loss 0.000304881, acc 1
2017-08-08T17:12:57.538123: step 9727, loss 0.00110693, acc 1
2017-08-08T17:12:57.720445: step 9728, loss 1.64961e-05, acc 1
2017-08-08T17:12:57.928182: step 9729, loss 1.39004e-05, acc 1
2017-08-08T17:12:58.144812: step 9730, loss 0.00142875, acc 1
2017-08-08T17:12:58.574043: step 9731, loss 0.00197053, acc 1
2017-08-08T17:12:58.994962: step 9732, loss 2.0171e-05, acc 1
2017-08-08T17:12:59.306065: step 9733, loss 0.000813804, acc 1
2017-08-08T17:12:59.566156: step 9734, loss 0.00080266, acc 1
2017-08-08T17:13:00.031027: step 9735, loss 0.000403983, acc 1
2017-08-08T17:13:00.375049: step 9736, loss 7.04021e-06, acc 1
2017-08-08T17:13:00.667356: step 9737, loss 0.00533286, acc 1
2017-08-08T17:13:00.945330: step 9738, loss 4.21481e-06, acc 1
2017-08-08T17:13:01.420426: step 9739, loss 0.000554063, acc 1
2017-08-08T17:13:01.872054: step 9740, loss 0.0048106, acc 1
2017-08-08T17:13:02.301527: step 9741, loss 0.0407071, acc 0.984375
2017-08-08T17:13:02.599767: step 9742, loss 0.00639323, acc 1
2017-08-08T17:13:02.862972: step 9743, loss 0.0362532, acc 0.984375
2017-08-08T17:13:03.344839: step 9744, loss 0.000781042, acc 1
2017-08-08T17:13:03.643972: step 9745, loss 4.03056e-06, acc 1
2017-08-08T17:13:03.920816: step 9746, loss 6.33911e-05, acc 1
2017-08-08T17:13:04.186066: step 9747, loss 0.000881651, acc 1
2017-08-08T17:13:04.498344: step 9748, loss 0.000201527, acc 1
2017-08-08T17:13:04.978676: step 9749, loss 0.000869764, acc 1
2017-08-08T17:13:05.347925: step 9750, loss 0.00186928, acc 1
2017-08-08T17:13:05.767116: step 9751, loss 6.40047e-05, acc 1
2017-08-08T17:13:06.030913: step 9752, loss 8.54589e-05, acc 1
2017-08-08T17:13:06.309368: step 9753, loss 4.05329e-05, acc 1
2017-08-08T17:13:06.698329: step 9754, loss 0.000122315, acc 1
2017-08-08T17:13:06.933937: step 9755, loss 0.00466923, acc 1
2017-08-08T17:13:07.231413: step 9756, loss 5.58837e-05, acc 1
2017-08-08T17:13:07.474097: step 9757, loss 3.90632e-05, acc 1
2017-08-08T17:13:07.857324: step 9758, loss 3.31446e-05, acc 1
2017-08-08T17:13:08.139491: step 9759, loss 0.000122873, acc 1
2017-08-08T17:13:08.392386: step 9760, loss 0.000180286, acc 1
2017-08-08T17:13:08.612367: step 9761, loss 0.000201294, acc 1
2017-08-08T17:13:08.964766: step 9762, loss 0.000809945, acc 1
2017-08-08T17:13:09.319674: step 9763, loss 1.99668e-05, acc 1
2017-08-08T17:13:09.533757: step 9764, loss 3.03147e-05, acc 1
2017-08-08T17:13:09.744730: step 9765, loss 0.000115439, acc 1
2017-08-08T17:13:09.981319: step 9766, loss 0.000207216, acc 1
2017-08-08T17:13:10.353400: step 9767, loss 4.52792e-06, acc 1
2017-08-08T17:13:10.735032: step 9768, loss 3.83856e-05, acc 1
2017-08-08T17:13:11.144272: step 9769, loss 3.81349e-05, acc 1
2017-08-08T17:13:11.408625: step 9770, loss 0.000120292, acc 1
2017-08-08T17:13:11.680941: step 9771, loss 0.00188842, acc 1
2017-08-08T17:13:12.041356: step 9772, loss 0.00134524, acc 1
2017-08-08T17:13:12.336418: step 9773, loss 3.43006e-05, acc 1
2017-08-08T17:13:12.629673: step 9774, loss 0.000175743, acc 1
2017-08-08T17:13:12.912028: step 9775, loss 0.000306628, acc 1
2017-08-08T17:13:13.224633: step 9776, loss 2.58526e-06, acc 1
2017-08-08T17:13:13.664210: step 9777, loss 6.1223e-06, acc 1
2017-08-08T17:13:14.009607: step 9778, loss 2.98573e-06, acc 1
2017-08-08T17:13:14.281108: step 9779, loss 0.000203734, acc 1
2017-08-08T17:13:14.523043: step 9780, loss 0.000215728, acc 1
2017-08-08T17:13:14.766643: step 9781, loss 0.000664892, acc 1
2017-08-08T17:13:15.153573: step 9782, loss 1.09352e-05, acc 1
2017-08-08T17:13:15.431368: step 9783, loss 0.000204648, acc 1
2017-08-08T17:13:15.682101: step 9784, loss 0.00130477, acc 1
2017-08-08T17:13:15.971354: step 9785, loss 8.63717e-05, acc 1
2017-08-08T17:13:16.342149: step 9786, loss 0.000133411, acc 1
2017-08-08T17:13:16.753880: step 9787, loss 3.95515e-05, acc 1
2017-08-08T17:13:17.187589: step 9788, loss 0.00146914, acc 1
2017-08-08T17:13:17.457768: step 9789, loss 0.00409504, acc 1
2017-08-08T17:13:17.737388: step 9790, loss 0.00309942, acc 1
2017-08-08T17:13:18.140020: step 9791, loss 0.000135259, acc 1
2017-08-08T17:13:18.356927: step 9792, loss 1.56547e-05, acc 1
2017-08-08T17:13:18.613618: step 9793, loss 1.03189e-06, acc 1
2017-08-08T17:13:18.846712: step 9794, loss 0.000176244, acc 1
2017-08-08T17:13:19.309009: step 9795, loss 8.05185e-05, acc 1
2017-08-08T17:13:19.652740: step 9796, loss 0.000615589, acc 1
2017-08-08T17:13:20.051988: step 9797, loss 0.000207005, acc 1
2017-08-08T17:13:20.340527: step 9798, loss 3.25809e-05, acc 1
2017-08-08T17:13:20.582826: step 9799, loss 0.000503055, acc 1
2017-08-08T17:13:20.943182: step 9800, loss 0.000712798, acc 1

Evaluation:
2017-08-08T17:13:21.519868: step 9800, loss 3.29311, acc 0.719512

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-9800

2017-08-08T17:13:22.103020: step 9801, loss 1.97744e-05, acc 1
2017-08-08T17:13:22.553910: step 9802, loss 0.000107876, acc 1
2017-08-08T17:13:22.905096: step 9803, loss 0.000108578, acc 1
2017-08-08T17:13:23.155522: step 9804, loss 8.55566e-05, acc 1
2017-08-08T17:13:23.372857: step 9805, loss 0.000235467, acc 1
2017-08-08T17:13:23.755126: step 9806, loss 0.0302883, acc 0.984375
2017-08-08T17:13:24.020578: step 9807, loss 0.00157991, acc 1
2017-08-08T17:13:24.304625: step 9808, loss 0.00065185, acc 1
2017-08-08T17:13:24.600195: step 9809, loss 4.99765e-05, acc 1
2017-08-08T17:13:25.027633: step 9810, loss 7.91905e-05, acc 1
2017-08-08T17:13:25.389500: step 9811, loss 0.00314645, acc 1
2017-08-08T17:13:25.662952: step 9812, loss 0.000358704, acc 1
2017-08-08T17:13:25.917670: step 9813, loss 3.49236e-06, acc 1
2017-08-08T17:13:26.142851: step 9814, loss 0.000117055, acc 1
2017-08-08T17:13:26.397571: step 9815, loss 0.000144461, acc 1
2017-08-08T17:13:26.749278: step 9816, loss 0.000365974, acc 1
2017-08-08T17:13:26.945587: step 9817, loss 0.000142386, acc 1
2017-08-08T17:13:27.195454: step 9818, loss 0.000530547, acc 1
2017-08-08T17:13:27.458253: step 9819, loss 2.51616e-05, acc 1
2017-08-08T17:13:27.809361: step 9820, loss 1.51618e-05, acc 1
2017-08-08T17:13:28.136471: step 9821, loss 1.63271e-05, acc 1
2017-08-08T17:13:28.455695: step 9822, loss 0.0364877, acc 0.984375
2017-08-08T17:13:28.660795: step 9823, loss 1.09236e-05, acc 1
2017-08-08T17:13:28.957332: step 9824, loss 0.000512071, acc 1
2017-08-08T17:13:29.223916: step 9825, loss 0.000645143, acc 1
2017-08-08T17:13:29.488836: step 9826, loss 0.00175013, acc 1
2017-08-08T17:13:29.701969: step 9827, loss 2.742e-05, acc 1
2017-08-08T17:13:30.028482: step 9828, loss 0.000942817, acc 1
2017-08-08T17:13:30.475051: step 9829, loss 0.000238673, acc 1
2017-08-08T17:13:30.812778: step 9830, loss 6.67366e-05, acc 1
2017-08-08T17:13:31.110978: step 9831, loss 7.23014e-06, acc 1
2017-08-08T17:13:31.352116: step 9832, loss 0.002402, acc 1
2017-08-08T17:13:31.705364: step 9833, loss 0.00195468, acc 1
2017-08-08T17:13:31.946341: step 9834, loss 7.07552e-06, acc 1
2017-08-08T17:13:32.278214: step 9835, loss 0.00157229, acc 1
2017-08-08T17:13:32.521441: step 9836, loss 0.000256938, acc 1
2017-08-08T17:13:32.950258: step 9837, loss 0.00303396, acc 1
2017-08-08T17:13:33.304337: step 9838, loss 0.000333997, acc 1
2017-08-08T17:13:33.619521: step 9839, loss 9.06013e-05, acc 1
2017-08-08T17:13:33.811994: step 9840, loss 3.29544e-05, acc 1
2017-08-08T17:13:34.185763: step 9841, loss 0.000104523, acc 1
2017-08-08T17:13:34.431334: step 9842, loss 0.00227074, acc 1
2017-08-08T17:13:34.701050: step 9843, loss 0.000374388, acc 1
2017-08-08T17:13:34.989379: step 9844, loss 0.000131303, acc 1
2017-08-08T17:13:35.358354: step 9845, loss 0.000204355, acc 1
2017-08-08T17:13:35.714259: step 9846, loss 2.01329e-05, acc 1
2017-08-08T17:13:35.965482: step 9847, loss 9.30855e-05, acc 1
2017-08-08T17:13:36.148176: step 9848, loss 0.0137301, acc 0.984375
2017-08-08T17:13:36.370794: step 9849, loss 3.28807e-05, acc 1
2017-08-08T17:13:36.788388: step 9850, loss 7.9268e-05, acc 1
2017-08-08T17:13:37.066382: step 9851, loss 0.000106735, acc 1
2017-08-08T17:13:37.356589: step 9852, loss 0.00101491, acc 1
2017-08-08T17:13:37.639148: step 9853, loss 0.000369672, acc 1
2017-08-08T17:13:38.062051: step 9854, loss 0.00115689, acc 1
2017-08-08T17:13:38.426674: step 9855, loss 0.000338168, acc 1
2017-08-08T17:13:38.760484: step 9856, loss 0.000223394, acc 1
2017-08-08T17:13:39.026726: step 9857, loss 0.000240912, acc 1
2017-08-08T17:13:39.271703: step 9858, loss 3.35897e-05, acc 1
2017-08-08T17:13:39.726838: step 9859, loss 0.00654276, acc 1
2017-08-08T17:13:40.026137: step 9860, loss 0.0015505, acc 1
2017-08-08T17:13:40.253372: step 9861, loss 0.00124513, acc 1
2017-08-08T17:13:40.489962: step 9862, loss 0.0647265, acc 0.984375
2017-08-08T17:13:40.769626: step 9863, loss 0.000477783, acc 1
2017-08-08T17:13:41.058020: step 9864, loss 0.00439308, acc 1
2017-08-08T17:13:41.321202: step 9865, loss 0.00399148, acc 1
2017-08-08T17:13:41.563709: step 9866, loss 0.000146765, acc 1
2017-08-08T17:13:41.764990: step 9867, loss 0.0123321, acc 0.984375
2017-08-08T17:13:42.018523: step 9868, loss 0.00450314, acc 1
2017-08-08T17:13:42.237944: step 9869, loss 8.39065e-05, acc 1
2017-08-08T17:13:42.412046: step 9870, loss 0.00111235, acc 1
2017-08-08T17:13:42.619035: step 9871, loss 0.000162787, acc 1
2017-08-08T17:13:42.957387: step 9872, loss 4.31576e-05, acc 1
2017-08-08T17:13:43.211975: step 9873, loss 0.000347042, acc 1
2017-08-08T17:13:43.418643: step 9874, loss 0.000192628, acc 1
2017-08-08T17:13:43.573922: step 9875, loss 0.000223536, acc 1
2017-08-08T17:13:43.736938: step 9876, loss 0.00423244, acc 1
2017-08-08T17:13:44.031107: step 9877, loss 1.89014e-05, acc 1
2017-08-08T17:13:44.184921: step 9878, loss 0.000413837, acc 1
2017-08-08T17:13:44.379327: step 9879, loss 3.17217e-05, acc 1
2017-08-08T17:13:44.639500: step 9880, loss 0.00161799, acc 1
2017-08-08T17:13:44.931837: step 9881, loss 2.12869e-05, acc 1
2017-08-08T17:13:45.225893: step 9882, loss 0.0492763, acc 0.984375
2017-08-08T17:13:45.579202: step 9883, loss 0.00372536, acc 1
2017-08-08T17:13:45.800920: step 9884, loss 0.000644219, acc 1
2017-08-08T17:13:46.057887: step 9885, loss 0.00070737, acc 1
2017-08-08T17:13:46.449310: step 9886, loss 0.000347649, acc 1
2017-08-08T17:13:46.794829: step 9887, loss 1.47729e-05, acc 1
2017-08-08T17:13:47.101401: step 9888, loss 0.000398296, acc 1
2017-08-08T17:13:47.368576: step 9889, loss 7.36739e-05, acc 1
2017-08-08T17:13:47.667497: step 9890, loss 0.00495338, acc 1
2017-08-08T17:13:47.983782: step 9891, loss 0.000241277, acc 1
2017-08-08T17:13:48.333378: step 9892, loss 0.000307427, acc 1
2017-08-08T17:13:48.583916: step 9893, loss 0.000636531, acc 1
2017-08-08T17:13:48.810925: step 9894, loss 0.000376414, acc 1
2017-08-08T17:13:49.165398: step 9895, loss 9.22208e-05, acc 1
2017-08-08T17:13:49.452589: step 9896, loss 4.21327e-05, acc 1
2017-08-08T17:13:49.757830: step 9897, loss 0.00028987, acc 1
2017-08-08T17:13:50.010214: step 9898, loss 2.42062e-05, acc 1
2017-08-08T17:13:50.329377: step 9899, loss 0.000238815, acc 1
2017-08-08T17:13:50.667317: step 9900, loss 0.000432532, acc 1

Evaluation:
2017-08-08T17:13:51.264587: step 9900, loss 3.40152, acc 0.714822

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-9900

2017-08-08T17:13:51.833591: step 9901, loss 1.44616e-05, acc 1
2017-08-08T17:13:52.071249: step 9902, loss 0.000731673, acc 1
2017-08-08T17:13:52.311640: step 9903, loss 0.000827296, acc 1
2017-08-08T17:13:52.532868: step 9904, loss 3.99255e-05, acc 1
2017-08-08T17:13:52.902825: step 9905, loss 6.8162e-06, acc 1
2017-08-08T17:13:53.257341: step 9906, loss 0.00015943, acc 1
2017-08-08T17:13:53.497369: step 9907, loss 0.000925964, acc 1
2017-08-08T17:13:53.767452: step 9908, loss 0.0221399, acc 0.984375
2017-08-08T17:13:53.955411: step 9909, loss 1.41707e-05, acc 1
2017-08-08T17:13:54.358028: step 9910, loss 0.00047553, acc 1
2017-08-08T17:13:54.626621: step 9911, loss 2.72097e-05, acc 1
2017-08-08T17:13:54.838488: step 9912, loss 5.52022e-06, acc 1
2017-08-08T17:13:55.012231: step 9913, loss 3.32953e-05, acc 1
2017-08-08T17:13:55.313123: step 9914, loss 0.000455747, acc 1
2017-08-08T17:13:55.569334: step 9915, loss 2.88054e-05, acc 1
2017-08-08T17:13:55.780276: step 9916, loss 8.11241e-06, acc 1
2017-08-08T17:13:55.969332: step 9917, loss 5.42294e-05, acc 1
2017-08-08T17:13:56.216248: step 9918, loss 7.84956e-05, acc 1
2017-08-08T17:13:56.407968: step 9919, loss 0.000374086, acc 1
2017-08-08T17:13:56.586852: step 9920, loss 8.14671e-05, acc 1
2017-08-08T17:13:56.865212: step 9921, loss 1.46589e-06, acc 1
2017-08-08T17:13:57.251460: step 9922, loss 0.000307048, acc 1
2017-08-08T17:13:57.544406: step 9923, loss 1.15617e-05, acc 1
2017-08-08T17:13:57.798757: step 9924, loss 0.000206921, acc 1
2017-08-08T17:13:57.999519: step 9925, loss 0.000159996, acc 1
2017-08-08T17:13:58.321724: step 9926, loss 2.92423e-06, acc 1
2017-08-08T17:13:58.555322: step 9927, loss 0.00227074, acc 1
2017-08-08T17:13:58.804358: step 9928, loss 2.63462e-05, acc 1
2017-08-08T17:13:59.150876: step 9929, loss 0.0016794, acc 1
2017-08-08T17:13:59.511542: step 9930, loss 0.000771015, acc 1
2017-08-08T17:13:59.878184: step 9931, loss 0.000573682, acc 1
2017-08-08T17:14:00.127838: step 9932, loss 2.23141e-06, acc 1
2017-08-08T17:14:00.409387: step 9933, loss 2.00488e-05, acc 1
2017-08-08T17:14:00.826747: step 9934, loss 0.000111793, acc 1
2017-08-08T17:14:01.041816: step 9935, loss 0.000249445, acc 1
2017-08-08T17:14:01.323236: step 9936, loss 5.17406e-06, acc 1
2017-08-08T17:14:01.573739: step 9937, loss 0.00688318, acc 1
2017-08-08T17:14:02.233636: step 9938, loss 0.00137731, acc 1
2017-08-08T17:14:02.655839: step 9939, loss 2.82805e-05, acc 1
2017-08-08T17:14:03.069794: step 9940, loss 9.24483e-05, acc 1
2017-08-08T17:14:03.341225: step 9941, loss 9.74773e-06, acc 1
2017-08-08T17:14:03.743762: step 9942, loss 8.6197e-06, acc 1
2017-08-08T17:14:04.102537: step 9943, loss 0.000431371, acc 1
2017-08-08T17:14:04.364019: step 9944, loss 0.00584951, acc 1
2017-08-08T17:14:04.674178: step 9945, loss 0.0752873, acc 0.984375
2017-08-08T17:14:04.934334: step 9946, loss 3.5872e-05, acc 1
2017-08-08T17:14:05.370136: step 9947, loss 0.00129593, acc 1
2017-08-08T17:14:05.714943: step 9948, loss 0.000380169, acc 1
2017-08-08T17:14:06.120623: step 9949, loss 4.03994e-05, acc 1
2017-08-08T17:14:06.393815: step 9950, loss 3.09003e-05, acc 1
2017-08-08T17:14:06.661624: step 9951, loss 0.000220773, acc 1
2017-08-08T17:14:07.050470: step 9952, loss 0.000779196, acc 1
2017-08-08T17:14:07.283982: step 9953, loss 0.000916677, acc 1
2017-08-08T17:14:07.553898: step 9954, loss 0.001158, acc 1
2017-08-08T17:14:07.825396: step 9955, loss 0.00144852, acc 1
2017-08-08T17:14:08.101359: step 9956, loss 0.0253103, acc 1
2017-08-08T17:14:08.497136: step 9957, loss 0.00258163, acc 1
2017-08-08T17:14:08.951146: step 9958, loss 0.000301136, acc 1
2017-08-08T17:14:09.280211: step 9959, loss 1.3936e-05, acc 1
2017-08-08T17:14:09.574371: step 9960, loss 0.00121783, acc 1
2017-08-08T17:14:09.938943: step 9961, loss 0.000718912, acc 1
2017-08-08T17:14:10.305366: step 9962, loss 0.00729058, acc 1
2017-08-08T17:14:10.567277: step 9963, loss 1.68223e-05, acc 1
2017-08-08T17:14:10.922839: step 9964, loss 0.000120069, acc 1
2017-08-08T17:14:11.325507: step 9965, loss 0.000912846, acc 1
2017-08-08T17:14:11.664366: step 9966, loss 6.92238e-06, acc 1
2017-08-08T17:14:12.046855: step 9967, loss 0.000562853, acc 1
2017-08-08T17:14:12.286198: step 9968, loss 0.000100619, acc 1
2017-08-08T17:14:12.597992: step 9969, loss 9.93975e-05, acc 1
2017-08-08T17:14:12.955148: step 9970, loss 0.088573, acc 0.984375
2017-08-08T17:14:13.228067: step 9971, loss 1.43235e-06, acc 1
2017-08-08T17:14:13.465398: step 9972, loss 0.00147165, acc 1
2017-08-08T17:14:13.790547: step 9973, loss 9.17163e-05, acc 1
2017-08-08T17:14:14.229750: step 9974, loss 1.21029e-05, acc 1
2017-08-08T17:14:14.677395: step 9975, loss 0.00011048, acc 1
2017-08-08T17:14:15.026341: step 9976, loss 5.20391e-05, acc 1
2017-08-08T17:14:15.229315: step 9977, loss 8.32167e-05, acc 1
2017-08-08T17:14:15.613685: step 9978, loss 0.000113197, acc 1
2017-08-08T17:14:15.885262: step 9979, loss 0.000126236, acc 1
2017-08-08T17:14:16.192985: step 9980, loss 0.000861521, acc 1
2017-08-08T17:14:16.443612: step 9981, loss 0.0501243, acc 0.984375
2017-08-08T17:14:16.929357: step 9982, loss 2.56093e-05, acc 1
2017-08-08T17:14:17.341368: step 9983, loss 4.49418e-05, acc 1
2017-08-08T17:14:17.681793: step 9984, loss 5.14051e-05, acc 1
2017-08-08T17:14:17.940901: step 9985, loss 0.00112326, acc 1
2017-08-08T17:14:18.359816: step 9986, loss 0.000182452, acc 1
2017-08-08T17:14:18.704380: step 9987, loss 0.000926924, acc 1
2017-08-08T17:14:18.979488: step 9988, loss 9.17626e-06, acc 1
2017-08-08T17:14:19.276798: step 9989, loss 3.17374e-06, acc 1
2017-08-08T17:14:19.636145: step 9990, loss 1.81656e-05, acc 1
2017-08-08T17:14:19.997920: step 9991, loss 0.00139189, acc 1
2017-08-08T17:14:20.355429: step 9992, loss 0.000113035, acc 1
2017-08-08T17:14:20.633958: step 9993, loss 0.00018389, acc 1
2017-08-08T17:14:20.877396: step 9994, loss 0.163567, acc 0.984375
2017-08-08T17:14:21.207772: step 9995, loss 0.000455983, acc 1
2017-08-08T17:14:21.607998: step 9996, loss 0.000346129, acc 1
2017-08-08T17:14:21.900484: step 9997, loss 3.87033e-06, acc 1
2017-08-08T17:14:22.167844: step 9998, loss 0.00118314, acc 1
2017-08-08T17:14:22.577363: step 9999, loss 0.00030244, acc 1
2017-08-08T17:14:23.017379: step 10000, loss 4.11624e-05, acc 1

Evaluation:
2017-08-08T17:14:23.782752: step 10000, loss 3.33061, acc 0.718574

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-10000

2017-08-08T17:14:24.444288: step 10001, loss 0.000218311, acc 1
2017-08-08T17:14:24.715108: step 10002, loss 0.0957209, acc 0.984375
2017-08-08T17:14:24.965137: step 10003, loss 3.82637e-05, acc 1
2017-08-08T17:14:25.222113: step 10004, loss 0.000285976, acc 1
2017-08-08T17:14:25.633591: step 10005, loss 6.39201e-06, acc 1
2017-08-08T17:14:26.055313: step 10006, loss 0.00231004, acc 1
2017-08-08T17:14:26.392823: step 10007, loss 0.000200327, acc 1
2017-08-08T17:14:26.641562: step 10008, loss 1.0802e-05, acc 1
2017-08-08T17:14:26.859838: step 10009, loss 4.36554e-05, acc 1
2017-08-08T17:14:27.304666: step 10010, loss 0.00022897, acc 1
2017-08-08T17:14:27.517607: step 10011, loss 0.00155253, acc 1
2017-08-08T17:14:27.781051: step 10012, loss 0.000140768, acc 1
2017-08-08T17:14:28.051244: step 10013, loss 0.00497921, acc 1
2017-08-08T17:14:28.509417: step 10014, loss 8.87661e-06, acc 1
2017-08-08T17:14:28.953220: step 10015, loss 7.50708e-05, acc 1
2017-08-08T17:14:29.285659: step 10016, loss 0.00156397, acc 1
2017-08-08T17:14:29.525691: step 10017, loss 1.33362e-06, acc 1
2017-08-08T17:14:29.833495: step 10018, loss 0.000388676, acc 1
2017-08-08T17:14:30.161575: step 10019, loss 0.000182722, acc 1
2017-08-08T17:14:30.412964: step 10020, loss 1.47865e-05, acc 1
2017-08-08T17:14:30.712294: step 10021, loss 9.56046e-05, acc 1
2017-08-08T17:14:31.017369: step 10022, loss 0.00054207, acc 1
2017-08-08T17:14:31.464425: step 10023, loss 0.000275724, acc 1
2017-08-08T17:14:31.836767: step 10024, loss 3.19806e-06, acc 1
2017-08-08T17:14:32.200786: step 10025, loss 0.00123231, acc 1
2017-08-08T17:14:32.425731: step 10026, loss 0.00341885, acc 1
2017-08-08T17:14:32.821762: step 10027, loss 0.000274464, acc 1
2017-08-08T17:14:33.171828: step 10028, loss 0.000845045, acc 1
2017-08-08T17:14:33.409254: step 10029, loss 0.00101882, acc 1
2017-08-08T17:14:33.674253: step 10030, loss 0.00019691, acc 1
2017-08-08T17:14:34.075522: step 10031, loss 0.000206123, acc 1
2017-08-08T17:14:34.535767: step 10032, loss 2.6077e-07, acc 1
2017-08-08T17:14:34.874663: step 10033, loss 0.00145411, acc 1
2017-08-08T17:14:35.112769: step 10034, loss 0.00088594, acc 1
2017-08-08T17:14:35.512220: step 10035, loss 7.92774e-06, acc 1
2017-08-08T17:14:35.851872: step 10036, loss 0.000447052, acc 1
2017-08-08T17:14:36.155312: step 10037, loss 2.73812e-05, acc 1
2017-08-08T17:14:36.410170: step 10038, loss 4.20568e-05, acc 1
2017-08-08T17:14:36.832625: step 10039, loss 4.90314e-05, acc 1
2017-08-08T17:14:37.269723: step 10040, loss 7.89438e-05, acc 1
2017-08-08T17:14:37.665398: step 10041, loss 0.000149537, acc 1
2017-08-08T17:14:38.021836: step 10042, loss 2.31034e-05, acc 1
2017-08-08T17:14:38.403060: step 10043, loss 0.000288227, acc 1
2017-08-08T17:14:38.736092: step 10044, loss 7.65512e-05, acc 1
2017-08-08T17:14:39.064944: step 10045, loss 0.000499375, acc 1
2017-08-08T17:14:39.366161: step 10046, loss 0.000437449, acc 1
2017-08-08T17:14:39.657335: step 10047, loss 0.00012035, acc 1
2017-08-08T17:14:39.988692: step 10048, loss 0.000151571, acc 1
2017-08-08T17:14:40.229524: step 10049, loss 0.000219828, acc 1
2017-08-08T17:14:40.429664: step 10050, loss 0.000162637, acc 1
2017-08-08T17:14:40.815720: step 10051, loss 2.31543e-05, acc 1
2017-08-08T17:14:41.015762: step 10052, loss 7.22464e-05, acc 1
2017-08-08T17:14:41.243982: step 10053, loss 0.000435828, acc 1
2017-08-08T17:14:41.518355: step 10054, loss 0.00137457, acc 1
2017-08-08T17:14:41.880455: step 10055, loss 3.4445e-05, acc 1
2017-08-08T17:14:42.217738: step 10056, loss 3.95858e-05, acc 1
2017-08-08T17:14:42.445767: step 10057, loss 0.00024906, acc 1
2017-08-08T17:14:42.632383: step 10058, loss 0.0020813, acc 1
2017-08-08T17:14:42.836341: step 10059, loss 1.4814e-05, acc 1
2017-08-08T17:14:43.236941: step 10060, loss 0.000104596, acc 1
2017-08-08T17:14:43.478737: step 10061, loss 1.28299e-05, acc 1
2017-08-08T17:14:43.696137: step 10062, loss 1.80299e-06, acc 1
2017-08-08T17:14:43.923837: step 10063, loss 0.0107229, acc 1
2017-08-08T17:14:44.185387: step 10064, loss 8.83627e-06, acc 1
2017-08-08T17:14:44.601379: step 10065, loss 8.57442e-05, acc 1
2017-08-08T17:14:44.887866: step 10066, loss 0.00315819, acc 1
2017-08-08T17:14:45.228803: step 10067, loss 1.67883e-05, acc 1
2017-08-08T17:14:45.406187: step 10068, loss 0.0118862, acc 0.984375
2017-08-08T17:14:45.617280: step 10069, loss 2.31303e-05, acc 1
2017-08-08T17:14:45.956845: step 10070, loss 0.000170987, acc 1
2017-08-08T17:14:46.199450: step 10071, loss 0.000371409, acc 1
2017-08-08T17:14:46.449965: step 10072, loss 7.96953e-05, acc 1
2017-08-08T17:14:46.682765: step 10073, loss 0.002423, acc 1
2017-08-08T17:14:47.021156: step 10074, loss 0.000660793, acc 1
2017-08-08T17:14:47.331018: step 10075, loss 4.12443e-05, acc 1
2017-08-08T17:14:47.603548: step 10076, loss 0.00348993, acc 1
2017-08-08T17:14:47.799585: step 10077, loss 0.000135639, acc 1
2017-08-08T17:14:48.081427: step 10078, loss 4.23316e-05, acc 1
2017-08-08T17:14:48.459497: step 10079, loss 0.000107808, acc 1
2017-08-08T17:14:48.678121: step 10080, loss 3.64694e-06, acc 1
2017-08-08T17:14:48.934443: step 10081, loss 0.000138852, acc 1
2017-08-08T17:14:49.297019: step 10082, loss 0.00571865, acc 1
2017-08-08T17:14:49.663636: step 10083, loss 0.000264244, acc 1
2017-08-08T17:14:49.914534: step 10084, loss 0.000125856, acc 1
2017-08-08T17:14:50.158290: step 10085, loss 4.52908e-05, acc 1
2017-08-08T17:14:50.374994: step 10086, loss 0.000166974, acc 1
2017-08-08T17:14:50.869851: step 10087, loss 4.86302e-05, acc 1
2017-08-08T17:14:51.102094: step 10088, loss 7.16072e-05, acc 1
2017-08-08T17:14:51.319207: step 10089, loss 0.000873703, acc 1
2017-08-08T17:14:51.612924: step 10090, loss 5.15733e-05, acc 1
2017-08-08T17:14:51.944388: step 10091, loss 3.76041e-06, acc 1
2017-08-08T17:14:52.280343: step 10092, loss 3.87517e-05, acc 1
2017-08-08T17:14:52.468367: step 10093, loss 9.05066e-05, acc 1
2017-08-08T17:14:52.713375: step 10094, loss 1.8384e-06, acc 1
2017-08-08T17:14:53.150765: step 10095, loss 8.59212e-05, acc 1
2017-08-08T17:14:53.461468: step 10096, loss 0.00022819, acc 1
2017-08-08T17:14:53.747842: step 10097, loss 0.000158061, acc 1
2017-08-08T17:14:54.087777: step 10098, loss 0.000184948, acc 1
2017-08-08T17:14:54.512971: step 10099, loss 0.000275448, acc 1
2017-08-08T17:14:54.835487: step 10100, loss 1.87308e-05, acc 1

Evaluation:
2017-08-08T17:14:55.324115: step 10100, loss 3.39528, acc 0.713884

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-10100

2017-08-08T17:14:55.724458: step 10101, loss 7.49212e-06, acc 1
2017-08-08T17:14:56.016233: step 10102, loss 5.26821e-05, acc 1
2017-08-08T17:14:56.244538: step 10103, loss 1.62328e-05, acc 1
2017-08-08T17:14:56.489350: step 10104, loss 0.000105417, acc 1
2017-08-08T17:14:56.745321: step 10105, loss 6.46115e-06, acc 1
2017-08-08T17:14:56.977323: step 10106, loss 0.00107534, acc 1
2017-08-08T17:14:57.276953: step 10107, loss 1.62327e-05, acc 1
2017-08-08T17:14:57.436784: step 10108, loss 2.83289e-05, acc 1
2017-08-08T17:14:57.679013: step 10109, loss 4.73576e-05, acc 1
2017-08-08T17:14:57.931306: step 10110, loss 0.000544726, acc 1
2017-08-08T17:14:58.174137: step 10111, loss 1.69049e-05, acc 1
2017-08-08T17:14:58.465894: step 10112, loss 7.4258e-05, acc 1
2017-08-08T17:14:58.857509: step 10113, loss 0.0020431, acc 1
2017-08-08T17:14:59.135695: step 10114, loss 0.000516541, acc 1
2017-08-08T17:14:59.408444: step 10115, loss 0.00297964, acc 1
2017-08-08T17:14:59.634303: step 10116, loss 2.15627e-05, acc 1
2017-08-08T17:14:59.911052: step 10117, loss 3.70043e-05, acc 1
2017-08-08T17:15:00.196970: step 10118, loss 0.000890993, acc 1
2017-08-08T17:15:00.540214: step 10119, loss 0.000136257, acc 1
2017-08-08T17:15:00.833622: step 10120, loss 9.52571e-05, acc 1
2017-08-08T17:15:01.073374: step 10121, loss 2.38888e-05, acc 1
2017-08-08T17:15:01.522352: step 10122, loss 0.000748206, acc 1
2017-08-08T17:15:01.938460: step 10123, loss 0.00216621, acc 1
2017-08-08T17:15:02.337924: step 10124, loss 0.000877227, acc 1
2017-08-08T17:15:02.798403: step 10125, loss 2.66164e-06, acc 1
2017-08-08T17:15:03.268764: step 10126, loss 0.000704569, acc 1
2017-08-08T17:15:03.565265: step 10127, loss 0.00101565, acc 1
2017-08-08T17:15:03.869768: step 10128, loss 0.000242579, acc 1
2017-08-08T17:15:04.165228: step 10129, loss 1.41373e-06, acc 1
2017-08-08T17:15:04.555533: step 10130, loss 0.00124253, acc 1
2017-08-08T17:15:04.940963: step 10131, loss 9.22113e-05, acc 1
2017-08-08T17:15:05.318696: step 10132, loss 7.47489e-05, acc 1
2017-08-08T17:15:05.592682: step 10133, loss 1.82385e-05, acc 1
2017-08-08T17:15:05.942325: step 10134, loss 0.000258354, acc 1
2017-08-08T17:15:06.372214: step 10135, loss 0.00062992, acc 1
2017-08-08T17:15:06.615449: step 10136, loss 2.1119e-05, acc 1
2017-08-08T17:15:06.934644: step 10137, loss 0.000177558, acc 1
2017-08-08T17:15:07.213101: step 10138, loss 0.000145933, acc 1
2017-08-08T17:15:07.628956: step 10139, loss 6.04916e-05, acc 1
2017-08-08T17:15:07.977375: step 10140, loss 0.00114851, acc 1
2017-08-08T17:15:08.320662: step 10141, loss 2.51355e-05, acc 1
2017-08-08T17:15:08.539135: step 10142, loss 0.000595885, acc 1
2017-08-08T17:15:08.936705: step 10143, loss 0.00127971, acc 1
2017-08-08T17:15:09.334180: step 10144, loss 7.60988e-05, acc 1
2017-08-08T17:15:09.617868: step 10145, loss 7.04866e-06, acc 1
2017-08-08T17:15:09.891394: step 10146, loss 7.03906e-05, acc 1
2017-08-08T17:15:10.145571: step 10147, loss 0.000581989, acc 1
2017-08-08T17:15:10.542791: step 10148, loss 1.80758e-05, acc 1
2017-08-08T17:15:10.900561: step 10149, loss 0.000115512, acc 1
2017-08-08T17:15:11.197516: step 10150, loss 0.000772804, acc 1
2017-08-08T17:15:11.450707: step 10151, loss 0.000513545, acc 1
2017-08-08T17:15:11.710372: step 10152, loss 0.000475785, acc 1
2017-08-08T17:15:12.152837: step 10153, loss 4.27806e-06, acc 1
2017-08-08T17:15:12.416148: step 10154, loss 0.000192959, acc 1
2017-08-08T17:15:12.851940: step 10155, loss 0.000304961, acc 1
2017-08-08T17:15:13.107539: step 10156, loss 1.21333e-05, acc 1
2017-08-08T17:15:13.404033: step 10157, loss 0.00010364, acc 1
2017-08-08T17:15:13.811792: step 10158, loss 9.07979e-05, acc 1
2017-08-08T17:15:14.112147: step 10159, loss 0.000317793, acc 1
2017-08-08T17:15:14.627150: step 10160, loss 0.00482471, acc 1
2017-08-08T17:15:14.875639: step 10161, loss 2.98204e-06, acc 1
2017-08-08T17:15:15.376948: step 10162, loss 1.55715e-06, acc 1
2017-08-08T17:15:15.707871: step 10163, loss 0.000577892, acc 1
2017-08-08T17:15:16.073427: step 10164, loss 0.000920671, acc 1
2017-08-08T17:15:16.444823: step 10165, loss 7.39712e-05, acc 1
2017-08-08T17:15:16.721054: step 10166, loss 1.02912e-05, acc 1
2017-08-08T17:15:17.105173: step 10167, loss 0.000204054, acc 1
2017-08-08T17:15:17.386236: step 10168, loss 0.000288084, acc 1
2017-08-08T17:15:17.670531: step 10169, loss 0.000167053, acc 1
2017-08-08T17:15:18.029373: step 10170, loss 2.06799e-05, acc 1
2017-08-08T17:15:18.334033: step 10171, loss 0.0155761, acc 0.984375
2017-08-08T17:15:18.690191: step 10172, loss 7.27307e-05, acc 1
2017-08-08T17:15:19.014824: step 10173, loss 0.000132267, acc 1
2017-08-08T17:15:19.317288: step 10174, loss 0.0114792, acc 1
2017-08-08T17:15:19.585435: step 10175, loss 8.37533e-05, acc 1
2017-08-08T17:15:19.821740: step 10176, loss 3.41556e-05, acc 1
2017-08-08T17:15:20.213813: step 10177, loss 2.09391e-05, acc 1
2017-08-08T17:15:20.474001: step 10178, loss 0.00790891, acc 1
2017-08-08T17:15:20.846274: step 10179, loss 6.22488e-05, acc 1
2017-08-08T17:15:21.098818: step 10180, loss 3.86668e-06, acc 1
2017-08-08T17:15:21.306597: step 10181, loss 2.56063e-05, acc 1
2017-08-08T17:15:21.548073: step 10182, loss 0.000201898, acc 1
2017-08-08T17:15:21.755838: step 10183, loss 0.000622556, acc 1
2017-08-08T17:15:22.017947: step 10184, loss 1.39137e-06, acc 1
2017-08-08T17:15:22.208324: step 10185, loss 9.83052e-05, acc 1
2017-08-08T17:15:22.454562: step 10186, loss 0.00142848, acc 1
2017-08-08T17:15:22.697733: step 10187, loss 9.24867e-06, acc 1
2017-08-08T17:15:22.893502: step 10188, loss 3.2039e-05, acc 1
2017-08-08T17:15:23.233322: step 10189, loss 7.30715e-05, acc 1
2017-08-08T17:15:23.650281: step 10190, loss 0.000101124, acc 1
2017-08-08T17:15:23.959014: step 10191, loss 0.000207112, acc 1
2017-08-08T17:15:24.251594: step 10192, loss 0.00267414, acc 1
2017-08-08T17:15:24.613391: step 10193, loss 0.00684047, acc 1
2017-08-08T17:15:24.933889: step 10194, loss 0.000637528, acc 1
2017-08-08T17:15:25.293454: step 10195, loss 0.0035344, acc 1
2017-08-08T17:15:25.470056: step 10196, loss 0.0140334, acc 0.984375
2017-08-08T17:15:25.685343: step 10197, loss 0.000193484, acc 1
2017-08-08T17:15:25.940667: step 10198, loss 6.76824e-05, acc 1
2017-08-08T17:15:26.142880: step 10199, loss 6.00529e-05, acc 1
2017-08-08T17:15:26.329438: step 10200, loss 1.80401e-06, acc 1

Evaluation:
2017-08-08T17:15:26.943370: step 10200, loss 3.33091, acc 0.723265

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-10200

2017-08-08T17:15:27.408398: step 10201, loss 2.45399e-05, acc 1
2017-08-08T17:15:27.646547: step 10202, loss 5.34549e-06, acc 1
2017-08-08T17:15:27.965928: step 10203, loss 0.000807957, acc 1
2017-08-08T17:15:28.233779: step 10204, loss 7.03608e-05, acc 1
2017-08-08T17:15:28.437512: step 10205, loss 7.31554e-06, acc 1
2017-08-08T17:15:28.751249: step 10206, loss 0.000865533, acc 1
2017-08-08T17:15:29.176782: step 10207, loss 0.000592989, acc 1
2017-08-08T17:15:29.472732: step 10208, loss 0.00574006, acc 1
2017-08-08T17:15:29.910401: step 10209, loss 3.03587e-05, acc 1
2017-08-08T17:15:30.192713: step 10210, loss 0.000721577, acc 1
2017-08-08T17:15:30.593848: step 10211, loss 1.07962e-05, acc 1
2017-08-08T17:15:30.873515: step 10212, loss 0.000172789, acc 1
2017-08-08T17:15:31.143764: step 10213, loss 1.4264e-05, acc 1
2017-08-08T17:15:31.526273: step 10214, loss 2.97412e-05, acc 1
2017-08-08T17:15:31.977739: step 10215, loss 0.00618608, acc 1
2017-08-08T17:15:32.307917: step 10216, loss 7.23168e-05, acc 1
2017-08-08T17:15:32.636256: step 10217, loss 0.000145173, acc 1
2017-08-08T17:15:32.883918: step 10218, loss 3.41313e-05, acc 1
2017-08-08T17:15:33.113366: step 10219, loss 0.000287111, acc 1
2017-08-08T17:15:33.578752: step 10220, loss 7.26769e-05, acc 1
2017-08-08T17:15:33.829006: step 10221, loss 1.75885e-05, acc 1
2017-08-08T17:15:34.140414: step 10222, loss 1.32853e-05, acc 1
2017-08-08T17:15:34.404944: step 10223, loss 1.07288e-06, acc 1
2017-08-08T17:15:34.866897: step 10224, loss 0.000824919, acc 1
2017-08-08T17:15:35.320113: step 10225, loss 8.44408e-06, acc 1
2017-08-08T17:15:35.654865: step 10226, loss 9.513e-05, acc 1
2017-08-08T17:15:35.941371: step 10227, loss 1.84963e-05, acc 1
2017-08-08T17:15:36.295323: step 10228, loss 0.000674689, acc 1
2017-08-08T17:15:36.538982: step 10229, loss 0.000121092, acc 1
2017-08-08T17:15:36.736594: step 10230, loss 0.000126448, acc 1
2017-08-08T17:15:36.994604: step 10231, loss 0.000509335, acc 1
2017-08-08T17:15:37.414480: step 10232, loss 1.24979e-06, acc 1
2017-08-08T17:15:37.860013: step 10233, loss 0.00047082, acc 1
2017-08-08T17:15:38.183563: step 10234, loss 0.00514851, acc 1
2017-08-08T17:15:38.452425: step 10235, loss 0.000156513, acc 1
2017-08-08T17:15:38.746256: step 10236, loss 2.15561e-05, acc 1
2017-08-08T17:15:39.114764: step 10237, loss 1.53477e-06, acc 1
2017-08-08T17:15:39.436819: step 10238, loss 0.000235431, acc 1
2017-08-08T17:15:39.754601: step 10239, loss 3.65761e-05, acc 1
2017-08-08T17:15:40.127225: step 10240, loss 0.000397056, acc 1
2017-08-08T17:15:40.548448: step 10241, loss 6.82109e-05, acc 1
2017-08-08T17:15:40.957357: step 10242, loss 0.000240947, acc 1
2017-08-08T17:15:41.299080: step 10243, loss 4.4485e-05, acc 1
2017-08-08T17:15:41.528543: step 10244, loss 0.000572803, acc 1
2017-08-08T17:15:41.868596: step 10245, loss 2.07523e-05, acc 1
2017-08-08T17:15:42.179830: step 10246, loss 0.000418013, acc 1
2017-08-08T17:15:42.452127: step 10247, loss 2.52942e-06, acc 1
2017-08-08T17:15:42.718344: step 10248, loss 0.024724, acc 0.984375
2017-08-08T17:15:42.965312: step 10249, loss 0.0021374, acc 1
2017-08-08T17:15:43.422387: step 10250, loss 0.000311674, acc 1
2017-08-08T17:15:43.689706: step 10251, loss 6.76137e-07, acc 1
2017-08-08T17:15:43.906263: step 10252, loss 0.000160742, acc 1
2017-08-08T17:15:44.094525: step 10253, loss 1.84468e-05, acc 1
2017-08-08T17:15:44.463277: step 10254, loss 8.03211e-05, acc 1
2017-08-08T17:15:44.765494: step 10255, loss 0.000166349, acc 1
2017-08-08T17:15:44.952160: step 10256, loss 7.16093e-06, acc 1
2017-08-08T17:15:45.190910: step 10257, loss 7.67404e-07, acc 1
2017-08-08T17:15:45.532569: step 10258, loss 1.50134e-05, acc 1
2017-08-08T17:15:45.825041: step 10259, loss 0.00054295, acc 1
2017-08-08T17:15:46.099956: step 10260, loss 6.33911e-05, acc 1
2017-08-08T17:15:46.352941: step 10261, loss 0.00038628, acc 1
2017-08-08T17:15:46.706232: step 10262, loss 0.000324241, acc 1
2017-08-08T17:15:46.969813: step 10263, loss 0.000188894, acc 1
2017-08-08T17:15:47.203585: step 10264, loss 0.000144789, acc 1
2017-08-08T17:15:47.455226: step 10265, loss 7.54366e-07, acc 1
2017-08-08T17:15:47.773099: step 10266, loss 0.000348269, acc 1
2017-08-08T17:15:48.106804: step 10267, loss 0.000362725, acc 1
2017-08-08T17:15:48.416990: step 10268, loss 0.000104156, acc 1
2017-08-08T17:15:48.639755: step 10269, loss 0.000279708, acc 1
2017-08-08T17:15:48.978326: step 10270, loss 0.0045246, acc 1
2017-08-08T17:15:49.301583: step 10271, loss 0.000555166, acc 1
2017-08-08T17:15:49.539832: step 10272, loss 0.00119933, acc 1
2017-08-08T17:15:49.832423: step 10273, loss 0.0101317, acc 1
2017-08-08T17:15:50.165502: step 10274, loss 0.00967662, acc 1
2017-08-08T17:15:50.413743: step 10275, loss 0.00138388, acc 1
2017-08-08T17:15:50.702764: step 10276, loss 3.66359e-05, acc 1
2017-08-08T17:15:50.913999: step 10277, loss 0.00787564, acc 1
2017-08-08T17:15:51.275756: step 10278, loss 8.92989e-06, acc 1
2017-08-08T17:15:51.490821: step 10279, loss 0.000208181, acc 1
2017-08-08T17:15:51.697424: step 10280, loss 5.55573e-06, acc 1
2017-08-08T17:15:51.981449: step 10281, loss 0.000370577, acc 1
2017-08-08T17:15:52.202645: step 10282, loss 0.000252689, acc 1
2017-08-08T17:15:52.506238: step 10283, loss 3.40204e-05, acc 1
2017-08-08T17:15:52.784316: step 10284, loss 0.00122889, acc 1
2017-08-08T17:15:53.056175: step 10285, loss 4.40876e-05, acc 1
2017-08-08T17:15:53.460921: step 10286, loss 4.88174e-06, acc 1
2017-08-08T17:15:53.872125: step 10287, loss 0.00617926, acc 1
2017-08-08T17:15:54.166111: step 10288, loss 0.00015567, acc 1
2017-08-08T17:15:54.432251: step 10289, loss 1.09523e-06, acc 1
2017-08-08T17:15:54.819359: step 10290, loss 1.83058e-05, acc 1
2017-08-08T17:15:55.091926: step 10291, loss 0.00343779, acc 1
2017-08-08T17:15:55.356671: step 10292, loss 0.00122255, acc 1
2017-08-08T17:15:55.638086: step 10293, loss 7.12489e-06, acc 1
2017-08-08T17:15:55.813468: step 10294, loss 5.86889e-06, acc 1
2017-08-08T17:15:56.004389: step 10295, loss 3.1373e-05, acc 1
2017-08-08T17:15:56.303463: step 10296, loss 0.00064251, acc 1
2017-08-08T17:15:56.518702: step 10297, loss 1.02875e-05, acc 1
2017-08-08T17:15:56.741731: step 10298, loss 0.000301268, acc 1
2017-08-08T17:15:56.970440: step 10299, loss 2.58902e-06, acc 1
2017-08-08T17:15:57.284152: step 10300, loss 1.09137e-05, acc 1

Evaluation:
2017-08-08T17:15:58.015383: step 10300, loss 3.3494, acc 0.723265

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-10300

2017-08-08T17:15:58.367804: step 10301, loss 3.48118e-06, acc 1
2017-08-08T17:15:58.673362: step 10302, loss 0.000181077, acc 1
2017-08-08T17:15:58.853381: step 10303, loss 0.000231092, acc 1
2017-08-08T17:15:59.136939: step 10304, loss 0.00291124, acc 1
2017-08-08T17:15:59.424987: step 10305, loss 0.000187195, acc 1
2017-08-08T17:15:59.796381: step 10306, loss 0.000238205, acc 1
2017-08-08T17:16:00.117368: step 10307, loss 0.000133943, acc 1
2017-08-08T17:16:00.416109: step 10308, loss 0.000367535, acc 1
2017-08-08T17:16:00.630649: step 10309, loss 0.0338488, acc 0.984375
2017-08-08T17:16:00.808953: step 10310, loss 5.46695e-05, acc 1
2017-08-08T17:16:01.095135: step 10311, loss 0.000485727, acc 1
2017-08-08T17:16:01.351889: step 10312, loss 9.02386e-05, acc 1
2017-08-08T17:16:01.542915: step 10313, loss 2.16157e-05, acc 1
2017-08-08T17:16:01.771905: step 10314, loss 5.08665e-06, acc 1
2017-08-08T17:16:02.105580: step 10315, loss 0.000345997, acc 1
2017-08-08T17:16:02.449595: step 10316, loss 0.000667026, acc 1
2017-08-08T17:16:02.830141: step 10317, loss 6.39595e-05, acc 1
2017-08-08T17:16:03.134830: step 10318, loss 1.05298e-05, acc 1
2017-08-08T17:16:03.386669: step 10319, loss 9.74803e-05, acc 1
2017-08-08T17:16:03.783766: step 10320, loss 6.09597e-05, acc 1
2017-08-08T17:16:04.204816: step 10321, loss 4.56499e-05, acc 1
2017-08-08T17:16:04.454112: step 10322, loss 2.0663e-05, acc 1
2017-08-08T17:16:04.696012: step 10323, loss 0.00101249, acc 1
2017-08-08T17:16:05.066584: step 10324, loss 0.00129083, acc 1
2017-08-08T17:16:05.424628: step 10325, loss 1.53448e-05, acc 1
2017-08-08T17:16:05.772194: step 10326, loss 0.000194407, acc 1
2017-08-08T17:16:06.083891: step 10327, loss 0.0177673, acc 0.984375
2017-08-08T17:16:06.326470: step 10328, loss 7.54489e-05, acc 1
2017-08-08T17:16:06.777375: step 10329, loss 5.54455e-05, acc 1
2017-08-08T17:16:07.080024: step 10330, loss 0.000983217, acc 1
2017-08-08T17:16:07.345828: step 10331, loss 0.0338327, acc 0.984375
2017-08-08T17:16:07.589313: step 10332, loss 4.21267e-05, acc 1
2017-08-08T17:16:08.034597: step 10333, loss 2.01535e-06, acc 1
2017-08-08T17:16:08.496930: step 10334, loss 5.23874e-05, acc 1
2017-08-08T17:16:08.838031: step 10335, loss 0.000142149, acc 1
2017-08-08T17:16:09.085163: step 10336, loss 0.0356674, acc 0.984375
2017-08-08T17:16:09.340216: step 10337, loss 3.93003e-06, acc 1
2017-08-08T17:16:09.801621: step 10338, loss 0.0001066, acc 1
2017-08-08T17:16:10.011473: step 10339, loss 0.00221516, acc 1
2017-08-08T17:16:10.281132: step 10340, loss 0.0186648, acc 0.984375
2017-08-08T17:16:10.499660: step 10341, loss 0.000104718, acc 1
2017-08-08T17:16:10.853416: step 10342, loss 2.22099e-05, acc 1
2017-08-08T17:16:11.232250: step 10343, loss 0.00157704, acc 1
2017-08-08T17:16:11.519204: step 10344, loss 0.000141018, acc 1
2017-08-08T17:16:11.715316: step 10345, loss 0.000643682, acc 1
2017-08-08T17:16:11.932706: step 10346, loss 0.00102184, acc 1
2017-08-08T17:16:12.250590: step 10347, loss 0.000113162, acc 1
2017-08-08T17:16:12.500493: step 10348, loss 0.000191777, acc 1
2017-08-08T17:16:12.764263: step 10349, loss 1.6419e-05, acc 1
2017-08-08T17:16:12.945954: step 10350, loss 1.72454e-05, acc 1
2017-08-08T17:16:13.302824: step 10351, loss 1.39324e-06, acc 1
2017-08-08T17:16:13.623221: step 10352, loss 0.000129379, acc 1
2017-08-08T17:16:13.862591: step 10353, loss 0.00522344, acc 1
2017-08-08T17:16:14.059353: step 10354, loss 0.00062554, acc 1
2017-08-08T17:16:14.291424: step 10355, loss 2.26804e-05, acc 1
2017-08-08T17:16:14.603822: step 10356, loss 0.000104613, acc 1
2017-08-08T17:16:14.807381: step 10357, loss 0.000381388, acc 1
2017-08-08T17:16:14.980771: step 10358, loss 0.00404095, acc 1
2017-08-08T17:16:15.272077: step 10359, loss 0.0113811, acc 0.984375
2017-08-08T17:16:15.644380: step 10360, loss 0.000121806, acc 1
2017-08-08T17:16:15.972649: step 10361, loss 3.09192e-06, acc 1
2017-08-08T17:16:16.184000: step 10362, loss 0.000203544, acc 1
2017-08-08T17:16:16.505496: step 10363, loss 0.0268575, acc 0.984375
2017-08-08T17:16:16.784444: step 10364, loss 6.08875e-06, acc 1
2017-08-08T17:16:17.035271: step 10365, loss 0.000546612, acc 1
2017-08-08T17:16:17.266532: step 10366, loss 2.89441e-06, acc 1
2017-08-08T17:16:17.659798: step 10367, loss 1.5273e-05, acc 1
2017-08-08T17:16:17.973326: step 10368, loss 1.05539e-05, acc 1
2017-08-08T17:16:18.178413: step 10369, loss 5.80304e-05, acc 1
2017-08-08T17:16:18.352551: step 10370, loss 2.1606e-06, acc 1
2017-08-08T17:16:18.644976: step 10371, loss 3.42486e-05, acc 1
2017-08-08T17:16:19.008144: step 10372, loss 0.000848516, acc 1
2017-08-08T17:16:19.266520: step 10373, loss 4.75252e-05, acc 1
2017-08-08T17:16:19.512663: step 10374, loss 0.00126656, acc 1
2017-08-08T17:16:19.835716: step 10375, loss 1.34387e-05, acc 1
2017-08-08T17:16:20.125863: step 10376, loss 0.000142564, acc 1
2017-08-08T17:16:20.383961: step 10377, loss 0.00244634, acc 1
2017-08-08T17:16:20.609604: step 10378, loss 3.80259e-05, acc 1
2017-08-08T17:16:20.804134: step 10379, loss 0.000262266, acc 1
2017-08-08T17:16:21.133384: step 10380, loss 8.20796e-05, acc 1
2017-08-08T17:16:21.363833: step 10381, loss 4.51665e-05, acc 1
2017-08-08T17:16:21.538032: step 10382, loss 0.00243145, acc 1
2017-08-08T17:16:21.766246: step 10383, loss 0.000680289, acc 1
2017-08-08T17:16:21.969411: step 10384, loss 2.22882e-05, acc 1
2017-08-08T17:16:22.304185: step 10385, loss 0.0118729, acc 0.984375
2017-08-08T17:16:22.610151: step 10386, loss 0.000158194, acc 1
2017-08-08T17:16:22.905323: step 10387, loss 0.000696924, acc 1
2017-08-08T17:16:23.111276: step 10388, loss 0.000586528, acc 1
2017-08-08T17:16:23.352271: step 10389, loss 0.000456778, acc 1
2017-08-08T17:16:23.767861: step 10390, loss 2.06544e-05, acc 1
2017-08-08T17:16:24.045247: step 10391, loss 2.65603e-05, acc 1
2017-08-08T17:16:24.326333: step 10392, loss 0.000121045, acc 1
2017-08-08T17:16:24.593509: step 10393, loss 1.13467e-05, acc 1
2017-08-08T17:16:25.032665: step 10394, loss 0.00967528, acc 1
2017-08-08T17:16:25.410242: step 10395, loss 0.000584068, acc 1
2017-08-08T17:16:25.679937: step 10396, loss 2.50145e-05, acc 1
2017-08-08T17:16:25.889778: step 10397, loss 6.78829e-05, acc 1
2017-08-08T17:16:26.154580: step 10398, loss 0.000275086, acc 1
2017-08-08T17:16:26.525659: step 10399, loss 1.68211e-05, acc 1
2017-08-08T17:16:26.710019: step 10400, loss 1.70614e-06, acc 1

Evaluation:
2017-08-08T17:16:27.177037: step 10400, loss 3.48714, acc 0.71576

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-10400

2017-08-08T17:16:27.725413: step 10401, loss 0.00797055, acc 1
2017-08-08T17:16:28.072331: step 10402, loss 0.000104233, acc 1
2017-08-08T17:16:28.294510: step 10403, loss 0.000635302, acc 1
2017-08-08T17:16:28.659732: step 10404, loss 0.000801944, acc 1
2017-08-08T17:16:28.864728: step 10405, loss 4.00085e-06, acc 1
2017-08-08T17:16:29.170199: step 10406, loss 5.21037e-05, acc 1
2017-08-08T17:16:29.611051: step 10407, loss 0.000351871, acc 1
2017-08-08T17:16:29.961480: step 10408, loss 0.000374017, acc 1
2017-08-08T17:16:30.378802: step 10409, loss 5.68859e-05, acc 1
2017-08-08T17:16:30.622404: step 10410, loss 2.40865e-05, acc 1
2017-08-08T17:16:30.860998: step 10411, loss 0.000225187, acc 1
2017-08-08T17:16:31.062852: step 10412, loss 4.10604e-05, acc 1
2017-08-08T17:16:31.453309: step 10413, loss 4.91643e-05, acc 1
2017-08-08T17:16:31.775898: step 10414, loss 0.000646677, acc 1
2017-08-08T17:16:32.040954: step 10415, loss 4.80173e-06, acc 1
2017-08-08T17:16:32.278612: step 10416, loss 0.000356309, acc 1
2017-08-08T17:16:32.584088: step 10417, loss 0.000294236, acc 1
2017-08-08T17:16:32.914366: step 10418, loss 5.62636e-05, acc 1
2017-08-08T17:16:33.118528: step 10419, loss 0.00013395, acc 1
2017-08-08T17:16:33.368858: step 10420, loss 1.61518e-05, acc 1
2017-08-08T17:16:33.663163: step 10421, loss 8.98549e-05, acc 1
2017-08-08T17:16:33.893924: step 10422, loss 0.000142177, acc 1
2017-08-08T17:16:34.121367: step 10423, loss 8.10566e-05, acc 1
2017-08-08T17:16:34.386611: step 10424, loss 0.000343153, acc 1
2017-08-08T17:16:34.617302: step 10425, loss 7.61209e-06, acc 1
2017-08-08T17:16:34.996900: step 10426, loss 0.000146755, acc 1
2017-08-08T17:16:35.272122: step 10427, loss 6.28013e-05, acc 1
2017-08-08T17:16:35.519461: step 10428, loss 0.00045048, acc 1
2017-08-08T17:16:35.749328: step 10429, loss 3.56156e-05, acc 1
2017-08-08T17:16:36.041337: step 10430, loss 0.000259509, acc 1
2017-08-08T17:16:36.377387: step 10431, loss 1.3576e-05, acc 1
2017-08-08T17:16:36.633397: step 10432, loss 0.00017098, acc 1
2017-08-08T17:16:36.877397: step 10433, loss 3.13077e-05, acc 1
2017-08-08T17:16:37.258406: step 10434, loss 4.61431e-05, acc 1
2017-08-08T17:16:37.472910: step 10435, loss 6.09196e-05, acc 1
2017-08-08T17:16:37.689516: step 10436, loss 1.67076e-06, acc 1
2017-08-08T17:16:37.937625: step 10437, loss 1.70192e-05, acc 1
2017-08-08T17:16:38.303668: step 10438, loss 0.00137899, acc 1
2017-08-08T17:16:38.597460: step 10439, loss 1.30382e-06, acc 1
2017-08-08T17:16:38.878178: step 10440, loss 4.15108e-05, acc 1
2017-08-08T17:16:39.149134: step 10441, loss 2.10009e-05, acc 1
2017-08-08T17:16:39.377291: step 10442, loss 1.35423e-05, acc 1
2017-08-08T17:16:39.754519: step 10443, loss 5.29808e-05, acc 1
2017-08-08T17:16:40.026965: step 10444, loss 6.8411e-05, acc 1
2017-08-08T17:16:40.306250: step 10445, loss 2.22204e-06, acc 1
2017-08-08T17:16:40.587933: step 10446, loss 0.00418467, acc 1
2017-08-08T17:16:40.952407: step 10447, loss 0.00125824, acc 1
2017-08-08T17:16:41.267845: step 10448, loss 0.000261147, acc 1
2017-08-08T17:16:41.485483: step 10449, loss 0.000460331, acc 1
2017-08-08T17:16:41.671235: step 10450, loss 4.63724e-05, acc 1
2017-08-08T17:16:41.969439: step 10451, loss 0.111372, acc 0.984375
2017-08-08T17:16:42.270632: step 10452, loss 0.000114915, acc 1
2017-08-08T17:16:42.471562: step 10453, loss 0.00328191, acc 1
2017-08-08T17:16:42.703966: step 10454, loss 9.42809e-05, acc 1
2017-08-08T17:16:42.929520: step 10455, loss 0.00316897, acc 1
2017-08-08T17:16:43.328195: step 10456, loss 7.59332e-05, acc 1
2017-08-08T17:16:43.707482: step 10457, loss 2.59729e-05, acc 1
2017-08-08T17:16:44.041365: step 10458, loss 0.00017875, acc 1
2017-08-08T17:16:44.328976: step 10459, loss 3.95616e-05, acc 1
2017-08-08T17:16:44.721644: step 10460, loss 4.39196e-06, acc 1
2017-08-08T17:16:44.937875: step 10461, loss 1.62049e-06, acc 1
2017-08-08T17:16:45.222009: step 10462, loss 1.59889e-05, acc 1
2017-08-08T17:16:45.610553: step 10463, loss 5.10362e-07, acc 1
2017-08-08T17:16:46.028637: step 10464, loss 1.78681e-05, acc 1
2017-08-08T17:16:46.381175: step 10465, loss 6.18723e-05, acc 1
2017-08-08T17:16:46.652075: step 10466, loss 0.0523539, acc 0.984375
2017-08-08T17:16:46.861285: step 10467, loss 0.000756439, acc 1
2017-08-08T17:16:47.229732: step 10468, loss 4.35928e-05, acc 1
2017-08-08T17:16:47.416053: step 10469, loss 0.000332113, acc 1
2017-08-08T17:16:47.617199: step 10470, loss 1.57579e-05, acc 1
2017-08-08T17:16:47.819419: step 10471, loss 1.56374e-05, acc 1
2017-08-08T17:16:48.129067: step 10472, loss 0.000719656, acc 1
2017-08-08T17:16:48.423201: step 10473, loss 0.000220367, acc 1
2017-08-08T17:16:48.729684: step 10474, loss 0.000227463, acc 1
2017-08-08T17:16:48.975616: step 10475, loss 0.00566253, acc 1
2017-08-08T17:16:49.233162: step 10476, loss 5.49829e-06, acc 1
2017-08-08T17:16:49.556116: step 10477, loss 4.34607e-05, acc 1
2017-08-08T17:16:49.873863: step 10478, loss 0.000176164, acc 1
2017-08-08T17:16:50.141508: step 10479, loss 7.54063e-06, acc 1
2017-08-08T17:16:50.421157: step 10480, loss 1.5207e-05, acc 1
2017-08-08T17:16:50.794557: step 10481, loss 0.0604821, acc 0.984375
2017-08-08T17:16:51.089734: step 10482, loss 8.67172e-06, acc 1
2017-08-08T17:16:51.341160: step 10483, loss 4.64332e-06, acc 1
2017-08-08T17:16:51.563780: step 10484, loss 1.02665e-05, acc 1
2017-08-08T17:16:51.852605: step 10485, loss 0.00047698, acc 1
2017-08-08T17:16:52.185483: step 10486, loss 3.57626e-07, acc 1
2017-08-08T17:16:52.426413: step 10487, loss 7.87005e-05, acc 1
2017-08-08T17:16:52.690569: step 10488, loss 6.82746e-05, acc 1
2017-08-08T17:16:52.995712: step 10489, loss 5.46832e-05, acc 1
2017-08-08T17:16:53.315032: step 10490, loss 5.05493e-06, acc 1
2017-08-08T17:16:53.678621: step 10491, loss 0.00124464, acc 1
2017-08-08T17:16:53.880937: step 10492, loss 3.04913e-05, acc 1
2017-08-08T17:16:54.112229: step 10493, loss 0.000258579, acc 1
2017-08-08T17:16:54.528897: step 10494, loss 7.02589e-05, acc 1
2017-08-08T17:16:54.773000: step 10495, loss 0.000313241, acc 1
2017-08-08T17:16:55.040294: step 10496, loss 0.000172066, acc 1
2017-08-08T17:16:55.380092: step 10497, loss 7.02062e-06, acc 1
2017-08-08T17:16:55.835086: step 10498, loss 0.00492245, acc 1
2017-08-08T17:16:56.089330: step 10499, loss 0.000211792, acc 1
2017-08-08T17:16:56.310350: step 10500, loss 0.000493461, acc 1

Evaluation:
2017-08-08T17:16:56.874276: step 10500, loss 3.48781, acc 0.721388

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-10500

2017-08-08T17:16:57.315724: step 10501, loss 0.00119311, acc 1
2017-08-08T17:16:57.524729: step 10502, loss 3.07316e-06, acc 1
2017-08-08T17:16:57.744099: step 10503, loss 1.86656e-05, acc 1
2017-08-08T17:16:58.037738: step 10504, loss 7.18511e-05, acc 1
2017-08-08T17:16:58.293780: step 10505, loss 7.47382e-06, acc 1
2017-08-08T17:16:58.560355: step 10506, loss 8.2344e-05, acc 1
2017-08-08T17:16:58.727624: step 10507, loss 2.24256e-06, acc 1
2017-08-08T17:16:58.887217: step 10508, loss 2.87071e-05, acc 1
2017-08-08T17:16:59.177360: step 10509, loss 0.000203022, acc 1
2017-08-08T17:16:59.371747: step 10510, loss 1.57034e-05, acc 1
2017-08-08T17:16:59.583089: step 10511, loss 0.000912954, acc 1
2017-08-08T17:16:59.785355: step 10512, loss 3.96084e-05, acc 1
2017-08-08T17:17:00.126437: step 10513, loss 0.000143188, acc 1
2017-08-08T17:17:00.455430: step 10514, loss 2.92965e-05, acc 1
2017-08-08T17:17:00.769820: step 10515, loss 0.000834851, acc 1
2017-08-08T17:17:00.997217: step 10516, loss 5.12225e-07, acc 1
2017-08-08T17:17:01.367124: step 10517, loss 4.61682e-05, acc 1
2017-08-08T17:17:01.731605: step 10518, loss 3.21781e-05, acc 1
2017-08-08T17:17:02.011946: step 10519, loss 0.00641247, acc 1
2017-08-08T17:17:02.339199: step 10520, loss 2.59806e-05, acc 1
2017-08-08T17:17:02.665478: step 10521, loss 0.000135386, acc 1
2017-08-08T17:17:02.985402: step 10522, loss 0.000231036, acc 1
2017-08-08T17:17:03.442096: step 10523, loss 2.31381e-05, acc 1
2017-08-08T17:17:03.815754: step 10524, loss 7.08493e-06, acc 1
2017-08-08T17:17:04.144352: step 10525, loss 1.8223e-05, acc 1
2017-08-08T17:17:04.378712: step 10526, loss 2.61204e-05, acc 1
2017-08-08T17:17:04.616335: step 10527, loss 1.44078e-05, acc 1
2017-08-08T17:17:04.915640: step 10528, loss 0.0133362, acc 0.984375
2017-08-08T17:17:05.179728: step 10529, loss 0.000185612, acc 1
2017-08-08T17:17:05.409458: step 10530, loss 4.69616e-05, acc 1
2017-08-08T17:17:05.661996: step 10531, loss 9.13177e-06, acc 1
2017-08-08T17:17:06.049638: step 10532, loss 7.42051e-06, acc 1
2017-08-08T17:17:06.403038: step 10533, loss 2.11257e-05, acc 1
2017-08-08T17:17:06.679967: step 10534, loss 2.49766e-06, acc 1
2017-08-08T17:17:07.027748: step 10535, loss 0.000179682, acc 1
2017-08-08T17:17:07.379593: step 10536, loss 0.000121795, acc 1
2017-08-08T17:17:07.733491: step 10537, loss 0.000284841, acc 1
2017-08-08T17:17:07.943512: step 10538, loss 2.10089e-05, acc 1
2017-08-08T17:17:08.178947: step 10539, loss 8.23271e-05, acc 1
2017-08-08T17:17:08.461664: step 10540, loss 4.5271e-05, acc 1
2017-08-08T17:17:08.878781: step 10541, loss 0.00014229, acc 1
2017-08-08T17:17:09.224899: step 10542, loss 2.43642e-05, acc 1
2017-08-08T17:17:09.670106: step 10543, loss 0.000225792, acc 1
2017-08-08T17:17:09.912170: step 10544, loss 0.000543407, acc 1
2017-08-08T17:17:10.262837: step 10545, loss 0.00214677, acc 1
2017-08-08T17:17:10.637283: step 10546, loss 4.63949e-06, acc 1
2017-08-08T17:17:10.886478: step 10547, loss 9.03131e-05, acc 1
2017-08-08T17:17:11.130402: step 10548, loss 1.28601e-05, acc 1
2017-08-08T17:17:11.419390: step 10549, loss 0.00088004, acc 1
2017-08-08T17:17:11.753418: step 10550, loss 0.000200479, acc 1
2017-08-08T17:17:12.125635: step 10551, loss 0.000325114, acc 1
2017-08-08T17:17:12.448485: step 10552, loss 5.66575e-05, acc 1
2017-08-08T17:17:12.680897: step 10553, loss 0.0017283, acc 1
2017-08-08T17:17:13.042541: step 10554, loss 0.0185163, acc 0.984375
2017-08-08T17:17:13.425523: step 10555, loss 0.000667683, acc 1
2017-08-08T17:17:13.664305: step 10556, loss 0.00133343, acc 1
2017-08-08T17:17:13.947909: step 10557, loss 0.00101535, acc 1
2017-08-08T17:17:14.232442: step 10558, loss 1.36321e-05, acc 1
2017-08-08T17:17:14.545333: step 10559, loss 4.83944e-05, acc 1
2017-08-08T17:17:14.821319: step 10560, loss 5.71163e-05, acc 1
2017-08-08T17:17:15.097709: step 10561, loss 7.20522e-05, acc 1
2017-08-08T17:17:15.320549: step 10562, loss 1.07966e-05, acc 1
2017-08-08T17:17:15.597715: step 10563, loss 0.000100003, acc 1
2017-08-08T17:17:15.925676: step 10564, loss 1.00583e-07, acc 1
2017-08-08T17:17:16.172174: step 10565, loss 7.65214e-05, acc 1
2017-08-08T17:17:16.462851: step 10566, loss 2.97849e-05, acc 1
2017-08-08T17:17:16.825221: step 10567, loss 3.36196e-05, acc 1
2017-08-08T17:17:17.141356: step 10568, loss 8.75442e-08, acc 1
2017-08-08T17:17:17.397472: step 10569, loss 0.000139184, acc 1
2017-08-08T17:17:17.664930: step 10570, loss 0.00460684, acc 1
2017-08-08T17:17:17.857346: step 10571, loss 1.53103e-06, acc 1
2017-08-08T17:17:18.209393: step 10572, loss 8.33349e-06, acc 1
2017-08-08T17:17:18.470424: step 10573, loss 4.79598e-06, acc 1
2017-08-08T17:17:18.695152: step 10574, loss 0.00250574, acc 1
2017-08-08T17:17:18.882837: step 10575, loss 3.15337e-06, acc 1
2017-08-08T17:17:19.178911: step 10576, loss 9.47152e-05, acc 1
2017-08-08T17:17:19.437359: step 10577, loss 1.73782e-06, acc 1
2017-08-08T17:17:19.709332: step 10578, loss 0.0232532, acc 0.984375
2017-08-08T17:17:19.918851: step 10579, loss 0.000196833, acc 1
2017-08-08T17:17:20.136187: step 10580, loss 1.17613e-05, acc 1
2017-08-08T17:17:20.600416: step 10581, loss 0.00946078, acc 1
2017-08-08T17:17:20.852570: step 10582, loss 5.01802e-05, acc 1
2017-08-08T17:17:21.135022: step 10583, loss 1.16905e-05, acc 1
2017-08-08T17:17:21.433244: step 10584, loss 8.3063e-05, acc 1
2017-08-08T17:17:21.783360: step 10585, loss 3.50029e-05, acc 1
2017-08-08T17:17:22.047072: step 10586, loss 0.000120329, acc 1
2017-08-08T17:17:22.240102: step 10587, loss 2.2939e-05, acc 1
2017-08-08T17:17:22.470536: step 10588, loss 0.00111194, acc 1
2017-08-08T17:17:22.715179: step 10589, loss 5.96552e-06, acc 1
2017-08-08T17:17:23.074829: step 10590, loss 9.04145e-05, acc 1
2017-08-08T17:17:23.370643: step 10591, loss 0.000289756, acc 1
2017-08-08T17:17:23.598089: step 10592, loss 0.00121773, acc 1
2017-08-08T17:17:23.923221: step 10593, loss 0.00381303, acc 1
2017-08-08T17:17:24.217324: step 10594, loss 5.3457e-05, acc 1
2017-08-08T17:17:24.560948: step 10595, loss 0.0225062, acc 0.984375
2017-08-08T17:17:24.864470: step 10596, loss 2.16619e-06, acc 1
2017-08-08T17:17:25.113314: step 10597, loss 0.00102109, acc 1
2017-08-08T17:17:25.477645: step 10598, loss 0.000482544, acc 1
2017-08-08T17:17:25.788331: step 10599, loss 0.000998228, acc 1
2017-08-08T17:17:26.026433: step 10600, loss 0.00165241, acc 1

Evaluation:
2017-08-08T17:17:26.802361: step 10600, loss 3.6823, acc 0.703565

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-10600

2017-08-08T17:17:27.382984: step 10601, loss 0.000124909, acc 1
2017-08-08T17:17:27.574940: step 10602, loss 4.09022e-06, acc 1
2017-08-08T17:17:27.936984: step 10603, loss 0.000416845, acc 1
2017-08-08T17:17:28.188928: step 10604, loss 1.85312e-05, acc 1
2017-08-08T17:17:28.470331: step 10605, loss 2.4122e-05, acc 1
2017-08-08T17:17:28.736009: step 10606, loss 0.000173474, acc 1
2017-08-08T17:17:29.150711: step 10607, loss 0.000788693, acc 1
2017-08-08T17:17:29.492227: step 10608, loss 0.00137874, acc 1
2017-08-08T17:17:29.917668: step 10609, loss 0.000131222, acc 1
2017-08-08T17:17:30.214835: step 10610, loss 0.000746437, acc 1
2017-08-08T17:17:30.488525: step 10611, loss 8.81728e-05, acc 1
2017-08-08T17:17:30.911038: step 10612, loss 0.00184229, acc 1
2017-08-08T17:17:31.118172: step 10613, loss 1.26586e-05, acc 1
2017-08-08T17:17:31.353972: step 10614, loss 0.000143551, acc 1
2017-08-08T17:17:31.629229: step 10615, loss 8.51016e-05, acc 1
2017-08-08T17:17:31.957876: step 10616, loss 0.00605333, acc 1
2017-08-08T17:17:32.232749: step 10617, loss 0.000352804, acc 1
2017-08-08T17:17:32.457315: step 10618, loss 3.34538e-05, acc 1
2017-08-08T17:17:32.663173: step 10619, loss 0.000236701, acc 1
2017-08-08T17:17:32.871211: step 10620, loss 1.26828e-05, acc 1
2017-08-08T17:17:33.180134: step 10621, loss 1.35964e-05, acc 1
2017-08-08T17:17:33.448052: step 10622, loss 2.75654e-05, acc 1
2017-08-08T17:17:33.653009: step 10623, loss 1.01671e-05, acc 1
2017-08-08T17:17:33.852349: step 10624, loss 4.04738e-06, acc 1
2017-08-08T17:17:34.091982: step 10625, loss 3.96173e-06, acc 1
2017-08-08T17:17:34.373815: step 10626, loss 6.61705e-06, acc 1
2017-08-08T17:17:34.684630: step 10627, loss 3.95758e-05, acc 1
2017-08-08T17:17:34.899799: step 10628, loss 3.91225e-05, acc 1
2017-08-08T17:17:35.197218: step 10629, loss 5.11456e-05, acc 1
2017-08-08T17:17:35.559710: step 10630, loss 0.00140289, acc 1
2017-08-08T17:17:35.796536: step 10631, loss 0.000158598, acc 1
2017-08-08T17:17:36.079162: step 10632, loss 2.45228e-05, acc 1
2017-08-08T17:17:36.434586: step 10633, loss 6.14364e-05, acc 1
2017-08-08T17:17:36.843824: step 10634, loss 0.00644853, acc 1
2017-08-08T17:17:37.201743: step 10635, loss 4.04158e-05, acc 1
2017-08-08T17:17:37.443374: step 10636, loss 0.000173075, acc 1
2017-08-08T17:17:37.673373: step 10637, loss 3.23261e-05, acc 1
2017-08-08T17:17:38.139988: step 10638, loss 0.000746698, acc 1
2017-08-08T17:17:38.393291: step 10639, loss 1.7862e-05, acc 1
2017-08-08T17:17:38.623430: step 10640, loss 0.000161792, acc 1
2017-08-08T17:17:38.950617: step 10641, loss 4.81918e-05, acc 1
2017-08-08T17:17:39.347253: step 10642, loss 9.35043e-05, acc 1
2017-08-08T17:17:39.752502: step 10643, loss 0.000198541, acc 1
2017-08-08T17:17:40.177734: step 10644, loss 0.000135912, acc 1
2017-08-08T17:17:40.453753: step 10645, loss 6.33992e-06, acc 1
2017-08-08T17:17:40.740737: step 10646, loss 3.70481e-05, acc 1
2017-08-08T17:17:41.191827: step 10647, loss 0.000239693, acc 1
2017-08-08T17:17:41.443140: step 10648, loss 2.99873e-05, acc 1
2017-08-08T17:17:41.713047: step 10649, loss 0.000188128, acc 1
2017-08-08T17:17:41.957403: step 10650, loss 0.00151443, acc 1
2017-08-08T17:17:42.297862: step 10651, loss 0.000109071, acc 1
2017-08-08T17:17:42.567201: step 10652, loss 0.0005065, acc 1
2017-08-08T17:17:42.929297: step 10653, loss 0.000155156, acc 1
2017-08-08T17:17:43.206971: step 10654, loss 0.00016238, acc 1
2017-08-08T17:17:43.517531: step 10655, loss 0.000678313, acc 1
2017-08-08T17:17:43.782718: step 10656, loss 8.6638e-06, acc 1
2017-08-08T17:17:44.066077: step 10657, loss 6.18393e-05, acc 1
2017-08-08T17:17:44.319819: step 10658, loss 1.42163e-05, acc 1
2017-08-08T17:17:44.536371: step 10659, loss 9.90441e-05, acc 1
2017-08-08T17:17:45.060172: step 10660, loss 3.14222e-05, acc 1
2017-08-08T17:17:45.436841: step 10661, loss 5.84099e-06, acc 1
2017-08-08T17:17:45.777010: step 10662, loss 4.12742e-06, acc 1
2017-08-08T17:17:46.037538: step 10663, loss 0.000123158, acc 1
2017-08-08T17:17:46.353772: step 10664, loss 2.46935e-05, acc 1
2017-08-08T17:17:46.654324: step 10665, loss 5.56995e-05, acc 1
2017-08-08T17:17:46.901871: step 10666, loss 0.000316067, acc 1
2017-08-08T17:17:47.188118: step 10667, loss 5.85846e-05, acc 1
2017-08-08T17:17:47.666468: step 10668, loss 3.62823e-06, acc 1
2017-08-08T17:17:48.071325: step 10669, loss 0.00081353, acc 1
2017-08-08T17:17:48.336384: step 10670, loss 0.00011566, acc 1
2017-08-08T17:17:48.568217: step 10671, loss 6.14492e-05, acc 1
2017-08-08T17:17:48.801403: step 10672, loss 2.60389e-06, acc 1
2017-08-08T17:17:49.230845: step 10673, loss 1.09109e-05, acc 1
2017-08-08T17:17:49.430573: step 10674, loss 3.98801e-05, acc 1
2017-08-08T17:17:49.671727: step 10675, loss 0.000302891, acc 1
2017-08-08T17:17:49.965491: step 10676, loss 0.000114171, acc 1
2017-08-08T17:17:50.392102: step 10677, loss 0.00014405, acc 1
2017-08-08T17:17:50.769428: step 10678, loss 0.0197794, acc 0.984375
2017-08-08T17:17:51.174720: step 10679, loss 0.00788002, acc 1
2017-08-08T17:17:51.477533: step 10680, loss 0.0109542, acc 1
2017-08-08T17:17:51.714811: step 10681, loss 4.60856e-05, acc 1
2017-08-08T17:17:52.026085: step 10682, loss 0.0542532, acc 0.984375
2017-08-08T17:17:52.226289: step 10683, loss 0.00909891, acc 1
2017-08-08T17:17:52.433377: step 10684, loss 4.78138e-05, acc 1
2017-08-08T17:17:52.676070: step 10685, loss 0.000279945, acc 1
2017-08-08T17:17:52.989996: step 10686, loss 0.000935902, acc 1
2017-08-08T17:17:53.271416: step 10687, loss 6.12808e-07, acc 1
2017-08-08T17:17:53.514004: step 10688, loss 6.04142e-05, acc 1
2017-08-08T17:17:53.789926: step 10689, loss 9.39776e-05, acc 1
2017-08-08T17:17:54.009406: step 10690, loss 0.00401528, acc 1
2017-08-08T17:17:54.411694: step 10691, loss 0.000105177, acc 1
2017-08-08T17:17:54.676073: step 10692, loss 0.00858834, acc 1
2017-08-08T17:17:54.953834: step 10693, loss 3.5629e-06, acc 1
2017-08-08T17:17:55.201492: step 10694, loss 3.12501e-05, acc 1
2017-08-08T17:17:55.419715: step 10695, loss 2.38469e-05, acc 1
2017-08-08T17:17:55.753377: step 10696, loss 0.00773808, acc 1
2017-08-08T17:17:56.104935: step 10697, loss 0.00526288, acc 1
2017-08-08T17:17:56.465251: step 10698, loss 0.000113662, acc 1
2017-08-08T17:17:56.742927: step 10699, loss 2.39527e-06, acc 1
2017-08-08T17:17:57.098354: step 10700, loss 0.00683921, acc 1

Evaluation:
2017-08-08T17:17:57.740331: step 10700, loss 3.54589, acc 0.725141

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-10700

2017-08-08T17:17:58.372435: step 10701, loss 5.74592e-05, acc 1
2017-08-08T17:17:58.744910: step 10702, loss 0.00183828, acc 1
2017-08-08T17:17:59.021628: step 10703, loss 2.10131e-05, acc 1
2017-08-08T17:17:59.224062: step 10704, loss 0.000238574, acc 1
2017-08-08T17:17:59.570805: step 10705, loss 5.66769e-06, acc 1
2017-08-08T17:17:59.807164: step 10706, loss 8.46909e-06, acc 1
2017-08-08T17:18:00.031408: step 10707, loss 0.000675437, acc 1
2017-08-08T17:18:00.270225: step 10708, loss 0.0005932, acc 1
2017-08-08T17:18:00.621900: step 10709, loss 2.06561e-06, acc 1
2017-08-08T17:18:00.936423: step 10710, loss 0.000110585, acc 1
2017-08-08T17:18:01.265362: step 10711, loss 3.36178e-06, acc 1
2017-08-08T17:18:01.507843: step 10712, loss 4.17475e-05, acc 1
2017-08-08T17:18:01.748254: step 10713, loss 0.00437049, acc 1
2017-08-08T17:18:02.165909: step 10714, loss 5.35525e-05, acc 1
2017-08-08T17:18:02.477716: step 10715, loss 6.33604e-06, acc 1
2017-08-08T17:18:02.801060: step 10716, loss 0.000302217, acc 1
2017-08-08T17:18:03.107188: step 10717, loss 6.0675e-05, acc 1
2017-08-08T17:18:03.512847: step 10718, loss 5.31842e-05, acc 1
2017-08-08T17:18:03.973532: step 10719, loss 0.000972521, acc 1
2017-08-08T17:18:04.360454: step 10720, loss 0.000523348, acc 1
2017-08-08T17:18:04.645479: step 10721, loss 3.53491e-05, acc 1
2017-08-08T17:18:04.999503: step 10722, loss 0.00329268, acc 1
2017-08-08T17:18:05.309307: step 10723, loss 0.000165563, acc 1
2017-08-08T17:18:05.593761: step 10724, loss 0.000101109, acc 1
2017-08-08T17:18:05.827609: step 10725, loss 4.99513e-05, acc 1
2017-08-08T17:18:06.140418: step 10726, loss 4.19512e-05, acc 1
2017-08-08T17:18:06.513611: step 10727, loss 1.21437e-05, acc 1
2017-08-08T17:18:06.817621: step 10728, loss 0.000539513, acc 1
2017-08-08T17:18:07.094347: step 10729, loss 0.000252574, acc 1
2017-08-08T17:18:07.275416: step 10730, loss 1.56586e-05, acc 1
2017-08-08T17:18:07.548323: step 10731, loss 1.75088e-07, acc 1
2017-08-08T17:18:07.798506: step 10732, loss 0.000251088, acc 1
2017-08-08T17:18:07.975832: step 10733, loss 5.51726e-05, acc 1
2017-08-08T17:18:08.220455: step 10734, loss 7.72487e-06, acc 1
2017-08-08T17:18:08.637313: step 10735, loss 1.18706e-05, acc 1
2017-08-08T17:18:08.915155: step 10736, loss 0.000276008, acc 1
2017-08-08T17:18:09.147862: step 10737, loss 6.16981e-05, acc 1
2017-08-08T17:18:09.344415: step 10738, loss 1.37585e-05, acc 1
2017-08-08T17:18:09.674245: step 10739, loss 0.00032435, acc 1
2017-08-08T17:18:09.884056: step 10740, loss 2.49028e-06, acc 1
2017-08-08T17:18:10.083124: step 10741, loss 0.000169301, acc 1
2017-08-08T17:18:10.332918: step 10742, loss 2.30677e-05, acc 1
2017-08-08T17:18:10.728156: step 10743, loss 0.000199207, acc 1
2017-08-08T17:18:11.133833: step 10744, loss 0.000167151, acc 1
2017-08-08T17:18:11.491915: step 10745, loss 3.61141e-06, acc 1
2017-08-08T17:18:11.750818: step 10746, loss 3.75956e-05, acc 1
2017-08-08T17:18:12.115464: step 10747, loss 0.00253492, acc 1
2017-08-08T17:18:12.486253: step 10748, loss 0.000262927, acc 1
2017-08-08T17:18:12.742469: step 10749, loss 3.23155e-06, acc 1
2017-08-08T17:18:13.030077: step 10750, loss 2.89987e-05, acc 1
2017-08-08T17:18:13.313043: step 10751, loss 0.00311118, acc 1
2017-08-08T17:18:13.711176: step 10752, loss 7.76948e-05, acc 1
2017-08-08T17:18:14.156321: step 10753, loss 2.33969e-05, acc 1
2017-08-08T17:18:14.553278: step 10754, loss 0.000162262, acc 1
2017-08-08T17:18:14.769471: step 10755, loss 0.000283707, acc 1
2017-08-08T17:18:15.057037: step 10756, loss 2.29449e-05, acc 1
2017-08-08T17:18:15.334295: step 10757, loss 0.00303472, acc 1
2017-08-08T17:18:15.571615: step 10758, loss 5.44222e-06, acc 1
2017-08-08T17:18:15.772201: step 10759, loss 1.68939e-06, acc 1
2017-08-08T17:18:16.124350: step 10760, loss 3.52431e-05, acc 1
2017-08-08T17:18:16.435929: step 10761, loss 2.4832e-05, acc 1
2017-08-08T17:18:16.697427: step 10762, loss 0.000537132, acc 1
2017-08-08T17:18:16.938213: step 10763, loss 0.000287541, acc 1
2017-08-08T17:18:17.194502: step 10764, loss 0.0150322, acc 0.984375
2017-08-08T17:18:17.554956: step 10765, loss 6.12742e-05, acc 1
2017-08-08T17:18:17.805379: step 10766, loss 2.69339e-05, acc 1
2017-08-08T17:18:18.052905: step 10767, loss 2.36762e-05, acc 1
2017-08-08T17:18:18.324770: step 10768, loss 9.63385e-05, acc 1
2017-08-08T17:18:18.663952: step 10769, loss 1.82904e-06, acc 1
2017-08-08T17:18:18.902328: step 10770, loss 8.86863e-05, acc 1
2017-08-08T17:18:19.149340: step 10771, loss 1.00904e-05, acc 1
2017-08-08T17:18:19.373017: step 10772, loss 2.95534e-05, acc 1
2017-08-08T17:18:19.569371: step 10773, loss 0.000624333, acc 1
2017-08-08T17:18:19.887659: step 10774, loss 1.66462e-05, acc 1
2017-08-08T17:18:20.161637: step 10775, loss 5.13334e-05, acc 1
2017-08-08T17:18:20.452723: step 10776, loss 1.75261e-05, acc 1
2017-08-08T17:18:20.728421: step 10777, loss 6.04575e-05, acc 1
2017-08-08T17:18:21.101309: step 10778, loss 0.000801343, acc 1
2017-08-08T17:18:21.441352: step 10779, loss 4.96275e-05, acc 1
2017-08-08T17:18:21.727357: step 10780, loss 3.02812e-05, acc 1
2017-08-08T17:18:21.969973: step 10781, loss 4.48788e-05, acc 1
2017-08-08T17:18:22.377754: step 10782, loss 0.000701588, acc 1
2017-08-08T17:18:22.648826: step 10783, loss 6.55246e-05, acc 1
2017-08-08T17:18:22.872740: step 10784, loss 3.01022e-05, acc 1
2017-08-08T17:18:23.130576: step 10785, loss 3.65076e-07, acc 1
2017-08-08T17:18:23.429829: step 10786, loss 5.94837e-05, acc 1
2017-08-08T17:18:23.811639: step 10787, loss 4.24103e-05, acc 1
2017-08-08T17:18:24.122852: step 10788, loss 0.000126252, acc 1
2017-08-08T17:18:24.379671: step 10789, loss 0.000673277, acc 1
2017-08-08T17:18:24.651708: step 10790, loss 7.79629e-06, acc 1
2017-08-08T17:18:24.922488: step 10791, loss 0.000459473, acc 1
2017-08-08T17:18:25.109568: step 10792, loss 0.000104778, acc 1
2017-08-08T17:18:25.320480: step 10793, loss 0.0107633, acc 1
2017-08-08T17:18:25.547808: step 10794, loss 0.000364052, acc 1
2017-08-08T17:18:26.019604: step 10795, loss 0.00124445, acc 1
2017-08-08T17:18:26.415141: step 10796, loss 0.0344214, acc 0.984375
2017-08-08T17:18:26.741015: step 10797, loss 9.07086e-05, acc 1
2017-08-08T17:18:26.989573: step 10798, loss 5.64983e-05, acc 1
2017-08-08T17:18:27.228708: step 10799, loss 3.82664e-05, acc 1
2017-08-08T17:18:27.520211: step 10800, loss 2.19002e-05, acc 1

Evaluation:
2017-08-08T17:18:28.159741: step 10800, loss 3.58842, acc 0.718574

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-10800

2017-08-08T17:18:28.568457: step 10801, loss 9.87885e-06, acc 1
2017-08-08T17:18:28.925963: step 10802, loss 0.00186125, acc 1
2017-08-08T17:18:29.204334: step 10803, loss 2.12503e-05, acc 1
2017-08-08T17:18:29.424811: step 10804, loss 3.20638e-05, acc 1
2017-08-08T17:18:29.630031: step 10805, loss 7.6883e-06, acc 1
2017-08-08T17:18:29.889418: step 10806, loss 0.00028951, acc 1
2017-08-08T17:18:30.257963: step 10807, loss 0.00025822, acc 1
2017-08-08T17:18:30.480943: step 10808, loss 0.00801208, acc 1
2017-08-08T17:18:30.683738: step 10809, loss 0.142103, acc 0.984375
2017-08-08T17:18:31.016743: step 10810, loss 0.00199384, acc 1
2017-08-08T17:18:31.293376: step 10811, loss 7.45174e-05, acc 1
2017-08-08T17:18:31.509424: step 10812, loss 1.08399e-05, acc 1
2017-08-08T17:18:31.706996: step 10813, loss 2.29282e-05, acc 1
2017-08-08T17:18:31.901328: step 10814, loss 0.000176658, acc 1
2017-08-08T17:18:32.149183: step 10815, loss 1.65025e-06, acc 1
2017-08-08T17:18:32.340059: step 10816, loss 2.37077e-05, acc 1
2017-08-08T17:18:32.606047: step 10817, loss 1.67659e-05, acc 1
2017-08-08T17:18:32.829784: step 10818, loss 0.000122323, acc 1
2017-08-08T17:18:33.116034: step 10819, loss 0.000303702, acc 1
2017-08-08T17:18:33.386208: step 10820, loss 3.11031e-05, acc 1
2017-08-08T17:18:33.708881: step 10821, loss 0.0002752, acc 1
2017-08-08T17:18:33.955448: step 10822, loss 1.94704e-05, acc 1
2017-08-08T17:18:34.148425: step 10823, loss 0.000112145, acc 1
2017-08-08T17:18:34.517454: step 10824, loss 5.31732e-06, acc 1
2017-08-08T17:18:34.750840: step 10825, loss 3.39398e-05, acc 1
2017-08-08T17:18:34.953626: step 10826, loss 1.61509e-05, acc 1
2017-08-08T17:18:35.128092: step 10827, loss 0.000206447, acc 1
2017-08-08T17:18:35.397286: step 10828, loss 0.000100204, acc 1
2017-08-08T17:18:35.822989: step 10829, loss 2.25123e-05, acc 1
2017-08-08T17:18:36.192961: step 10830, loss 1.08972e-05, acc 1
2017-08-08T17:18:36.407635: step 10831, loss 4.87898e-05, acc 1
2017-08-08T17:18:36.621372: step 10832, loss 8.11657e-05, acc 1
2017-08-08T17:18:36.978894: step 10833, loss 0.00012923, acc 1
2017-08-08T17:18:37.245408: step 10834, loss 0.000253267, acc 1
2017-08-08T17:18:37.462955: step 10835, loss 3.77719e-06, acc 1
2017-08-08T17:18:37.769378: step 10836, loss 8.96062e-05, acc 1
2017-08-08T17:18:38.093308: step 10837, loss 4.22592e-05, acc 1
2017-08-08T17:18:38.422899: step 10838, loss 2.37292e-06, acc 1
2017-08-08T17:18:38.645431: step 10839, loss 0.000233481, acc 1
2017-08-08T17:18:38.964027: step 10840, loss 2.22601e-05, acc 1
2017-08-08T17:18:39.222774: step 10841, loss 0.000123716, acc 1
2017-08-08T17:18:39.455278: step 10842, loss 3.63757e-06, acc 1
2017-08-08T17:18:39.658995: step 10843, loss 0.000389748, acc 1
2017-08-08T17:18:39.960025: step 10844, loss 0.00447474, acc 1
2017-08-08T17:18:40.507574: step 10845, loss 3.44579e-05, acc 1
2017-08-08T17:18:40.871185: step 10846, loss 6.48308e-05, acc 1
2017-08-08T17:18:41.076615: step 10847, loss 4.40778e-05, acc 1
2017-08-08T17:18:41.256487: step 10848, loss 4.778e-05, acc 1
2017-08-08T17:18:41.567131: step 10849, loss 0.00118397, acc 1
2017-08-08T17:18:41.763952: step 10850, loss 9.29231e-05, acc 1
2017-08-08T17:18:41.982610: step 10851, loss 2.16066e-07, acc 1
2017-08-08T17:18:42.200400: step 10852, loss 1.35411e-06, acc 1
2017-08-08T17:18:42.497390: step 10853, loss 0.000569911, acc 1
2017-08-08T17:18:42.883164: step 10854, loss 0.00134295, acc 1
2017-08-08T17:18:43.220178: step 10855, loss 1.36619e-05, acc 1
2017-08-08T17:18:43.481543: step 10856, loss 3.98968e-06, acc 1
2017-08-08T17:18:43.732767: step 10857, loss 0.000156601, acc 1
2017-08-08T17:18:44.067489: step 10858, loss 0.000134103, acc 1
2017-08-08T17:18:44.262720: step 10859, loss 6.89708e-06, acc 1
2017-08-08T17:18:44.468889: step 10860, loss 0.000111095, acc 1
2017-08-08T17:18:44.658499: step 10861, loss 0.000450788, acc 1
2017-08-08T17:18:44.993328: step 10862, loss 0.000192647, acc 1
2017-08-08T17:18:45.242276: step 10863, loss 2.28415e-05, acc 1
2017-08-08T17:18:45.530684: step 10864, loss 0.000136595, acc 1
2017-08-08T17:18:45.743579: step 10865, loss 0.000121642, acc 1
2017-08-08T17:18:45.924341: step 10866, loss 2.00948e-05, acc 1
2017-08-08T17:18:46.268648: step 10867, loss 0.00339458, acc 1
2017-08-08T17:18:46.487685: step 10868, loss 4.63338e-05, acc 1
2017-08-08T17:18:46.739106: step 10869, loss 0.000295956, acc 1
2017-08-08T17:18:47.089375: step 10870, loss 9.40836e-05, acc 1
2017-08-08T17:18:47.398868: step 10871, loss 0.000339682, acc 1
2017-08-08T17:18:47.653657: step 10872, loss 0.00100066, acc 1
2017-08-08T17:18:47.848412: step 10873, loss 0.000188803, acc 1
2017-08-08T17:18:48.059014: step 10874, loss 0.000751654, acc 1
2017-08-08T17:18:48.431323: step 10875, loss 8.82186e-05, acc 1
2017-08-08T17:18:48.640708: step 10876, loss 0.00016753, acc 1
2017-08-08T17:18:48.836082: step 10877, loss 7.70628e-06, acc 1
2017-08-08T17:18:49.038416: step 10878, loss 0.00138261, acc 1
2017-08-08T17:18:49.369312: step 10879, loss 0.0112269, acc 0.984375
2017-08-08T17:18:49.665876: step 10880, loss 3.35674e-05, acc 1
2017-08-08T17:18:49.948790: step 10881, loss 2.42128e-06, acc 1
2017-08-08T17:18:50.175985: step 10882, loss 0.000102188, acc 1
2017-08-08T17:18:50.370778: step 10883, loss 6.57048e-05, acc 1
2017-08-08T17:18:50.599380: step 10884, loss 5.19927e-05, acc 1
2017-08-08T17:18:50.791656: step 10885, loss 0.00014417, acc 1
2017-08-08T17:18:51.003316: step 10886, loss 0.000263599, acc 1
2017-08-08T17:18:51.256627: step 10887, loss 3.84973e-06, acc 1
2017-08-08T17:18:51.505207: step 10888, loss 3.24662e-05, acc 1
2017-08-08T17:18:51.836854: step 10889, loss 0.00094509, acc 1
2017-08-08T17:18:52.089934: step 10890, loss 0.021364, acc 0.984375
2017-08-08T17:18:52.283641: step 10891, loss 0.000117081, acc 1
2017-08-08T17:18:52.581874: step 10892, loss 0.00787246, acc 1
2017-08-08T17:18:52.823816: step 10893, loss 0.000605533, acc 1
2017-08-08T17:18:53.087092: step 10894, loss 0.000768885, acc 1
2017-08-08T17:18:53.385372: step 10895, loss 0.0015929, acc 1
2017-08-08T17:18:53.776415: step 10896, loss 0.0501629, acc 0.984375
2017-08-08T17:18:54.090912: step 10897, loss 0.00144985, acc 1
2017-08-08T17:18:54.468291: step 10898, loss 4.28578e-05, acc 1
2017-08-08T17:18:54.728422: step 10899, loss 2.39203e-05, acc 1
2017-08-08T17:18:55.002665: step 10900, loss 0.00326573, acc 1

Evaluation:
2017-08-08T17:18:55.853415: step 10900, loss 3.73096, acc 0.699812

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-10900

2017-08-08T17:18:56.308148: step 10901, loss 0.00601657, acc 1
2017-08-08T17:18:56.647541: step 10902, loss 0.000189357, acc 1
2017-08-08T17:18:56.977670: step 10903, loss 0.0157978, acc 0.984375
2017-08-08T17:18:57.246813: step 10904, loss 0.00816535, acc 1
2017-08-08T17:18:57.456917: step 10905, loss 1.98736e-06, acc 1
2017-08-08T17:18:57.702650: step 10906, loss 4.61336e-05, acc 1
2017-08-08T17:18:58.042270: step 10907, loss 6.06685e-05, acc 1
2017-08-08T17:18:58.256410: step 10908, loss 1.25215e-05, acc 1
2017-08-08T17:18:58.543921: step 10909, loss 6.43908e-05, acc 1
2017-08-08T17:18:58.794577: step 10910, loss 5.49352e-05, acc 1
2017-08-08T17:18:59.217306: step 10911, loss 0.00400304, acc 1
2017-08-08T17:18:59.578256: step 10912, loss 0.0847508, acc 0.984375
2017-08-08T17:18:59.849395: step 10913, loss 1.31147e-05, acc 1
2017-08-08T17:19:00.056731: step 10914, loss 9.36298e-05, acc 1
2017-08-08T17:19:00.356597: step 10915, loss 0.00257791, acc 1
2017-08-08T17:19:00.697612: step 10916, loss 0.00354715, acc 1
2017-08-08T17:19:00.978200: step 10917, loss 0.0005811, acc 1
2017-08-08T17:19:01.277487: step 10918, loss 0.000201259, acc 1
2017-08-08T17:19:01.705459: step 10919, loss 0.000109591, acc 1
2017-08-08T17:19:02.081479: step 10920, loss 1.81976e-06, acc 1
2017-08-08T17:19:02.389495: step 10921, loss 3.88536e-06, acc 1
2017-08-08T17:19:02.648590: step 10922, loss 0.000162537, acc 1
2017-08-08T17:19:02.960500: step 10923, loss 4.16732e-05, acc 1
2017-08-08T17:19:03.364685: step 10924, loss 0.000704722, acc 1
2017-08-08T17:19:03.633212: step 10925, loss 1.32677e-05, acc 1
2017-08-08T17:19:03.953693: step 10926, loss 0.00705494, acc 1
2017-08-08T17:19:04.204631: step 10927, loss 2.02035e-05, acc 1
2017-08-08T17:19:04.554762: step 10928, loss 0.0142425, acc 0.984375
2017-08-08T17:19:04.906598: step 10929, loss 2.38218e-06, acc 1
2017-08-08T17:19:05.245354: step 10930, loss 2.6869e-05, acc 1
2017-08-08T17:19:05.516855: step 10931, loss 2.75916e-05, acc 1
2017-08-08T17:19:05.877365: step 10932, loss 6.37548e-06, acc 1
2017-08-08T17:19:06.243885: step 10933, loss 0.00963544, acc 1
2017-08-08T17:19:06.556585: step 10934, loss 1.58507e-06, acc 1
2017-08-08T17:19:06.834926: step 10935, loss 2.5511e-05, acc 1
2017-08-08T17:19:07.201397: step 10936, loss 0.000317475, acc 1
2017-08-08T17:19:07.664012: step 10937, loss 0.00937852, acc 1
2017-08-08T17:19:08.018547: step 10938, loss 0.000201817, acc 1
2017-08-08T17:19:08.351492: step 10939, loss 0.00122655, acc 1
2017-08-08T17:19:08.631168: step 10940, loss 1.24796e-06, acc 1
2017-08-08T17:19:09.110610: step 10941, loss 6.85795e-05, acc 1
2017-08-08T17:19:09.418922: step 10942, loss 0.0457367, acc 0.984375
2017-08-08T17:19:09.710353: step 10943, loss 9.27855e-05, acc 1
2017-08-08T17:19:09.954361: step 10944, loss 8.46565e-06, acc 1
2017-08-08T17:19:10.260802: step 10945, loss 5.97862e-06, acc 1
2017-08-08T17:19:10.643529: step 10946, loss 3.11491e-05, acc 1
2017-08-08T17:19:11.009050: step 10947, loss 0.00126586, acc 1
2017-08-08T17:19:11.346595: step 10948, loss 3.91166e-05, acc 1
2017-08-08T17:19:11.640605: step 10949, loss 0.00594706, acc 1
2017-08-08T17:19:11.903055: step 10950, loss 0.000153721, acc 1
2017-08-08T17:19:12.339058: step 10951, loss 6.16107e-06, acc 1
2017-08-08T17:19:12.574295: step 10952, loss 0.000135594, acc 1
2017-08-08T17:19:12.893391: step 10953, loss 1.41561e-07, acc 1
2017-08-08T17:19:13.243773: step 10954, loss 4.80929e-05, acc 1
2017-08-08T17:19:13.609505: step 10955, loss 0.000108884, acc 1
2017-08-08T17:19:13.920056: step 10956, loss 0.000246036, acc 1
2017-08-08T17:19:14.224350: step 10957, loss 0.00509628, acc 1
2017-08-08T17:19:14.465405: step 10958, loss 0.000148547, acc 1
2017-08-08T17:19:14.773431: step 10959, loss 0.000918686, acc 1
2017-08-08T17:19:14.997708: step 10960, loss 0.00147391, acc 1
2017-08-08T17:19:15.268868: step 10961, loss 3.14774e-06, acc 1
2017-08-08T17:19:15.457208: step 10962, loss 1.97748e-05, acc 1
2017-08-08T17:19:15.742547: step 10963, loss 0.00897581, acc 1
2017-08-08T17:19:16.061495: step 10964, loss 4.89874e-07, acc 1
2017-08-08T17:19:16.398583: step 10965, loss 0.00301557, acc 1
2017-08-08T17:19:16.641163: step 10966, loss 4.72888e-06, acc 1
2017-08-08T17:19:16.889969: step 10967, loss 0.000201225, acc 1
2017-08-08T17:19:17.224125: step 10968, loss 8.95351e-05, acc 1
2017-08-08T17:19:17.437635: step 10969, loss 2.48515e-05, acc 1
2017-08-08T17:19:17.701547: step 10970, loss 0.000487639, acc 1
2017-08-08T17:19:18.066981: step 10971, loss 0.000155137, acc 1
2017-08-08T17:19:18.502204: step 10972, loss 0.000184916, acc 1
2017-08-08T17:19:18.822244: step 10973, loss 8.49605e-05, acc 1
2017-08-08T17:19:19.054617: step 10974, loss 5.05592e-05, acc 1
2017-08-08T17:19:19.263323: step 10975, loss 0.000350529, acc 1
2017-08-08T17:19:19.510772: step 10976, loss 3.65163e-05, acc 1
2017-08-08T17:19:19.765891: step 10977, loss 0.00955789, acc 1
2017-08-08T17:19:19.964517: step 10978, loss 0.00014129, acc 1
2017-08-08T17:19:20.175465: step 10979, loss 0.000262786, acc 1
2017-08-08T17:19:20.374125: step 10980, loss 0.000103947, acc 1
2017-08-08T17:19:20.640746: step 10981, loss 5.64343e-06, acc 1
2017-08-08T17:19:20.910463: step 10982, loss 6.89175e-07, acc 1
2017-08-08T17:19:21.155691: step 10983, loss 1.35222e-05, acc 1
2017-08-08T17:19:21.384532: step 10984, loss 2.04891e-08, acc 1
2017-08-08T17:19:21.637328: step 10985, loss 2.5927e-05, acc 1
2017-08-08T17:19:21.967730: step 10986, loss 0.00355192, acc 1
2017-08-08T17:19:22.191084: step 10987, loss 8.21418e-07, acc 1
2017-08-08T17:19:22.399003: step 10988, loss 0.000352888, acc 1
2017-08-08T17:19:22.757376: step 10989, loss 0.00136947, acc 1
2017-08-08T17:19:23.024742: step 10990, loss 1.80258e-05, acc 1
2017-08-08T17:19:23.378881: step 10991, loss 1.33984e-05, acc 1
2017-08-08T17:19:23.565053: step 10992, loss 1.77148e-05, acc 1
2017-08-08T17:19:23.866443: step 10993, loss 1.71542e-06, acc 1
2017-08-08T17:19:24.158902: step 10994, loss 0.00119039, acc 1
2017-08-08T17:19:24.412672: step 10995, loss 0.000197236, acc 1
2017-08-08T17:19:24.630304: step 10996, loss 0.000732865, acc 1
2017-08-08T17:19:24.948094: step 10997, loss 5.49631e-06, acc 1
2017-08-08T17:19:25.265113: step 10998, loss 2.72296e-06, acc 1
2017-08-08T17:19:25.537002: step 10999, loss 3.778e-05, acc 1
2017-08-08T17:19:25.734304: step 11000, loss 3.68216e-05, acc 1

Evaluation:
2017-08-08T17:19:26.407569: step 11000, loss 3.65713, acc 0.709193

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-11000

2017-08-08T17:19:26.823176: step 11001, loss 0.000339954, acc 1
2017-08-08T17:19:27.028139: step 11002, loss 0.00298143, acc 1
2017-08-08T17:19:27.337333: step 11003, loss 0.0016959, acc 1
2017-08-08T17:19:27.706325: step 11004, loss 0.000432164, acc 1
2017-08-08T17:19:28.066568: step 11005, loss 0.00102492, acc 1
2017-08-08T17:19:28.289220: step 11006, loss 2.03859e-05, acc 1
2017-08-08T17:19:28.570798: step 11007, loss 1.16867e-05, acc 1
2017-08-08T17:19:28.980193: step 11008, loss 2.58907e-07, acc 1
2017-08-08T17:19:29.248683: step 11009, loss 0.0239854, acc 0.984375
2017-08-08T17:19:29.604059: step 11010, loss 0.000182657, acc 1
2017-08-08T17:19:29.908720: step 11011, loss 0.000114893, acc 1
2017-08-08T17:19:30.281328: step 11012, loss 8.17694e-07, acc 1
2017-08-08T17:19:30.703552: step 11013, loss 1.27866e-05, acc 1
2017-08-08T17:19:31.072011: step 11014, loss 0.000702029, acc 1
2017-08-08T17:19:31.247816: step 11015, loss 1.60185e-06, acc 1
2017-08-08T17:19:31.517333: step 11016, loss 1.27914e-05, acc 1
2017-08-08T17:19:31.802219: step 11017, loss 1.00023e-06, acc 1
2017-08-08T17:19:32.079828: step 11018, loss 0.000274257, acc 1
2017-08-08T17:19:32.297183: step 11019, loss 0.00153388, acc 1
2017-08-08T17:19:32.628381: step 11020, loss 9.90667e-05, acc 1
2017-08-08T17:19:32.986179: step 11021, loss 2.86646e-05, acc 1
2017-08-08T17:19:33.369689: step 11022, loss 3.51162e-05, acc 1
2017-08-08T17:19:33.568921: step 11023, loss 0.000211641, acc 1
2017-08-08T17:19:33.850885: step 11024, loss 0.000286366, acc 1
2017-08-08T17:19:34.230920: step 11025, loss 3.31996e-05, acc 1
2017-08-08T17:19:34.524872: step 11026, loss 1.14887e-05, acc 1
2017-08-08T17:19:35.021987: step 11027, loss 6.49166e-05, acc 1
2017-08-08T17:19:35.284092: step 11028, loss 1.8831e-06, acc 1
2017-08-08T17:19:35.629375: step 11029, loss 0.000308263, acc 1
2017-08-08T17:19:35.869105: step 11030, loss 0.000857329, acc 1
2017-08-08T17:19:36.230823: step 11031, loss 0.000157453, acc 1
2017-08-08T17:19:36.482049: step 11032, loss 0.000370012, acc 1
2017-08-08T17:19:36.797387: step 11033, loss 4.49624e-06, acc 1
2017-08-08T17:19:37.059103: step 11034, loss 0.00038496, acc 1
2017-08-08T17:19:37.274864: step 11035, loss 0.000721819, acc 1
2017-08-08T17:19:37.697353: step 11036, loss 5.68104e-07, acc 1
2017-08-08T17:19:37.936828: step 11037, loss 4.75372e-05, acc 1
2017-08-08T17:19:38.209443: step 11038, loss 0.00110311, acc 1
2017-08-08T17:19:38.474255: step 11039, loss 0.000565086, acc 1
2017-08-08T17:19:38.690589: step 11040, loss 0.000699119, acc 1
2017-08-08T17:19:39.141332: step 11041, loss 2.73748e-05, acc 1
2017-08-08T17:19:39.395894: step 11042, loss 2.33488e-05, acc 1
2017-08-08T17:19:39.750081: step 11043, loss 2.26867e-06, acc 1
2017-08-08T17:19:40.102008: step 11044, loss 0.000362423, acc 1
2017-08-08T17:19:40.363558: step 11045, loss 0.000142972, acc 1
2017-08-08T17:19:40.738818: step 11046, loss 0.000986958, acc 1
2017-08-08T17:19:41.017679: step 11047, loss 4.58696e-05, acc 1
2017-08-08T17:19:41.265340: step 11048, loss 2.63358e-05, acc 1
2017-08-08T17:19:41.568178: step 11049, loss 2.31962e-05, acc 1
2017-08-08T17:19:41.744334: step 11050, loss 7.19398e-06, acc 1
2017-08-08T17:19:41.982125: step 11051, loss 5.387e-05, acc 1
2017-08-08T17:19:42.240669: step 11052, loss 0.000953597, acc 1
2017-08-08T17:19:42.501331: step 11053, loss 4.32109e-06, acc 1
2017-08-08T17:19:42.809570: step 11054, loss 0.00104638, acc 1
2017-08-08T17:19:43.002886: step 11055, loss 3.04345e-06, acc 1
2017-08-08T17:19:43.263322: step 11056, loss 0.0860609, acc 0.96875
2017-08-08T17:19:43.473053: step 11057, loss 0.00340891, acc 1
2017-08-08T17:19:43.730972: step 11058, loss 4.02052e-05, acc 1
2017-08-08T17:19:44.008520: step 11059, loss 6.77892e-06, acc 1
2017-08-08T17:19:44.225406: step 11060, loss 2.38638e-05, acc 1
2017-08-08T17:19:44.445383: step 11061, loss 3.98815e-05, acc 1
2017-08-08T17:19:44.789861: step 11062, loss 0.000190033, acc 1
2017-08-08T17:19:45.095356: step 11063, loss 7.99113e-06, acc 1
2017-08-08T17:19:45.379855: step 11064, loss 1.28664e-05, acc 1
2017-08-08T17:19:45.608275: step 11065, loss 3.48979e-05, acc 1
2017-08-08T17:19:46.022728: step 11066, loss 5.66153e-06, acc 1
2017-08-08T17:19:46.449631: step 11067, loss 6.35273e-06, acc 1
2017-08-08T17:19:46.860716: step 11068, loss 0.000220206, acc 1
2017-08-08T17:19:47.072603: step 11069, loss 0.000265245, acc 1
2017-08-08T17:19:47.301461: step 11070, loss 1.1405e-05, acc 1
2017-08-08T17:19:47.756063: step 11071, loss 0.000114558, acc 1
2017-08-08T17:19:48.019535: step 11072, loss 5.41966e-06, acc 1
2017-08-08T17:19:48.281202: step 11073, loss 0.000659289, acc 1
2017-08-08T17:19:48.637066: step 11074, loss 0.000127303, acc 1
2017-08-08T17:19:49.038606: step 11075, loss 0.00434974, acc 1
2017-08-08T17:19:49.446289: step 11076, loss 0.000193919, acc 1
2017-08-08T17:19:49.814235: step 11077, loss 1.70237e-06, acc 1
2017-08-08T17:19:50.030426: step 11078, loss 0.000878577, acc 1
2017-08-08T17:19:50.203816: step 11079, loss 2.79847e-05, acc 1
2017-08-08T17:19:50.593919: step 11080, loss 0.000123199, acc 1
2017-08-08T17:19:50.868968: step 11081, loss 2.56104e-06, acc 1
2017-08-08T17:19:51.172947: step 11082, loss 7.93873e-05, acc 1
2017-08-08T17:19:51.381670: step 11083, loss 3.49968e-05, acc 1
2017-08-08T17:19:51.812659: step 11084, loss 2.71871e-05, acc 1
2017-08-08T17:19:52.137328: step 11085, loss 7.16824e-05, acc 1
2017-08-08T17:19:52.340480: step 11086, loss 1.79496e-05, acc 1
2017-08-08T17:19:52.586628: step 11087, loss 7.90395e-05, acc 1
2017-08-08T17:19:52.874721: step 11088, loss 7.75348e-06, acc 1
2017-08-08T17:19:53.128643: step 11089, loss 6.37281e-05, acc 1
2017-08-08T17:19:53.300214: step 11090, loss 0.000267033, acc 1
2017-08-08T17:19:53.497396: step 11091, loss 0.00413737, acc 1
2017-08-08T17:19:53.740530: step 11092, loss 6.70771e-05, acc 1
2017-08-08T17:19:54.154628: step 11093, loss 0.000279759, acc 1
2017-08-08T17:19:54.463896: step 11094, loss 1.68448e-05, acc 1
2017-08-08T17:19:54.723653: step 11095, loss 4.00401e-05, acc 1
2017-08-08T17:19:54.900894: step 11096, loss 3.32614e-05, acc 1
2017-08-08T17:19:55.299348: step 11097, loss 1.24793e-06, acc 1
2017-08-08T17:19:55.519704: step 11098, loss 5.14034e-05, acc 1
2017-08-08T17:19:55.701546: step 11099, loss 0.000369805, acc 1
2017-08-08T17:19:55.934696: step 11100, loss 1.01038e-05, acc 1

Evaluation:
2017-08-08T17:19:56.552244: step 11100, loss 3.70064, acc 0.709193

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-11100

2017-08-08T17:19:57.029629: step 11101, loss 1.39682e-05, acc 1
2017-08-08T17:19:57.198049: step 11102, loss 5.05065e-05, acc 1
2017-08-08T17:19:57.376158: step 11103, loss 3.4581e-05, acc 1
2017-08-08T17:19:57.790084: step 11104, loss 0.00192883, acc 1
2017-08-08T17:19:58.154417: step 11105, loss 6.07902e-06, acc 1
2017-08-08T17:19:58.334423: step 11106, loss 0.000273212, acc 1
2017-08-08T17:19:58.601801: step 11107, loss 0.000120244, acc 1
2017-08-08T17:19:58.914272: step 11108, loss 0.00504102, acc 1
2017-08-08T17:19:59.214008: step 11109, loss 1.33152e-05, acc 1
2017-08-08T17:19:59.436621: step 11110, loss 1.91664e-06, acc 1
2017-08-08T17:19:59.624314: step 11111, loss 8.9531e-06, acc 1
2017-08-08T17:19:59.882269: step 11112, loss 2.04096e-05, acc 1
2017-08-08T17:20:00.231031: step 11113, loss 7.54218e-05, acc 1
2017-08-08T17:20:00.431120: step 11114, loss 9.95334e-05, acc 1
2017-08-08T17:20:00.650104: step 11115, loss 2.86822e-06, acc 1
2017-08-08T17:20:01.022100: step 11116, loss 0.000158216, acc 1
2017-08-08T17:20:01.440964: step 11117, loss 0.000100826, acc 1
2017-08-08T17:20:01.876466: step 11118, loss 5.34772e-05, acc 1
2017-08-08T17:20:02.201359: step 11119, loss 3.468e-06, acc 1
2017-08-08T17:20:02.506491: step 11120, loss 5.84027e-06, acc 1
2017-08-08T17:20:02.969040: step 11121, loss 8.35475e-06, acc 1
2017-08-08T17:20:03.244613: step 11122, loss 0.00248343, acc 1
2017-08-08T17:20:03.544816: step 11123, loss 7.33894e-05, acc 1
2017-08-08T17:20:03.941888: step 11124, loss 3.36014e-05, acc 1
2017-08-08T17:20:04.343341: step 11125, loss 3.18392e-05, acc 1
2017-08-08T17:20:04.717377: step 11126, loss 0.000418247, acc 1
2017-08-08T17:20:05.053771: step 11127, loss 1.20961e-05, acc 1
2017-08-08T17:20:05.338766: step 11128, loss 2.93882e-05, acc 1
2017-08-08T17:20:05.831105: step 11129, loss 2.43546e-05, acc 1
2017-08-08T17:20:06.072184: step 11130, loss 3.58357e-06, acc 1
2017-08-08T17:20:06.324881: step 11131, loss 0.000132681, acc 1
2017-08-08T17:20:06.624700: step 11132, loss 1.49991e-05, acc 1
2017-08-08T17:20:06.989105: step 11133, loss 0.00282046, acc 1
2017-08-08T17:20:07.349845: step 11134, loss 0.0011563, acc 1
2017-08-08T17:20:07.593236: step 11135, loss 0.00032642, acc 1
2017-08-08T17:20:07.958472: step 11136, loss 6.9588e-05, acc 1
2017-08-08T17:20:08.340792: step 11137, loss 1.01954e-05, acc 1
2017-08-08T17:20:08.551971: step 11138, loss 0.0022004, acc 1
2017-08-08T17:20:08.847734: step 11139, loss 6.32026e-05, acc 1
2017-08-08T17:20:09.071092: step 11140, loss 0.000341046, acc 1
2017-08-08T17:20:09.410786: step 11141, loss 0.00542691, acc 1
2017-08-08T17:20:09.671368: step 11142, loss 0.000241145, acc 1
2017-08-08T17:20:09.971254: step 11143, loss 0.000625486, acc 1
2017-08-08T17:20:10.165416: step 11144, loss 2.27721e-05, acc 1
2017-08-08T17:20:10.375870: step 11145, loss 3.99089e-05, acc 1
2017-08-08T17:20:10.645166: step 11146, loss 0.000198749, acc 1
2017-08-08T17:20:10.834378: step 11147, loss 1.14933e-05, acc 1
2017-08-08T17:20:11.041766: step 11148, loss 0.000390547, acc 1
2017-08-08T17:20:11.323130: step 11149, loss 0.00145735, acc 1
2017-08-08T17:20:11.564110: step 11150, loss 0.000502655, acc 1
2017-08-08T17:20:11.922613: step 11151, loss 6.52627e-05, acc 1
2017-08-08T17:20:12.405633: step 11152, loss 0.0505557, acc 0.984375
2017-08-08T17:20:12.761676: step 11153, loss 3.0952e-05, acc 1
2017-08-08T17:20:13.012475: step 11154, loss 0.000351283, acc 1
2017-08-08T17:20:13.292442: step 11155, loss 1.24831e-05, acc 1
2017-08-08T17:20:13.715513: step 11156, loss 2.35802e-05, acc 1
2017-08-08T17:20:13.979495: step 11157, loss 5.608e-05, acc 1
2017-08-08T17:20:14.259625: step 11158, loss 7.02999e-05, acc 1
2017-08-08T17:20:14.579505: step 11159, loss 1.07845e-06, acc 1
2017-08-08T17:20:14.937356: step 11160, loss 6.94817e-05, acc 1
2017-08-08T17:20:15.407804: step 11161, loss 6.68686e-07, acc 1
2017-08-08T17:20:15.761582: step 11162, loss 0.0235869, acc 0.984375
2017-08-08T17:20:16.058346: step 11163, loss 0.000243911, acc 1
2017-08-08T17:20:16.367564: step 11164, loss 9.46688e-05, acc 1
2017-08-08T17:20:16.814084: step 11165, loss 1.08218e-06, acc 1
2017-08-08T17:20:17.088175: step 11166, loss 5.31574e-06, acc 1
2017-08-08T17:20:17.390209: step 11167, loss 7.1527e-05, acc 1
2017-08-08T17:20:17.709662: step 11168, loss 1.59229e-05, acc 1
2017-08-08T17:20:18.164875: step 11169, loss 4.89866e-05, acc 1
2017-08-08T17:20:18.528654: step 11170, loss 0.000289059, acc 1
2017-08-08T17:20:18.745049: step 11171, loss 2.21052e-05, acc 1
2017-08-08T17:20:18.979290: step 11172, loss 3.34704e-06, acc 1
2017-08-08T17:20:19.411814: step 11173, loss 3.31906e-06, acc 1
2017-08-08T17:20:19.699819: step 11174, loss 1.04308e-07, acc 1
2017-08-08T17:20:19.918004: step 11175, loss 6.85924e-06, acc 1
2017-08-08T17:20:20.142392: step 11176, loss 0.0495607, acc 0.984375
2017-08-08T17:20:20.551870: step 11177, loss 0.0013487, acc 1
2017-08-08T17:20:20.849853: step 11178, loss 8.0644e-05, acc 1
2017-08-08T17:20:21.079454: step 11179, loss 0.00151159, acc 1
2017-08-08T17:20:21.331763: step 11180, loss 1.50006e-05, acc 1
2017-08-08T17:20:21.733555: step 11181, loss 2.38597e-06, acc 1
2017-08-08T17:20:21.947779: step 11182, loss 2.26672e-05, acc 1
2017-08-08T17:20:22.184683: step 11183, loss 5.75648e-05, acc 1
2017-08-08T17:20:22.464436: step 11184, loss 7.12246e-05, acc 1
2017-08-08T17:20:22.815307: step 11185, loss 0.002445, acc 1
2017-08-08T17:20:23.244029: step 11186, loss 0.000767349, acc 1
2017-08-08T17:20:23.524520: step 11187, loss 1.20958e-05, acc 1
2017-08-08T17:20:23.726298: step 11188, loss 0.000148459, acc 1
2017-08-08T17:20:23.989362: step 11189, loss 4.52751e-05, acc 1
2017-08-08T17:20:24.427142: step 11190, loss 0.00167323, acc 1
2017-08-08T17:20:24.707637: step 11191, loss 0.0002594, acc 1
2017-08-08T17:20:24.987284: step 11192, loss 1.60187e-07, acc 1
2017-08-08T17:20:25.340057: step 11193, loss 3.45976e-05, acc 1
2017-08-08T17:20:25.669373: step 11194, loss 1.69946e-05, acc 1
2017-08-08T17:20:26.001317: step 11195, loss 0.00489948, acc 1
2017-08-08T17:20:26.267534: step 11196, loss 8.8099e-05, acc 1
2017-08-08T17:20:26.463526: step 11197, loss 1.30886e-05, acc 1
2017-08-08T17:20:26.838959: step 11198, loss 0.00124733, acc 1
2017-08-08T17:20:27.061722: step 11199, loss 0.0301452, acc 0.984375
2017-08-08T17:20:27.270695: step 11200, loss 2.63964e-05, acc 1

Evaluation:
2017-08-08T17:20:27.860106: step 11200, loss 3.62783, acc 0.713884

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-11200

2017-08-08T17:20:28.373366: step 11201, loss 3.66875e-05, acc 1
2017-08-08T17:20:28.651995: step 11202, loss 0.00371117, acc 1
2017-08-08T17:20:28.822420: step 11203, loss 7.68162e-05, acc 1
2017-08-08T17:20:29.066559: step 11204, loss 1.88654e-05, acc 1
2017-08-08T17:20:29.380498: step 11205, loss 9.46172e-05, acc 1
2017-08-08T17:20:29.652136: step 11206, loss 0.000111554, acc 1
2017-08-08T17:20:30.028167: step 11207, loss 0.0006342, acc 1
2017-08-08T17:20:30.457625: step 11208, loss 3.08618e-06, acc 1
2017-08-08T17:20:30.836519: step 11209, loss 0.000622286, acc 1
2017-08-08T17:20:31.025979: step 11210, loss 1.76789e-05, acc 1
2017-08-08T17:20:31.189334: step 11211, loss 0.00545976, acc 1
2017-08-08T17:20:31.390834: step 11212, loss 0.000227113, acc 1
2017-08-08T17:20:31.656915: step 11213, loss 3.22798e-05, acc 1
2017-08-08T17:20:31.871135: step 11214, loss 4.33231e-05, acc 1
2017-08-08T17:20:32.160116: step 11215, loss 0.0126637, acc 0.984375
2017-08-08T17:20:32.525665: step 11216, loss 9.84835e-05, acc 1
2017-08-08T17:20:32.856989: step 11217, loss 0.000529695, acc 1
2017-08-08T17:20:33.211515: step 11218, loss 1.77735e-05, acc 1
2017-08-08T17:20:33.443025: step 11219, loss 6.91894e-06, acc 1
2017-08-08T17:20:33.739424: step 11220, loss 0.00015926, acc 1
2017-08-08T17:20:34.048425: step 11221, loss 0.0106622, acc 1
2017-08-08T17:20:34.250223: step 11222, loss 0.000519931, acc 1
2017-08-08T17:20:34.430528: step 11223, loss 0.000820004, acc 1
2017-08-08T17:20:34.664203: step 11224, loss 7.02651e-06, acc 1
2017-08-08T17:20:35.096349: step 11225, loss 0.0790099, acc 0.984375
2017-08-08T17:20:35.403262: step 11226, loss 0.000262059, acc 1
2017-08-08T17:20:35.666709: step 11227, loss 7.88387e-05, acc 1
2017-08-08T17:20:35.978239: step 11228, loss 3.14956e-05, acc 1
2017-08-08T17:20:36.159259: step 11229, loss 1.07125e-05, acc 1
2017-08-08T17:20:36.473815: step 11230, loss 1.59029e-05, acc 1
2017-08-08T17:20:36.677842: step 11231, loss 4.69907e-06, acc 1
2017-08-08T17:20:36.930934: step 11232, loss 4.79606e-06, acc 1
2017-08-08T17:20:37.162307: step 11233, loss 0.00203621, acc 1
2017-08-08T17:20:37.456150: step 11234, loss 1.96572e-05, acc 1
2017-08-08T17:20:37.780515: step 11235, loss 9.36453e-05, acc 1
2017-08-08T17:20:37.986759: step 11236, loss 1.76601e-05, acc 1
2017-08-08T17:20:38.269154: step 11237, loss 0.000128582, acc 1
2017-08-08T17:20:38.520886: step 11238, loss 7.94454e-06, acc 1
2017-08-08T17:20:38.856463: step 11239, loss 1.11011e-06, acc 1
2017-08-08T17:20:39.167907: step 11240, loss 5.50179e-06, acc 1
2017-08-08T17:20:39.440651: step 11241, loss 0.030867, acc 0.984375
2017-08-08T17:20:39.733385: step 11242, loss 1.7236e-05, acc 1
2017-08-08T17:20:40.074994: step 11243, loss 1.76203e-06, acc 1
2017-08-08T17:20:40.415594: step 11244, loss 5.24062e-05, acc 1
2017-08-08T17:20:40.743797: step 11245, loss 0.00147406, acc 1
2017-08-08T17:20:40.964797: step 11246, loss 0.00197882, acc 1
2017-08-08T17:20:41.160407: step 11247, loss 0.00501593, acc 1
2017-08-08T17:20:41.544004: step 11248, loss 3.32069e-05, acc 1
2017-08-08T17:20:41.758291: step 11249, loss 2.96329e-05, acc 1
2017-08-08T17:20:41.946314: step 11250, loss 0.000108772, acc 1
2017-08-08T17:20:42.258666: step 11251, loss 9.37738e-05, acc 1
2017-08-08T17:20:42.609162: step 11252, loss 3.83456e-05, acc 1
2017-08-08T17:20:42.929988: step 11253, loss 1.0437e-05, acc 1
2017-08-08T17:20:43.154971: step 11254, loss 1.30385e-08, acc 1
2017-08-08T17:20:43.567718: step 11255, loss 6.83676e-06, acc 1
2017-08-08T17:20:43.924174: step 11256, loss 0.000539121, acc 1
2017-08-08T17:20:44.183190: step 11257, loss 4.03848e-05, acc 1
2017-08-08T17:20:44.466789: step 11258, loss 0.00356539, acc 1
2017-08-08T17:20:44.881909: step 11259, loss 0.00134923, acc 1
2017-08-08T17:20:45.213380: step 11260, loss 0.00149176, acc 1
2017-08-08T17:20:45.560920: step 11261, loss 8.92589e-06, acc 1
2017-08-08T17:20:45.851309: step 11262, loss 6.87305e-07, acc 1
2017-08-08T17:20:46.129928: step 11263, loss 0.000370871, acc 1
2017-08-08T17:20:46.412808: step 11264, loss 0.000310043, acc 1
2017-08-08T17:20:46.706165: step 11265, loss 7.49088e-06, acc 1
2017-08-08T17:20:46.981006: step 11266, loss 0.0289785, acc 0.984375
2017-08-08T17:20:47.254828: step 11267, loss 6.75251e-05, acc 1
2017-08-08T17:20:47.532410: step 11268, loss 0.0103687, acc 1
2017-08-08T17:20:47.864391: step 11269, loss 0.0223389, acc 0.984375
2017-08-08T17:20:48.231939: step 11270, loss 1.26233e-05, acc 1
2017-08-08T17:20:48.621641: step 11271, loss 3.37125e-06, acc 1
2017-08-08T17:20:48.893623: step 11272, loss 0.00741708, acc 1
2017-08-08T17:20:49.366437: step 11273, loss 3.21464e-05, acc 1
2017-08-08T17:20:49.607806: step 11274, loss 1.75088e-07, acc 1
2017-08-08T17:20:49.847606: step 11275, loss 6.12381e-06, acc 1
2017-08-08T17:20:50.153689: step 11276, loss 1.78814e-05, acc 1
2017-08-08T17:20:50.487369: step 11277, loss 1.37835e-07, acc 1
2017-08-08T17:20:50.809313: step 11278, loss 1.76014e-06, acc 1
2017-08-08T17:20:51.050411: step 11279, loss 0.000273403, acc 1
2017-08-08T17:20:51.260330: step 11280, loss 2.46471e-05, acc 1
2017-08-08T17:20:51.567185: step 11281, loss 0.00103509, acc 1
2017-08-08T17:20:51.890103: step 11282, loss 2.066e-05, acc 1
2017-08-08T17:20:52.112575: step 11283, loss 0.0802218, acc 0.984375
2017-08-08T17:20:52.388556: step 11284, loss 0.000151732, acc 1
2017-08-08T17:20:52.682995: step 11285, loss 0.000153925, acc 1
2017-08-08T17:20:53.097350: step 11286, loss 0.000174243, acc 1
2017-08-08T17:20:53.435694: step 11287, loss 2.67517e-05, acc 1
2017-08-08T17:20:53.729379: step 11288, loss 0.000832872, acc 1
2017-08-08T17:20:53.932505: step 11289, loss 1.78533e-05, acc 1
2017-08-08T17:20:54.245194: step 11290, loss 0.000301822, acc 1
2017-08-08T17:20:54.696327: step 11291, loss 0.000114549, acc 1
2017-08-08T17:20:54.973200: step 11292, loss 6.86802e-05, acc 1
2017-08-08T17:20:55.278131: step 11293, loss 7.03531e-05, acc 1
2017-08-08T17:20:55.557387: step 11294, loss 0.000397796, acc 1
2017-08-08T17:20:56.046505: step 11295, loss 2.03283e-05, acc 1
2017-08-08T17:20:56.509175: step 11296, loss 0.0170271, acc 0.984375
2017-08-08T17:20:56.834469: step 11297, loss 0.000347038, acc 1
2017-08-08T17:20:57.088401: step 11298, loss 3.55291e-05, acc 1
2017-08-08T17:20:57.473370: step 11299, loss 0.000137067, acc 1
2017-08-08T17:20:57.819899: step 11300, loss 0.0187112, acc 0.984375

Evaluation:
2017-08-08T17:20:58.525352: step 11300, loss 3.83793, acc 0.72045

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-11300

2017-08-08T17:20:59.162575: step 11301, loss 0.0114531, acc 0.984375
2017-08-08T17:20:59.554648: step 11302, loss 3.08814e-06, acc 1
2017-08-08T17:20:59.850010: step 11303, loss 1.9022e-05, acc 1
2017-08-08T17:21:00.056613: step 11304, loss 4.53097e-05, acc 1
2017-08-08T17:21:00.442884: step 11305, loss 5.6578e-05, acc 1
2017-08-08T17:21:00.657780: step 11306, loss 1.91296e-05, acc 1
2017-08-08T17:21:00.881359: step 11307, loss 2.85275e-05, acc 1
2017-08-08T17:21:01.164069: step 11308, loss 1.0114e-06, acc 1
2017-08-08T17:21:01.589363: step 11309, loss 0.00037424, acc 1
2017-08-08T17:21:02.094470: step 11310, loss 0.000110726, acc 1
2017-08-08T17:21:02.517737: step 11311, loss 9.43251e-06, acc 1
2017-08-08T17:21:02.843676: step 11312, loss 0.000496692, acc 1
2017-08-08T17:21:03.145438: step 11313, loss 0.0107404, acc 1
2017-08-08T17:21:03.631360: step 11314, loss 2.22426e-05, acc 1
2017-08-08T17:21:03.922857: step 11315, loss 0.000274914, acc 1
2017-08-08T17:21:04.222014: step 11316, loss 3.8349e-06, acc 1
2017-08-08T17:21:04.524402: step 11317, loss 6.66825e-07, acc 1
2017-08-08T17:21:04.924154: step 11318, loss 4.02875e-05, acc 1
2017-08-08T17:21:05.364945: step 11319, loss 0.000370242, acc 1
2017-08-08T17:21:05.762756: step 11320, loss 9.94385e-05, acc 1
2017-08-08T17:21:06.100040: step 11321, loss 0.00458575, acc 1
2017-08-08T17:21:06.386016: step 11322, loss 4.41322e-05, acc 1
2017-08-08T17:21:06.711495: step 11323, loss 1.54656e-05, acc 1
2017-08-08T17:21:07.098405: step 11324, loss 0.010251, acc 1
2017-08-08T17:21:07.339160: step 11325, loss 0.00103743, acc 1
2017-08-08T17:21:07.599871: step 11326, loss 1.91852e-07, acc 1
2017-08-08T17:21:07.861886: step 11327, loss 5.41788e-06, acc 1
2017-08-08T17:21:08.292693: step 11328, loss 0.000109561, acc 1
2017-08-08T17:21:08.690959: step 11329, loss 5.6046e-05, acc 1
2017-08-08T17:21:09.013318: step 11330, loss 5.38397e-05, acc 1
2017-08-08T17:21:09.254142: step 11331, loss 0.0013617, acc 1
2017-08-08T17:21:09.668090: step 11332, loss 0.000973804, acc 1
2017-08-08T17:21:10.027239: step 11333, loss 1.99482e-06, acc 1
2017-08-08T17:21:10.280352: step 11334, loss 0.00520545, acc 1
2017-08-08T17:21:10.614150: step 11335, loss 0.000414774, acc 1
2017-08-08T17:21:10.976822: step 11336, loss 5.30627e-06, acc 1
2017-08-08T17:21:11.375416: step 11337, loss 0.000580486, acc 1
2017-08-08T17:21:11.697138: step 11338, loss 0.0198952, acc 0.984375
2017-08-08T17:21:11.919003: step 11339, loss 4.50354e-05, acc 1
2017-08-08T17:21:12.188943: step 11340, loss 1.78813e-07, acc 1
2017-08-08T17:21:12.552982: step 11341, loss 0.000347385, acc 1
2017-08-08T17:21:12.739249: step 11342, loss 4.71246e-07, acc 1
2017-08-08T17:21:12.997617: step 11343, loss 4.25311e-05, acc 1
2017-08-08T17:21:13.301994: step 11344, loss 0.000278243, acc 1
2017-08-08T17:21:13.592285: step 11345, loss 3.00995e-06, acc 1
2017-08-08T17:21:13.959857: step 11346, loss 3.301e-05, acc 1
2017-08-08T17:21:14.196240: step 11347, loss 0.00027536, acc 1
2017-08-08T17:21:14.509053: step 11348, loss 0.000651912, acc 1
2017-08-08T17:21:14.777226: step 11349, loss 0.002523, acc 1
2017-08-08T17:21:14.996410: step 11350, loss 0.0226275, acc 0.984375
2017-08-08T17:21:15.258279: step 11351, loss 2.15315e-06, acc 1
2017-08-08T17:21:15.670439: step 11352, loss 4.52378e-06, acc 1
2017-08-08T17:21:15.991335: step 11353, loss 1.41559e-06, acc 1
2017-08-08T17:21:16.272497: step 11354, loss 1.22374e-06, acc 1
2017-08-08T17:21:16.480085: step 11355, loss 1.90092e-05, acc 1
2017-08-08T17:21:16.840657: step 11356, loss 8.66366e-06, acc 1
2017-08-08T17:21:17.028971: step 11357, loss 0.00293201, acc 1
2017-08-08T17:21:17.240307: step 11358, loss 2.60568e-06, acc 1
2017-08-08T17:21:17.459614: step 11359, loss 3.09217e-05, acc 1
2017-08-08T17:21:17.826883: step 11360, loss 2.59823e-06, acc 1
2017-08-08T17:21:18.234024: step 11361, loss 0.000153449, acc 1
2017-08-08T17:21:18.565659: step 11362, loss 7.41686e-05, acc 1
2017-08-08T17:21:18.837275: step 11363, loss 6.65106e-06, acc 1
2017-08-08T17:21:19.097370: step 11364, loss 0.00020211, acc 1
2017-08-08T17:21:19.505345: step 11365, loss 0.000657382, acc 1
2017-08-08T17:21:19.726572: step 11366, loss 6.38886e-05, acc 1
2017-08-08T17:21:19.966088: step 11367, loss 2.05111e-05, acc 1
2017-08-08T17:21:20.240165: step 11368, loss 0.000244003, acc 1
2017-08-08T17:21:20.631796: step 11369, loss 0.000368397, acc 1
2017-08-08T17:21:20.928807: step 11370, loss 3.0191e-06, acc 1
2017-08-08T17:21:21.305913: step 11371, loss 5.29395e-05, acc 1
2017-08-08T17:21:21.562916: step 11372, loss 0.000529074, acc 1
2017-08-08T17:21:21.865344: step 11373, loss 0.000115636, acc 1
2017-08-08T17:21:22.155379: step 11374, loss 1.45283e-06, acc 1
2017-08-08T17:21:22.404322: step 11375, loss 0.000105726, acc 1
2017-08-08T17:21:22.642432: step 11376, loss 0.00245805, acc 1
2017-08-08T17:21:22.967698: step 11377, loss 0.000178667, acc 1
2017-08-08T17:21:23.326411: step 11378, loss 3.43937e-05, acc 1
2017-08-08T17:21:23.562847: step 11379, loss 4.20251e-05, acc 1
2017-08-08T17:21:23.802607: step 11380, loss 0.000325176, acc 1
2017-08-08T17:21:24.015727: step 11381, loss 1.35192e-05, acc 1
2017-08-08T17:21:24.333482: step 11382, loss 2.1215e-06, acc 1
2017-08-08T17:21:24.537607: step 11383, loss 2.34663e-05, acc 1
2017-08-08T17:21:24.767294: step 11384, loss 0.0512901, acc 0.984375
2017-08-08T17:21:25.005826: step 11385, loss 0.000102633, acc 1
2017-08-08T17:21:25.277377: step 11386, loss 0.000369551, acc 1
2017-08-08T17:21:25.720565: step 11387, loss 1.4294e-05, acc 1
2017-08-08T17:21:26.029564: step 11388, loss 4.22728e-05, acc 1
2017-08-08T17:21:26.230432: step 11389, loss 0.006817, acc 1
2017-08-08T17:21:26.570454: step 11390, loss 0.013968, acc 0.984375
2017-08-08T17:21:26.913863: step 11391, loss 6.787e-06, acc 1
2017-08-08T17:21:27.198036: step 11392, loss 0.00016439, acc 1
2017-08-08T17:21:27.492317: step 11393, loss 0.000106462, acc 1
2017-08-08T17:21:27.761372: step 11394, loss 0.000614859, acc 1
2017-08-08T17:21:28.153056: step 11395, loss 1.13994e-05, acc 1
2017-08-08T17:21:28.513346: step 11396, loss 0.000140129, acc 1
2017-08-08T17:21:28.779157: step 11397, loss 0.000275867, acc 1
2017-08-08T17:21:29.027062: step 11398, loss 1.69622e-05, acc 1
2017-08-08T17:21:29.334112: step 11399, loss 1.11015e-05, acc 1
2017-08-08T17:21:29.518458: step 11400, loss 4.51774e-06, acc 1

Evaluation:
2017-08-08T17:21:30.174930: step 11400, loss 3.94762, acc 0.705441

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-11400

2017-08-08T17:21:30.847166: step 11401, loss 1.776e-05, acc 1
2017-08-08T17:21:31.101804: step 11402, loss 0.000716911, acc 1
2017-08-08T17:21:31.456739: step 11403, loss 1.46486e-05, acc 1
2017-08-08T17:21:31.807041: step 11404, loss 3.7134e-05, acc 1
2017-08-08T17:21:32.086631: step 11405, loss 0.000319904, acc 1
2017-08-08T17:21:32.375745: step 11406, loss 4.12909e-05, acc 1
2017-08-08T17:21:32.724128: step 11407, loss 1.49007e-06, acc 1
2017-08-08T17:21:33.033368: step 11408, loss 0.00041536, acc 1
2017-08-08T17:21:33.416799: step 11409, loss 6.31227e-05, acc 1
2017-08-08T17:21:33.714103: step 11410, loss 1.56566e-05, acc 1
2017-08-08T17:21:33.946975: step 11411, loss 0.000276757, acc 1
2017-08-08T17:21:34.397843: step 11412, loss 0.000284028, acc 1
2017-08-08T17:21:34.710835: step 11413, loss 0.0073132, acc 1
2017-08-08T17:21:34.947950: step 11414, loss 6.3203e-05, acc 1
2017-08-08T17:21:35.250540: step 11415, loss 0.000134094, acc 1
2017-08-08T17:21:35.625680: step 11416, loss 9.44162e-05, acc 1
2017-08-08T17:21:36.022627: step 11417, loss 0.000114145, acc 1
2017-08-08T17:21:36.401802: step 11418, loss 1.01345e-05, acc 1
2017-08-08T17:21:36.724544: step 11419, loss 8.81046e-05, acc 1
2017-08-08T17:21:37.045326: step 11420, loss 1.20407e-05, acc 1
2017-08-08T17:21:37.264551: step 11421, loss 1.8228e-05, acc 1
2017-08-08T17:21:37.505249: step 11422, loss 0.0013914, acc 1
2017-08-08T17:21:37.858629: step 11423, loss 5.04416e-05, acc 1
2017-08-08T17:21:38.263156: step 11424, loss 0.00397905, acc 1
2017-08-08T17:21:38.577376: step 11425, loss 2.5195e-05, acc 1
2017-08-08T17:21:38.873678: step 11426, loss 0.00114451, acc 1
2017-08-08T17:21:39.082606: step 11427, loss 6.26645e-05, acc 1
2017-08-08T17:21:39.353544: step 11428, loss 1.72406e-05, acc 1
2017-08-08T17:21:39.618966: step 11429, loss 6.61011e-05, acc 1
2017-08-08T17:21:39.824919: step 11430, loss 0.000502611, acc 1
2017-08-08T17:21:40.058859: step 11431, loss 0.00221245, acc 1
2017-08-08T17:21:40.387993: step 11432, loss 1.51618e-06, acc 1
2017-08-08T17:21:40.817725: step 11433, loss 0.000483915, acc 1
2017-08-08T17:21:41.161319: step 11434, loss 0.000786732, acc 1
2017-08-08T17:21:41.404845: step 11435, loss 5.72722e-06, acc 1
2017-08-08T17:21:41.661985: step 11436, loss 3.38607e-06, acc 1
2017-08-08T17:21:41.959402: step 11437, loss 7.01137e-05, acc 1
2017-08-08T17:21:42.177417: step 11438, loss 0.00067175, acc 1
2017-08-08T17:21:42.368222: step 11439, loss 1.85969e-05, acc 1
2017-08-08T17:21:42.605355: step 11440, loss 1.0412e-06, acc 1
2017-08-08T17:21:42.938537: step 11441, loss 0.000106843, acc 1
2017-08-08T17:21:43.192176: step 11442, loss 7.97076e-05, acc 1
2017-08-08T17:21:43.493596: step 11443, loss 0.00275893, acc 1
2017-08-08T17:21:43.699613: step 11444, loss 0.000248777, acc 1
2017-08-08T17:21:43.973804: step 11445, loss 0.000126657, acc 1
2017-08-08T17:21:44.350473: step 11446, loss 4.0827e-06, acc 1
2017-08-08T17:21:44.574929: step 11447, loss 8.83678e-05, acc 1
2017-08-08T17:21:44.853415: step 11448, loss 0.00091334, acc 1
2017-08-08T17:21:45.290005: step 11449, loss 0.00562254, acc 1
2017-08-08T17:21:45.673213: step 11450, loss 7.19383e-05, acc 1
2017-08-08T17:21:46.097968: step 11451, loss 0.000538866, acc 1
2017-08-08T17:21:46.278730: step 11452, loss 4.91148e-05, acc 1
2017-08-08T17:21:46.532904: step 11453, loss 0.000147729, acc 1
2017-08-08T17:21:46.900982: step 11454, loss 3.38766e-05, acc 1
2017-08-08T17:21:47.129440: step 11455, loss 1.09522e-06, acc 1
2017-08-08T17:21:47.355440: step 11456, loss 1.9785e-05, acc 1
2017-08-08T17:21:47.586117: step 11457, loss 2.96521e-06, acc 1
2017-08-08T17:21:47.929485: step 11458, loss 7.83733e-06, acc 1
2017-08-08T17:21:48.209686: step 11459, loss 0.00107318, acc 1
2017-08-08T17:21:48.523228: step 11460, loss 9.17049e-05, acc 1
2017-08-08T17:21:48.786541: step 11461, loss 4.73515e-05, acc 1
2017-08-08T17:21:49.206029: step 11462, loss 0.000247376, acc 1
2017-08-08T17:21:49.455453: step 11463, loss 0.0013331, acc 1
2017-08-08T17:21:49.692135: step 11464, loss 1.0408e-05, acc 1
2017-08-08T17:21:49.960431: step 11465, loss 1.86068e-06, acc 1
2017-08-08T17:21:50.373351: step 11466, loss 0.000134976, acc 1
2017-08-08T17:21:50.696972: step 11467, loss 4.20119e-05, acc 1
2017-08-08T17:21:51.039260: step 11468, loss 0.000169374, acc 1
2017-08-08T17:21:51.351240: step 11469, loss 6.88259e-05, acc 1
2017-08-08T17:21:51.610469: step 11470, loss 5.20387e-05, acc 1
2017-08-08T17:21:52.034735: step 11471, loss 0.000195608, acc 1
2017-08-08T17:21:52.304419: step 11472, loss 0.000247548, acc 1
2017-08-08T17:21:52.594871: step 11473, loss 4.65821e-06, acc 1
2017-08-08T17:21:52.829922: step 11474, loss 0.000140743, acc 1
2017-08-08T17:21:53.147325: step 11475, loss 0.000458031, acc 1
2017-08-08T17:21:53.404692: step 11476, loss 0.00018971, acc 1
2017-08-08T17:21:53.745422: step 11477, loss 0.0047459, acc 1
2017-08-08T17:21:53.989579: step 11478, loss 5.33612e-06, acc 1
2017-08-08T17:21:54.261451: step 11479, loss 2.93516e-05, acc 1
2017-08-08T17:21:54.511983: step 11480, loss 1.48074e-06, acc 1
2017-08-08T17:21:54.743417: step 11481, loss 9.67241e-06, acc 1
2017-08-08T17:21:55.010225: step 11482, loss 0.000617837, acc 1
2017-08-08T17:21:55.371854: step 11483, loss 4.70811e-05, acc 1
2017-08-08T17:21:55.691836: step 11484, loss 3.09154e-05, acc 1
2017-08-08T17:21:55.987180: step 11485, loss 2.12152e-06, acc 1
2017-08-08T17:21:56.253453: step 11486, loss 0.000406758, acc 1
2017-08-08T17:21:56.490598: step 11487, loss 0.000141086, acc 1
2017-08-08T17:21:56.901153: step 11488, loss 4.6835e-05, acc 1
2017-08-08T17:21:57.209393: step 11489, loss 2.88137e-06, acc 1
2017-08-08T17:21:57.465381: step 11490, loss 8.05728e-06, acc 1
2017-08-08T17:21:57.757909: step 11491, loss 4.76249e-06, acc 1
2017-08-08T17:21:58.125403: step 11492, loss 0.00302915, acc 1
2017-08-08T17:21:58.572928: step 11493, loss 0.000499103, acc 1
2017-08-08T17:21:58.957344: step 11494, loss 0.000120786, acc 1
2017-08-08T17:21:59.214446: step 11495, loss 5.37892e-06, acc 1
2017-08-08T17:21:59.426617: step 11496, loss 0.000512211, acc 1
2017-08-08T17:21:59.827013: step 11497, loss 0.00143523, acc 1
2017-08-08T17:22:00.161357: step 11498, loss 2.1348e-05, acc 1
2017-08-08T17:22:00.442487: step 11499, loss 2.84293e-05, acc 1
2017-08-08T17:22:00.736402: step 11500, loss 1.07845e-06, acc 1

Evaluation:
2017-08-08T17:22:01.753401: step 11500, loss 3.86858, acc 0.711069

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-11500

2017-08-08T17:22:02.338810: step 11501, loss 0.00088553, acc 1
2017-08-08T17:22:02.609174: step 11502, loss 8.0624e-06, acc 1
2017-08-08T17:22:03.049727: step 11503, loss 7.42955e-06, acc 1
2017-08-08T17:22:03.401590: step 11504, loss 3.68789e-06, acc 1
2017-08-08T17:22:03.758348: step 11505, loss 0.000781001, acc 1
2017-08-08T17:22:04.147316: step 11506, loss 4.69905e-06, acc 1
2017-08-08T17:22:04.462474: step 11507, loss 0.000836544, acc 1
2017-08-08T17:22:04.935118: step 11508, loss 1.64901e-05, acc 1
2017-08-08T17:22:05.391194: step 11509, loss 4.13506e-07, acc 1
2017-08-08T17:22:05.748443: step 11510, loss 4.43685e-05, acc 1
2017-08-08T17:22:06.009920: step 11511, loss 0.000948791, acc 1
2017-08-08T17:22:06.380139: step 11512, loss 3.20725e-06, acc 1
2017-08-08T17:22:06.705063: step 11513, loss 2.59519e-05, acc 1
2017-08-08T17:22:06.920734: step 11514, loss 0.00410714, acc 1
2017-08-08T17:22:07.141919: step 11515, loss 4.65068e-05, acc 1
2017-08-08T17:22:07.393373: step 11516, loss 0.000485959, acc 1
2017-08-08T17:22:07.804073: step 11517, loss 1.78653e-05, acc 1
2017-08-08T17:22:08.169374: step 11518, loss 3.32396e-05, acc 1
2017-08-08T17:22:08.480322: step 11519, loss 5.88215e-05, acc 1
2017-08-08T17:22:08.697436: step 11520, loss 4.40723e-05, acc 1
2017-08-08T17:22:09.061555: step 11521, loss 2.81071e-05, acc 1
2017-08-08T17:22:09.407239: step 11522, loss 3.58983e-05, acc 1
2017-08-08T17:22:09.683246: step 11523, loss 5.45127e-06, acc 1
2017-08-08T17:22:09.981562: step 11524, loss 3.20057e-05, acc 1
2017-08-08T17:22:10.361710: step 11525, loss 0.000111565, acc 1
2017-08-08T17:22:10.798904: step 11526, loss 5.64801e-05, acc 1
2017-08-08T17:22:11.169829: step 11527, loss 1.41373e-06, acc 1
2017-08-08T17:22:11.440812: step 11528, loss 0.000336145, acc 1
2017-08-08T17:22:11.678045: step 11529, loss 7.99054e-07, acc 1
2017-08-08T17:22:12.052547: step 11530, loss 0.0029424, acc 1
2017-08-08T17:22:12.281593: step 11531, loss 6.27922e-05, acc 1
2017-08-08T17:22:12.508489: step 11532, loss 2.95458e-05, acc 1
2017-08-08T17:22:12.825549: step 11533, loss 0.0935377, acc 0.984375
2017-08-08T17:22:13.202188: step 11534, loss 2.7335e-05, acc 1
2017-08-08T17:22:13.575520: step 11535, loss 3.6339e-06, acc 1
2017-08-08T17:22:13.961249: step 11536, loss 0.000799522, acc 1
2017-08-08T17:22:14.166892: step 11537, loss 3.48481e-06, acc 1
2017-08-08T17:22:14.421455: step 11538, loss 1.22807e-05, acc 1
2017-08-08T17:22:14.776338: step 11539, loss 3.57985e-05, acc 1
2017-08-08T17:22:15.020297: step 11540, loss 6.5093e-06, acc 1
2017-08-08T17:22:15.276016: step 11541, loss 0.00644857, acc 1
2017-08-08T17:22:15.625344: step 11542, loss 3.31884e-05, acc 1
2017-08-08T17:22:16.046680: step 11543, loss 0.0011882, acc 1
2017-08-08T17:22:16.381106: step 11544, loss 0.000404528, acc 1
2017-08-08T17:22:16.685605: step 11545, loss 0.00155127, acc 1
2017-08-08T17:22:16.881927: step 11546, loss 7.60998e-05, acc 1
2017-08-08T17:22:17.101508: step 11547, loss 5.45584e-05, acc 1
2017-08-08T17:22:17.504725: step 11548, loss 1.63704e-05, acc 1
2017-08-08T17:22:17.747537: step 11549, loss 6.29541e-06, acc 1
2017-08-08T17:22:18.015416: step 11550, loss 0.00353304, acc 1
2017-08-08T17:22:18.275264: step 11551, loss 1.16703e-05, acc 1
2017-08-08T17:22:18.574485: step 11552, loss 0.000148225, acc 1
2017-08-08T17:22:18.975325: step 11553, loss 3.10477e-06, acc 1
2017-08-08T17:22:19.295858: step 11554, loss 0.00492105, acc 1
2017-08-08T17:22:19.529689: step 11555, loss 1.18327e-05, acc 1
2017-08-08T17:22:19.709254: step 11556, loss 0.00208958, acc 1
2017-08-08T17:22:19.905401: step 11557, loss 0.000592307, acc 1
2017-08-08T17:22:20.195515: step 11558, loss 0.0027285, acc 1
2017-08-08T17:22:20.404461: step 11559, loss 0.000390551, acc 1
2017-08-08T17:22:20.627527: step 11560, loss 0.000119802, acc 1
2017-08-08T17:22:20.941066: step 11561, loss 8.8882e-05, acc 1
2017-08-08T17:22:21.217331: step 11562, loss 1.78691e-05, acc 1
2017-08-08T17:22:21.509366: step 11563, loss 3.00995e-06, acc 1
2017-08-08T17:22:21.783544: step 11564, loss 9.47178e-05, acc 1
2017-08-08T17:22:22.008485: step 11565, loss 0.00620337, acc 1
2017-08-08T17:22:22.319660: step 11566, loss 0.0174734, acc 0.984375
2017-08-08T17:22:22.737404: step 11567, loss 0.000141935, acc 1
2017-08-08T17:22:23.038650: step 11568, loss 5.86388e-05, acc 1
2017-08-08T17:22:23.348052: step 11569, loss 9.16826e-05, acc 1
2017-08-08T17:22:23.630433: step 11570, loss 1.26286e-06, acc 1
2017-08-08T17:22:24.078912: step 11571, loss 7.23355e-05, acc 1
2017-08-08T17:22:24.506621: step 11572, loss 1.32862e-05, acc 1
2017-08-08T17:22:24.901571: step 11573, loss 0.00122753, acc 1
2017-08-08T17:22:25.141362: step 11574, loss 0.00168802, acc 1
2017-08-08T17:22:25.485239: step 11575, loss 0.00652751, acc 1
2017-08-08T17:22:25.914892: step 11576, loss 2.71938e-06, acc 1
2017-08-08T17:22:26.197219: step 11577, loss 0.000188894, acc 1
2017-08-08T17:22:26.494985: step 11578, loss 0.000675337, acc 1
2017-08-08T17:22:26.863421: step 11579, loss 0.0033231, acc 1
2017-08-08T17:22:27.315567: step 11580, loss 0.0977189, acc 0.984375
2017-08-08T17:22:27.700579: step 11581, loss 0.000107175, acc 1
2017-08-08T17:22:28.015703: step 11582, loss 1.18152e-05, acc 1
2017-08-08T17:22:28.221574: step 11583, loss 6.01631e-07, acc 1
2017-08-08T17:22:28.512221: step 11584, loss 0.000195972, acc 1
2017-08-08T17:22:28.765299: step 11585, loss 0.000146649, acc 1
2017-08-08T17:22:28.939670: step 11586, loss 7.7299e-07, acc 1
2017-08-08T17:22:29.159354: step 11587, loss 0.00167601, acc 1
2017-08-08T17:22:29.361631: step 11588, loss 8.58347e-06, acc 1
2017-08-08T17:22:29.729311: step 11589, loss 0.00017211, acc 1
2017-08-08T17:22:30.025869: step 11590, loss 7.93188e-06, acc 1
2017-08-08T17:22:30.285128: step 11591, loss 0.00205189, acc 1
2017-08-08T17:22:30.477329: step 11592, loss 0.00016223, acc 1
2017-08-08T17:22:30.747169: step 11593, loss 3.63936e-06, acc 1
2017-08-08T17:22:30.994023: step 11594, loss 0.00010532, acc 1
2017-08-08T17:22:31.259838: step 11595, loss 6.56006e-05, acc 1
2017-08-08T17:22:31.608643: step 11596, loss 3.55375e-06, acc 1
2017-08-08T17:22:31.872793: step 11597, loss 2.97261e-06, acc 1
2017-08-08T17:22:32.062265: step 11598, loss 2.98022e-07, acc 1
2017-08-08T17:22:32.351951: step 11599, loss 0.000169908, acc 1
2017-08-08T17:22:32.545627: step 11600, loss 3.21948e-05, acc 1

Evaluation:
2017-08-08T17:22:33.018936: step 11600, loss 3.82671, acc 0.712946

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-11600

2017-08-08T17:22:33.489284: step 11601, loss 2.54614e-06, acc 1
2017-08-08T17:22:33.686062: step 11602, loss 1.7732e-06, acc 1
2017-08-08T17:22:33.891956: step 11603, loss 0.000431816, acc 1
2017-08-08T17:22:34.177164: step 11604, loss 0.00014142, acc 1
2017-08-08T17:22:34.382431: step 11605, loss 9.62975e-07, acc 1
2017-08-08T17:22:34.566196: step 11606, loss 0.00022445, acc 1
2017-08-08T17:22:34.746969: step 11607, loss 0.000537781, acc 1
2017-08-08T17:22:35.057927: step 11608, loss 1.41578e-05, acc 1
2017-08-08T17:22:35.408548: step 11609, loss 0.00012675, acc 1
2017-08-08T17:22:35.635691: step 11610, loss 4.27796e-06, acc 1
2017-08-08T17:22:35.850478: step 11611, loss 0.000212065, acc 1
2017-08-08T17:22:36.059472: step 11612, loss 7.96327e-05, acc 1
2017-08-08T17:22:36.424208: step 11613, loss 0.00310882, acc 1
2017-08-08T17:22:36.631937: step 11614, loss 0.00874223, acc 1
2017-08-08T17:22:36.888839: step 11615, loss 1.4249e-06, acc 1
2017-08-08T17:22:37.216517: step 11616, loss 2.43434e-06, acc 1
2017-08-08T17:22:37.472703: step 11617, loss 9.28749e-05, acc 1
2017-08-08T17:22:37.697321: step 11618, loss 1.69527e-05, acc 1
2017-08-08T17:22:37.892192: step 11619, loss 0.000108969, acc 1
2017-08-08T17:22:38.093416: step 11620, loss 0.000129083, acc 1
2017-08-08T17:22:38.483413: step 11621, loss 0.070359, acc 0.984375
2017-08-08T17:22:38.738763: step 11622, loss 0.0569309, acc 0.984375
2017-08-08T17:22:39.018176: step 11623, loss 3.49627e-05, acc 1
2017-08-08T17:22:39.249312: step 11624, loss 0.000386471, acc 1
2017-08-08T17:22:39.545876: step 11625, loss 2.40088e-06, acc 1
2017-08-08T17:22:39.881752: step 11626, loss 6.81376e-05, acc 1
2017-08-08T17:22:40.177744: step 11627, loss 9.18516e-06, acc 1
2017-08-08T17:22:40.411055: step 11628, loss 0.00031808, acc 1
2017-08-08T17:22:40.570587: step 11629, loss 0.0010149, acc 1
2017-08-08T17:22:40.762612: step 11630, loss 0.000747587, acc 1
2017-08-08T17:22:41.132403: step 11631, loss 1.77184e-05, acc 1
2017-08-08T17:22:41.343889: step 11632, loss 0.000443831, acc 1
2017-08-08T17:22:41.567891: step 11633, loss 0.000550804, acc 1
2017-08-08T17:22:41.964504: step 11634, loss 0.000121983, acc 1
2017-08-08T17:22:42.301537: step 11635, loss 0.00045786, acc 1
2017-08-08T17:22:42.600626: step 11636, loss 0.00043408, acc 1
2017-08-08T17:22:42.829523: step 11637, loss 0.0020183, acc 1
2017-08-08T17:22:42.997487: step 11638, loss 0.00057824, acc 1
2017-08-08T17:22:43.291688: step 11639, loss 0.000325107, acc 1
2017-08-08T17:22:43.558501: step 11640, loss 0.00204098, acc 1
2017-08-08T17:22:43.835653: step 11641, loss 0.00355772, acc 1
2017-08-08T17:22:44.109158: step 11642, loss 0.00335373, acc 1
2017-08-08T17:22:44.372668: step 11643, loss 0.000771794, acc 1
2017-08-08T17:22:44.793144: step 11644, loss 0.00127018, acc 1
2017-08-08T17:22:45.106958: step 11645, loss 0.000114207, acc 1
2017-08-08T17:22:45.400173: step 11646, loss 8.94013e-06, acc 1
2017-08-08T17:22:45.603174: step 11647, loss 0.0106127, acc 1
2017-08-08T17:22:45.840385: step 11648, loss 0.000438272, acc 1
2017-08-08T17:22:46.241855: step 11649, loss 0.000376895, acc 1
2017-08-08T17:22:46.432545: step 11650, loss 0.00088366, acc 1
2017-08-08T17:22:46.622849: step 11651, loss 3.19467e-05, acc 1
2017-08-08T17:22:46.872896: step 11652, loss 2.26057e-05, acc 1
2017-08-08T17:22:47.153309: step 11653, loss 0.000792964, acc 1
2017-08-08T17:22:47.371783: step 11654, loss 0.0310551, acc 0.984375
2017-08-08T17:22:47.598259: step 11655, loss 4.53589e-05, acc 1
2017-08-08T17:22:47.780301: step 11656, loss 0.000425478, acc 1
2017-08-08T17:22:48.025393: step 11657, loss 9.95224e-05, acc 1
2017-08-08T17:22:48.352996: step 11658, loss 0.00303453, acc 1
2017-08-08T17:22:48.638965: step 11659, loss 3.6561e-06, acc 1
2017-08-08T17:22:48.928783: step 11660, loss 3.04488e-05, acc 1
2017-08-08T17:22:49.225365: step 11661, loss 1.61353e-05, acc 1
2017-08-08T17:22:49.689851: step 11662, loss 0.000440068, acc 1
2017-08-08T17:22:49.967775: step 11663, loss 5.35091e-05, acc 1
2017-08-08T17:22:50.227057: step 11664, loss 3.70223e-05, acc 1
2017-08-08T17:22:50.453739: step 11665, loss 0.000553731, acc 1
2017-08-08T17:22:50.838762: step 11666, loss 0.000291048, acc 1
2017-08-08T17:22:51.089609: step 11667, loss 1.24432e-05, acc 1
2017-08-08T17:22:51.354586: step 11668, loss 0.0311135, acc 0.984375
2017-08-08T17:22:51.627012: step 11669, loss 6.88474e-06, acc 1
2017-08-08T17:22:51.873409: step 11670, loss 1.93239e-05, acc 1
2017-08-08T17:22:52.265763: step 11671, loss 3.35251e-05, acc 1
2017-08-08T17:22:52.597619: step 11672, loss 5.74847e-05, acc 1
2017-08-08T17:22:52.963372: step 11673, loss 1.09708e-06, acc 1
2017-08-08T17:22:53.205641: step 11674, loss 3.38177e-05, acc 1
2017-08-08T17:22:53.617385: step 11675, loss 0.000895875, acc 1
2017-08-08T17:22:53.947231: step 11676, loss 0.000447101, acc 1
2017-08-08T17:22:54.242991: step 11677, loss 0.00126993, acc 1
2017-08-08T17:22:54.497315: step 11678, loss 0.00229105, acc 1
2017-08-08T17:22:54.823623: step 11679, loss 3.63195e-06, acc 1
2017-08-08T17:22:55.177742: step 11680, loss 5.77805e-05, acc 1
2017-08-08T17:22:55.555124: step 11681, loss 0.000491887, acc 1
2017-08-08T17:22:55.922936: step 11682, loss 8.35765e-05, acc 1
2017-08-08T17:22:56.153748: step 11683, loss 0.000126864, acc 1
2017-08-08T17:22:56.505692: step 11684, loss 0.000941599, acc 1
2017-08-08T17:22:56.961565: step 11685, loss 0.000770937, acc 1
2017-08-08T17:22:57.240397: step 11686, loss 0.0184422, acc 0.984375
2017-08-08T17:22:57.469576: step 11687, loss 0.00827475, acc 1
2017-08-08T17:22:57.844875: step 11688, loss 0.000110106, acc 1
2017-08-08T17:22:58.172927: step 11689, loss 0.000219225, acc 1
2017-08-08T17:22:58.509298: step 11690, loss 0.000800825, acc 1
2017-08-08T17:22:58.783650: step 11691, loss 0.00403581, acc 1
2017-08-08T17:22:59.125468: step 11692, loss 0.00240024, acc 1
2017-08-08T17:22:59.339199: step 11693, loss 7.09592e-05, acc 1
2017-08-08T17:22:59.612330: step 11694, loss 1.40624e-06, acc 1
2017-08-08T17:22:59.880590: step 11695, loss 2.87499e-05, acc 1
2017-08-08T17:23:00.232521: step 11696, loss 0.000339811, acc 1
2017-08-08T17:23:00.633849: step 11697, loss 0.000453944, acc 1
2017-08-08T17:23:01.030512: step 11698, loss 2.44441e-05, acc 1
2017-08-08T17:23:01.294926: step 11699, loss 0.000110936, acc 1
2017-08-08T17:23:01.626568: step 11700, loss 0.0070748, acc 1

Evaluation:
2017-08-08T17:23:02.523242: step 11700, loss 3.97885, acc 0.711069

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-11700

2017-08-08T17:23:03.161409: step 11701, loss 5.85608e-05, acc 1
2017-08-08T17:23:03.544201: step 11702, loss 3.78099e-06, acc 1
2017-08-08T17:23:03.961695: step 11703, loss 0.00918113, acc 1
2017-08-08T17:23:04.308733: step 11704, loss 4.86477e-05, acc 1
2017-08-08T17:23:04.561348: step 11705, loss 0.00332514, acc 1
2017-08-08T17:23:04.977370: step 11706, loss 4.06788e-06, acc 1
2017-08-08T17:23:05.287753: step 11707, loss 3.1633e-05, acc 1
2017-08-08T17:23:05.571407: step 11708, loss 1.35827e-05, acc 1
2017-08-08T17:23:05.881612: step 11709, loss 8.68833e-06, acc 1
2017-08-08T17:23:06.298694: step 11710, loss 0.000152261, acc 1
2017-08-08T17:23:06.638285: step 11711, loss 9.32762e-05, acc 1
2017-08-08T17:23:06.997617: step 11712, loss 0.000608607, acc 1
2017-08-08T17:23:07.241476: step 11713, loss 7.91147e-06, acc 1
2017-08-08T17:23:07.569359: step 11714, loss 1.5901e-05, acc 1
2017-08-08T17:23:07.870306: step 11715, loss 0.00851315, acc 1
2017-08-08T17:23:08.135988: step 11716, loss 0.00036064, acc 1
2017-08-08T17:23:08.414096: step 11717, loss 4.52015e-06, acc 1
2017-08-08T17:23:08.664322: step 11718, loss 0.000108268, acc 1
2017-08-08T17:23:08.968066: step 11719, loss 4.52488e-05, acc 1
2017-08-08T17:23:09.285170: step 11720, loss 1.21374e-05, acc 1
2017-08-08T17:23:09.583909: step 11721, loss 1.30564e-05, acc 1
2017-08-08T17:23:09.826493: step 11722, loss 7.14877e-05, acc 1
2017-08-08T17:23:10.070247: step 11723, loss 0.000117515, acc 1
2017-08-08T17:23:10.456247: step 11724, loss 0.0010453, acc 1
2017-08-08T17:23:10.707427: step 11725, loss 1.54597e-06, acc 1
2017-08-08T17:23:10.918991: step 11726, loss 1.55155e-06, acc 1
2017-08-08T17:23:11.246546: step 11727, loss 2.59263e-05, acc 1
2017-08-08T17:23:11.577389: step 11728, loss 0.000351631, acc 1
2017-08-08T17:23:11.917356: step 11729, loss 0.00363603, acc 1
2017-08-08T17:23:12.157900: step 11730, loss 0.00117247, acc 1
2017-08-08T17:23:12.359830: step 11731, loss 2.57037e-06, acc 1
2017-08-08T17:23:12.731285: step 11732, loss 2.56287e-06, acc 1
2017-08-08T17:23:12.968111: step 11733, loss 5.04887e-06, acc 1
2017-08-08T17:23:13.259729: step 11734, loss 0.000355311, acc 1
2017-08-08T17:23:13.608678: step 11735, loss 0.000905341, acc 1
2017-08-08T17:23:13.941385: step 11736, loss 0.000269877, acc 1
2017-08-08T17:23:14.272103: step 11737, loss 6.25841e-07, acc 1
2017-08-08T17:23:14.521375: step 11738, loss 0.000239379, acc 1
2017-08-08T17:23:14.756799: step 11739, loss 1.70983e-05, acc 1
2017-08-08T17:23:15.180390: step 11740, loss 1.59423e-05, acc 1
2017-08-08T17:23:15.515458: step 11741, loss 0.000258313, acc 1
2017-08-08T17:23:15.784128: step 11742, loss 4.67276e-05, acc 1
2017-08-08T17:23:16.047345: step 11743, loss 8.19624e-05, acc 1
2017-08-08T17:23:16.417867: step 11744, loss 1.17021e-05, acc 1
2017-08-08T17:23:16.735025: step 11745, loss 9.55624e-06, acc 1
2017-08-08T17:23:17.034410: step 11746, loss 0.000485717, acc 1
2017-08-08T17:23:17.321855: step 11747, loss 6.42599e-05, acc 1
2017-08-08T17:23:17.531391: step 11748, loss 2.48114e-05, acc 1
2017-08-08T17:23:17.953865: step 11749, loss 0.000201261, acc 1
2017-08-08T17:23:18.155173: step 11750, loss 1.8915e-05, acc 1
2017-08-08T17:23:18.439154: step 11751, loss 0.000376415, acc 1
2017-08-08T17:23:18.754864: step 11752, loss 2.57224e-06, acc 1
2017-08-08T17:23:19.061332: step 11753, loss 6.77998e-07, acc 1
2017-08-08T17:23:19.327357: step 11754, loss 1.01139e-06, acc 1
2017-08-08T17:23:19.530799: step 11755, loss 8.38639e-06, acc 1
2017-08-08T17:23:19.751532: step 11756, loss 0.000343173, acc 1
2017-08-08T17:23:20.120837: step 11757, loss 0.000216689, acc 1
2017-08-08T17:23:20.336430: step 11758, loss 5.63949e-06, acc 1
2017-08-08T17:23:20.622048: step 11759, loss 8.77561e-05, acc 1
2017-08-08T17:23:20.831596: step 11760, loss 7.88092e-05, acc 1
2017-08-08T17:23:21.184442: step 11761, loss 2.62427e-06, acc 1
2017-08-08T17:23:21.506849: step 11762, loss 0.000514039, acc 1
2017-08-08T17:23:21.734815: step 11763, loss 5.82769e-05, acc 1
2017-08-08T17:23:21.935781: step 11764, loss 0.000539812, acc 1
2017-08-08T17:23:22.201350: step 11765, loss 4.13805e-05, acc 1
2017-08-08T17:23:22.534623: step 11766, loss 4.20801e-05, acc 1
2017-08-08T17:23:22.812882: step 11767, loss 4.78521e-05, acc 1
2017-08-08T17:23:23.143481: step 11768, loss 2.26381e-05, acc 1
2017-08-08T17:23:23.524433: step 11769, loss 0.000213126, acc 1
2017-08-08T17:23:23.966659: step 11770, loss 1.67049e-05, acc 1
2017-08-08T17:23:24.380770: step 11771, loss 0.0471942, acc 0.984375
2017-08-08T17:23:24.686114: step 11772, loss 0.00104806, acc 1
2017-08-08T17:23:24.929007: step 11773, loss 0.00105736, acc 1
2017-08-08T17:23:25.284564: step 11774, loss 0.000388716, acc 1
2017-08-08T17:23:25.474985: step 11775, loss 1.70772e-05, acc 1
2017-08-08T17:23:25.673762: step 11776, loss 0.000180663, acc 1
2017-08-08T17:23:25.869379: step 11777, loss 0.0133836, acc 0.984375
2017-08-08T17:23:26.187203: step 11778, loss 0.00867097, acc 1
2017-08-08T17:23:26.510922: step 11779, loss 0.00239401, acc 1
2017-08-08T17:23:26.831130: step 11780, loss 0.000232577, acc 1
2017-08-08T17:23:27.073298: step 11781, loss 1.38562e-05, acc 1
2017-08-08T17:23:27.288981: step 11782, loss 3.34684e-05, acc 1
2017-08-08T17:23:27.665896: step 11783, loss 6.35471e-06, acc 1
2017-08-08T17:23:28.009443: step 11784, loss 0.000566833, acc 1
2017-08-08T17:23:28.305113: step 11785, loss 0.0225538, acc 0.984375
2017-08-08T17:23:28.578461: step 11786, loss 0.00333713, acc 1
2017-08-08T17:23:28.866907: step 11787, loss 0.00240793, acc 1
2017-08-08T17:23:29.189380: step 11788, loss 9.76735e-06, acc 1
2017-08-08T17:23:29.607466: step 11789, loss 0.00409675, acc 1
2017-08-08T17:23:29.979181: step 11790, loss 0.00185005, acc 1
2017-08-08T17:23:30.252452: step 11791, loss 3.00058e-06, acc 1
2017-08-08T17:23:30.497337: step 11792, loss 3.73624e-05, acc 1
2017-08-08T17:23:30.817384: step 11793, loss 2.8684e-06, acc 1
2017-08-08T17:23:31.064444: step 11794, loss 0.000301912, acc 1
2017-08-08T17:23:31.320224: step 11795, loss 3.75937e-05, acc 1
2017-08-08T17:23:31.567531: step 11796, loss 0.0230408, acc 0.984375
2017-08-08T17:23:31.912412: step 11797, loss 0.00102969, acc 1
2017-08-08T17:23:32.257341: step 11798, loss 0.00185258, acc 1
2017-08-08T17:23:32.558681: step 11799, loss 0.000135117, acc 1
2017-08-08T17:23:32.722260: step 11800, loss 0.00928968, acc 1

Evaluation:
2017-08-08T17:23:33.372656: step 11800, loss 3.99031, acc 0.713884

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-11800

2017-08-08T17:23:33.873408: step 11801, loss 7.59294e-05, acc 1
2017-08-08T17:23:34.121225: step 11802, loss 3.82687e-05, acc 1
2017-08-08T17:23:34.539574: step 11803, loss 7.30662e-06, acc 1
2017-08-08T17:23:34.855280: step 11804, loss 3.91703e-06, acc 1
2017-08-08T17:23:35.213304: step 11805, loss 1.50669e-05, acc 1
2017-08-08T17:23:35.446967: step 11806, loss 8.99854e-06, acc 1
2017-08-08T17:23:35.687318: step 11807, loss 4.73618e-05, acc 1
2017-08-08T17:23:36.196199: step 11808, loss 2.25928e-05, acc 1
2017-08-08T17:23:36.524946: step 11809, loss 0.00154016, acc 1
2017-08-08T17:23:36.797220: step 11810, loss 1.80672e-06, acc 1
2017-08-08T17:23:37.120287: step 11811, loss 4.9078e-06, acc 1
2017-08-08T17:23:37.566163: step 11812, loss 3.00982e-05, acc 1
2017-08-08T17:23:37.820433: step 11813, loss 3.06447e-05, acc 1
2017-08-08T17:23:38.009503: step 11814, loss 3.04147e-06, acc 1
2017-08-08T17:23:38.267930: step 11815, loss 0.00139994, acc 1
2017-08-08T17:23:38.642624: step 11816, loss 7.88107e-05, acc 1
2017-08-08T17:23:38.968545: step 11817, loss 2.84602e-06, acc 1
2017-08-08T17:23:39.168853: step 11818, loss 6.01627e-07, acc 1
2017-08-08T17:23:39.370664: step 11819, loss 1.01699e-06, acc 1
2017-08-08T17:23:39.605623: step 11820, loss 9.14472e-05, acc 1
2017-08-08T17:23:39.944889: step 11821, loss 0.000189384, acc 1
2017-08-08T17:23:40.225118: step 11822, loss 0.0214364, acc 0.984375
2017-08-08T17:23:40.609358: step 11823, loss 6.35623e-06, acc 1
2017-08-08T17:23:40.879948: step 11824, loss 0.000149706, acc 1
2017-08-08T17:23:41.105731: step 11825, loss 2.14489e-05, acc 1
2017-08-08T17:23:41.517944: step 11826, loss 0.000200319, acc 1
2017-08-08T17:23:41.731606: step 11827, loss 4.27233e-05, acc 1
2017-08-08T17:23:41.973835: step 11828, loss 0.000403852, acc 1
2017-08-08T17:23:42.258410: step 11829, loss 1.41186e-06, acc 1
2017-08-08T17:23:42.695106: step 11830, loss 3.21721e-05, acc 1
2017-08-08T17:23:43.061329: step 11831, loss 0.000100403, acc 1
2017-08-08T17:23:43.430817: step 11832, loss 0.00282447, acc 1
2017-08-08T17:23:43.708964: step 11833, loss 2.51637e-06, acc 1
2017-08-08T17:23:44.047612: step 11834, loss 0.000229333, acc 1
2017-08-08T17:23:44.454770: step 11835, loss 2.16787e-05, acc 1
2017-08-08T17:23:44.725326: step 11836, loss 5.45624e-05, acc 1
2017-08-08T17:23:44.951773: step 11837, loss 6.4236e-06, acc 1
2017-08-08T17:23:45.254595: step 11838, loss 1.75088e-07, acc 1
2017-08-08T17:23:45.637092: step 11839, loss 5.4761e-07, acc 1
2017-08-08T17:23:45.916202: step 11840, loss 4.10675e-06, acc 1
2017-08-08T17:23:46.152917: step 11841, loss 0.0296551, acc 0.984375
2017-08-08T17:23:46.367471: step 11842, loss 0.000146206, acc 1
2017-08-08T17:23:46.555151: step 11843, loss 0.000732862, acc 1
2017-08-08T17:23:46.906166: step 11844, loss 2.80136e-06, acc 1
2017-08-08T17:23:47.232018: step 11845, loss 1.69252e-05, acc 1
2017-08-08T17:23:47.525364: step 11846, loss 0.000390747, acc 1
2017-08-08T17:23:47.754688: step 11847, loss 2.98011e-06, acc 1
2017-08-08T17:23:48.153726: step 11848, loss 0.00011014, acc 1
2017-08-08T17:23:48.583335: step 11849, loss 2.70972e-05, acc 1
2017-08-08T17:23:48.887947: step 11850, loss 4.41072e-07, acc 1
2017-08-08T17:23:49.143253: step 11851, loss 3.57475e-05, acc 1
2017-08-08T17:23:49.478198: step 11852, loss 0.000584557, acc 1
2017-08-08T17:23:49.810415: step 11853, loss 8.94905e-05, acc 1
2017-08-08T17:23:50.183056: step 11854, loss 0.000131062, acc 1
2017-08-08T17:23:50.408071: step 11855, loss 1.45296e-05, acc 1
2017-08-08T17:23:50.758565: step 11856, loss 5.652e-05, acc 1
2017-08-08T17:23:51.005492: step 11857, loss 9.25397e-05, acc 1
2017-08-08T17:23:51.298010: step 11858, loss 0.000235847, acc 1
2017-08-08T17:23:51.582296: step 11859, loss 1.39973e-05, acc 1
2017-08-08T17:23:51.788242: step 11860, loss 0.000165713, acc 1
2017-08-08T17:23:52.194634: step 11861, loss 0.000238421, acc 1
2017-08-08T17:23:52.496101: step 11862, loss 0.000391247, acc 1
2017-08-08T17:23:52.779348: step 11863, loss 3.31717e-06, acc 1
2017-08-08T17:23:53.023616: step 11864, loss 0.000470201, acc 1
2017-08-08T17:23:53.419206: step 11865, loss 0.000182494, acc 1
2017-08-08T17:23:53.736686: step 11866, loss 0.000957, acc 1
2017-08-08T17:23:53.967223: step 11867, loss 7.56095e-06, acc 1
2017-08-08T17:23:54.167046: step 11868, loss 0.000536648, acc 1
2017-08-08T17:23:54.527026: step 11869, loss 0.000189248, acc 1
2017-08-08T17:23:54.719993: step 11870, loss 0.00123249, acc 1
2017-08-08T17:23:54.969106: step 11871, loss 0.000756188, acc 1
2017-08-08T17:23:55.217366: step 11872, loss 0.00162998, acc 1
2017-08-08T17:23:55.662338: step 11873, loss 0.000158165, acc 1
2017-08-08T17:23:56.050750: step 11874, loss 0.000154504, acc 1
2017-08-08T17:23:56.281409: step 11875, loss 1.88248e-05, acc 1
2017-08-08T17:23:56.482545: step 11876, loss 0.00132819, acc 1
2017-08-08T17:23:56.710070: step 11877, loss 0.000641058, acc 1
2017-08-08T17:23:57.047052: step 11878, loss 7.03776e-05, acc 1
2017-08-08T17:23:57.272586: step 11879, loss 0.000496744, acc 1
2017-08-08T17:23:57.574376: step 11880, loss 7.58092e-07, acc 1
2017-08-08T17:23:57.931684: step 11881, loss 0.000226251, acc 1
2017-08-08T17:23:58.245164: step 11882, loss 0.000148907, acc 1
2017-08-08T17:23:58.572843: step 11883, loss 5.86165e-05, acc 1
2017-08-08T17:23:58.876254: step 11884, loss 0.000893565, acc 1
2017-08-08T17:23:59.134486: step 11885, loss 0.00169841, acc 1
2017-08-08T17:23:59.451181: step 11886, loss 1.33312e-05, acc 1
2017-08-08T17:23:59.746763: step 11887, loss 0.000259175, acc 1
2017-08-08T17:23:59.973314: step 11888, loss 4.55569e-06, acc 1
2017-08-08T17:24:00.282172: step 11889, loss 0.000244228, acc 1
2017-08-08T17:24:00.529314: step 11890, loss 5.57046e-06, acc 1
2017-08-08T17:24:00.856768: step 11891, loss 0.000438038, acc 1
2017-08-08T17:24:01.160751: step 11892, loss 6.44906e-06, acc 1
2017-08-08T17:24:01.550663: step 11893, loss 2.00408e-06, acc 1
2017-08-08T17:24:01.824544: step 11894, loss 2.56807e-05, acc 1
2017-08-08T17:24:02.133438: step 11895, loss 7.61501e-05, acc 1
2017-08-08T17:24:02.594105: step 11896, loss 6.32929e-05, acc 1
2017-08-08T17:24:02.879963: step 11897, loss 4.02997e-05, acc 1
2017-08-08T17:24:03.198857: step 11898, loss 0.000792458, acc 1
2017-08-08T17:24:03.508072: step 11899, loss 0.0353373, acc 0.984375
2017-08-08T17:24:04.019808: step 11900, loss 0.000593265, acc 1

Evaluation:
2017-08-08T17:24:05.049343: step 11900, loss 4.06239, acc 0.713884

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-11900

2017-08-08T17:24:05.656833: step 11901, loss 3.14785e-07, acc 1
2017-08-08T17:24:05.966180: step 11902, loss 0.000565302, acc 1
2017-08-08T17:24:06.348524: step 11903, loss 1.77401e-05, acc 1
2017-08-08T17:24:06.650196: step 11904, loss 0.000114076, acc 1
2017-08-08T17:24:06.877663: step 11905, loss 9.87511e-05, acc 1
2017-08-08T17:24:07.255738: step 11906, loss 2.81767e-05, acc 1
2017-08-08T17:24:07.476560: step 11907, loss 0.000456074, acc 1
2017-08-08T17:24:07.775682: step 11908, loss 0.0297254, acc 0.984375
2017-08-08T17:24:08.109375: step 11909, loss 0.000581866, acc 1
2017-08-08T17:24:08.382981: step 11910, loss 0.00110218, acc 1
2017-08-08T17:24:08.638947: step 11911, loss 0.000177059, acc 1
2017-08-08T17:24:09.049355: step 11912, loss 0.000192978, acc 1
2017-08-08T17:24:09.295379: step 11913, loss 1.20883e-05, acc 1
2017-08-08T17:24:09.579646: step 11914, loss 0.0118244, acc 1
2017-08-08T17:24:09.916342: step 11915, loss 0.000367338, acc 1
2017-08-08T17:24:10.187379: step 11916, loss 4.10813e-05, acc 1
2017-08-08T17:24:10.510569: step 11917, loss 2.8562e-05, acc 1
2017-08-08T17:24:10.722367: step 11918, loss 0.000252933, acc 1
2017-08-08T17:24:11.061416: step 11919, loss 0.0180127, acc 0.984375
2017-08-08T17:24:11.349144: step 11920, loss 0.017254, acc 0.984375
2017-08-08T17:24:11.689217: step 11921, loss 1.23122e-05, acc 1
2017-08-08T17:24:11.891070: step 11922, loss 4.65842e-05, acc 1
2017-08-08T17:24:12.114409: step 11923, loss 0.000494655, acc 1
2017-08-08T17:24:12.428565: step 11924, loss 5.06641e-05, acc 1
2017-08-08T17:24:12.639630: step 11925, loss 9.54514e-05, acc 1
2017-08-08T17:24:12.949302: step 11926, loss 0.00181134, acc 1
2017-08-08T17:24:13.201400: step 11927, loss 0.000503993, acc 1
2017-08-08T17:24:13.481553: step 11928, loss 0.00109644, acc 1
2017-08-08T17:24:13.740489: step 11929, loss 2.87521e-05, acc 1
2017-08-08T17:24:13.950619: step 11930, loss 2.53496e-06, acc 1
2017-08-08T17:24:14.316047: step 11931, loss 0.0284671, acc 0.984375
2017-08-08T17:24:14.545150: step 11932, loss 0.00112972, acc 1
2017-08-08T17:24:14.837318: step 11933, loss 3.91219e-05, acc 1
2017-08-08T17:24:15.064016: step 11934, loss 5.11399e-05, acc 1
2017-08-08T17:24:15.267289: step 11935, loss 2.96159e-07, acc 1
2017-08-08T17:24:15.566492: step 11936, loss 1.96689e-06, acc 1
2017-08-08T17:24:15.774080: step 11937, loss 0.00251009, acc 1
2017-08-08T17:24:16.036866: step 11938, loss 1.57203e-06, acc 1
2017-08-08T17:24:16.222867: step 11939, loss 0.000116628, acc 1
2017-08-08T17:24:16.405342: step 11940, loss 4.46825e-06, acc 1
2017-08-08T17:24:16.594233: step 11941, loss 0.00734232, acc 1
2017-08-08T17:24:16.869369: step 11942, loss 0.0202323, acc 0.984375
2017-08-08T17:24:17.097322: step 11943, loss 0.000188974, acc 1
2017-08-08T17:24:17.366459: step 11944, loss 4.22404e-06, acc 1
2017-08-08T17:24:17.546889: step 11945, loss 0.00119643, acc 1
2017-08-08T17:24:17.712346: step 11946, loss 2.58066e-05, acc 1
2017-08-08T17:24:17.972826: step 11947, loss 3.60925e-05, acc 1
2017-08-08T17:24:18.195731: step 11948, loss 0.00183992, acc 1
2017-08-08T17:24:18.393524: step 11949, loss 0.00261529, acc 1
2017-08-08T17:24:18.563719: step 11950, loss 4.64891e-06, acc 1
2017-08-08T17:24:18.824801: step 11951, loss 0.00186088, acc 1
2017-08-08T17:24:19.052628: step 11952, loss 0.000144207, acc 1
2017-08-08T17:24:19.274643: step 11953, loss 0.000294331, acc 1
2017-08-08T17:24:19.495947: step 11954, loss 1.8535e-05, acc 1
2017-08-08T17:24:19.722961: step 11955, loss 0.00773978, acc 1
2017-08-08T17:24:20.202940: step 11956, loss 2.10288e-06, acc 1
2017-08-08T17:24:20.405760: step 11957, loss 5.11044e-05, acc 1
2017-08-08T17:24:20.595967: step 11958, loss 9.26232e-06, acc 1
2017-08-08T17:24:20.824010: step 11959, loss 6.51925e-08, acc 1
2017-08-08T17:24:21.117316: step 11960, loss 9.57712e-06, acc 1
2017-08-08T17:24:21.424300: step 11961, loss 4.58435e-05, acc 1
2017-08-08T17:24:21.648967: step 11962, loss 5.47615e-07, acc 1
2017-08-08T17:24:21.843640: step 11963, loss 3.38984e-06, acc 1
2017-08-08T17:24:22.149338: step 11964, loss 0.000168204, acc 1
2017-08-08T17:24:22.418525: step 11965, loss 0.000115505, acc 1
2017-08-08T17:24:22.657213: step 11966, loss 0.000218172, acc 1
2017-08-08T17:24:22.868178: step 11967, loss 7.55419e-06, acc 1
2017-08-08T17:24:23.104471: step 11968, loss 0.00103126, acc 1
2017-08-08T17:24:23.430528: step 11969, loss 1.79752e-05, acc 1
2017-08-08T17:24:23.750296: step 11970, loss 0.0131189, acc 0.984375
2017-08-08T17:24:24.016631: step 11971, loss 0.000226379, acc 1
2017-08-08T17:24:24.258405: step 11972, loss 8.15576e-06, acc 1
2017-08-08T17:24:24.500028: step 11973, loss 0.000509577, acc 1
2017-08-08T17:24:24.818838: step 11974, loss 8.77295e-07, acc 1
2017-08-08T17:24:25.118792: step 11975, loss 9.9109e-05, acc 1
2017-08-08T17:24:25.398849: step 11976, loss 0.00594224, acc 1
2017-08-08T17:24:25.637503: step 11977, loss 1.52313e-05, acc 1
2017-08-08T17:24:25.984028: step 11978, loss 9.62759e-06, acc 1
2017-08-08T17:24:26.374578: step 11979, loss 0.000794847, acc 1
2017-08-08T17:24:26.620740: step 11980, loss 0.000317193, acc 1
2017-08-08T17:24:26.864271: step 11981, loss 0.000467437, acc 1
2017-08-08T17:24:27.261393: step 11982, loss 8.85843e-06, acc 1
2017-08-08T17:24:27.585626: step 11983, loss 2.2794e-05, acc 1
2017-08-08T17:24:27.791219: step 11984, loss 1.06886e-05, acc 1
2017-08-08T17:24:28.084997: step 11985, loss 0.000539513, acc 1
2017-08-08T17:24:28.398255: step 11986, loss 5.623e-06, acc 1
2017-08-08T17:24:28.817382: step 11987, loss 1.93098e-05, acc 1
2017-08-08T17:24:29.229363: step 11988, loss 3.46264e-05, acc 1
2017-08-08T17:24:29.618662: step 11989, loss 4.32113e-06, acc 1
2017-08-08T17:24:29.879324: step 11990, loss 0.000588797, acc 1
2017-08-08T17:24:30.192055: step 11991, loss 1.56462e-07, acc 1
2017-08-08T17:24:30.629826: step 11992, loss 0.0245168, acc 0.984375
2017-08-08T17:24:30.820373: step 11993, loss 0.000156988, acc 1
2017-08-08T17:24:31.081503: step 11994, loss 0.00289994, acc 1
2017-08-08T17:24:31.295890: step 11995, loss 7.89372e-05, acc 1
2017-08-08T17:24:31.593311: step 11996, loss 0.000180329, acc 1
2017-08-08T17:24:31.876591: step 11997, loss 1.34708e-05, acc 1
2017-08-08T17:24:32.138904: step 11998, loss 7.60198e-05, acc 1
2017-08-08T17:24:32.335750: step 11999, loss 0.000528024, acc 1
2017-08-08T17:24:32.706372: step 12000, loss 2.57882e-06, acc 1

Evaluation:
2017-08-08T17:24:33.380996: step 12000, loss 4.11675, acc 0.712008

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-12000

2017-08-08T17:24:33.972919: step 12001, loss 3.68371e-05, acc 1
2017-08-08T17:24:34.308620: step 12002, loss 1.65775e-07, acc 1
2017-08-08T17:24:34.728013: step 12003, loss 1.24795e-06, acc 1
2017-08-08T17:24:34.925491: step 12004, loss 4.77172e-06, acc 1
2017-08-08T17:24:35.265725: step 12005, loss 5.17579e-06, acc 1
2017-08-08T17:24:35.494781: step 12006, loss 5.09367e-05, acc 1
2017-08-08T17:24:35.709032: step 12007, loss 8.06945e-05, acc 1
2017-08-08T17:24:35.892530: step 12008, loss 5.3008e-06, acc 1
2017-08-08T17:24:36.113709: step 12009, loss 1.76201e-06, acc 1
2017-08-08T17:24:36.409316: step 12010, loss 2.39493e-05, acc 1
2017-08-08T17:24:36.689388: step 12011, loss 0.00015925, acc 1
2017-08-08T17:24:36.946796: step 12012, loss 9.16052e-05, acc 1
2017-08-08T17:24:37.275908: step 12013, loss 0.00020049, acc 1
2017-08-08T17:24:37.519645: step 12014, loss 3.09198e-07, acc 1
2017-08-08T17:24:37.925228: step 12015, loss 6.55762e-05, acc 1
2017-08-08T17:24:38.192663: step 12016, loss 0.000169048, acc 1
2017-08-08T17:24:38.405553: step 12017, loss 0.000176786, acc 1
2017-08-08T17:24:38.653378: step 12018, loss 7.35023e-05, acc 1
2017-08-08T17:24:39.045208: step 12019, loss 0.000147646, acc 1
2017-08-08T17:24:39.327045: step 12020, loss 0.000467901, acc 1
2017-08-08T17:24:39.609805: step 12021, loss 0.000141291, acc 1
2017-08-08T17:24:39.823006: step 12022, loss 0.000149129, acc 1
2017-08-08T17:24:40.079683: step 12023, loss 0.000527686, acc 1
2017-08-08T17:24:40.376154: step 12024, loss 0.00028515, acc 1
2017-08-08T17:24:40.582160: step 12025, loss 0.027122, acc 0.984375
2017-08-08T17:24:40.806136: step 12026, loss 1.50805e-05, acc 1
2017-08-08T17:24:41.138271: step 12027, loss 1.41373e-06, acc 1
2017-08-08T17:24:41.508706: step 12028, loss 5.31197e-06, acc 1
2017-08-08T17:24:41.790251: step 12029, loss 0.000543511, acc 1
2017-08-08T17:24:42.006856: step 12030, loss 4.42187e-05, acc 1
2017-08-08T17:24:42.181381: step 12031, loss 1.35199e-05, acc 1
2017-08-08T17:24:42.481442: step 12032, loss 2.43391e-05, acc 1
2017-08-08T17:24:42.748982: step 12033, loss 2.58531e-05, acc 1
2017-08-08T17:24:42.962285: step 12034, loss 2.07116e-06, acc 1
2017-08-08T17:24:43.236892: step 12035, loss 2.00927e-05, acc 1
2017-08-08T17:24:43.585365: step 12036, loss 1.39695e-06, acc 1
2017-08-08T17:24:43.946056: step 12037, loss 0.000349175, acc 1
2017-08-08T17:24:44.244981: step 12038, loss 4.32694e-05, acc 1
2017-08-08T17:24:44.482291: step 12039, loss 3.91677e-06, acc 1
2017-08-08T17:24:44.710869: step 12040, loss 9.23496e-05, acc 1
2017-08-08T17:24:45.065383: step 12041, loss 0.000290568, acc 1
2017-08-08T17:24:45.381197: step 12042, loss 0.0603655, acc 0.984375
2017-08-08T17:24:45.613597: step 12043, loss 0.00232256, acc 1
2017-08-08T17:24:45.864020: step 12044, loss 2.87315e-05, acc 1
2017-08-08T17:24:46.187437: step 12045, loss 4.6052e-05, acc 1
2017-08-08T17:24:46.486181: step 12046, loss 0.0310063, acc 0.96875
2017-08-08T17:24:46.895867: step 12047, loss 0.0372496, acc 0.984375
2017-08-08T17:24:47.194286: step 12048, loss 0.000125111, acc 1
2017-08-08T17:24:47.420342: step 12049, loss 0.043322, acc 0.984375
2017-08-08T17:24:47.613393: step 12050, loss 0.000250242, acc 1
2017-08-08T17:24:48.022334: step 12051, loss 3.34489e-05, acc 1
2017-08-08T17:24:48.306108: step 12052, loss 0.00876638, acc 1
2017-08-08T17:24:48.595881: step 12053, loss 0.000178712, acc 1
2017-08-08T17:24:48.843502: step 12054, loss 0.000781256, acc 1
2017-08-08T17:24:49.255438: step 12055, loss 4.24284e-06, acc 1
2017-08-08T17:24:49.579794: step 12056, loss 6.41522e-05, acc 1
2017-08-08T17:24:49.876962: step 12057, loss 1.92403e-06, acc 1
2017-08-08T17:24:50.076859: step 12058, loss 0.000562894, acc 1
2017-08-08T17:24:50.479358: step 12059, loss 0.0175034, acc 0.984375
2017-08-08T17:24:50.734811: step 12060, loss 3.51447e-05, acc 1
2017-08-08T17:24:50.973491: step 12061, loss 6.16673e-06, acc 1
2017-08-08T17:24:51.233630: step 12062, loss 1.61488e-06, acc 1
2017-08-08T17:24:51.598266: step 12063, loss 0.000113091, acc 1
2017-08-08T17:24:51.973035: step 12064, loss 2.38418e-07, acc 1
2017-08-08T17:24:52.353265: step 12065, loss 0.000131499, acc 1
2017-08-08T17:24:52.643429: step 12066, loss 1.42763e-05, acc 1
2017-08-08T17:24:52.856988: step 12067, loss 1.60557e-06, acc 1
2017-08-08T17:24:53.143360: step 12068, loss 0.00118824, acc 1
2017-08-08T17:24:53.391677: step 12069, loss 0.00686446, acc 1
2017-08-08T17:24:53.631915: step 12070, loss 0.000165356, acc 1
2017-08-08T17:24:53.850534: step 12071, loss 9.58783e-05, acc 1
2017-08-08T17:24:54.229137: step 12072, loss 2.38651e-05, acc 1
2017-08-08T17:24:54.541370: step 12073, loss 9.03765e-06, acc 1
2017-08-08T17:24:54.896847: step 12074, loss 9.63425e-06, acc 1
2017-08-08T17:24:55.118541: step 12075, loss 0.000229927, acc 1
2017-08-08T17:24:55.358659: step 12076, loss 2.36169e-06, acc 1
2017-08-08T17:24:55.711921: step 12077, loss 0.00578476, acc 1
2017-08-08T17:24:55.956145: step 12078, loss 0.000166849, acc 1
2017-08-08T17:24:56.229400: step 12079, loss 0.000198181, acc 1
2017-08-08T17:24:56.501398: step 12080, loss 0.00534788, acc 1
2017-08-08T17:24:56.957382: step 12081, loss 3.54757e-05, acc 1
2017-08-08T17:24:57.375674: step 12082, loss 0.00168025, acc 1
2017-08-08T17:24:57.670145: step 12083, loss 0.000474475, acc 1
2017-08-08T17:24:57.856032: step 12084, loss 0.000123762, acc 1
2017-08-08T17:24:58.103086: step 12085, loss 2.696e-05, acc 1
2017-08-08T17:24:58.438653: step 12086, loss 0.000589403, acc 1
2017-08-08T17:24:58.653332: step 12087, loss 0.000269376, acc 1
2017-08-08T17:24:58.871861: step 12088, loss 4.53399e-05, acc 1
2017-08-08T17:24:59.074810: step 12089, loss 0.000113524, acc 1
2017-08-08T17:24:59.467083: step 12090, loss 0.000447248, acc 1
2017-08-08T17:24:59.736350: step 12091, loss 3.98614e-05, acc 1
2017-08-08T17:25:00.025161: step 12092, loss 2.08107e-05, acc 1
2017-08-08T17:25:00.197452: step 12093, loss 8.21065e-06, acc 1
2017-08-08T17:25:00.461359: step 12094, loss 1.73055e-05, acc 1
2017-08-08T17:25:00.758848: step 12095, loss 6.56088e-06, acc 1
2017-08-08T17:25:00.962728: step 12096, loss 0.000402623, acc 1
2017-08-08T17:25:01.187377: step 12097, loss 0.000966316, acc 1
2017-08-08T17:25:01.506098: step 12098, loss 8.79296e-05, acc 1
2017-08-08T17:25:02.019434: step 12099, loss 9.25685e-06, acc 1
2017-08-08T17:25:02.420112: step 12100, loss 0.000194977, acc 1

Evaluation:
2017-08-08T17:25:03.385018: step 12100, loss 4.22149, acc 0.704503

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-12100

2017-08-08T17:25:04.046028: step 12101, loss 0.000350397, acc 1
2017-08-08T17:25:04.366054: step 12102, loss 1.3411e-07, acc 1
2017-08-08T17:25:04.642422: step 12103, loss 0.000278959, acc 1
2017-08-08T17:25:05.099350: step 12104, loss 0.000421197, acc 1
2017-08-08T17:25:05.501708: step 12105, loss 2.91567e-05, acc 1
2017-08-08T17:25:05.961461: step 12106, loss 9.91242e-05, acc 1
2017-08-08T17:25:06.352465: step 12107, loss 0.000235538, acc 1
2017-08-08T17:25:06.638852: step 12108, loss 3.91154e-07, acc 1
2017-08-08T17:25:07.122556: step 12109, loss 1.95502e-05, acc 1
2017-08-08T17:25:07.388100: step 12110, loss 8.59848e-06, acc 1
2017-08-08T17:25:07.640560: step 12111, loss 0.000228764, acc 1
2017-08-08T17:25:07.911929: step 12112, loss 8.75443e-08, acc 1
2017-08-08T17:25:08.316134: step 12113, loss 9.92951e-06, acc 1
2017-08-08T17:25:08.559177: step 12114, loss 6.09091e-05, acc 1
2017-08-08T17:25:08.806485: step 12115, loss 1.04634e-05, acc 1
2017-08-08T17:25:09.024561: step 12116, loss 7.89083e-05, acc 1
2017-08-08T17:25:09.431665: step 12117, loss 5.22441e-05, acc 1
2017-08-08T17:25:09.761846: step 12118, loss 1.14849e-05, acc 1
2017-08-08T17:25:10.020434: step 12119, loss 9.01543e-05, acc 1
2017-08-08T17:25:10.321932: step 12120, loss 1.26209e-05, acc 1
2017-08-08T17:25:10.725751: step 12121, loss 1.36156e-06, acc 1
2017-08-08T17:25:11.134918: step 12122, loss 1.86633e-06, acc 1
2017-08-08T17:25:11.520660: step 12123, loss 0.000277514, acc 1
2017-08-08T17:25:11.787254: step 12124, loss 1.35968e-06, acc 1
2017-08-08T17:25:12.086737: step 12125, loss 0.00259907, acc 1
2017-08-08T17:25:12.554338: step 12126, loss 0.000208046, acc 1
2017-08-08T17:25:12.841702: step 12127, loss 2.8839e-05, acc 1
2017-08-08T17:25:13.065818: step 12128, loss 0.000471917, acc 1
2017-08-08T17:25:13.322093: step 12129, loss 1.9744e-07, acc 1
2017-08-08T17:25:13.738474: step 12130, loss 7.59712e-06, acc 1
2017-08-08T17:25:14.207158: step 12131, loss 0.000259951, acc 1
2017-08-08T17:25:14.549589: step 12132, loss 0.0257514, acc 0.984375
2017-08-08T17:25:14.829314: step 12133, loss 0.000174112, acc 1
2017-08-08T17:25:15.149435: step 12134, loss 6.30441e-06, acc 1
2017-08-08T17:25:15.676133: step 12135, loss 0.000133534, acc 1
2017-08-08T17:25:15.919967: step 12136, loss 8.52009e-05, acc 1
2017-08-08T17:25:16.131191: step 12137, loss 0.00677557, acc 1
2017-08-08T17:25:16.465818: step 12138, loss 0.000167969, acc 1
2017-08-08T17:25:16.776016: step 12139, loss 3.84979e-05, acc 1
2017-08-08T17:25:17.129428: step 12140, loss 0.00021687, acc 1
2017-08-08T17:25:17.513527: step 12141, loss 0.000172323, acc 1
2017-08-08T17:25:17.776054: step 12142, loss 6.6313e-05, acc 1
2017-08-08T17:25:18.147929: step 12143, loss 0.000917402, acc 1
2017-08-08T17:25:18.445598: step 12144, loss 0.000115248, acc 1
2017-08-08T17:25:18.685961: step 12145, loss 0.000422303, acc 1
2017-08-08T17:25:18.912193: step 12146, loss 1.32432e-06, acc 1
2017-08-08T17:25:19.338779: step 12147, loss 0.0313193, acc 0.984375
2017-08-08T17:25:19.761053: step 12148, loss 7.02619e-05, acc 1
2017-08-08T17:25:20.075769: step 12149, loss 0.000232559, acc 1
2017-08-08T17:25:20.319619: step 12150, loss 0.000459916, acc 1
2017-08-08T17:25:20.808582: step 12151, loss 6.20249e-07, acc 1
2017-08-08T17:25:21.082083: step 12152, loss 4.48808e-05, acc 1
2017-08-08T17:25:21.345780: step 12153, loss 0.000108287, acc 1
2017-08-08T17:25:21.614969: step 12154, loss 2.87402e-05, acc 1
2017-08-08T17:25:21.899091: step 12155, loss 8.17901e-05, acc 1
2017-08-08T17:25:22.177646: step 12156, loss 0.000518671, acc 1
2017-08-08T17:25:22.432837: step 12157, loss 0.00081544, acc 1
2017-08-08T17:25:22.700376: step 12158, loss 0.00115639, acc 1
2017-08-08T17:25:22.881921: step 12159, loss 5.31757e-05, acc 1
2017-08-08T17:25:23.171200: step 12160, loss 0.000192021, acc 1
2017-08-08T17:25:23.425572: step 12161, loss 1.55374e-05, acc 1
2017-08-08T17:25:23.687394: step 12162, loss 0.000376294, acc 1
2017-08-08T17:25:23.987905: step 12163, loss 4.6905e-05, acc 1
2017-08-08T17:25:24.353221: step 12164, loss 0.000658826, acc 1
2017-08-08T17:25:24.763232: step 12165, loss 0.00151746, acc 1
2017-08-08T17:25:25.094509: step 12166, loss 5.59849e-06, acc 1
2017-08-08T17:25:25.422776: step 12167, loss 1.52736e-07, acc 1
2017-08-08T17:25:25.657634: step 12168, loss 0.0017123, acc 1
2017-08-08T17:25:25.945446: step 12169, loss 0.00052962, acc 1
2017-08-08T17:25:26.327174: step 12170, loss 0.000464076, acc 1
2017-08-08T17:25:26.626242: step 12171, loss 0.00109732, acc 1
2017-08-08T17:25:26.917857: step 12172, loss 4.47389e-05, acc 1
2017-08-08T17:25:27.354108: step 12173, loss 0.106858, acc 0.984375
2017-08-08T17:25:27.781116: step 12174, loss 3.28001e-06, acc 1
2017-08-08T17:25:28.136147: step 12175, loss 9.82885e-06, acc 1
2017-08-08T17:25:28.429312: step 12176, loss 0.0856502, acc 0.984375
2017-08-08T17:25:28.711774: step 12177, loss 5.40457e-06, acc 1
2017-08-08T17:25:29.130171: step 12178, loss 2.44006e-07, acc 1
2017-08-08T17:25:29.417932: step 12179, loss 1.31322e-05, acc 1
2017-08-08T17:25:29.697119: step 12180, loss 0.00349693, acc 1
2017-08-08T17:25:29.971582: step 12181, loss 2.55259e-05, acc 1
2017-08-08T17:25:30.365372: step 12182, loss 4.63856e-05, acc 1
2017-08-08T17:25:30.745395: step 12183, loss 1.99298e-06, acc 1
2017-08-08T17:25:31.078981: step 12184, loss 0.000429599, acc 1
2017-08-08T17:25:31.298000: step 12185, loss 0.000336453, acc 1
2017-08-08T17:25:31.523701: step 12186, loss 0.000485666, acc 1
2017-08-08T17:25:31.846383: step 12187, loss 0.00101168, acc 1
2017-08-08T17:25:32.102513: step 12188, loss 3.09043e-05, acc 1
2017-08-08T17:25:32.313875: step 12189, loss 0.00257683, acc 1
2017-08-08T17:25:32.555201: step 12190, loss 5.02713e-05, acc 1
2017-08-08T17:25:32.881481: step 12191, loss 1.0059e-05, acc 1
2017-08-08T17:25:33.093534: step 12192, loss 4.84782e-05, acc 1
2017-08-08T17:25:33.341632: step 12193, loss 3.52772e-05, acc 1
2017-08-08T17:25:33.536274: step 12194, loss 0.00438037, acc 1
2017-08-08T17:25:33.716860: step 12195, loss 0.0191665, acc 0.984375
2017-08-08T17:25:34.048363: step 12196, loss 0.0269529, acc 0.984375
2017-08-08T17:25:34.400743: step 12197, loss 3.32263e-06, acc 1
2017-08-08T17:25:34.646239: step 12198, loss 0.000214172, acc 1
2017-08-08T17:25:34.915092: step 12199, loss 0.000129563, acc 1
2017-08-08T17:25:35.211518: step 12200, loss 0.000211377, acc 1

Evaluation:
2017-08-08T17:25:36.201156: step 12200, loss 4.26285, acc 0.708255

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-12200

2017-08-08T17:25:36.794095: step 12201, loss 1.51431e-05, acc 1
2017-08-08T17:25:37.199791: step 12202, loss 1.5292e-06, acc 1
2017-08-08T17:25:37.432108: step 12203, loss 4.84727e-05, acc 1
2017-08-08T17:25:37.672285: step 12204, loss 0.000110146, acc 1
2017-08-08T17:25:37.922587: step 12205, loss 1.43421e-05, acc 1
2017-08-08T17:25:38.368659: step 12206, loss 1.17512e-05, acc 1
2017-08-08T17:25:38.772746: step 12207, loss 0.00362387, acc 1
2017-08-08T17:25:39.134431: step 12208, loss 0.000129452, acc 1
2017-08-08T17:25:39.379799: step 12209, loss 1.38825e-05, acc 1
2017-08-08T17:25:39.677173: step 12210, loss 7.62869e-05, acc 1
2017-08-08T17:25:39.925899: step 12211, loss 8.17825e-06, acc 1
2017-08-08T17:25:40.126271: step 12212, loss 1.14846e-05, acc 1
2017-08-08T17:25:40.375639: step 12213, loss 0.000129391, acc 1
2017-08-08T17:25:40.592833: step 12214, loss 1.94666e-05, acc 1
2017-08-08T17:25:40.968717: step 12215, loss 0.0402528, acc 0.984375
2017-08-08T17:25:41.210476: step 12216, loss 0.000292362, acc 1
2017-08-08T17:25:41.518612: step 12217, loss 7.61103e-05, acc 1
2017-08-08T17:25:41.756823: step 12218, loss 1.64694e-05, acc 1
2017-08-08T17:25:42.097771: step 12219, loss 2.74409e-05, acc 1
2017-08-08T17:25:42.484764: step 12220, loss 0.000232366, acc 1
2017-08-08T17:25:42.759908: step 12221, loss 0.000895429, acc 1
2017-08-08T17:25:43.069869: step 12222, loss 2.35997e-05, acc 1
2017-08-08T17:25:43.405451: step 12223, loss 0.00058309, acc 1
2017-08-08T17:25:43.851330: step 12224, loss 0.000320026, acc 1
2017-08-08T17:25:44.271480: step 12225, loss 9.12542e-05, acc 1
2017-08-08T17:25:44.635481: step 12226, loss 1.389e-05, acc 1
2017-08-08T17:25:44.904402: step 12227, loss 0.000255109, acc 1
2017-08-08T17:25:45.233668: step 12228, loss 6.62745e-05, acc 1
2017-08-08T17:25:45.539422: step 12229, loss 3.25961e-07, acc 1
2017-08-08T17:25:45.806131: step 12230, loss 8.30522e-06, acc 1
2017-08-08T17:25:46.057466: step 12231, loss 0.0012749, acc 1
2017-08-08T17:25:46.429417: step 12232, loss 0.00491544, acc 1
2017-08-08T17:25:46.871980: step 12233, loss 0.000953851, acc 1
2017-08-08T17:25:47.306999: step 12234, loss 3.33284e-05, acc 1
2017-08-08T17:25:47.645483: step 12235, loss 6.86881e-06, acc 1
2017-08-08T17:25:47.965311: step 12236, loss 3.2054e-06, acc 1
2017-08-08T17:25:48.215317: step 12237, loss 5.01048e-07, acc 1
2017-08-08T17:25:48.432325: step 12238, loss 0.000332322, acc 1
2017-08-08T17:25:48.673990: step 12239, loss 3.34315e-06, acc 1
2017-08-08T17:25:48.914438: step 12240, loss 0.00162857, acc 1
2017-08-08T17:25:49.340255: step 12241, loss 2.50203e-05, acc 1
2017-08-08T17:25:49.710000: step 12242, loss 0.000122367, acc 1
2017-08-08T17:25:50.049447: step 12243, loss 2.35989e-06, acc 1
2017-08-08T17:25:50.328104: step 12244, loss 9.57302e-06, acc 1
2017-08-08T17:25:50.673484: step 12245, loss 9.5966e-05, acc 1
2017-08-08T17:25:51.047618: step 12246, loss 0.000785843, acc 1
2017-08-08T17:25:51.284666: step 12247, loss 0.00602902, acc 1
2017-08-08T17:25:51.662284: step 12248, loss 8.51834e-06, acc 1
2017-08-08T17:25:52.022107: step 12249, loss 0.000118938, acc 1
2017-08-08T17:25:52.360569: step 12250, loss 0.000140535, acc 1
2017-08-08T17:25:52.807952: step 12251, loss 4.22819e-07, acc 1
2017-08-08T17:25:53.163545: step 12252, loss 8.12106e-07, acc 1
2017-08-08T17:25:53.460981: step 12253, loss 5.78381e-05, acc 1
2017-08-08T17:25:53.654131: step 12254, loss 2.4142e-05, acc 1
2017-08-08T17:25:54.035404: step 12255, loss 2.15315e-06, acc 1
2017-08-08T17:25:54.357131: step 12256, loss 3.10858e-06, acc 1
2017-08-08T17:25:54.674345: step 12257, loss 7.23786e-05, acc 1
2017-08-08T17:25:54.989283: step 12258, loss 4.58153e-06, acc 1
2017-08-08T17:25:55.513488: step 12259, loss 8.02488e-05, acc 1
2017-08-08T17:25:55.921434: step 12260, loss 2.92139e-05, acc 1
2017-08-08T17:25:56.261972: step 12261, loss 3.27057e-06, acc 1
2017-08-08T17:25:56.509960: step 12262, loss 0.149656, acc 0.984375
2017-08-08T17:25:56.748763: step 12263, loss 3.64868e-06, acc 1
2017-08-08T17:25:57.069381: step 12264, loss 0.000907564, acc 1
2017-08-08T17:25:57.407783: step 12265, loss 3.53632e-05, acc 1
2017-08-08T17:25:57.712037: step 12266, loss 0.000495662, acc 1
2017-08-08T17:25:57.914491: step 12267, loss 1.01471e-05, acc 1
2017-08-08T17:25:58.259816: step 12268, loss 0.000153169, acc 1
2017-08-08T17:25:58.633200: step 12269, loss 0.00034963, acc 1
2017-08-08T17:25:58.978555: step 12270, loss 1.99673e-05, acc 1
2017-08-08T17:25:59.251669: step 12271, loss 0.000192647, acc 1
2017-08-08T17:25:59.473124: step 12272, loss 1.94827e-06, acc 1
2017-08-08T17:25:59.778946: step 12273, loss 0.000795213, acc 1
2017-08-08T17:26:00.056101: step 12274, loss 8.87593e-05, acc 1
2017-08-08T17:26:00.294230: step 12275, loss 4.73878e-05, acc 1
2017-08-08T17:26:00.530172: step 12276, loss 0.00497578, acc 1
2017-08-08T17:26:00.813376: step 12277, loss 0.000556681, acc 1
2017-08-08T17:26:01.129388: step 12278, loss 0.000913466, acc 1
2017-08-08T17:26:01.476983: step 12279, loss 6.51498e-05, acc 1
2017-08-08T17:26:01.796249: step 12280, loss 0.000219901, acc 1
2017-08-08T17:26:02.039369: step 12281, loss 0.000347237, acc 1
2017-08-08T17:26:02.268372: step 12282, loss 0.000336092, acc 1
2017-08-08T17:26:02.721304: step 12283, loss 0.000296014, acc 1
2017-08-08T17:26:03.065742: step 12284, loss 3.72527e-07, acc 1
2017-08-08T17:26:03.302562: step 12285, loss 0.00151087, acc 1
2017-08-08T17:26:03.559565: step 12286, loss 0.0172703, acc 0.984375
2017-08-08T17:26:03.807530: step 12287, loss 0.000992635, acc 1
2017-08-08T17:26:04.088948: step 12288, loss 0.0171143, acc 0.984375
2017-08-08T17:26:04.562801: step 12289, loss 1.45298e-05, acc 1
2017-08-08T17:26:04.933707: step 12290, loss 1.05238e-06, acc 1
2017-08-08T17:26:05.201630: step 12291, loss 0.000202659, acc 1
2017-08-08T17:26:05.447313: step 12292, loss 0.000189877, acc 1
2017-08-08T17:26:05.849013: step 12293, loss 8.85069e-05, acc 1
2017-08-08T17:26:06.113288: step 12294, loss 7.97095e-06, acc 1
2017-08-08T17:26:06.413063: step 12295, loss 0.000416335, acc 1
2017-08-08T17:26:06.705444: step 12296, loss 0.00148377, acc 1
2017-08-08T17:26:07.193400: step 12297, loss 2.34936e-05, acc 1
2017-08-08T17:26:07.584647: step 12298, loss 3.84403e-06, acc 1
2017-08-08T17:26:07.901674: step 12299, loss 1.29079e-06, acc 1
2017-08-08T17:26:08.113992: step 12300, loss 0.00183905, acc 1

Evaluation:
2017-08-08T17:26:08.807633: step 12300, loss 4.25784, acc 0.705441

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-12300

2017-08-08T17:26:09.364082: step 12301, loss 0.000605196, acc 1
2017-08-08T17:26:09.653660: step 12302, loss 0.0372732, acc 0.984375
2017-08-08T17:26:09.903465: step 12303, loss 0.000254827, acc 1
2017-08-08T17:26:10.062781: step 12304, loss 1.75424e-05, acc 1
2017-08-08T17:26:10.305208: step 12305, loss 0.00134183, acc 1
2017-08-08T17:26:10.670210: step 12306, loss 0.000158409, acc 1
2017-08-08T17:26:10.954366: step 12307, loss 0.00027755, acc 1
2017-08-08T17:26:11.247370: step 12308, loss 3.60697e-05, acc 1
2017-08-08T17:26:11.602601: step 12309, loss 0.00192066, acc 1
2017-08-08T17:26:12.006206: step 12310, loss 2.42494e-05, acc 1
2017-08-08T17:26:12.421441: step 12311, loss 0.0787535, acc 0.984375
2017-08-08T17:26:12.669541: step 12312, loss 1.59998e-05, acc 1
2017-08-08T17:26:12.846998: step 12313, loss 0.000104453, acc 1
2017-08-08T17:26:13.185412: step 12314, loss 0.000726935, acc 1
2017-08-08T17:26:13.478042: step 12315, loss 4.90228e-06, acc 1
2017-08-08T17:26:13.738670: step 12316, loss 0.0103196, acc 1
2017-08-08T17:26:13.965375: step 12317, loss 2.70706e-05, acc 1
2017-08-08T17:26:14.219521: step 12318, loss 0.000605832, acc 1
2017-08-08T17:26:14.611443: step 12319, loss 0.000145385, acc 1
2017-08-08T17:26:14.933157: step 12320, loss 2.67465e-06, acc 1
2017-08-08T17:26:15.237836: step 12321, loss 0.000143438, acc 1
2017-08-08T17:26:15.491654: step 12322, loss 0.00021331, acc 1
2017-08-08T17:26:15.900732: step 12323, loss 0.0025642, acc 1
2017-08-08T17:26:16.168212: step 12324, loss 3.24099e-07, acc 1
2017-08-08T17:26:16.414852: step 12325, loss 1.21774e-05, acc 1
2017-08-08T17:26:16.745619: step 12326, loss 1.86093e-05, acc 1
2017-08-08T17:26:17.059953: step 12327, loss 1.73123e-05, acc 1
2017-08-08T17:26:17.325339: step 12328, loss 2.42194e-05, acc 1
2017-08-08T17:26:17.578778: step 12329, loss 0.000117243, acc 1
2017-08-08T17:26:17.853627: step 12330, loss 7.8989e-06, acc 1
2017-08-08T17:26:18.095352: step 12331, loss 8.17956e-06, acc 1
2017-08-08T17:26:18.358941: step 12332, loss 2.56595e-05, acc 1
2017-08-08T17:26:18.752262: step 12333, loss 9.36895e-07, acc 1
2017-08-08T17:26:19.012968: step 12334, loss 9.42472e-07, acc 1
2017-08-08T17:26:19.223943: step 12335, loss 3.35433e-06, acc 1
2017-08-08T17:26:19.486380: step 12336, loss 5.84374e-05, acc 1
2017-08-08T17:26:19.860373: step 12337, loss 4.63792e-07, acc 1
2017-08-08T17:26:20.259671: step 12338, loss 3.77948e-05, acc 1
2017-08-08T17:26:20.622095: step 12339, loss 2.45158e-05, acc 1
2017-08-08T17:26:20.881403: step 12340, loss 6.35779e-06, acc 1
2017-08-08T17:26:21.135199: step 12341, loss 1.60877e-05, acc 1
2017-08-08T17:26:21.514968: step 12342, loss 1.04275e-05, acc 1
2017-08-08T17:26:21.743046: step 12343, loss 7.21751e-05, acc 1
2017-08-08T17:26:22.031161: step 12344, loss 0.00012909, acc 1
2017-08-08T17:26:22.345592: step 12345, loss 3.10414e-05, acc 1
2017-08-08T17:26:22.735369: step 12346, loss 0.000250913, acc 1
2017-08-08T17:26:23.207667: step 12347, loss 0.000172538, acc 1
2017-08-08T17:26:23.536017: step 12348, loss 1.71967e-05, acc 1
2017-08-08T17:26:23.794369: step 12349, loss 4.74963e-05, acc 1
2017-08-08T17:26:24.060162: step 12350, loss 0.000794156, acc 1
2017-08-08T17:26:24.457365: step 12351, loss 3.69772e-05, acc 1
2017-08-08T17:26:24.748652: step 12352, loss 2.75671e-07, acc 1
2017-08-08T17:26:25.043933: step 12353, loss 0.000277659, acc 1
2017-08-08T17:26:25.334029: step 12354, loss 0.000703776, acc 1
2017-08-08T17:26:25.801531: step 12355, loss 5.3317e-05, acc 1
2017-08-08T17:26:26.140708: step 12356, loss 0.00129283, acc 1
2017-08-08T17:26:26.416568: step 12357, loss 0.000125883, acc 1
2017-08-08T17:26:26.637680: step 12358, loss 3.27825e-07, acc 1
2017-08-08T17:26:26.858407: step 12359, loss 3.07335e-07, acc 1
2017-08-08T17:26:27.348841: step 12360, loss 6.76037e-06, acc 1
2017-08-08T17:26:27.628474: step 12361, loss 3.06025e-06, acc 1
2017-08-08T17:26:27.917245: step 12362, loss 0.00180532, acc 1
2017-08-08T17:26:28.221927: step 12363, loss 7.61816e-07, acc 1
2017-08-08T17:26:28.671355: step 12364, loss 1.50171e-05, acc 1
2017-08-08T17:26:28.970209: step 12365, loss 4.81268e-06, acc 1
2017-08-08T17:26:29.246888: step 12366, loss 1.46633e-05, acc 1
2017-08-08T17:26:29.462983: step 12367, loss 0.0002913, acc 1
2017-08-08T17:26:29.695108: step 12368, loss 4.79273e-05, acc 1
2017-08-08T17:26:29.958759: step 12369, loss 0.00172249, acc 1
2017-08-08T17:26:30.320333: step 12370, loss 8.77296e-07, acc 1
2017-08-08T17:26:30.642461: step 12371, loss 0.00240749, acc 1
2017-08-08T17:26:30.892730: step 12372, loss 1.42861e-06, acc 1
2017-08-08T17:26:31.288523: step 12373, loss 0.000407976, acc 1
2017-08-08T17:26:31.759154: step 12374, loss 9.08057e-05, acc 1
2017-08-08T17:26:32.167187: step 12375, loss 1.89847e-05, acc 1
2017-08-08T17:26:32.433742: step 12376, loss 1.92962e-06, acc 1
2017-08-08T17:26:32.700354: step 12377, loss 6.53785e-07, acc 1
2017-08-08T17:26:33.183674: step 12378, loss 1.58432e-05, acc 1
2017-08-08T17:26:33.450817: step 12379, loss 0.000293869, acc 1
2017-08-08T17:26:33.716011: step 12380, loss 3.68159e-05, acc 1
2017-08-08T17:26:34.007389: step 12381, loss 1.99724e-05, acc 1
2017-08-08T17:26:34.457282: step 12382, loss 4.89101e-06, acc 1
2017-08-08T17:26:34.926257: step 12383, loss 4.67621e-05, acc 1
2017-08-08T17:26:35.270691: step 12384, loss 4.33601e-06, acc 1
2017-08-08T17:26:35.521802: step 12385, loss 4.27608e-05, acc 1
2017-08-08T17:26:35.859089: step 12386, loss 0.000188072, acc 1
2017-08-08T17:26:36.251637: step 12387, loss 8.38188e-08, acc 1
2017-08-08T17:26:36.538698: step 12388, loss 3.43305e-05, acc 1
2017-08-08T17:26:36.782304: step 12389, loss 0.00221201, acc 1
2017-08-08T17:26:37.119215: step 12390, loss 0.000859181, acc 1
2017-08-08T17:26:37.533368: step 12391, loss 0.000195276, acc 1
2017-08-08T17:26:37.903339: step 12392, loss 0.0044631, acc 1
2017-08-08T17:26:38.167265: step 12393, loss 0.000206038, acc 1
2017-08-08T17:26:38.371642: step 12394, loss 0.000212283, acc 1
2017-08-08T17:26:38.709396: step 12395, loss 1.48819e-06, acc 1
2017-08-08T17:26:38.965414: step 12396, loss 2.10474e-06, acc 1
2017-08-08T17:26:39.203895: step 12397, loss 0.00034672, acc 1
2017-08-08T17:26:39.471152: step 12398, loss 1.68935e-06, acc 1
2017-08-08T17:26:39.749026: step 12399, loss 0.00849871, acc 1
2017-08-08T17:26:40.207154: step 12400, loss 0.000227744, acc 1

Evaluation:
2017-08-08T17:26:41.018565: step 12400, loss 4.31434, acc 0.712008

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-12400

2017-08-08T17:26:41.494828: step 12401, loss 0.000161386, acc 1
2017-08-08T17:26:41.792006: step 12402, loss 4.77856e-05, acc 1
2017-08-08T17:26:42.012558: step 12403, loss 7.25645e-05, acc 1
2017-08-08T17:26:42.238186: step 12404, loss 0.000102033, acc 1
2017-08-08T17:26:42.464713: step 12405, loss 0.000855746, acc 1
2017-08-08T17:26:42.827547: step 12406, loss 3.83626e-05, acc 1
2017-08-08T17:26:43.158325: step 12407, loss 3.35276e-08, acc 1
2017-08-08T17:26:43.444353: step 12408, loss 0.000252138, acc 1
2017-08-08T17:26:43.686361: step 12409, loss 0.00172729, acc 1
2017-08-08T17:26:44.012141: step 12410, loss 0.00141075, acc 1
2017-08-08T17:26:44.221358: step 12411, loss 0.000190325, acc 1
2017-08-08T17:26:44.435943: step 12412, loss 0.000204975, acc 1
2017-08-08T17:26:44.689430: step 12413, loss 2.03577e-06, acc 1
2017-08-08T17:26:44.983653: step 12414, loss 0.000376812, acc 1
2017-08-08T17:26:45.229567: step 12415, loss 0.000259438, acc 1
2017-08-08T17:26:45.433325: step 12416, loss 4.997e-05, acc 1
2017-08-08T17:26:45.617969: step 12417, loss 0.000829131, acc 1
2017-08-08T17:26:45.880822: step 12418, loss 5.35094e-06, acc 1
2017-08-08T17:26:46.054420: step 12419, loss 0.00242109, acc 1
2017-08-08T17:26:46.267206: step 12420, loss 0.000453043, acc 1
2017-08-08T17:26:46.530781: step 12421, loss 0.00080776, acc 1
2017-08-08T17:26:46.876732: step 12422, loss 0.000324767, acc 1
2017-08-08T17:26:47.290556: step 12423, loss 8.25134e-07, acc 1
2017-08-08T17:26:47.632612: step 12424, loss 0.000715756, acc 1
2017-08-08T17:26:47.836520: step 12425, loss 3.04646e-05, acc 1
2017-08-08T17:26:48.129322: step 12426, loss 0.00024586, acc 1
2017-08-08T17:26:48.537461: step 12427, loss 0.000179125, acc 1
2017-08-08T17:26:48.790175: step 12428, loss 0.000590549, acc 1
2017-08-08T17:26:49.002356: step 12429, loss 2.59089e-06, acc 1
2017-08-08T17:26:49.218627: step 12430, loss 0.00903062, acc 1
2017-08-08T17:26:49.472309: step 12431, loss 3.6843e-05, acc 1
2017-08-08T17:26:49.749438: step 12432, loss 1.73593e-06, acc 1
2017-08-08T17:26:50.033419: step 12433, loss 1.66516e-06, acc 1
2017-08-08T17:26:50.275981: step 12434, loss 0.00272431, acc 1
2017-08-08T17:26:50.490381: step 12435, loss 0.00185243, acc 1
2017-08-08T17:26:50.740196: step 12436, loss 0.000800399, acc 1
2017-08-08T17:26:51.157359: step 12437, loss 9.38454e-05, acc 1
2017-08-08T17:26:51.486107: step 12438, loss 0.0232505, acc 0.984375
2017-08-08T17:26:51.771769: step 12439, loss 1.52731e-06, acc 1
2017-08-08T17:26:52.015892: step 12440, loss 0.000125826, acc 1
2017-08-08T17:26:52.459364: step 12441, loss 1.07658e-06, acc 1
2017-08-08T17:26:52.880720: step 12442, loss 8.99977e-05, acc 1
2017-08-08T17:26:53.258665: step 12443, loss 2.9036e-05, acc 1
2017-08-08T17:26:53.497544: step 12444, loss 2.75671e-07, acc 1
2017-08-08T17:26:53.833309: step 12445, loss 0.000503937, acc 1
2017-08-08T17:26:54.261952: step 12446, loss 0.000594378, acc 1
2017-08-08T17:26:54.519110: step 12447, loss 0.000119475, acc 1
2017-08-08T17:26:54.791525: step 12448, loss 4.5446e-06, acc 1
2017-08-08T17:26:55.244750: step 12449, loss 0.0141477, acc 0.984375
2017-08-08T17:26:55.568927: step 12450, loss 0.00380857, acc 1
2017-08-08T17:26:55.890794: step 12451, loss 0.000391488, acc 1
2017-08-08T17:26:56.111993: step 12452, loss 0.00105234, acc 1
2017-08-08T17:26:56.331465: step 12453, loss 8.61729e-05, acc 1
2017-08-08T17:26:56.595887: step 12454, loss 0.00024845, acc 1
2017-08-08T17:26:56.830897: step 12455, loss 0.000808878, acc 1
2017-08-08T17:26:57.025475: step 12456, loss 2.72119e-06, acc 1
2017-08-08T17:26:57.412877: step 12457, loss 2.08799e-06, acc 1
2017-08-08T17:26:57.702665: step 12458, loss 0.00102866, acc 1
2017-08-08T17:26:58.149770: step 12459, loss 0.00442928, acc 1
2017-08-08T17:26:58.404562: step 12460, loss 2.94296e-07, acc 1
2017-08-08T17:26:58.750521: step 12461, loss 2.61957e-05, acc 1
2017-08-08T17:26:59.095119: step 12462, loss 6.11548e-05, acc 1
2017-08-08T17:26:59.359626: step 12463, loss 6.92569e-06, acc 1
2017-08-08T17:26:59.660433: step 12464, loss 3.78941e-05, acc 1
2017-08-08T17:26:59.950901: step 12465, loss 0.000227699, acc 1
2017-08-08T17:27:00.363447: step 12466, loss 2.62632e-07, acc 1
2017-08-08T17:27:00.768622: step 12467, loss 0.000266823, acc 1
2017-08-08T17:27:01.121283: step 12468, loss 0.000119958, acc 1
2017-08-08T17:27:01.416913: step 12469, loss 0.000247949, acc 1
2017-08-08T17:27:01.927369: step 12470, loss 4.19072e-06, acc 1
2017-08-08T17:27:02.242265: step 12471, loss 6.80423e-05, acc 1
2017-08-08T17:27:02.587805: step 12472, loss 0.000643125, acc 1
2017-08-08T17:27:02.869512: step 12473, loss 5.15353e-06, acc 1
2017-08-08T17:27:03.271052: step 12474, loss 1.86264e-08, acc 1
2017-08-08T17:27:03.659603: step 12475, loss 4.12182e-06, acc 1
2017-08-08T17:27:04.015284: step 12476, loss 5.66143e-05, acc 1
2017-08-08T17:27:04.334470: step 12477, loss 3.46346e-05, acc 1
2017-08-08T17:27:04.613002: step 12478, loss 7.47598e-05, acc 1
2017-08-08T17:27:05.041407: step 12479, loss 0.000313865, acc 1
2017-08-08T17:27:05.350581: step 12480, loss 0.000363803, acc 1
2017-08-08T17:27:05.600658: step 12481, loss 6.80486e-05, acc 1
2017-08-08T17:27:05.857340: step 12482, loss 0.00383468, acc 1
2017-08-08T17:27:06.221454: step 12483, loss 2.54055e-06, acc 1
2017-08-08T17:27:06.565165: step 12484, loss 7.94087e-06, acc 1
2017-08-08T17:27:06.830156: step 12485, loss 0.000157672, acc 1
2017-08-08T17:27:07.058817: step 12486, loss 0.000687033, acc 1
2017-08-08T17:27:07.520442: step 12487, loss 1.00582e-06, acc 1
2017-08-08T17:27:07.836988: step 12488, loss 9.47926e-06, acc 1
2017-08-08T17:27:08.092442: step 12489, loss 7.67977e-06, acc 1
2017-08-08T17:27:08.397958: step 12490, loss 0.00142339, acc 1
2017-08-08T17:27:08.840101: step 12491, loss 0.00121717, acc 1
2017-08-08T17:27:09.276754: step 12492, loss 2.0656e-06, acc 1
2017-08-08T17:27:09.647543: step 12493, loss 1.15295e-06, acc 1
2017-08-08T17:27:09.918353: step 12494, loss 1.55897e-06, acc 1
2017-08-08T17:27:10.263467: step 12495, loss 1.00135e-05, acc 1
2017-08-08T17:27:10.645259: step 12496, loss 0.000148298, acc 1
2017-08-08T17:27:10.922710: step 12497, loss 0.000340751, acc 1
2017-08-08T17:27:11.230935: step 12498, loss 1.21995e-05, acc 1
2017-08-08T17:27:11.525814: step 12499, loss 8.06443e-06, acc 1
2017-08-08T17:27:11.789308: step 12500, loss 0.000530806, acc 1

Evaluation:
2017-08-08T17:27:12.360600: step 12500, loss 4.37177, acc 0.705441

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-12500

2017-08-08T17:27:12.854660: step 12501, loss 0.000136794, acc 1
2017-08-08T17:27:13.049351: step 12502, loss 1.6399e-05, acc 1
2017-08-08T17:27:13.262440: step 12503, loss 3.43469e-05, acc 1
2017-08-08T17:27:13.499624: step 12504, loss 0.0166672, acc 0.984375
2017-08-08T17:27:13.850463: step 12505, loss 6.28587e-06, acc 1
2017-08-08T17:27:14.173313: step 12506, loss 9.70626e-05, acc 1
2017-08-08T17:27:14.490225: step 12507, loss 1.13434e-06, acc 1
2017-08-08T17:27:14.664374: step 12508, loss 0.000676096, acc 1
2017-08-08T17:27:14.938676: step 12509, loss 2.24089e-05, acc 1
2017-08-08T17:27:15.266828: step 12510, loss 2.13031e-05, acc 1
2017-08-08T17:27:15.475559: step 12511, loss 5.20191e-06, acc 1
2017-08-08T17:27:15.727395: step 12512, loss 0.000194667, acc 1
2017-08-08T17:27:16.107149: step 12513, loss 7.64276e-05, acc 1
2017-08-08T17:27:16.454181: step 12514, loss 9.93822e-06, acc 1
2017-08-08T17:27:16.816908: step 12515, loss 0.0001972, acc 1
2017-08-08T17:27:17.019342: step 12516, loss 0.03924, acc 0.984375
2017-08-08T17:27:17.359488: step 12517, loss 0.00013806, acc 1
2017-08-08T17:27:17.699178: step 12518, loss 1.146e-05, acc 1
2017-08-08T17:27:18.027450: step 12519, loss 3.25384e-06, acc 1
2017-08-08T17:27:18.312872: step 12520, loss 5.15294e-05, acc 1
2017-08-08T17:27:18.651112: step 12521, loss 0.000248331, acc 1
2017-08-08T17:27:19.095100: step 12522, loss 1.35542e-05, acc 1
2017-08-08T17:27:19.448736: step 12523, loss 3.25007e-06, acc 1
2017-08-08T17:27:19.824672: step 12524, loss 3.53321e-06, acc 1
2017-08-08T17:27:20.097321: step 12525, loss 3.85609e-05, acc 1
2017-08-08T17:27:20.474132: step 12526, loss 4.44382e-06, acc 1
2017-08-08T17:27:20.770817: step 12527, loss 0.000767379, acc 1
2017-08-08T17:27:21.033066: step 12528, loss 1.3802e-06, acc 1
2017-08-08T17:27:21.384556: step 12529, loss 0.0128925, acc 0.984375
2017-08-08T17:27:21.846084: step 12530, loss 0.0106426, acc 1
2017-08-08T17:27:22.185028: step 12531, loss 0.00162032, acc 1
2017-08-08T17:27:22.436541: step 12532, loss 0.115861, acc 0.984375
2017-08-08T17:27:22.692697: step 12533, loss 0.00273006, acc 1
2017-08-08T17:27:23.127804: step 12534, loss 0.000137117, acc 1
2017-08-08T17:27:23.444635: step 12535, loss 2.27241e-07, acc 1
2017-08-08T17:27:23.735451: step 12536, loss 0.000125888, acc 1
2017-08-08T17:27:23.997611: step 12537, loss 9.25249e-06, acc 1
2017-08-08T17:27:24.323300: step 12538, loss 1.36401e-05, acc 1
2017-08-08T17:27:24.773831: step 12539, loss 0.000324887, acc 1
2017-08-08T17:27:25.135979: step 12540, loss 1.49227e-05, acc 1
2017-08-08T17:27:25.432446: step 12541, loss 0.0345313, acc 0.984375
2017-08-08T17:27:25.641608: step 12542, loss 6.18157e-05, acc 1
2017-08-08T17:27:26.040719: step 12543, loss 0.00832426, acc 1
2017-08-08T17:27:26.397360: step 12544, loss 1.02072e-06, acc 1
2017-08-08T17:27:26.627444: step 12545, loss 9.11971e-06, acc 1
2017-08-08T17:27:26.913782: step 12546, loss 1.5373e-05, acc 1
2017-08-08T17:27:27.344466: step 12547, loss 2.79197e-06, acc 1
2017-08-08T17:27:27.741854: step 12548, loss 0.000223286, acc 1
2017-08-08T17:27:28.059294: step 12549, loss 0.000155194, acc 1
2017-08-08T17:27:28.304066: step 12550, loss 0.00013701, acc 1
2017-08-08T17:27:28.513327: step 12551, loss 3.04926e-05, acc 1
2017-08-08T17:27:28.825886: step 12552, loss 0.000119486, acc 1
2017-08-08T17:27:29.016411: step 12553, loss 0.000914653, acc 1
2017-08-08T17:27:29.254603: step 12554, loss 9.98863e-06, acc 1
2017-08-08T17:27:29.514296: step 12555, loss 1.54599e-07, acc 1
2017-08-08T17:27:29.932379: step 12556, loss 0.000158243, acc 1
2017-08-08T17:27:30.362477: step 12557, loss 2.67845e-06, acc 1
2017-08-08T17:27:30.719372: step 12558, loss 1.41832e-05, acc 1
2017-08-08T17:27:30.995737: step 12559, loss 0.000292277, acc 1
2017-08-08T17:27:31.371655: step 12560, loss 2.04891e-08, acc 1
2017-08-08T17:27:31.758544: step 12561, loss 9.93789e-06, acc 1
2017-08-08T17:27:32.057359: step 12562, loss 2.5424e-06, acc 1
2017-08-08T17:27:32.357848: step 12563, loss 2.03547e-05, acc 1
2017-08-08T17:27:32.618363: step 12564, loss 0.000566621, acc 1
2017-08-08T17:27:33.075066: step 12565, loss 0.0454416, acc 0.984375
2017-08-08T17:27:33.472269: step 12566, loss 3.23631e-05, acc 1
2017-08-08T17:27:33.844825: step 12567, loss 7.58943e-05, acc 1
2017-08-08T17:27:34.068870: step 12568, loss 4.84815e-06, acc 1
2017-08-08T17:27:34.287630: step 12569, loss 3.16647e-07, acc 1
2017-08-08T17:27:34.700401: step 12570, loss 0.000163137, acc 1
2017-08-08T17:27:34.929579: step 12571, loss 0.000265418, acc 1
2017-08-08T17:27:35.157993: step 12572, loss 4.71742e-06, acc 1
2017-08-08T17:27:35.385057: step 12573, loss 4.35189e-05, acc 1
2017-08-08T17:27:35.676435: step 12574, loss 2.6077e-08, acc 1
2017-08-08T17:27:36.000607: step 12575, loss 1.53749e-05, acc 1
2017-08-08T17:27:36.292195: step 12576, loss 1.89549e-05, acc 1
2017-08-08T17:27:36.491019: step 12577, loss 3.68977e-06, acc 1
2017-08-08T17:27:36.691772: step 12578, loss 6.5646e-06, acc 1
2017-08-08T17:27:37.087630: step 12579, loss 0.000123314, acc 1
2017-08-08T17:27:37.390145: step 12580, loss 0.000241226, acc 1
2017-08-08T17:27:37.622088: step 12581, loss 0.00140504, acc 1
2017-08-08T17:27:37.806829: step 12582, loss 5.51125e-06, acc 1
2017-08-08T17:27:38.096130: step 12583, loss 4.60069e-07, acc 1
2017-08-08T17:27:38.412419: step 12584, loss 8.85755e-06, acc 1
2017-08-08T17:27:38.783370: step 12585, loss 1.97957e-05, acc 1
2017-08-08T17:27:39.054739: step 12586, loss 1.01794e-05, acc 1
2017-08-08T17:27:39.394012: step 12587, loss 9.74154e-07, acc 1
2017-08-08T17:27:39.675982: step 12588, loss 0.0674119, acc 0.984375
2017-08-08T17:27:39.882105: step 12589, loss 0.00131022, acc 1
2017-08-08T17:27:40.105337: step 12590, loss 0.00048464, acc 1
2017-08-08T17:27:40.532039: step 12591, loss 3.70823e-06, acc 1
2017-08-08T17:27:40.873395: step 12592, loss 0.02091, acc 0.984375
2017-08-08T17:27:41.207037: step 12593, loss 3.23435e-05, acc 1
2017-08-08T17:27:41.414010: step 12594, loss 0.000192558, acc 1
2017-08-08T17:27:41.786449: step 12595, loss 0.000377174, acc 1
2017-08-08T17:27:41.983959: step 12596, loss 0.000934273, acc 1
2017-08-08T17:27:42.181037: step 12597, loss 0.000381992, acc 1
2017-08-08T17:27:42.397491: step 12598, loss 1.72921e-05, acc 1
2017-08-08T17:27:42.786737: step 12599, loss 0.000110705, acc 1
2017-08-08T17:27:43.173577: step 12600, loss 7.42736e-06, acc 1

Evaluation:
2017-08-08T17:27:43.957834: step 12600, loss 4.36708, acc 0.710131

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-12600

2017-08-08T17:27:44.518748: step 12601, loss 6.44977e-06, acc 1
2017-08-08T17:27:44.730908: step 12602, loss 0.000962794, acc 1
2017-08-08T17:27:44.966358: step 12603, loss 0.00019104, acc 1
2017-08-08T17:27:45.369886: step 12604, loss 2.55192e-05, acc 1
2017-08-08T17:27:45.780684: step 12605, loss 5.95608e-06, acc 1
2017-08-08T17:27:46.114747: step 12606, loss 0.000369399, acc 1
2017-08-08T17:27:46.382103: step 12607, loss 1.89453e-05, acc 1
2017-08-08T17:27:46.641653: step 12608, loss 2.86845e-07, acc 1
2017-08-08T17:27:46.984441: step 12609, loss 0.000345533, acc 1
2017-08-08T17:27:47.196582: step 12610, loss 3.27934e-05, acc 1
2017-08-08T17:27:47.384028: step 12611, loss 1.63912e-07, acc 1
2017-08-08T17:27:47.588831: step 12612, loss 5.90574e-05, acc 1
2017-08-08T17:27:47.832303: step 12613, loss 4.29923e-05, acc 1
2017-08-08T17:27:48.183432: step 12614, loss 1.06788e-05, acc 1
2017-08-08T17:27:48.529269: step 12615, loss 0.00147693, acc 1
2017-08-08T17:27:48.804490: step 12616, loss 0.000128806, acc 1
2017-08-08T17:27:48.980482: step 12617, loss 0.0275577, acc 0.984375
2017-08-08T17:27:49.325409: step 12618, loss 7.84818e-05, acc 1
2017-08-08T17:27:49.527316: step 12619, loss 0.00160852, acc 1
2017-08-08T17:27:49.772026: step 12620, loss 0.000121213, acc 1
2017-08-08T17:27:50.045314: step 12621, loss 5.78908e-05, acc 1
2017-08-08T17:27:50.314580: step 12622, loss 8.94216e-06, acc 1
2017-08-08T17:27:50.661776: step 12623, loss 2.02045e-05, acc 1
2017-08-08T17:27:50.847911: step 12624, loss 0.000172617, acc 1
2017-08-08T17:27:51.066102: step 12625, loss 0.00225686, acc 1
2017-08-08T17:27:51.511294: step 12626, loss 1.066e-05, acc 1
2017-08-08T17:27:51.746812: step 12627, loss 0.00074013, acc 1
2017-08-08T17:27:52.027829: step 12628, loss 4.82329e-05, acc 1
2017-08-08T17:27:52.301026: step 12629, loss 9.99501e-06, acc 1
2017-08-08T17:27:52.598046: step 12630, loss 3.14944e-06, acc 1
2017-08-08T17:27:52.944051: step 12631, loss 1.6558e-05, acc 1
2017-08-08T17:27:53.210126: step 12632, loss 6.58623e-05, acc 1
2017-08-08T17:27:53.458863: step 12633, loss 0.00587583, acc 1
2017-08-08T17:27:53.655491: step 12634, loss 1.19297e-05, acc 1
2017-08-08T17:27:53.925333: step 12635, loss 0.000256387, acc 1
2017-08-08T17:27:54.271375: step 12636, loss 4.01504e-05, acc 1
2017-08-08T17:27:54.537287: step 12637, loss 6.41269e-05, acc 1
2017-08-08T17:27:54.760144: step 12638, loss 4.55984e-05, acc 1
2017-08-08T17:27:55.175919: step 12639, loss 6.78629e-06, acc 1
2017-08-08T17:27:55.437373: step 12640, loss 7.71598e-06, acc 1
2017-08-08T17:27:55.711588: step 12641, loss 6.23976e-07, acc 1
2017-08-08T17:27:55.913405: step 12642, loss 0.00264767, acc 1
2017-08-08T17:27:56.072167: step 12643, loss 5.22063e-06, acc 1
2017-08-08T17:27:56.345374: step 12644, loss 2.34779e-05, acc 1
2017-08-08T17:27:56.529550: step 12645, loss 3.78115e-07, acc 1
2017-08-08T17:27:56.741925: step 12646, loss 1.49012e-08, acc 1
2017-08-08T17:27:56.941268: step 12647, loss 4.04748e-05, acc 1
2017-08-08T17:27:57.321375: step 12648, loss 0.000791717, acc 1
2017-08-08T17:27:57.826407: step 12649, loss 0.000385594, acc 1
2017-08-08T17:27:58.162205: step 12650, loss 0.000109512, acc 1
2017-08-08T17:27:58.364711: step 12651, loss 6.84103e-05, acc 1
2017-08-08T17:27:58.624427: step 12652, loss 1.54104e-05, acc 1
2017-08-08T17:27:59.044460: step 12653, loss 0.000223388, acc 1
2017-08-08T17:27:59.367042: step 12654, loss 6.13003e-05, acc 1
2017-08-08T17:27:59.649144: step 12655, loss 1.03748e-06, acc 1
2017-08-08T17:27:59.952660: step 12656, loss 2.90753e-06, acc 1
2017-08-08T17:28:00.277738: step 12657, loss 0.00282406, acc 1
2017-08-08T17:28:00.609380: step 12658, loss 0.00202435, acc 1
2017-08-08T17:28:01.063785: step 12659, loss 3.6754e-05, acc 1
2017-08-08T17:28:01.434127: step 12660, loss 6.66376e-06, acc 1
2017-08-08T17:28:01.644960: step 12661, loss 3.56677e-06, acc 1
2017-08-08T17:28:01.929400: step 12662, loss 0.000148345, acc 1
2017-08-08T17:28:02.339468: step 12663, loss 1.75563e-05, acc 1
2017-08-08T17:28:02.627721: step 12664, loss 0.0020446, acc 1
2017-08-08T17:28:02.976162: step 12665, loss 1.06617e-05, acc 1
2017-08-08T17:28:03.379569: step 12666, loss 0.00417826, acc 1
2017-08-08T17:28:03.728615: step 12667, loss 5.57812e-05, acc 1
2017-08-08T17:28:04.051751: step 12668, loss 4.96367e-06, acc 1
2017-08-08T17:28:04.339834: step 12669, loss 3.13469e-06, acc 1
2017-08-08T17:28:04.611121: step 12670, loss 4.3249e-06, acc 1
2017-08-08T17:28:04.929674: step 12671, loss 3.31548e-07, acc 1
2017-08-08T17:28:05.146163: step 12672, loss 4.58207e-07, acc 1
2017-08-08T17:28:05.346141: step 12673, loss 3.96741e-07, acc 1
2017-08-08T17:28:05.667025: step 12674, loss 0.0140624, acc 0.984375
2017-08-08T17:28:05.941143: step 12675, loss 2.40289e-05, acc 1
2017-08-08T17:28:06.222015: step 12676, loss 6.62041e-05, acc 1
2017-08-08T17:28:06.560250: step 12677, loss 5.46865e-05, acc 1
2017-08-08T17:28:06.782614: step 12678, loss 0.00300172, acc 1
2017-08-08T17:28:07.160480: step 12679, loss 0.000222423, acc 1
2017-08-08T17:28:07.502760: step 12680, loss 4.10848e-05, acc 1
2017-08-08T17:28:07.797116: step 12681, loss 0.000106678, acc 1
2017-08-08T17:28:08.083247: step 12682, loss 0.000118641, acc 1
2017-08-08T17:28:08.496054: step 12683, loss 7.13383e-07, acc 1
2017-08-08T17:28:08.875705: step 12684, loss 8.20009e-06, acc 1
2017-08-08T17:28:09.242309: step 12685, loss 5.58485e-05, acc 1
2017-08-08T17:28:09.464158: step 12686, loss 4.77199e-05, acc 1
2017-08-08T17:28:09.680265: step 12687, loss 2.06373e-06, acc 1
2017-08-08T17:28:10.013701: step 12688, loss 2.64067e-05, acc 1
2017-08-08T17:28:10.243027: step 12689, loss 0.000435577, acc 1
2017-08-08T17:28:10.482685: step 12690, loss 0.000156906, acc 1
2017-08-08T17:28:10.708082: step 12691, loss 0.00294477, acc 1
2017-08-08T17:28:11.059812: step 12692, loss 0.000134582, acc 1
2017-08-08T17:28:11.323640: step 12693, loss 1.9464e-06, acc 1
2017-08-08T17:28:11.552238: step 12694, loss 0.00348473, acc 1
2017-08-08T17:28:11.765843: step 12695, loss 0.00870564, acc 1
2017-08-08T17:28:12.017382: step 12696, loss 2.03837e-05, acc 1
2017-08-08T17:28:12.428603: step 12697, loss 7.20156e-05, acc 1
2017-08-08T17:28:12.719475: step 12698, loss 1.02445e-07, acc 1
2017-08-08T17:28:13.025778: step 12699, loss 5.14985e-06, acc 1
2017-08-08T17:28:13.303505: step 12700, loss 6.89178e-08, acc 1

Evaluation:
2017-08-08T17:28:14.358742: step 12700, loss 4.41405, acc 0.709193

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-12700

2017-08-08T17:28:14.892686: step 12701, loss 0.000251511, acc 1
2017-08-08T17:28:15.121377: step 12702, loss 0.000581772, acc 1
2017-08-08T17:28:15.529836: step 12703, loss 9.64554e-05, acc 1
2017-08-08T17:28:15.824334: step 12704, loss 7.76033e-06, acc 1
2017-08-08T17:28:16.096658: step 12705, loss 0.000253092, acc 1
2017-08-08T17:28:16.315695: step 12706, loss 2.168e-06, acc 1
2017-08-08T17:28:16.619313: step 12707, loss 2.90057e-05, acc 1
2017-08-08T17:28:16.891322: step 12708, loss 2.78533e-05, acc 1
2017-08-08T17:28:17.193323: step 12709, loss 0.000100211, acc 1
2017-08-08T17:28:17.450134: step 12710, loss 3.61351e-07, acc 1
2017-08-08T17:28:17.645606: step 12711, loss 3.74566e-05, acc 1
2017-08-08T17:28:17.925374: step 12712, loss 0.000928932, acc 1
2017-08-08T17:28:18.202633: step 12713, loss 8.19174e-05, acc 1
2017-08-08T17:28:18.449054: step 12714, loss 8.5896e-05, acc 1
2017-08-08T17:28:18.655998: step 12715, loss 6.21138e-05, acc 1
2017-08-08T17:28:18.917368: step 12716, loss 0.000492724, acc 1
2017-08-08T17:28:19.128836: step 12717, loss 0.000821169, acc 1
2017-08-08T17:28:19.404141: step 12718, loss 0.000131043, acc 1
2017-08-08T17:28:19.657327: step 12719, loss 3.04571e-05, acc 1
2017-08-08T17:28:19.867341: step 12720, loss 0.00118737, acc 1
2017-08-08T17:28:20.178722: step 12721, loss 1.11276e-05, acc 1
2017-08-08T17:28:20.391698: step 12722, loss 0.0108424, acc 1
2017-08-08T17:28:20.588384: step 12723, loss 2.69197e-05, acc 1
2017-08-08T17:28:20.839965: step 12724, loss 4.97322e-07, acc 1
2017-08-08T17:28:21.092315: step 12725, loss 5.83809e-05, acc 1
2017-08-08T17:28:21.384880: step 12726, loss 2.84092e-05, acc 1
2017-08-08T17:28:21.647548: step 12727, loss 3.32281e-06, acc 1
2017-08-08T17:28:21.867755: step 12728, loss 0.000145623, acc 1
2017-08-08T17:28:22.070410: step 12729, loss 0.000171232, acc 1
2017-08-08T17:28:22.325319: step 12730, loss 3.05656e-05, acc 1
2017-08-08T17:28:22.501594: step 12731, loss 6.4129e-05, acc 1
2017-08-08T17:28:22.687683: step 12732, loss 2.8872e-05, acc 1
2017-08-08T17:28:22.909974: step 12733, loss 5.24296e-06, acc 1
2017-08-08T17:28:23.230657: step 12734, loss 0.000660586, acc 1
2017-08-08T17:28:23.538978: step 12735, loss 5.92847e-05, acc 1
2017-08-08T17:28:23.800621: step 12736, loss 0.000173364, acc 1
2017-08-08T17:28:23.997311: step 12737, loss 0.0614085, acc 0.984375
2017-08-08T17:28:24.325303: step 12738, loss 1.92267e-05, acc 1
2017-08-08T17:28:24.544939: step 12739, loss 0.000208928, acc 1
2017-08-08T17:28:24.772603: step 12740, loss 0.00134421, acc 1
2017-08-08T17:28:24.998205: step 12741, loss 1.59807e-06, acc 1
2017-08-08T17:28:25.317310: step 12742, loss 1.68688e-05, acc 1
2017-08-08T17:28:25.662367: step 12743, loss 0.000569441, acc 1
2017-08-08T17:28:25.923815: step 12744, loss 2.8599e-05, acc 1
2017-08-08T17:28:26.117931: step 12745, loss 4.82195e-06, acc 1
2017-08-08T17:28:26.373544: step 12746, loss 1.25894e-05, acc 1
2017-08-08T17:28:26.704951: step 12747, loss 2.33083e-05, acc 1
2017-08-08T17:28:27.008762: step 12748, loss 4.95248e-06, acc 1
2017-08-08T17:28:27.236913: step 12749, loss 2.77573e-05, acc 1
2017-08-08T17:28:27.504079: step 12750, loss 1.21338e-05, acc 1
2017-08-08T17:28:27.789050: step 12751, loss 9.04754e-06, acc 1
2017-08-08T17:28:28.116758: step 12752, loss 0.000148053, acc 1
2017-08-08T17:28:28.424160: step 12753, loss 1.24134e-05, acc 1
2017-08-08T17:28:28.666957: step 12754, loss 0.000334386, acc 1
2017-08-08T17:28:28.926379: step 12755, loss 4.74972e-07, acc 1
2017-08-08T17:28:29.287649: step 12756, loss 7.74854e-07, acc 1
2017-08-08T17:28:29.607842: step 12757, loss 2.23957e-05, acc 1
2017-08-08T17:28:29.819123: step 12758, loss 2.19791e-07, acc 1
2017-08-08T17:28:30.173410: step 12759, loss 0.000767416, acc 1
2017-08-08T17:28:30.550863: step 12760, loss 8.19559e-07, acc 1
2017-08-08T17:28:30.838028: step 12761, loss 9.82822e-06, acc 1
2017-08-08T17:28:31.050403: step 12762, loss 8.50626e-05, acc 1
2017-08-08T17:28:31.383819: step 12763, loss 5.80899e-05, acc 1
2017-08-08T17:28:31.695071: step 12764, loss 4.35279e-05, acc 1
2017-08-08T17:28:31.924416: step 12765, loss 0.000160297, acc 1
2017-08-08T17:28:32.193526: step 12766, loss 0.000495952, acc 1
2017-08-08T17:28:32.409355: step 12767, loss 0.00298499, acc 1
2017-08-08T17:28:32.748203: step 12768, loss 3.50515e-05, acc 1
2017-08-08T17:28:33.057441: step 12769, loss 2.2258e-06, acc 1
2017-08-08T17:28:33.320001: step 12770, loss 6.6334e-06, acc 1
2017-08-08T17:28:33.534650: step 12771, loss 1.1995e-06, acc 1
2017-08-08T17:28:33.724441: step 12772, loss 8.70156e-06, acc 1
2017-08-08T17:28:34.039805: step 12773, loss 3.70069e-06, acc 1
2017-08-08T17:28:34.221474: step 12774, loss 5.17175e-05, acc 1
2017-08-08T17:28:34.430661: step 12775, loss 8.49781e-05, acc 1
2017-08-08T17:28:34.639365: step 12776, loss 0.00031596, acc 1
2017-08-08T17:28:34.972664: step 12777, loss 3.19796e-06, acc 1
2017-08-08T17:28:35.193328: step 12778, loss 0.000836311, acc 1
2017-08-08T17:28:35.430876: step 12779, loss 4.82686e-05, acc 1
2017-08-08T17:28:35.612566: step 12780, loss 6.29567e-07, acc 1
2017-08-08T17:28:35.900502: step 12781, loss 8.87894e-05, acc 1
2017-08-08T17:28:36.128058: step 12782, loss 3.49013e-05, acc 1
2017-08-08T17:28:36.296150: step 12783, loss 8.04759e-05, acc 1
2017-08-08T17:28:36.469535: step 12784, loss 1.30385e-08, acc 1
2017-08-08T17:28:36.627478: step 12785, loss 7.88276e-05, acc 1
2017-08-08T17:28:37.032093: step 12786, loss 0.000601096, acc 1
2017-08-08T17:28:37.418737: step 12787, loss 0.01555, acc 0.984375
2017-08-08T17:28:37.679208: step 12788, loss 0.000443396, acc 1
2017-08-08T17:28:37.907936: step 12789, loss 1.17317e-05, acc 1
2017-08-08T17:28:38.127318: step 12790, loss 0.000634655, acc 1
2017-08-08T17:28:38.437325: step 12791, loss 1.68682e-05, acc 1
2017-08-08T17:28:38.732734: step 12792, loss 2.51457e-07, acc 1
2017-08-08T17:28:39.002862: step 12793, loss 0.000366616, acc 1
2017-08-08T17:28:39.429252: step 12794, loss 0.000452134, acc 1
2017-08-08T17:28:39.727893: step 12795, loss 0.00123227, acc 1
2017-08-08T17:28:40.095523: step 12796, loss 4.31354e-06, acc 1
2017-08-08T17:28:40.438868: step 12797, loss 1.17346e-05, acc 1
2017-08-08T17:28:40.687073: step 12798, loss 2.13837e-05, acc 1
2017-08-08T17:28:41.121829: step 12799, loss 2.43068e-06, acc 1
2017-08-08T17:28:41.396760: step 12800, loss 3.03224e-06, acc 1

Evaluation:
2017-08-08T17:28:42.286719: step 12800, loss 4.46438, acc 0.705441

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-12800

2017-08-08T17:28:42.913822: step 12801, loss 4.73131e-05, acc 1
2017-08-08T17:28:43.200323: step 12802, loss 1.70055e-06, acc 1
2017-08-08T17:28:43.653701: step 12803, loss 5.02914e-08, acc 1
2017-08-08T17:28:43.988278: step 12804, loss 1.80301e-05, acc 1
2017-08-08T17:28:44.253359: step 12805, loss 0.00397803, acc 1
2017-08-08T17:28:44.609542: step 12806, loss 4.30414e-05, acc 1
2017-08-08T17:28:44.875683: step 12807, loss 8.28872e-07, acc 1
2017-08-08T17:28:45.097327: step 12808, loss 2.78132e-05, acc 1
2017-08-08T17:28:45.381227: step 12809, loss 2.23875e-06, acc 1
2017-08-08T17:28:45.620379: step 12810, loss 5.943e-05, acc 1
2017-08-08T17:28:45.969419: step 12811, loss 1.56282e-05, acc 1
2017-08-08T17:28:46.268025: step 12812, loss 1.30198e-06, acc 1
2017-08-08T17:28:46.585311: step 12813, loss 7.33399e-05, acc 1
2017-08-08T17:28:46.789455: step 12814, loss 2.03391e-06, acc 1
2017-08-08T17:28:47.066528: step 12815, loss 0.000915285, acc 1
2017-08-08T17:28:47.297677: step 12816, loss 3.01723e-06, acc 1
2017-08-08T17:28:47.573253: step 12817, loss 0.0017149, acc 1
2017-08-08T17:28:47.949825: step 12818, loss 3.9769e-05, acc 1
2017-08-08T17:28:48.182220: step 12819, loss 1.89888e-05, acc 1
2017-08-08T17:28:48.610943: step 12820, loss 0.000581791, acc 1
2017-08-08T17:28:48.841728: step 12821, loss 0.00031454, acc 1
2017-08-08T17:28:49.162123: step 12822, loss 5.68578e-06, acc 1
2017-08-08T17:28:49.343924: step 12823, loss 1.71732e-06, acc 1
2017-08-08T17:28:49.554661: step 12824, loss 3.15196e-05, acc 1
2017-08-08T17:28:49.910920: step 12825, loss 9.21571e-05, acc 1
2017-08-08T17:28:50.184835: step 12826, loss 0.00912273, acc 1
2017-08-08T17:28:50.557210: step 12827, loss 1.16413e-06, acc 1
2017-08-08T17:28:50.859450: step 12828, loss 1.62607e-05, acc 1
2017-08-08T17:28:51.131596: step 12829, loss 0.000555658, acc 1
2017-08-08T17:28:51.566179: step 12830, loss 4.33993e-07, acc 1
2017-08-08T17:28:51.800938: step 12831, loss 6.43128e-05, acc 1
2017-08-08T17:28:52.054870: step 12832, loss 5.2263e-06, acc 1
2017-08-08T17:28:52.337753: step 12833, loss 0.000100534, acc 1
2017-08-08T17:28:52.772550: step 12834, loss 3.69902e-06, acc 1
2017-08-08T17:28:53.112598: step 12835, loss 1.3323e-05, acc 1
2017-08-08T17:28:53.389375: step 12836, loss 6.8272e-05, acc 1
2017-08-08T17:28:53.690918: step 12837, loss 0.000212742, acc 1
2017-08-08T17:28:53.991449: step 12838, loss 1.86087e-05, acc 1
2017-08-08T17:28:54.306345: step 12839, loss 0.000102093, acc 1
2017-08-08T17:28:54.541247: step 12840, loss 2.65786e-06, acc 1
2017-08-08T17:28:54.733425: step 12841, loss 1.03002e-06, acc 1
2017-08-08T17:28:55.161387: step 12842, loss 1.15611e-05, acc 1
2017-08-08T17:28:55.567312: step 12843, loss 6.68405e-06, acc 1
2017-08-08T17:28:55.862026: step 12844, loss 2.7817e-05, acc 1
2017-08-08T17:28:56.128343: step 12845, loss 5.55603e-05, acc 1
2017-08-08T17:28:56.385257: step 12846, loss 5.00523e-05, acc 1
2017-08-08T17:28:56.611718: step 12847, loss 0.000569518, acc 1
2017-08-08T17:28:56.803670: step 12848, loss 1.43167e-05, acc 1
2017-08-08T17:28:57.019600: step 12849, loss 1.05812e-05, acc 1
2017-08-08T17:28:57.301802: step 12850, loss 1.52737e-07, acc 1
2017-08-08T17:28:57.569508: step 12851, loss 4.45113e-06, acc 1
2017-08-08T17:28:57.823642: step 12852, loss 2.73808e-07, acc 1
2017-08-08T17:28:58.014345: step 12853, loss 0.000109635, acc 1
2017-08-08T17:28:58.196491: step 12854, loss 0.0021467, acc 1
2017-08-08T17:28:58.444032: step 12855, loss 1.41187e-06, acc 1
2017-08-08T17:28:58.635923: step 12856, loss 0.000340984, acc 1
2017-08-08T17:28:58.861318: step 12857, loss 0.000208948, acc 1
2017-08-08T17:28:59.097770: step 12858, loss 8.75421e-07, acc 1
2017-08-08T17:28:59.484874: step 12859, loss 1.24794e-06, acc 1
2017-08-08T17:28:59.848863: step 12860, loss 2.77722e-05, acc 1
2017-08-08T17:29:00.184363: step 12861, loss 0.000185933, acc 1
2017-08-08T17:29:00.458103: step 12862, loss 0.00140867, acc 1
2017-08-08T17:29:00.705388: step 12863, loss 5.08296e-06, acc 1
2017-08-08T17:29:01.149981: step 12864, loss 2.45672e-06, acc 1
2017-08-08T17:29:01.395479: step 12865, loss 5.40097e-06, acc 1
2017-08-08T17:29:01.661720: step 12866, loss 0.000385892, acc 1
2017-08-08T17:29:02.030629: step 12867, loss 9.68077e-06, acc 1
2017-08-08T17:29:02.519670: step 12868, loss 0.0154998, acc 0.984375
2017-08-08T17:29:03.099160: step 12869, loss 2.09535e-06, acc 1
2017-08-08T17:29:03.409719: step 12870, loss 0.0785863, acc 0.984375
2017-08-08T17:29:03.747257: step 12871, loss 5.08816e-06, acc 1
2017-08-08T17:29:04.182164: step 12872, loss 1.11196e-06, acc 1
2017-08-08T17:29:04.485567: step 12873, loss 8.16284e-06, acc 1
2017-08-08T17:29:04.713933: step 12874, loss 9.85051e-06, acc 1
2017-08-08T17:29:04.977363: step 12875, loss 4.04917e-05, acc 1
2017-08-08T17:29:05.237120: step 12876, loss 8.77497e-06, acc 1
2017-08-08T17:29:05.537331: step 12877, loss 0.00426479, acc 1
2017-08-08T17:29:05.799420: step 12878, loss 0.0359851, acc 0.984375
2017-08-08T17:29:05.986674: step 12879, loss 7.84438e-06, acc 1
2017-08-08T17:29:06.261064: step 12880, loss 1.1192e-05, acc 1
2017-08-08T17:29:06.558855: step 12881, loss 2.27242e-07, acc 1
2017-08-08T17:29:06.852311: step 12882, loss 0.00832305, acc 1
2017-08-08T17:29:07.088426: step 12883, loss 0.000563597, acc 1
2017-08-08T17:29:07.489792: step 12884, loss 5.64373e-07, acc 1
2017-08-08T17:29:07.893851: step 12885, loss 2.23517e-08, acc 1
2017-08-08T17:29:08.177680: step 12886, loss 9.55589e-06, acc 1
2017-08-08T17:29:08.406376: step 12887, loss 3.48312e-07, acc 1
2017-08-08T17:29:08.669390: step 12888, loss 3.81735e-05, acc 1
2017-08-08T17:29:08.997526: step 12889, loss 0.0022788, acc 1
2017-08-08T17:29:09.209789: step 12890, loss 0.000206849, acc 1
2017-08-08T17:29:09.410957: step 12891, loss 0.000163467, acc 1
2017-08-08T17:29:09.689897: step 12892, loss 1.29796e-05, acc 1
2017-08-08T17:29:10.079688: step 12893, loss 1.3839e-06, acc 1
2017-08-08T17:29:10.356143: step 12894, loss 0.000123167, acc 1
2017-08-08T17:29:10.613817: step 12895, loss 0.0292071, acc 0.984375
2017-08-08T17:29:10.832286: step 12896, loss 0.000187131, acc 1
2017-08-08T17:29:11.175585: step 12897, loss 9.61095e-07, acc 1
2017-08-08T17:29:11.396767: step 12898, loss 0.0034013, acc 1
2017-08-08T17:29:11.604933: step 12899, loss 0.000965025, acc 1
2017-08-08T17:29:11.837312: step 12900, loss 0.00228494, acc 1

Evaluation:
2017-08-08T17:29:12.474252: step 12900, loss 4.51875, acc 0.707317

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-12900

2017-08-08T17:29:12.888364: step 12901, loss 3.84194e-05, acc 1
2017-08-08T17:29:13.262827: step 12902, loss 6.91149e-05, acc 1
2017-08-08T17:29:13.470685: step 12903, loss 0.0190511, acc 1
2017-08-08T17:29:13.722629: step 12904, loss 4.37878e-06, acc 1
2017-08-08T17:29:13.988438: step 12905, loss 0.000541655, acc 1
2017-08-08T17:29:14.288168: step 12906, loss 7.68394e-06, acc 1
2017-08-08T17:29:14.617033: step 12907, loss 0.000194947, acc 1
2017-08-08T17:29:14.813783: step 12908, loss 0.000121598, acc 1
2017-08-08T17:29:15.007105: step 12909, loss 8.33117e-05, acc 1
2017-08-08T17:29:15.217322: step 12910, loss 3.57626e-07, acc 1
2017-08-08T17:29:15.506244: step 12911, loss 8.20487e-05, acc 1
2017-08-08T17:29:15.695573: step 12912, loss 2.23693e-06, acc 1
2017-08-08T17:29:15.919743: step 12913, loss 1.07286e-06, acc 1
2017-08-08T17:29:16.116318: step 12914, loss 0.000463985, acc 1
2017-08-08T17:29:16.432636: step 12915, loss 3.32271e-06, acc 1
2017-08-08T17:29:16.697207: step 12916, loss 0.000355945, acc 1
2017-08-08T17:29:17.011544: step 12917, loss 2.48788e-05, acc 1
2017-08-08T17:29:17.206390: step 12918, loss 0.00110207, acc 1
2017-08-08T17:29:17.527242: step 12919, loss 9.52998e-06, acc 1
2017-08-08T17:29:17.793617: step 12920, loss 0.000335869, acc 1
2017-08-08T17:29:18.018577: step 12921, loss 0.0704488, acc 0.984375
2017-08-08T17:29:18.271502: step 12922, loss 1.14178e-06, acc 1
2017-08-08T17:29:18.618476: step 12923, loss 2.20715e-06, acc 1
2017-08-08T17:29:18.980576: step 12924, loss 4.76835e-07, acc 1
2017-08-08T17:29:19.222836: step 12925, loss 1.84841e-05, acc 1
2017-08-08T17:29:19.415846: step 12926, loss 0.000135043, acc 1
2017-08-08T17:29:19.731784: step 12927, loss 8.03942e-05, acc 1
2017-08-08T17:29:20.169513: step 12928, loss 1.53773e-05, acc 1
2017-08-08T17:29:20.378352: step 12929, loss 6.92407e-05, acc 1
2017-08-08T17:29:20.637310: step 12930, loss 5.25944e-06, acc 1
2017-08-08T17:29:20.897318: step 12931, loss 0.00046848, acc 1
2017-08-08T17:29:21.196440: step 12932, loss 5.02914e-08, acc 1
2017-08-08T17:29:21.380264: step 12933, loss 0.0159626, acc 0.984375
2017-08-08T17:29:21.595545: step 12934, loss 6.96618e-07, acc 1
2017-08-08T17:29:21.945440: step 12935, loss 4.48134e-06, acc 1
2017-08-08T17:29:22.150213: step 12936, loss 0.000313859, acc 1
2017-08-08T17:29:22.355244: step 12937, loss 0.000165948, acc 1
2017-08-08T17:29:22.602931: step 12938, loss 1.66317e-05, acc 1
2017-08-08T17:29:22.917310: step 12939, loss 0.00012238, acc 1
2017-08-08T17:29:23.202804: step 12940, loss 0.000135593, acc 1
2017-08-08T17:29:23.483501: step 12941, loss 0.00162323, acc 1
2017-08-08T17:29:23.667424: step 12942, loss 0.000315631, acc 1
2017-08-08T17:29:24.003988: step 12943, loss 4.76983e-05, acc 1
2017-08-08T17:29:24.234224: step 12944, loss 0.0123852, acc 0.984375
2017-08-08T17:29:24.509188: step 12945, loss 0.0908215, acc 0.96875
2017-08-08T17:29:24.792698: step 12946, loss 2.34686e-06, acc 1
2017-08-08T17:29:25.072694: step 12947, loss 2.43471e-05, acc 1
2017-08-08T17:29:25.369336: step 12948, loss 0.0319904, acc 0.984375
2017-08-08T17:29:25.604656: step 12949, loss 2.56102e-06, acc 1
2017-08-08T17:29:25.770019: step 12950, loss 0.012319, acc 0.984375
2017-08-08T17:29:25.957699: step 12951, loss 0.000109115, acc 1
2017-08-08T17:29:26.286905: step 12952, loss 3.68099e-05, acc 1
2017-08-08T17:29:26.550301: step 12953, loss 4.52017e-05, acc 1
2017-08-08T17:29:26.798778: step 12954, loss 0.00337251, acc 1
2017-08-08T17:29:26.983420: step 12955, loss 0.000460576, acc 1
2017-08-08T17:29:27.302135: step 12956, loss 0.000434661, acc 1
2017-08-08T17:29:27.587009: step 12957, loss 3.73386e-05, acc 1
2017-08-08T17:29:27.841520: step 12958, loss 9.65117e-06, acc 1
2017-08-08T17:29:28.104033: step 12959, loss 2.68454e-05, acc 1
2017-08-08T17:29:28.316160: step 12960, loss 0.0129779, acc 0.984375
2017-08-08T17:29:28.658471: step 12961, loss 1.64997e-05, acc 1
2017-08-08T17:29:28.937564: step 12962, loss 0.000100228, acc 1
2017-08-08T17:29:29.201574: step 12963, loss 0.000637144, acc 1
2017-08-08T17:29:29.529482: step 12964, loss 0.00851612, acc 1
2017-08-08T17:29:29.814151: step 12965, loss 7.42239e-05, acc 1
2017-08-08T17:29:30.184895: step 12966, loss 1.88528e-05, acc 1
2017-08-08T17:29:30.408172: step 12967, loss 0.000114081, acc 1
2017-08-08T17:29:30.673357: step 12968, loss 0.000388359, acc 1
2017-08-08T17:29:30.944895: step 12969, loss 0.0168004, acc 0.984375
2017-08-08T17:29:31.190681: step 12970, loss 5.00275e-06, acc 1
2017-08-08T17:29:31.434552: step 12971, loss 0.000388985, acc 1
2017-08-08T17:29:31.681310: step 12972, loss 1.39678e-05, acc 1
2017-08-08T17:29:31.960745: step 12973, loss 0.000358858, acc 1
2017-08-08T17:29:32.235163: step 12974, loss 2.17929e-07, acc 1
2017-08-08T17:29:32.441350: step 12975, loss 3.53902e-07, acc 1
2017-08-08T17:29:32.705339: step 12976, loss 0.000274414, acc 1
2017-08-08T17:29:33.141141: step 12977, loss 1.67464e-05, acc 1
2017-08-08T17:29:33.425096: step 12978, loss 0.000363288, acc 1
2017-08-08T17:29:33.741612: step 12979, loss 1.76948e-06, acc 1
2017-08-08T17:29:34.099579: step 12980, loss 0.000566581, acc 1
2017-08-08T17:29:34.543460: step 12981, loss 0.000138114, acc 1
2017-08-08T17:29:35.003844: step 12982, loss 3.18312e-06, acc 1
2017-08-08T17:29:35.290727: step 12983, loss 2.5964e-06, acc 1
2017-08-08T17:29:35.498012: step 12984, loss 2.73923e-05, acc 1
2017-08-08T17:29:35.881404: step 12985, loss 7.31926e-06, acc 1
2017-08-08T17:29:36.178239: step 12986, loss 9.73278e-05, acc 1
2017-08-08T17:29:36.546099: step 12987, loss 5.60647e-07, acc 1
2017-08-08T17:29:36.820982: step 12988, loss 2.40345e-05, acc 1
2017-08-08T17:29:37.213819: step 12989, loss 0.00304397, acc 1
2017-08-08T17:29:37.653422: step 12990, loss 1.10338e-05, acc 1
2017-08-08T17:29:38.045381: step 12991, loss 4.6938e-07, acc 1
2017-08-08T17:29:38.305686: step 12992, loss 0.00206652, acc 1
2017-08-08T17:29:38.533209: step 12993, loss 0.000251144, acc 1
2017-08-08T17:29:38.813395: step 12994, loss 0.0113978, acc 0.984375
2017-08-08T17:29:39.021136: step 12995, loss 4.20956e-07, acc 1
2017-08-08T17:29:39.234787: step 12996, loss 2.34692e-07, acc 1
2017-08-08T17:29:39.450209: step 12997, loss 0.000594519, acc 1
2017-08-08T17:29:39.743571: step 12998, loss 0.0400073, acc 0.984375
2017-08-08T17:29:40.033206: step 12999, loss 8.45668e-06, acc 1
2017-08-08T17:29:40.264662: step 13000, loss 5.58793e-09, acc 1

Evaluation:
2017-08-08T17:29:40.705557: step 13000, loss 4.56987, acc 0.713884

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-13000

2017-08-08T17:29:41.124712: step 13001, loss 9.23589e-06, acc 1
2017-08-08T17:29:41.360914: step 13002, loss 1.01871e-05, acc 1
2017-08-08T17:29:41.572324: step 13003, loss 9.73782e-05, acc 1
2017-08-08T17:29:41.858667: step 13004, loss 3.22766e-06, acc 1
2017-08-08T17:29:42.140922: step 13005, loss 0.000202122, acc 1
2017-08-08T17:29:42.406887: step 13006, loss 2.00036e-06, acc 1
2017-08-08T17:29:42.698329: step 13007, loss 2.49461e-05, acc 1
2017-08-08T17:29:43.001401: step 13008, loss 3.01747e-07, acc 1
2017-08-08T17:29:43.289491: step 13009, loss 1.9755e-05, acc 1
2017-08-08T17:29:43.527815: step 13010, loss 1.38193e-05, acc 1
2017-08-08T17:29:43.801716: step 13011, loss 4.4517e-07, acc 1
2017-08-08T17:29:44.068736: step 13012, loss 6.08189e-05, acc 1
2017-08-08T17:29:44.457486: step 13013, loss 0.00055621, acc 1
2017-08-08T17:29:44.813750: step 13014, loss 3.69001e-05, acc 1
2017-08-08T17:29:45.138611: step 13015, loss 8.14343e-05, acc 1
2017-08-08T17:29:45.367310: step 13016, loss 0.001344, acc 1
2017-08-08T17:29:45.710236: step 13017, loss 0.00118137, acc 1
2017-08-08T17:29:45.891862: step 13018, loss 0.000566584, acc 1
2017-08-08T17:29:46.084825: step 13019, loss 0.000290038, acc 1
2017-08-08T17:29:46.258211: step 13020, loss 0.00727182, acc 1
2017-08-08T17:29:46.604749: step 13021, loss 1.42116e-06, acc 1
2017-08-08T17:29:46.929588: step 13022, loss 4.84287e-08, acc 1
2017-08-08T17:29:47.193031: step 13023, loss 6.16683e-06, acc 1
2017-08-08T17:29:47.419954: step 13024, loss 0.000260925, acc 1
2017-08-08T17:29:47.609940: step 13025, loss 0.000131242, acc 1
2017-08-08T17:29:47.933805: step 13026, loss 0.000154117, acc 1
2017-08-08T17:29:48.161227: step 13027, loss 0.000335711, acc 1
2017-08-08T17:29:48.366939: step 13028, loss 0.000235975, acc 1
2017-08-08T17:29:48.582360: step 13029, loss 3.26805e-05, acc 1
2017-08-08T17:29:48.959569: step 13030, loss 0.0049869, acc 1
2017-08-08T17:29:49.281542: step 13031, loss 4.75834e-06, acc 1
2017-08-08T17:29:49.570264: step 13032, loss 1.44375e-05, acc 1
2017-08-08T17:29:49.774069: step 13033, loss 2.26111e-06, acc 1
2017-08-08T17:29:50.065364: step 13034, loss 3.47173e-06, acc 1
2017-08-08T17:29:50.303967: step 13035, loss 8.19558e-07, acc 1
2017-08-08T17:29:50.532181: step 13036, loss 6.70668e-06, acc 1
2017-08-08T17:29:50.743246: step 13037, loss 5.8032e-06, acc 1
2017-08-08T17:29:51.029381: step 13038, loss 4.86549e-05, acc 1
2017-08-08T17:29:51.395044: step 13039, loss 3.9955e-05, acc 1
2017-08-08T17:29:51.677362: step 13040, loss 0.00033009, acc 1
2017-08-08T17:29:51.844711: step 13041, loss 7.43404e-05, acc 1
2017-08-08T17:29:52.036331: step 13042, loss 0.0018875, acc 1
2017-08-08T17:29:52.328155: step 13043, loss 4.47035e-08, acc 1
2017-08-08T17:29:52.538252: step 13044, loss 3.56126e-06, acc 1
2017-08-08T17:29:52.746819: step 13045, loss 5.0086e-05, acc 1
2017-08-08T17:29:52.937353: step 13046, loss 1.34989e-05, acc 1
2017-08-08T17:29:53.249382: step 13047, loss 6.05334e-05, acc 1
2017-08-08T17:29:53.577359: step 13048, loss 4.58605e-05, acc 1
2017-08-08T17:29:53.847849: step 13049, loss 4.24112e-06, acc 1
2017-08-08T17:29:54.100468: step 13050, loss 0.0029324, acc 1
2017-08-08T17:29:54.462857: step 13051, loss 0.000391466, acc 1
2017-08-08T17:29:54.739351: step 13052, loss 1.9166e-06, acc 1
2017-08-08T17:29:54.982908: step 13053, loss 2.80032e-05, acc 1
2017-08-08T17:29:55.206224: step 13054, loss 1.9849e-05, acc 1
2017-08-08T17:29:55.425356: step 13055, loss 1.68302e-05, acc 1
2017-08-08T17:29:55.721369: step 13056, loss 0.000149378, acc 1
2017-08-08T17:29:56.131737: step 13057, loss 0.00241237, acc 1
2017-08-08T17:29:56.525477: step 13058, loss 0.000574655, acc 1
2017-08-08T17:29:56.822371: step 13059, loss 1.0908e-05, acc 1
2017-08-08T17:29:57.071752: step 13060, loss 2.04891e-08, acc 1
2017-08-08T17:29:57.477313: step 13061, loss 9.49957e-05, acc 1
2017-08-08T17:29:57.682067: step 13062, loss 2.24998e-06, acc 1
2017-08-08T17:29:57.856195: step 13063, loss 1.695e-07, acc 1
2017-08-08T17:29:58.030109: step 13064, loss 2.42144e-08, acc 1
2017-08-08T17:29:58.269355: step 13065, loss 6.70538e-07, acc 1
2017-08-08T17:29:58.524802: step 13066, loss 3.57403e-06, acc 1
2017-08-08T17:29:58.741888: step 13067, loss 4.02486e-06, acc 1
2017-08-08T17:29:58.930237: step 13068, loss 0.00146082, acc 1
2017-08-08T17:29:59.102505: step 13069, loss 5.299e-06, acc 1
2017-08-08T17:29:59.324530: step 13070, loss 9.68574e-08, acc 1
2017-08-08T17:29:59.559892: step 13071, loss 0.000181839, acc 1
2017-08-08T17:29:59.722059: step 13072, loss 1.41594e-05, acc 1
2017-08-08T17:29:59.988156: step 13073, loss 0.000111977, acc 1
2017-08-08T17:30:00.237346: step 13074, loss 9.84846e-06, acc 1
2017-08-08T17:30:00.564036: step 13075, loss 1.44725e-06, acc 1
2017-08-08T17:30:00.780445: step 13076, loss 0.00748479, acc 1
2017-08-08T17:30:01.059964: step 13077, loss 3.24076e-06, acc 1
2017-08-08T17:30:01.277885: step 13078, loss 5.94456e-05, acc 1
2017-08-08T17:30:01.560889: step 13079, loss 4.58648e-05, acc 1
2017-08-08T17:30:01.842892: step 13080, loss 8.35007e-06, acc 1
2017-08-08T17:30:02.078287: step 13081, loss 2.23324e-06, acc 1
2017-08-08T17:30:02.559895: step 13082, loss 0.00200669, acc 1
2017-08-08T17:30:02.872126: step 13083, loss 0.00033211, acc 1
2017-08-08T17:30:03.186103: step 13084, loss 0.000124766, acc 1
2017-08-08T17:30:03.455948: step 13085, loss 0.00015089, acc 1
2017-08-08T17:30:03.698439: step 13086, loss 7.60775e-05, acc 1
2017-08-08T17:30:04.171272: step 13087, loss 0.00293074, acc 1
2017-08-08T17:30:04.459940: step 13088, loss 1.08033e-07, acc 1
2017-08-08T17:30:04.710732: step 13089, loss 4.51165e-05, acc 1
2017-08-08T17:30:05.012924: step 13090, loss 0.000165132, acc 1
2017-08-08T17:30:05.362210: step 13091, loss 6.26008e-05, acc 1
2017-08-08T17:30:05.747337: step 13092, loss 1.06171e-07, acc 1
2017-08-08T17:30:06.077845: step 13093, loss 0.000542686, acc 1
2017-08-08T17:30:06.329561: step 13094, loss 8.58662e-07, acc 1
2017-08-08T17:30:06.576578: step 13095, loss 8.3817e-07, acc 1
2017-08-08T17:30:06.965355: step 13096, loss 0.000237551, acc 1
2017-08-08T17:30:07.168446: step 13097, loss 0.000106447, acc 1
2017-08-08T17:30:07.454773: step 13098, loss 5.66352e-05, acc 1
2017-08-08T17:30:07.755044: step 13099, loss 0.0173635, acc 0.984375
2017-08-08T17:30:08.203674: step 13100, loss 9.8842e-05, acc 1

Evaluation:
2017-08-08T17:30:08.791824: step 13100, loss 4.65895, acc 0.702627

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-13100

2017-08-08T17:30:09.227185: step 13101, loss 5.63518e-05, acc 1
2017-08-08T17:30:09.629013: step 13102, loss 0.00750827, acc 1
2017-08-08T17:30:09.880245: step 13103, loss 3.48668e-06, acc 1
2017-08-08T17:30:10.154887: step 13104, loss 9.71585e-05, acc 1
2017-08-08T17:30:10.550972: step 13105, loss 8.82884e-07, acc 1
2017-08-08T17:30:10.861613: step 13106, loss 9.72546e-06, acc 1
2017-08-08T17:30:11.174164: step 13107, loss 5.81382e-05, acc 1
2017-08-08T17:30:11.528227: step 13108, loss 0.000133413, acc 1
2017-08-08T17:30:11.755086: step 13109, loss 1.22931e-06, acc 1
2017-08-08T17:30:12.032587: step 13110, loss 3.76183e-05, acc 1
2017-08-08T17:30:12.452922: step 13111, loss 0.000114791, acc 1
2017-08-08T17:30:12.752409: step 13112, loss 6.37696e-06, acc 1
2017-08-08T17:30:13.038090: step 13113, loss 5.56089e-06, acc 1
2017-08-08T17:30:13.317677: step 13114, loss 2.1234e-07, acc 1
2017-08-08T17:30:13.764411: step 13115, loss 9.05429e-05, acc 1
2017-08-08T17:30:14.181391: step 13116, loss 0.000127671, acc 1
2017-08-08T17:30:14.533705: step 13117, loss 5.98554e-05, acc 1
2017-08-08T17:30:14.783218: step 13118, loss 9.53228e-05, acc 1
2017-08-08T17:30:15.059624: step 13119, loss 0.000250164, acc 1
2017-08-08T17:30:15.382360: step 13120, loss 8.8129e-05, acc 1
2017-08-08T17:30:15.759365: step 13121, loss 1.44642e-05, acc 1
2017-08-08T17:30:16.048259: step 13122, loss 1.63465e-05, acc 1
2017-08-08T17:30:16.434232: step 13123, loss 0.000336571, acc 1
2017-08-08T17:30:16.861225: step 13124, loss 0.000131489, acc 1
2017-08-08T17:30:17.244291: step 13125, loss 0.000196443, acc 1
2017-08-08T17:30:17.521326: step 13126, loss 2.23517e-08, acc 1
2017-08-08T17:30:17.766006: step 13127, loss 2.65651e-05, acc 1
2017-08-08T17:30:18.212728: step 13128, loss 0.00027952, acc 1
2017-08-08T17:30:18.492401: step 13129, loss 0.0675736, acc 0.984375
2017-08-08T17:30:18.762371: step 13130, loss 6.33583e-06, acc 1
2017-08-08T17:30:19.111147: step 13131, loss 5.05064e-05, acc 1
2017-08-08T17:30:19.517362: step 13132, loss 4.19264e-06, acc 1
2017-08-08T17:30:19.967492: step 13133, loss 6.11556e-05, acc 1
2017-08-08T17:30:20.242455: step 13134, loss 3.36848e-05, acc 1
2017-08-08T17:30:20.458750: step 13135, loss 4.65661e-08, acc 1
2017-08-08T17:30:20.813753: step 13136, loss 0.000526966, acc 1
2017-08-08T17:30:21.053205: step 13137, loss 5.73595e-06, acc 1
2017-08-08T17:30:21.286015: step 13138, loss 9.18808e-06, acc 1
2017-08-08T17:30:21.616967: step 13139, loss 3.17926e-06, acc 1
2017-08-08T17:30:21.994654: step 13140, loss 8.4507e-06, acc 1
2017-08-08T17:30:22.250090: step 13141, loss 8.04655e-07, acc 1
2017-08-08T17:30:22.440515: step 13142, loss 0.00011227, acc 1
2017-08-08T17:30:22.724775: step 13143, loss 2.23517e-07, acc 1
2017-08-08T17:30:23.014427: step 13144, loss 1.99811e-05, acc 1
2017-08-08T17:30:23.353864: step 13145, loss 0.0743989, acc 0.984375
2017-08-08T17:30:23.639056: step 13146, loss 0.000277915, acc 1
2017-08-08T17:30:24.101708: step 13147, loss 0.000347492, acc 1
2017-08-08T17:30:24.393459: step 13148, loss 0.000334372, acc 1
2017-08-08T17:30:24.668614: step 13149, loss 5.81719e-05, acc 1
2017-08-08T17:30:24.869342: step 13150, loss 2.30278e-05, acc 1
2017-08-08T17:30:25.063666: step 13151, loss 6.48191e-07, acc 1
2017-08-08T17:30:25.453500: step 13152, loss 1.98493e-05, acc 1
2017-08-08T17:30:25.708650: step 13153, loss 0.000104157, acc 1
2017-08-08T17:30:25.957195: step 13154, loss 5.02017e-05, acc 1
2017-08-08T17:30:26.240339: step 13155, loss 0.00140337, acc 1
2017-08-08T17:30:26.676039: step 13156, loss 1.08499e-05, acc 1
2017-08-08T17:30:27.029583: step 13157, loss 4.6955e-06, acc 1
2017-08-08T17:30:27.321635: step 13158, loss 5.12885e-05, acc 1
2017-08-08T17:30:27.501006: step 13159, loss 6.07371e-05, acc 1
2017-08-08T17:30:27.785365: step 13160, loss 0.000473804, acc 1
2017-08-08T17:30:28.026277: step 13161, loss 3.94835e-06, acc 1
2017-08-08T17:30:28.225795: step 13162, loss 0.000108693, acc 1
2017-08-08T17:30:28.478798: step 13163, loss 0.000829271, acc 1
2017-08-08T17:30:28.807210: step 13164, loss 0.0122972, acc 1
2017-08-08T17:30:29.239616: step 13165, loss 1.32618e-06, acc 1
2017-08-08T17:30:29.610593: step 13166, loss 4.34718e-06, acc 1
2017-08-08T17:30:29.941564: step 13167, loss 1.00443e-05, acc 1
2017-08-08T17:30:30.211957: step 13168, loss 1.39881e-06, acc 1
2017-08-08T17:30:30.561200: step 13169, loss 0.0010117, acc 1
2017-08-08T17:30:30.888468: step 13170, loss 1.32059e-06, acc 1
2017-08-08T17:30:31.133632: step 13171, loss 0.000925654, acc 1
2017-08-08T17:30:31.350432: step 13172, loss 1.84969e-05, acc 1
2017-08-08T17:30:31.697379: step 13173, loss 2.25554e-06, acc 1
2017-08-08T17:30:31.993502: step 13174, loss 0.00642295, acc 1
2017-08-08T17:30:32.241010: step 13175, loss 4.02474e-06, acc 1
2017-08-08T17:30:32.470038: step 13176, loss 0.00478936, acc 1
2017-08-08T17:30:32.636413: step 13177, loss 6.07487e-05, acc 1
2017-08-08T17:30:32.913671: step 13178, loss 1.27214e-06, acc 1
2017-08-08T17:30:33.189707: step 13179, loss 3.32269e-06, acc 1
2017-08-08T17:30:33.429694: step 13180, loss 0.000762957, acc 1
2017-08-08T17:30:33.684013: step 13181, loss 4.22058e-06, acc 1
2017-08-08T17:30:34.085329: step 13182, loss 1.64616e-05, acc 1
2017-08-08T17:30:34.521375: step 13183, loss 0.000148203, acc 1
2017-08-08T17:30:34.880263: step 13184, loss 0.000609285, acc 1
2017-08-08T17:30:35.104140: step 13185, loss 3.38805e-06, acc 1
2017-08-08T17:30:35.331684: step 13186, loss 4.7883e-06, acc 1
2017-08-08T17:30:35.684072: step 13187, loss 0.000703386, acc 1
2017-08-08T17:30:35.967505: step 13188, loss 1.67638e-08, acc 1
2017-08-08T17:30:36.224880: step 13189, loss 1.36691e-05, acc 1
2017-08-08T17:30:36.593004: step 13190, loss 1.13307e-05, acc 1
2017-08-08T17:30:37.018658: step 13191, loss 1.20199e-05, acc 1
2017-08-08T17:30:37.389771: step 13192, loss 5.36385e-06, acc 1
2017-08-08T17:30:37.707182: step 13193, loss 0.000229415, acc 1
2017-08-08T17:30:37.960747: step 13194, loss 3.16649e-08, acc 1
2017-08-08T17:30:38.218914: step 13195, loss 1.66354e-05, acc 1
2017-08-08T17:30:38.601030: step 13196, loss 0.00010279, acc 1
2017-08-08T17:30:38.823249: step 13197, loss 0.000349156, acc 1
2017-08-08T17:30:39.100190: step 13198, loss 8.32253e-05, acc 1
2017-08-08T17:30:39.421555: step 13199, loss 0.000170034, acc 1
2017-08-08T17:30:39.769393: step 13200, loss 0.000181477, acc 1

Evaluation:
2017-08-08T17:30:40.446414: step 13200, loss 4.65882, acc 0.707317

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-13200

2017-08-08T17:30:41.014863: step 13201, loss 0.000912568, acc 1
2017-08-08T17:30:41.317912: step 13202, loss 3.26879e-06, acc 1
2017-08-08T17:30:41.542755: step 13203, loss 0.000390761, acc 1
2017-08-08T17:30:41.793118: step 13204, loss 7.68742e-06, acc 1
2017-08-08T17:30:42.137310: step 13205, loss 1.24774e-05, acc 1
2017-08-08T17:30:42.529949: step 13206, loss 9.85267e-05, acc 1
2017-08-08T17:30:42.890250: step 13207, loss 0.0045884, acc 1
2017-08-08T17:30:43.230840: step 13208, loss 3.74025e-05, acc 1
2017-08-08T17:30:43.471677: step 13209, loss 3.02201e-05, acc 1
2017-08-08T17:30:43.819951: step 13210, loss 4.47034e-08, acc 1
2017-08-08T17:30:44.160582: step 13211, loss 5.54758e-05, acc 1
2017-08-08T17:30:44.381184: step 13212, loss 0.000143379, acc 1
2017-08-08T17:30:44.644498: step 13213, loss 1.54339e-05, acc 1
2017-08-08T17:30:44.930131: step 13214, loss 2.51666e-05, acc 1
2017-08-08T17:30:45.253619: step 13215, loss 2.86596e-05, acc 1
2017-08-08T17:30:45.471653: step 13216, loss 0.000225668, acc 1
2017-08-08T17:30:45.664782: step 13217, loss 0.000261348, acc 1
2017-08-08T17:30:45.913470: step 13218, loss 2.98021e-07, acc 1
2017-08-08T17:30:46.197205: step 13219, loss 0.000187936, acc 1
2017-08-08T17:30:46.491228: step 13220, loss 2.78073e-06, acc 1
2017-08-08T17:30:46.770195: step 13221, loss 9.04581e-06, acc 1
2017-08-08T17:30:47.176829: step 13222, loss 0.000416968, acc 1
2017-08-08T17:30:47.577990: step 13223, loss 1.55341e-06, acc 1
2017-08-08T17:30:47.907362: step 13224, loss 0.00139345, acc 1
2017-08-08T17:30:48.190312: step 13225, loss 0.000221326, acc 1
2017-08-08T17:30:48.376414: step 13226, loss 0.00244664, acc 1
2017-08-08T17:30:48.680050: step 13227, loss 0.000230974, acc 1
2017-08-08T17:30:48.976308: step 13228, loss 1.93061e-05, acc 1
2017-08-08T17:30:49.217878: step 13229, loss 0.00105214, acc 1
2017-08-08T17:30:49.442757: step 13230, loss 2.0526e-05, acc 1
2017-08-08T17:30:49.884661: step 13231, loss 1.01512e-06, acc 1
2017-08-08T17:30:50.245062: step 13232, loss 3.51192e-05, acc 1
2017-08-08T17:30:50.563481: step 13233, loss 1.20883e-06, acc 1
2017-08-08T17:30:50.801604: step 13234, loss 9.88454e-06, acc 1
2017-08-08T17:30:51.195274: step 13235, loss 0.000107729, acc 1
2017-08-08T17:30:51.562186: step 13236, loss 9.49537e-05, acc 1
2017-08-08T17:30:51.881669: step 13237, loss 8.26354e-05, acc 1
2017-08-08T17:30:52.242368: step 13238, loss 2.87658e-05, acc 1
2017-08-08T17:30:52.580725: step 13239, loss 0.000484019, acc 1
2017-08-08T17:30:52.862174: step 13240, loss 1.13621e-07, acc 1
2017-08-08T17:30:53.082005: step 13241, loss 6.27058e-05, acc 1
2017-08-08T17:30:53.270113: step 13242, loss 8.28117e-05, acc 1
2017-08-08T17:30:53.483933: step 13243, loss 6.24675e-06, acc 1
2017-08-08T17:30:53.735662: step 13244, loss 0.00342398, acc 1
2017-08-08T17:30:53.971112: step 13245, loss 0.000301854, acc 1
2017-08-08T17:30:54.230757: step 13246, loss 2.12147e-06, acc 1
2017-08-08T17:30:54.630626: step 13247, loss 1.71594e-05, acc 1
2017-08-08T17:30:54.949976: step 13248, loss 4.84973e-05, acc 1
2017-08-08T17:30:55.228472: step 13249, loss 2.18354e-05, acc 1
2017-08-08T17:30:55.458853: step 13250, loss 7.63007e-06, acc 1
2017-08-08T17:30:55.633158: step 13251, loss 0.000612573, acc 1
2017-08-08T17:30:55.973327: step 13252, loss 1.18651e-05, acc 1
2017-08-08T17:30:56.217893: step 13253, loss 5.69903e-06, acc 1
2017-08-08T17:30:56.495091: step 13254, loss 8.7013e-05, acc 1
2017-08-08T17:30:56.733519: step 13255, loss 3.19345e-05, acc 1
2017-08-08T17:30:56.967878: step 13256, loss 1.28863e-05, acc 1
2017-08-08T17:30:57.370376: step 13257, loss 0.000998527, acc 1
2017-08-08T17:30:57.797677: step 13258, loss 5.52902e-05, acc 1
2017-08-08T17:30:58.161975: step 13259, loss 3.68831e-05, acc 1
2017-08-08T17:30:58.440237: step 13260, loss 4.70817e-06, acc 1
2017-08-08T17:30:58.657370: step 13261, loss 5.87555e-06, acc 1
2017-08-08T17:30:59.004620: step 13262, loss 2.18107e-06, acc 1
2017-08-08T17:30:59.190395: step 13263, loss 7.93865e-05, acc 1
2017-08-08T17:30:59.433510: step 13264, loss 2.7639e-05, acc 1
2017-08-08T17:30:59.766873: step 13265, loss 0.000520938, acc 1
2017-08-08T17:31:00.147547: step 13266, loss 6.90182e-06, acc 1
2017-08-08T17:31:00.496461: step 13267, loss 3.4581e-05, acc 1
2017-08-08T17:31:00.761570: step 13268, loss 0.000124258, acc 1
2017-08-08T17:31:01.114613: step 13269, loss 0.000278522, acc 1
2017-08-08T17:31:01.527491: step 13270, loss 4.47924e-05, acc 1
2017-08-08T17:31:01.823206: step 13271, loss 3.61265e-05, acc 1
2017-08-08T17:31:02.116884: step 13272, loss 0.000270793, acc 1
2017-08-08T17:31:02.550115: step 13273, loss 0.000137076, acc 1
2017-08-08T17:31:03.004060: step 13274, loss 0.000287999, acc 1
2017-08-08T17:31:03.368631: step 13275, loss 6.65934e-05, acc 1
2017-08-08T17:31:03.637498: step 13276, loss 2.05251e-06, acc 1
2017-08-08T17:31:03.870557: step 13277, loss 0.00387338, acc 1
2017-08-08T17:31:04.193661: step 13278, loss 8.4563e-07, acc 1
2017-08-08T17:31:04.501956: step 13279, loss 0.000308555, acc 1
2017-08-08T17:31:04.715575: step 13280, loss 1.95577e-07, acc 1
2017-08-08T17:31:04.927638: step 13281, loss 4.40314e-05, acc 1
2017-08-08T17:31:05.165897: step 13282, loss 0.000240813, acc 1
2017-08-08T17:31:05.629346: step 13283, loss 0.000885735, acc 1
2017-08-08T17:31:06.031484: step 13284, loss 4.76662e-05, acc 1
2017-08-08T17:31:06.359561: step 13285, loss 8.62831e-06, acc 1
2017-08-08T17:31:06.588309: step 13286, loss 6.97423e-06, acc 1
2017-08-08T17:31:06.862308: step 13287, loss 8.56816e-08, acc 1
2017-08-08T17:31:07.222904: step 13288, loss 1.96726e-05, acc 1
2017-08-08T17:31:07.476627: step 13289, loss 1.23489e-06, acc 1
2017-08-08T17:31:07.717197: step 13290, loss 1.6819e-06, acc 1
2017-08-08T17:31:08.103474: step 13291, loss 2.91884e-05, acc 1
2017-08-08T17:31:08.507438: step 13292, loss 6.53002e-05, acc 1
2017-08-08T17:31:08.801111: step 13293, loss 5.40167e-08, acc 1
2017-08-08T17:31:09.018126: step 13294, loss 1.21342e-05, acc 1
2017-08-08T17:31:09.228726: step 13295, loss 1.0859e-06, acc 1
2017-08-08T17:31:09.559324: step 13296, loss 7.71409e-06, acc 1
2017-08-08T17:31:09.841988: step 13297, loss 2.73807e-07, acc 1
2017-08-08T17:31:10.083493: step 13298, loss 1.26776e-05, acc 1
2017-08-08T17:31:10.345682: step 13299, loss 8.1273e-06, acc 1
2017-08-08T17:31:10.657392: step 13300, loss 1.02257e-06, acc 1

Evaluation:
2017-08-08T17:31:11.308616: step 13300, loss 4.64742, acc 0.711069

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-13300

2017-08-08T17:31:11.724820: step 13301, loss 3.37093e-05, acc 1
2017-08-08T17:31:11.967628: step 13302, loss 0.00493545, acc 1
2017-08-08T17:31:12.257124: step 13303, loss 5.70111e-06, acc 1
2017-08-08T17:31:12.666508: step 13304, loss 4.41801e-05, acc 1
2017-08-08T17:31:12.870230: step 13305, loss 1.2405e-06, acc 1
2017-08-08T17:31:13.097298: step 13306, loss 8.92098e-05, acc 1
2017-08-08T17:31:13.335103: step 13307, loss 6.70299e-06, acc 1
2017-08-08T17:31:13.657401: step 13308, loss 1.37461e-06, acc 1
2017-08-08T17:31:14.009148: step 13309, loss 1.0412e-06, acc 1
2017-08-08T17:31:14.274910: step 13310, loss 0.0012383, acc 1
2017-08-08T17:31:14.497702: step 13311, loss 1.20208e-05, acc 1
2017-08-08T17:31:14.800967: step 13312, loss 9.72767e-06, acc 1
2017-08-08T17:31:14.983036: step 13313, loss 2.79397e-08, acc 1
2017-08-08T17:31:15.198319: step 13314, loss 1.6115e-05, acc 1
2017-08-08T17:31:15.501381: step 13315, loss 5.78805e-06, acc 1
2017-08-08T17:31:15.971365: step 13316, loss 4.07915e-07, acc 1
2017-08-08T17:31:16.356398: step 13317, loss 3.37942e-05, acc 1
2017-08-08T17:31:16.629568: step 13318, loss 0.000143551, acc 1
2017-08-08T17:31:16.856832: step 13319, loss 7.09663e-07, acc 1
2017-08-08T17:31:17.199731: step 13320, loss 1.08221e-05, acc 1
2017-08-08T17:31:17.576850: step 13321, loss 5.55059e-05, acc 1
2017-08-08T17:31:17.769036: step 13322, loss 0.000346755, acc 1
2017-08-08T17:31:17.951192: step 13323, loss 0.000592404, acc 1
2017-08-08T17:31:18.296558: step 13324, loss 0.000191072, acc 1
2017-08-08T17:31:18.630422: step 13325, loss 6.41802e-06, acc 1
2017-08-08T17:31:19.048798: step 13326, loss 0.000154855, acc 1
2017-08-08T17:31:19.286659: step 13327, loss 6.75444e-06, acc 1
2017-08-08T17:31:19.523131: step 13328, loss 1.06956e-05, acc 1
2017-08-08T17:31:19.874869: step 13329, loss 8.40201e-06, acc 1
2017-08-08T17:31:20.189645: step 13330, loss 0.000171047, acc 1
2017-08-08T17:31:20.498486: step 13331, loss 0.00558317, acc 1
2017-08-08T17:31:20.737345: step 13332, loss 1.91998e-05, acc 1
2017-08-08T17:31:21.025363: step 13333, loss 2.69605e-05, acc 1
2017-08-08T17:31:21.419344: step 13334, loss 1.1362e-06, acc 1
2017-08-08T17:31:21.823452: step 13335, loss 0.00214541, acc 1
2017-08-08T17:31:22.119712: step 13336, loss 0.000258762, acc 1
2017-08-08T17:31:22.317373: step 13337, loss 0.000709552, acc 1
2017-08-08T17:31:22.565264: step 13338, loss 3.9766e-06, acc 1
2017-08-08T17:31:22.776450: step 13339, loss 0.000244485, acc 1
2017-08-08T17:31:23.012762: step 13340, loss 0.000130129, acc 1
2017-08-08T17:31:23.300549: step 13341, loss 0.000467318, acc 1
2017-08-08T17:31:23.654934: step 13342, loss 1.3411e-07, acc 1
2017-08-08T17:31:23.933363: step 13343, loss 3.89835e-05, acc 1
2017-08-08T17:31:24.161437: step 13344, loss 4.88207e-05, acc 1
2017-08-08T17:31:24.419762: step 13345, loss 7.32244e-06, acc 1
2017-08-08T17:31:24.864492: step 13346, loss 0.00927398, acc 1
2017-08-08T17:31:25.138996: step 13347, loss 2.5498e-06, acc 1
2017-08-08T17:31:25.382517: step 13348, loss 7.68466e-06, acc 1
2017-08-08T17:31:25.610852: step 13349, loss 0.000764361, acc 1
2017-08-08T17:31:26.022801: step 13350, loss 2.01603e-05, acc 1
2017-08-08T17:31:26.487880: step 13351, loss 2.43554e-05, acc 1
2017-08-08T17:31:26.804672: step 13352, loss 0.00267426, acc 1
2017-08-08T17:31:27.039085: step 13353, loss 0.000558667, acc 1
2017-08-08T17:31:27.317380: step 13354, loss 0.000126996, acc 1
2017-08-08T17:31:27.713150: step 13355, loss 0.0496873, acc 0.984375
2017-08-08T17:31:27.959042: step 13356, loss 2.18271e-05, acc 1
2017-08-08T17:31:28.266387: step 13357, loss 0.00039659, acc 1
2017-08-08T17:31:28.557413: step 13358, loss 1.12872e-06, acc 1
2017-08-08T17:31:28.920260: step 13359, loss 3.30227e-05, acc 1
2017-08-08T17:31:29.413001: step 13360, loss 3.06767e-05, acc 1
2017-08-08T17:31:29.705310: step 13361, loss 4.7497e-07, acc 1
2017-08-08T17:31:29.975111: step 13362, loss 2.71945e-07, acc 1
2017-08-08T17:31:30.201859: step 13363, loss 0.000218583, acc 1
2017-08-08T17:31:30.664069: step 13364, loss 1.06169e-06, acc 1
2017-08-08T17:31:30.934014: step 13365, loss 0.000291524, acc 1
2017-08-08T17:31:31.238858: step 13366, loss 0.0003839, acc 1
2017-08-08T17:31:31.441328: step 13367, loss 0.000958721, acc 1
2017-08-08T17:31:31.728317: step 13368, loss 1.26063e-05, acc 1
2017-08-08T17:31:32.036885: step 13369, loss 1.35973e-07, acc 1
2017-08-08T17:31:32.268870: step 13370, loss 2.86846e-07, acc 1
2017-08-08T17:31:32.513443: step 13371, loss 0.000101676, acc 1
2017-08-08T17:31:32.771035: step 13372, loss 6.65096e-06, acc 1
2017-08-08T17:31:33.113512: step 13373, loss 0.000184895, acc 1
2017-08-08T17:31:33.299965: step 13374, loss 1.36549e-05, acc 1
2017-08-08T17:31:33.543849: step 13375, loss 0.00303251, acc 1
2017-08-08T17:31:33.845362: step 13376, loss 1.70057e-06, acc 1
2017-08-08T17:31:34.196023: step 13377, loss 1.34328e-05, acc 1
2017-08-08T17:31:34.549278: step 13378, loss 4.86978e-05, acc 1
2017-08-08T17:31:34.769042: step 13379, loss 3.9374e-06, acc 1
2017-08-08T17:31:34.999522: step 13380, loss 1.71541e-06, acc 1
2017-08-08T17:31:35.305400: step 13381, loss 3.61352e-07, acc 1
2017-08-08T17:31:35.617374: step 13382, loss 8.75187e-05, acc 1
2017-08-08T17:31:35.844972: step 13383, loss 0.00151663, acc 1
2017-08-08T17:31:36.035087: step 13384, loss 8.3823e-05, acc 1
2017-08-08T17:31:36.284438: step 13385, loss 6.64562e-06, acc 1
2017-08-08T17:31:36.677400: step 13386, loss 2.04327e-06, acc 1
2017-08-08T17:31:36.910014: step 13387, loss 1.28042e-05, acc 1
2017-08-08T17:31:37.152017: step 13388, loss 0.000236053, acc 1
2017-08-08T17:31:37.332447: step 13389, loss 8.82621e-05, acc 1
2017-08-08T17:31:37.696599: step 13390, loss 0.000245648, acc 1
2017-08-08T17:31:37.956099: step 13391, loss 0.000572696, acc 1
2017-08-08T17:31:38.191244: step 13392, loss 4.64105e-06, acc 1
2017-08-08T17:31:38.414212: step 13393, loss 8.11487e-05, acc 1
2017-08-08T17:31:38.625203: step 13394, loss 7.50182e-06, acc 1
2017-08-08T17:31:38.929048: step 13395, loss 0.000635898, acc 1
2017-08-08T17:31:39.261859: step 13396, loss 0.000340604, acc 1
2017-08-08T17:31:39.453348: step 13397, loss 0.00408289, acc 1
2017-08-08T17:31:39.689301: step 13398, loss 5.23958e-05, acc 1
2017-08-08T17:31:40.104101: step 13399, loss 0.000220846, acc 1
2017-08-08T17:31:40.364003: step 13400, loss 0.000112004, acc 1

Evaluation:
2017-08-08T17:31:40.879706: step 13400, loss 4.69979, acc 0.705441

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-13400

2017-08-08T17:31:41.489376: step 13401, loss 7.74131e-05, acc 1
2017-08-08T17:31:41.787064: step 13402, loss 0.000432782, acc 1
2017-08-08T17:31:41.987549: step 13403, loss 0.00259281, acc 1
2017-08-08T17:31:42.207199: step 13404, loss 2.03955e-05, acc 1
2017-08-08T17:31:42.550213: step 13405, loss 4.54775e-05, acc 1
2017-08-08T17:31:42.733309: step 13406, loss 1.21405e-05, acc 1
2017-08-08T17:31:42.969899: step 13407, loss 9.90913e-07, acc 1
2017-08-08T17:31:43.177627: step 13408, loss 0.000418594, acc 1
2017-08-08T17:31:43.523920: step 13409, loss 0.00397941, acc 1
2017-08-08T17:31:43.889017: step 13410, loss 1.1064e-06, acc 1
2017-08-08T17:31:44.183656: step 13411, loss 1.11759e-08, acc 1
2017-08-08T17:31:44.396753: step 13412, loss 0.000210918, acc 1
2017-08-08T17:31:44.691110: step 13413, loss 0.000137523, acc 1
2017-08-08T17:31:45.023961: step 13414, loss 2.0489e-07, acc 1
2017-08-08T17:31:45.263753: step 13415, loss 6.18389e-07, acc 1
2017-08-08T17:31:45.504331: step 13416, loss 4.21898e-05, acc 1
2017-08-08T17:31:45.750772: step 13417, loss 9.11869e-05, acc 1
2017-08-08T17:31:46.083862: step 13418, loss 4.48477e-06, acc 1
2017-08-08T17:31:46.333482: step 13419, loss 0.0352319, acc 0.984375
2017-08-08T17:31:46.561475: step 13420, loss 0.0010722, acc 1
2017-08-08T17:31:46.820770: step 13421, loss 0.0144247, acc 0.984375
2017-08-08T17:31:47.270327: step 13422, loss 7.19982e-05, acc 1
2017-08-08T17:31:47.575799: step 13423, loss 2.31469e-05, acc 1
2017-08-08T17:31:47.839887: step 13424, loss 2.48833e-06, acc 1
2017-08-08T17:31:48.094082: step 13425, loss 2.93918e-05, acc 1
2017-08-08T17:31:48.441397: step 13426, loss 8.84074e-06, acc 1
2017-08-08T17:31:48.825319: step 13427, loss 0.000222685, acc 1
2017-08-08T17:31:49.257091: step 13428, loss 3.63e-05, acc 1
2017-08-08T17:31:49.530022: step 13429, loss 2.65788e-06, acc 1
2017-08-08T17:31:49.843616: step 13430, loss 4.10252e-05, acc 1
2017-08-08T17:31:50.200262: step 13431, loss 4.53528e-06, acc 1
2017-08-08T17:31:50.492752: step 13432, loss 0.000103526, acc 1
2017-08-08T17:31:50.782765: step 13433, loss 0.000121015, acc 1
2017-08-08T17:31:51.169732: step 13434, loss 0.000794566, acc 1
2017-08-08T17:31:51.585391: step 13435, loss 0.000116305, acc 1
2017-08-08T17:31:51.934494: step 13436, loss 0.000105551, acc 1
2017-08-08T17:31:52.258846: step 13437, loss 0.00032193, acc 1
2017-08-08T17:31:52.518795: step 13438, loss 1.91654e-05, acc 1
2017-08-08T17:31:52.857420: step 13439, loss 5.95566e-06, acc 1
2017-08-08T17:31:53.140061: step 13440, loss 0.000387688, acc 1
2017-08-08T17:31:53.345389: step 13441, loss 2.48282e-06, acc 1
2017-08-08T17:31:53.576852: step 13442, loss 4.97319e-07, acc 1
2017-08-08T17:31:53.873369: step 13443, loss 0.000472629, acc 1
2017-08-08T17:31:54.298702: step 13444, loss 4.24834e-05, acc 1
2017-08-08T17:31:54.696794: step 13445, loss 1.07759e-05, acc 1
2017-08-08T17:31:54.962057: step 13446, loss 2.01165e-07, acc 1
2017-08-08T17:31:55.148551: step 13447, loss 0.000127586, acc 1
2017-08-08T17:31:55.446673: step 13448, loss 5.96041e-07, acc 1
2017-08-08T17:31:55.695615: step 13449, loss 0.000180595, acc 1
2017-08-08T17:31:55.894005: step 13450, loss 0.0269314, acc 0.984375
2017-08-08T17:31:56.125828: step 13451, loss 0.00543623, acc 1
2017-08-08T17:31:56.457944: step 13452, loss 2.23177e-05, acc 1
2017-08-08T17:31:56.910131: step 13453, loss 0.000255484, acc 1
2017-08-08T17:31:57.347906: step 13454, loss 9.09516e-05, acc 1
2017-08-08T17:31:57.567703: step 13455, loss 0.00346398, acc 1
2017-08-08T17:31:57.912325: step 13456, loss 1.21442e-06, acc 1
2017-08-08T17:31:58.120876: step 13457, loss 3.91155e-08, acc 1
2017-08-08T17:31:58.394743: step 13458, loss 0.000649212, acc 1
2017-08-08T17:31:58.611870: step 13459, loss 1.76592e-05, acc 1
2017-08-08T17:31:58.913579: step 13460, loss 4.09782e-08, acc 1
2017-08-08T17:31:59.200608: step 13461, loss 1.17531e-06, acc 1
2017-08-08T17:31:59.448588: step 13462, loss 2.26491e-06, acc 1
2017-08-08T17:31:59.616651: step 13463, loss 9.53974e-06, acc 1
2017-08-08T17:31:59.933723: step 13464, loss 4.09746e-06, acc 1
2017-08-08T17:32:00.348625: step 13465, loss 0.000340607, acc 1
2017-08-08T17:32:00.541363: step 13466, loss 0.000349661, acc 1
2017-08-08T17:32:00.747559: step 13467, loss 1.81598e-06, acc 1
2017-08-08T17:32:01.081365: step 13468, loss 9.32094e-05, acc 1
2017-08-08T17:32:01.432976: step 13469, loss 1.69401e-05, acc 1
2017-08-08T17:32:01.810934: step 13470, loss 2.51456e-07, acc 1
2017-08-08T17:32:02.137540: step 13471, loss 7.39111e-06, acc 1
2017-08-08T17:32:02.458887: step 13472, loss 7.82296e-07, acc 1
2017-08-08T17:32:02.836337: step 13473, loss 9.3132e-08, acc 1
2017-08-08T17:32:03.120455: step 13474, loss 1.70532e-05, acc 1
2017-08-08T17:32:03.369803: step 13475, loss 7.79767e-06, acc 1
2017-08-08T17:32:03.673763: step 13476, loss 5.81005e-05, acc 1
2017-08-08T17:32:04.067980: step 13477, loss 1.97144e-05, acc 1
2017-08-08T17:32:04.404356: step 13478, loss 3.1823e-05, acc 1
2017-08-08T17:32:04.760352: step 13479, loss 0.000306997, acc 1
2017-08-08T17:32:05.085480: step 13480, loss 1.21254e-06, acc 1
2017-08-08T17:32:05.373381: step 13481, loss 0.000171648, acc 1
2017-08-08T17:32:05.765024: step 13482, loss 0.000117107, acc 1
2017-08-08T17:32:05.974622: step 13483, loss 0.00048559, acc 1
2017-08-08T17:32:06.243144: step 13484, loss 4.26542e-07, acc 1
2017-08-08T17:32:06.533608: step 13485, loss 4.37717e-07, acc 1
2017-08-08T17:32:06.915934: step 13486, loss 6.37494e-06, acc 1
2017-08-08T17:32:07.178608: step 13487, loss 0.0167746, acc 0.984375
2017-08-08T17:32:07.436054: step 13488, loss 3.05472e-07, acc 1
2017-08-08T17:32:07.680800: step 13489, loss 0.000682256, acc 1
2017-08-08T17:32:07.998660: step 13490, loss 1.26453e-05, acc 1
2017-08-08T17:32:08.239395: step 13491, loss 5.12097e-05, acc 1
2017-08-08T17:32:08.552008: step 13492, loss 2.60028e-05, acc 1
2017-08-08T17:32:08.885795: step 13493, loss 1.3059e-05, acc 1
2017-08-08T17:32:09.407095: step 13494, loss 8.64258e-07, acc 1
2017-08-08T17:32:09.780797: step 13495, loss 0.00044828, acc 1
2017-08-08T17:32:10.171948: step 13496, loss 6.51925e-08, acc 1
2017-08-08T17:32:10.448849: step 13497, loss 0.000113281, acc 1
2017-08-08T17:32:10.899949: step 13498, loss 0.000463916, acc 1
2017-08-08T17:32:11.186045: step 13499, loss 0.000432914, acc 1
2017-08-08T17:32:11.441616: step 13500, loss 2.09851e-05, acc 1

Evaluation:
2017-08-08T17:32:12.134798: step 13500, loss 4.73649, acc 0.713884

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-13500

2017-08-08T17:32:12.685970: step 13501, loss 2.21837e-06, acc 1
2017-08-08T17:32:12.889451: step 13502, loss 3.09927e-06, acc 1
2017-08-08T17:32:13.209346: step 13503, loss 2.57522e-05, acc 1
2017-08-08T17:32:13.458635: step 13504, loss 0.000435742, acc 1
2017-08-08T17:32:13.679844: step 13505, loss 0.00598917, acc 1
2017-08-08T17:32:13.973366: step 13506, loss 0.000157042, acc 1
2017-08-08T17:32:14.483420: step 13507, loss 3.14438e-05, acc 1
2017-08-08T17:32:14.902227: step 13508, loss 0.0581186, acc 0.984375
2017-08-08T17:32:15.160094: step 13509, loss 1.43836e-05, acc 1
2017-08-08T17:32:15.355433: step 13510, loss 2.9035e-05, acc 1
2017-08-08T17:32:15.646121: step 13511, loss 5.40124e-06, acc 1
2017-08-08T17:32:15.863684: step 13512, loss 8.25144e-07, acc 1
2017-08-08T17:32:16.078163: step 13513, loss 2.2239e-06, acc 1
2017-08-08T17:32:16.330874: step 13514, loss 1.71028e-05, acc 1
2017-08-08T17:32:16.700665: step 13515, loss 3.82367e-05, acc 1
2017-08-08T17:32:17.037356: step 13516, loss 0.000169643, acc 1
2017-08-08T17:32:17.314598: step 13517, loss 0.00132833, acc 1
2017-08-08T17:32:17.577524: step 13518, loss 2.86845e-07, acc 1
2017-08-08T17:32:17.830501: step 13519, loss 2.23502e-06, acc 1
2017-08-08T17:32:18.234000: step 13520, loss 0.000169399, acc 1
2017-08-08T17:32:18.551784: step 13521, loss 8.46271e-05, acc 1
2017-08-08T17:32:18.768820: step 13522, loss 7.07805e-08, acc 1
2017-08-08T17:32:19.068004: step 13523, loss 4.73867e-05, acc 1
2017-08-08T17:32:19.304828: step 13524, loss 0.000700197, acc 1
2017-08-08T17:32:19.713350: step 13525, loss 4.25956e-06, acc 1
2017-08-08T17:32:20.032115: step 13526, loss 1.78834e-05, acc 1
2017-08-08T17:32:20.418986: step 13527, loss 2.66778e-05, acc 1
2017-08-08T17:32:20.688074: step 13528, loss 0.00814381, acc 1
2017-08-08T17:32:21.222841: step 13529, loss 4.72445e-05, acc 1
2017-08-08T17:32:21.430316: step 13530, loss 1.06539e-05, acc 1
2017-08-08T17:32:21.620710: step 13531, loss 1.10453e-06, acc 1
2017-08-08T17:32:21.904639: step 13532, loss 8.26643e-06, acc 1
2017-08-08T17:32:22.251054: step 13533, loss 0.00136122, acc 1
2017-08-08T17:32:22.606596: step 13534, loss 7.07228e-05, acc 1
2017-08-08T17:32:22.857830: step 13535, loss 3.70665e-07, acc 1
2017-08-08T17:32:23.101129: step 13536, loss 0.000562077, acc 1
2017-08-08T17:32:23.388768: step 13537, loss 1.01332e-05, acc 1
2017-08-08T17:32:23.669168: step 13538, loss 0.00142883, acc 1
2017-08-08T17:32:23.942118: step 13539, loss 1.22059e-05, acc 1
2017-08-08T17:32:24.210506: step 13540, loss 9.77694e-05, acc 1
2017-08-08T17:32:24.512847: step 13541, loss 2.09036e-05, acc 1
2017-08-08T17:32:24.961588: step 13542, loss 0.0441043, acc 0.984375
2017-08-08T17:32:25.309347: step 13543, loss 2.86627e-05, acc 1
2017-08-08T17:32:25.556718: step 13544, loss 0.000139761, acc 1
2017-08-08T17:32:25.769040: step 13545, loss 7.81486e-05, acc 1
2017-08-08T17:32:26.080143: step 13546, loss 4.24919e-05, acc 1
2017-08-08T17:32:26.409737: step 13547, loss 0.00560549, acc 1
2017-08-08T17:32:26.728078: step 13548, loss 4.19622e-05, acc 1
2017-08-08T17:32:27.055035: step 13549, loss 0.00174794, acc 1
2017-08-08T17:32:27.461659: step 13550, loss 1.53249e-05, acc 1
2017-08-08T17:32:27.803745: step 13551, loss 7.60615e-06, acc 1
2017-08-08T17:32:28.064175: step 13552, loss 0.000149788, acc 1
2017-08-08T17:32:28.296284: step 13553, loss 1.71363e-07, acc 1
2017-08-08T17:32:28.581031: step 13554, loss 4.65999e-06, acc 1
2017-08-08T17:32:28.844455: step 13555, loss 2.53124e-05, acc 1
2017-08-08T17:32:29.179238: step 13556, loss 1.91473e-06, acc 1
2017-08-08T17:32:29.446579: step 13557, loss 5.35978e-06, acc 1
2017-08-08T17:32:29.822774: step 13558, loss 3.01541e-06, acc 1
2017-08-08T17:32:30.257395: step 13559, loss 5.55948e-05, acc 1
2017-08-08T17:32:30.617350: step 13560, loss 1.36921e-05, acc 1
2017-08-08T17:32:30.839165: step 13561, loss 6.74805e-06, acc 1
2017-08-08T17:32:31.009145: step 13562, loss 0.00845668, acc 1
2017-08-08T17:32:31.261349: step 13563, loss 6.31514e-05, acc 1
2017-08-08T17:32:31.507032: step 13564, loss 6.22114e-07, acc 1
2017-08-08T17:32:31.704358: step 13565, loss 8.11617e-06, acc 1
2017-08-08T17:32:31.886803: step 13566, loss 5.15662e-05, acc 1
2017-08-08T17:32:32.069614: step 13567, loss 7.52821e-06, acc 1
2017-08-08T17:32:32.341312: step 13568, loss 0.000345491, acc 1
2017-08-08T17:32:32.574661: step 13569, loss 3.07962e-05, acc 1
2017-08-08T17:32:32.783791: step 13570, loss 2.024e-05, acc 1
2017-08-08T17:32:32.961146: step 13571, loss 8.35244e-06, acc 1
2017-08-08T17:32:33.248480: step 13572, loss 0.000486627, acc 1
2017-08-08T17:32:33.422388: step 13573, loss 0.000400076, acc 1
2017-08-08T17:32:33.591963: step 13574, loss 1.53377e-05, acc 1
2017-08-08T17:32:33.748200: step 13575, loss 3.7439e-07, acc 1
2017-08-08T17:32:33.999332: step 13576, loss 3.79572e-06, acc 1
2017-08-08T17:32:34.351324: step 13577, loss 6.86664e-06, acc 1
2017-08-08T17:32:34.634325: step 13578, loss 5.64377e-07, acc 1
2017-08-08T17:32:34.839384: step 13579, loss 4.01174e-06, acc 1
2017-08-08T17:32:35.108636: step 13580, loss 0.00014239, acc 1
2017-08-08T17:32:35.530773: step 13581, loss 0.000425124, acc 1
2017-08-08T17:32:35.751859: step 13582, loss 0.000262612, acc 1
2017-08-08T17:32:35.968263: step 13583, loss 1.30194e-06, acc 1
2017-08-08T17:32:36.173737: step 13584, loss 1.79741e-06, acc 1
2017-08-08T17:32:36.491287: step 13585, loss 4.55653e-05, acc 1
2017-08-08T17:32:36.726975: step 13586, loss 3.99325e-06, acc 1
2017-08-08T17:32:36.981380: step 13587, loss 0.0234784, acc 0.984375
2017-08-08T17:32:37.207682: step 13588, loss 0.000616084, acc 1
2017-08-08T17:32:37.445398: step 13589, loss 2.94106e-05, acc 1
2017-08-08T17:32:37.805795: step 13590, loss 9.49947e-08, acc 1
2017-08-08T17:32:38.079586: step 13591, loss 7.65538e-07, acc 1
2017-08-08T17:32:38.375486: step 13592, loss 0.000101234, acc 1
2017-08-08T17:32:38.640439: step 13593, loss 2.40332e-05, acc 1
2017-08-08T17:32:39.060898: step 13594, loss 0.00141271, acc 1
2017-08-08T17:32:39.433374: step 13595, loss 0.000264237, acc 1
2017-08-08T17:32:39.701266: step 13596, loss 0.00109055, acc 1
2017-08-08T17:32:39.930185: step 13597, loss 0.000114692, acc 1
2017-08-08T17:32:40.261601: step 13598, loss 2.98746e-05, acc 1
2017-08-08T17:32:40.601212: step 13599, loss 1.05463e-05, acc 1
2017-08-08T17:32:40.875265: step 13600, loss 0.000344991, acc 1

Evaluation:
2017-08-08T17:32:41.413462: step 13600, loss 4.92921, acc 0.708255

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-13600

2017-08-08T17:32:42.036941: step 13601, loss 0.0242558, acc 0.984375
2017-08-08T17:32:42.431199: step 13602, loss 0.000108889, acc 1
2017-08-08T17:32:42.683349: step 13603, loss 8.36317e-07, acc 1
2017-08-08T17:32:42.948029: step 13604, loss 0.00166412, acc 1
2017-08-08T17:32:43.365366: step 13605, loss 1.21815e-06, acc 1
2017-08-08T17:32:43.668779: step 13606, loss 7.39906e-05, acc 1
2017-08-08T17:32:43.958534: step 13607, loss 2.04878e-06, acc 1
2017-08-08T17:32:44.231515: step 13608, loss 3.24098e-07, acc 1
2017-08-08T17:32:44.685814: step 13609, loss 0.000142118, acc 1
2017-08-08T17:32:45.074166: step 13610, loss 0.000115048, acc 1
2017-08-08T17:32:45.491719: step 13611, loss 3.2968e-06, acc 1
2017-08-08T17:32:45.733190: step 13612, loss 0.000418292, acc 1
2017-08-08T17:32:45.938533: step 13613, loss 4.34454e-05, acc 1
2017-08-08T17:32:46.347069: step 13614, loss 1.22132e-05, acc 1
2017-08-08T17:32:46.580503: step 13615, loss 2.75275e-06, acc 1
2017-08-08T17:32:46.815544: step 13616, loss 5.27126e-07, acc 1
2017-08-08T17:32:47.069945: step 13617, loss 6.91459e-06, acc 1
2017-08-08T17:32:47.344473: step 13618, loss 1.28333e-06, acc 1
2017-08-08T17:32:47.639382: step 13619, loss 1.33106e-05, acc 1
2017-08-08T17:32:47.909307: step 13620, loss 6.04891e-05, acc 1
2017-08-08T17:32:48.200959: step 13621, loss 3.43269e-06, acc 1
2017-08-08T17:32:48.388733: step 13622, loss 8.34455e-07, acc 1
2017-08-08T17:32:48.595448: step 13623, loss 0.000434207, acc 1
2017-08-08T17:32:48.992308: step 13624, loss 2.14204e-07, acc 1
2017-08-08T17:32:49.324156: step 13625, loss 1.28335e-05, acc 1
2017-08-08T17:32:49.613225: step 13626, loss 3.51282e-06, acc 1
2017-08-08T17:32:49.970965: step 13627, loss 7.89752e-07, acc 1
2017-08-08T17:32:50.364495: step 13628, loss 0.000458909, acc 1
2017-08-08T17:32:50.673340: step 13629, loss 3.33762e-06, acc 1
2017-08-08T17:32:50.927370: step 13630, loss 9.94632e-07, acc 1
2017-08-08T17:32:51.129417: step 13631, loss 0.000661323, acc 1
2017-08-08T17:32:51.570622: step 13632, loss 3.04691e-05, acc 1
2017-08-08T17:32:51.842148: step 13633, loss 0.00319203, acc 1
2017-08-08T17:32:52.176447: step 13634, loss 1.15594e-05, acc 1
2017-08-08T17:32:52.412707: step 13635, loss 6.88006e-06, acc 1
2017-08-08T17:32:52.737562: step 13636, loss 0.000539289, acc 1
2017-08-08T17:32:53.079155: step 13637, loss 6.02404e-05, acc 1
2017-08-08T17:32:53.386928: step 13638, loss 3.68802e-07, acc 1
2017-08-08T17:32:53.556218: step 13639, loss 4.93438e-05, acc 1
2017-08-08T17:32:53.760371: step 13640, loss 6.174e-06, acc 1
2017-08-08T17:32:54.101165: step 13641, loss 1.22126e-05, acc 1
2017-08-08T17:32:54.344098: step 13642, loss 0.0380767, acc 0.984375
2017-08-08T17:32:54.567996: step 13643, loss 0.000294131, acc 1
2017-08-08T17:32:54.785342: step 13644, loss 3.67866e-05, acc 1
2017-08-08T17:32:55.087530: step 13645, loss 0.0014506, acc 1
2017-08-08T17:32:55.521511: step 13646, loss 2.73807e-07, acc 1
2017-08-08T17:32:55.886008: step 13647, loss 8.18209e-05, acc 1
2017-08-08T17:32:56.164108: step 13648, loss 4.02846e-05, acc 1
2017-08-08T17:32:56.409500: step 13649, loss 1.14179e-06, acc 1
2017-08-08T17:32:56.757866: step 13650, loss 0.102869, acc 0.983333
2017-08-08T17:32:57.006622: step 13651, loss 0.000105309, acc 1
2017-08-08T17:32:57.305928: step 13652, loss 0.000120167, acc 1
2017-08-08T17:32:57.595488: step 13653, loss 8.14371e-05, acc 1
2017-08-08T17:32:58.039606: step 13654, loss 2.54614e-06, acc 1
2017-08-08T17:32:58.432826: step 13655, loss 1.06235e-05, acc 1
2017-08-08T17:32:58.776138: step 13656, loss 4.4195e-06, acc 1
2017-08-08T17:32:59.085248: step 13657, loss 0.00030796, acc 1
2017-08-08T17:32:59.286848: step 13658, loss 1.76804e-05, acc 1
2017-08-08T17:32:59.781009: step 13659, loss 0.0941426, acc 0.984375
2017-08-08T17:33:00.041469: step 13660, loss 0.000294238, acc 1
2017-08-08T17:33:00.423523: step 13661, loss 2.32112e-05, acc 1
2017-08-08T17:33:00.740796: step 13662, loss 3.40779e-05, acc 1
2017-08-08T17:33:01.018741: step 13663, loss 1.85634e-05, acc 1
2017-08-08T17:33:01.355979: step 13664, loss 1.02445e-07, acc 1
2017-08-08T17:33:01.639601: step 13665, loss 0.000102799, acc 1
2017-08-08T17:33:02.026302: step 13666, loss 4.23132e-05, acc 1
2017-08-08T17:33:02.338479: step 13667, loss 0.00037336, acc 1
2017-08-08T17:33:02.646184: step 13668, loss 7.46913e-07, acc 1
2017-08-08T17:33:03.104224: step 13669, loss 3.98558e-06, acc 1
2017-08-08T17:33:03.492123: step 13670, loss 2.28334e-05, acc 1
2017-08-08T17:33:03.804642: step 13671, loss 0.057204, acc 0.984375
2017-08-08T17:33:04.100340: step 13672, loss 0.000543499, acc 1
2017-08-08T17:33:04.367638: step 13673, loss 4.06053e-07, acc 1
2017-08-08T17:33:04.849373: step 13674, loss 2.9273e-05, acc 1
2017-08-08T17:33:05.106756: step 13675, loss 7.67068e-05, acc 1
2017-08-08T17:33:05.477625: step 13676, loss 0.0172605, acc 0.984375
2017-08-08T17:33:05.770017: step 13677, loss 0.017066, acc 0.984375
2017-08-08T17:33:06.008197: step 13678, loss 0.000199482, acc 1
2017-08-08T17:33:06.445388: step 13679, loss 1.43478e-05, acc 1
2017-08-08T17:33:06.719963: step 13680, loss 1.45059e-05, acc 1
2017-08-08T17:33:07.095061: step 13681, loss 1.26228e-05, acc 1
2017-08-08T17:33:07.458709: step 13682, loss 9.48409e-06, acc 1
2017-08-08T17:33:07.733011: step 13683, loss 0.000143283, acc 1
2017-08-08T17:33:08.015773: step 13684, loss 5.72605e-05, acc 1
2017-08-08T17:33:08.312412: step 13685, loss 0.000238356, acc 1
2017-08-08T17:33:08.643790: step 13686, loss 4.10541e-05, acc 1
2017-08-08T17:33:08.927850: step 13687, loss 0.000115724, acc 1
2017-08-08T17:33:09.305176: step 13688, loss 0.00049401, acc 1
2017-08-08T17:33:09.517413: step 13689, loss 7.93426e-06, acc 1
2017-08-08T17:33:09.858391: step 13690, loss 0.0104527, acc 1
2017-08-08T17:33:10.209231: step 13691, loss 3.24701e-05, acc 1
2017-08-08T17:33:10.433566: step 13692, loss 0.000582755, acc 1
2017-08-08T17:33:10.784632: step 13693, loss 0.000353035, acc 1
2017-08-08T17:33:11.005015: step 13694, loss 4.56308e-06, acc 1
2017-08-08T17:33:11.274279: step 13695, loss 0.000187975, acc 1
2017-08-08T17:33:11.561878: step 13696, loss 0.000202031, acc 1
2017-08-08T17:33:11.864558: step 13697, loss 3.33438e-05, acc 1
2017-08-08T17:33:12.065489: step 13698, loss 4.35219e-05, acc 1
2017-08-08T17:33:12.265521: step 13699, loss 0.00121611, acc 1
2017-08-08T17:33:12.644319: step 13700, loss 2.83513e-05, acc 1

Evaluation:
2017-08-08T17:33:13.360839: step 13700, loss 4.79571, acc 0.710131

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-13700

2017-08-08T17:33:13.790166: step 13701, loss 0.000652559, acc 1
2017-08-08T17:33:14.327088: step 13702, loss 0.000499972, acc 1
2017-08-08T17:33:14.630958: step 13703, loss 4.80763e-05, acc 1
2017-08-08T17:33:14.918757: step 13704, loss 1.02896e-05, acc 1
2017-08-08T17:33:15.197523: step 13705, loss 3.11983e-06, acc 1
2017-08-08T17:33:15.564820: step 13706, loss 1.37025e-05, acc 1
2017-08-08T17:33:16.002598: step 13707, loss 9.01821e-05, acc 1
2017-08-08T17:33:16.348130: step 13708, loss 2.37154e-05, acc 1
2017-08-08T17:33:16.664533: step 13709, loss 2.60918e-05, acc 1
2017-08-08T17:33:16.983534: step 13710, loss 0.000203902, acc 1
2017-08-08T17:33:17.244857: step 13711, loss 2.61953e-05, acc 1
2017-08-08T17:33:17.466451: step 13712, loss 1.43792e-06, acc 1
2017-08-08T17:33:17.746254: step 13713, loss 0.000883976, acc 1
2017-08-08T17:33:18.059746: step 13714, loss 6.44468e-07, acc 1
2017-08-08T17:33:18.325439: step 13715, loss 2.65783e-06, acc 1
2017-08-08T17:33:18.588714: step 13716, loss 2.1699e-06, acc 1
2017-08-08T17:33:18.802311: step 13717, loss 8.6045e-06, acc 1
2017-08-08T17:33:19.071903: step 13718, loss 5.93284e-05, acc 1
2017-08-08T17:33:19.480790: step 13719, loss 4.47034e-08, acc 1
2017-08-08T17:33:19.752837: step 13720, loss 2.72872e-06, acc 1
2017-08-08T17:33:20.022220: step 13721, loss 0.00034864, acc 1
2017-08-08T17:33:20.328100: step 13722, loss 0.00102956, acc 1
2017-08-08T17:33:20.703784: step 13723, loss 0.0010309, acc 1
2017-08-08T17:33:21.048058: step 13724, loss 0.000680667, acc 1
2017-08-08T17:33:21.317938: step 13725, loss 6.40068e-06, acc 1
2017-08-08T17:33:21.565458: step 13726, loss 7.95343e-07, acc 1
2017-08-08T17:33:21.925869: step 13727, loss 5.69925e-06, acc 1
2017-08-08T17:33:22.117882: step 13728, loss 0.000316137, acc 1
2017-08-08T17:33:22.300045: step 13729, loss 0.000142789, acc 1
2017-08-08T17:33:22.577489: step 13730, loss 6.0163e-07, acc 1
2017-08-08T17:33:22.871014: step 13731, loss 4.89874e-07, acc 1
2017-08-08T17:33:23.193634: step 13732, loss 1.59142e-05, acc 1
2017-08-08T17:33:23.400921: step 13733, loss 1.82538e-07, acc 1
2017-08-08T17:33:23.583733: step 13734, loss 2.98023e-08, acc 1
2017-08-08T17:33:23.851179: step 13735, loss 5.58785e-07, acc 1
2017-08-08T17:33:24.247942: step 13736, loss 5.13084e-06, acc 1
2017-08-08T17:33:24.459838: step 13737, loss 0.000269045, acc 1
2017-08-08T17:33:24.673138: step 13738, loss 0.000221848, acc 1
2017-08-08T17:33:25.017411: step 13739, loss 0.00795594, acc 1
2017-08-08T17:33:25.375266: step 13740, loss 3.26132e-06, acc 1
2017-08-08T17:33:25.748771: step 13741, loss 1.72193e-05, acc 1
2017-08-08T17:33:25.969066: step 13742, loss 0.00900689, acc 1
2017-08-08T17:33:26.210726: step 13743, loss 4.66559e-06, acc 1
2017-08-08T17:33:26.534314: step 13744, loss 4.73199e-05, acc 1
2017-08-08T17:33:26.720370: step 13745, loss 2.18025e-05, acc 1
2017-08-08T17:33:26.913462: step 13746, loss 5.58964e-05, acc 1
2017-08-08T17:33:27.113237: step 13747, loss 0.000288538, acc 1
2017-08-08T17:33:27.569406: step 13748, loss 3.70644e-06, acc 1
2017-08-08T17:33:27.901432: step 13749, loss 2.88314e-05, acc 1
2017-08-08T17:33:28.189284: step 13750, loss 0.000392047, acc 1
2017-08-08T17:33:28.410613: step 13751, loss 0.00011022, acc 1
2017-08-08T17:33:28.635474: step 13752, loss 7.75135e-06, acc 1
2017-08-08T17:33:29.026343: step 13753, loss 0.000290427, acc 1
2017-08-08T17:33:29.287035: step 13754, loss 4.91491e-05, acc 1
2017-08-08T17:33:29.513884: step 13755, loss 0.00216886, acc 1
2017-08-08T17:33:29.731663: step 13756, loss 5.85018e-05, acc 1
2017-08-08T17:33:29.944671: step 13757, loss 0.0015, acc 1
2017-08-08T17:33:30.337581: step 13758, loss 0.000175505, acc 1
2017-08-08T17:33:30.697393: step 13759, loss 1.21441e-06, acc 1
2017-08-08T17:33:30.969516: step 13760, loss 1.25737e-05, acc 1
2017-08-08T17:33:31.180975: step 13761, loss 0.000184485, acc 1
2017-08-08T17:33:31.525426: step 13762, loss 0.000161929, acc 1
2017-08-08T17:33:31.751027: step 13763, loss 6.73819e-06, acc 1
2017-08-08T17:33:32.014791: step 13764, loss 0.000131482, acc 1
2017-08-08T17:33:32.267179: step 13765, loss 0.000193864, acc 1
2017-08-08T17:33:32.646043: step 13766, loss 7.41893e-05, acc 1
2017-08-08T17:33:33.052391: step 13767, loss 9.26429e-06, acc 1
2017-08-08T17:33:33.397341: step 13768, loss 3.92124e-05, acc 1
2017-08-08T17:33:33.710982: step 13769, loss 8.56043e-06, acc 1
2017-08-08T17:33:33.928662: step 13770, loss 0.00865523, acc 1
2017-08-08T17:33:34.245363: step 13771, loss 0.00151189, acc 1
2017-08-08T17:33:34.457107: step 13772, loss 0.00158356, acc 1
2017-08-08T17:33:34.652199: step 13773, loss 1.98083e-05, acc 1
2017-08-08T17:33:34.867723: step 13774, loss 0.000133303, acc 1
2017-08-08T17:33:35.158792: step 13775, loss 6.67826e-06, acc 1
2017-08-08T17:33:35.423182: step 13776, loss 2.13267e-06, acc 1
2017-08-08T17:33:35.713834: step 13777, loss 0.000868728, acc 1
2017-08-08T17:33:35.897061: step 13778, loss 0.0160178, acc 0.984375
2017-08-08T17:33:36.065636: step 13779, loss 2.18527e-05, acc 1
2017-08-08T17:33:36.462191: step 13780, loss 1.99903e-05, acc 1
2017-08-08T17:33:36.684135: step 13781, loss 0.00420563, acc 1
2017-08-08T17:33:36.953716: step 13782, loss 2.46409e-06, acc 1
2017-08-08T17:33:37.230720: step 13783, loss 1.22561e-06, acc 1
2017-08-08T17:33:37.589378: step 13784, loss 2.01315e-05, acc 1
2017-08-08T17:33:37.940405: step 13785, loss 1.32509e-05, acc 1
2017-08-08T17:33:38.324200: step 13786, loss 6.51418e-06, acc 1
2017-08-08T17:33:38.560839: step 13787, loss 0.0333792, acc 0.984375
2017-08-08T17:33:38.787555: step 13788, loss 4.7686e-05, acc 1
2017-08-08T17:33:39.143191: step 13789, loss 1.05983e-06, acc 1
2017-08-08T17:33:39.378064: step 13790, loss 0.0334221, acc 0.984375
2017-08-08T17:33:39.692320: step 13791, loss 4.67476e-05, acc 1
2017-08-08T17:33:40.171781: step 13792, loss 1.78619e-06, acc 1
2017-08-08T17:33:40.552348: step 13793, loss 0.0443893, acc 0.984375
2017-08-08T17:33:40.831165: step 13794, loss 6.33739e-06, acc 1
2017-08-08T17:33:41.121828: step 13795, loss 2.75277e-06, acc 1
2017-08-08T17:33:41.437249: step 13796, loss 1.28007e-05, acc 1
2017-08-08T17:33:41.711626: step 13797, loss 6.91876e-05, acc 1
2017-08-08T17:33:41.977800: step 13798, loss 0.0303401, acc 0.984375
2017-08-08T17:33:42.295607: step 13799, loss 0.000891988, acc 1
2017-08-08T17:33:42.636426: step 13800, loss 0.0060594, acc 1

Evaluation:
2017-08-08T17:33:43.666747: step 13800, loss 4.84174, acc 0.704503

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-13800

2017-08-08T17:33:44.328912: step 13801, loss 3.82013e-06, acc 1
2017-08-08T17:33:44.607183: step 13802, loss 0.000494742, acc 1
2017-08-08T17:33:44.845480: step 13803, loss 0.00125045, acc 1
2017-08-08T17:33:45.053407: step 13804, loss 6.26747e-05, acc 1
2017-08-08T17:33:45.360627: step 13805, loss 2.18184e-05, acc 1
2017-08-08T17:33:45.733805: step 13806, loss 0.00136099, acc 1
2017-08-08T17:33:45.978011: step 13807, loss 0.00014832, acc 1
2017-08-08T17:33:46.239521: step 13808, loss 5.62515e-07, acc 1
2017-08-08T17:33:46.485416: step 13809, loss 0.00026994, acc 1
2017-08-08T17:33:46.947550: step 13810, loss 8.13751e-05, acc 1
2017-08-08T17:33:47.257803: step 13811, loss 6.89687e-06, acc 1
2017-08-08T17:33:47.543245: step 13812, loss 2.00657e-05, acc 1
2017-08-08T17:33:47.937606: step 13813, loss 1.38225e-05, acc 1
2017-08-08T17:33:48.300937: step 13814, loss 0.000227117, acc 1
2017-08-08T17:33:48.578725: step 13815, loss 0.000830551, acc 1
2017-08-08T17:33:48.857453: step 13816, loss 1.72664e-05, acc 1
2017-08-08T17:33:49.120010: step 13817, loss 5.96046e-08, acc 1
2017-08-08T17:33:49.515814: step 13818, loss 0.000237244, acc 1
2017-08-08T17:33:49.798747: step 13819, loss 6.89178e-08, acc 1
2017-08-08T17:33:50.092854: step 13820, loss 0.0126912, acc 0.984375
2017-08-08T17:33:50.490019: step 13821, loss 4.99731e-05, acc 1
2017-08-08T17:33:50.921599: step 13822, loss 8.00936e-08, acc 1
2017-08-08T17:33:51.318656: step 13823, loss 0.000266108, acc 1
2017-08-08T17:33:51.591288: step 13824, loss 0.000138961, acc 1
2017-08-08T17:33:51.832990: step 13825, loss 0.00220092, acc 1
2017-08-08T17:33:52.273359: step 13826, loss 5.32711e-07, acc 1
2017-08-08T17:33:52.579077: step 13827, loss 9.99419e-05, acc 1
2017-08-08T17:33:52.803378: step 13828, loss 0.000116705, acc 1
2017-08-08T17:33:53.012698: step 13829, loss 5.69793e-05, acc 1
2017-08-08T17:33:53.349516: step 13830, loss 0.000410493, acc 1
2017-08-08T17:33:53.701943: step 13831, loss 3.88751e-05, acc 1
2017-08-08T17:33:54.051908: step 13832, loss 3.48276e-06, acc 1
2017-08-08T17:33:54.328925: step 13833, loss 2.03735e-05, acc 1
2017-08-08T17:33:54.740657: step 13834, loss 6.00042e-05, acc 1
2017-08-08T17:33:55.095263: step 13835, loss 0.000130214, acc 1
2017-08-08T17:33:55.330838: step 13836, loss 6.15902e-06, acc 1
2017-08-08T17:33:55.562029: step 13837, loss 6.35159e-07, acc 1
2017-08-08T17:33:55.970098: step 13838, loss 0.0230345, acc 0.984375
2017-08-08T17:33:56.376204: step 13839, loss 0.000807134, acc 1
2017-08-08T17:33:56.660153: step 13840, loss 0.000707257, acc 1
2017-08-08T17:33:56.906708: step 13841, loss 2.70454e-05, acc 1
2017-08-08T17:33:57.191629: step 13842, loss 1.6995e-05, acc 1
2017-08-08T17:33:57.762109: step 13843, loss 0.000115566, acc 1
2017-08-08T17:33:58.019701: step 13844, loss 9.85782e-06, acc 1
2017-08-08T17:33:58.302310: step 13845, loss 0.000262957, acc 1
2017-08-08T17:33:58.573177: step 13846, loss 0.0010718, acc 1
2017-08-08T17:33:58.940931: step 13847, loss 1.30385e-07, acc 1
2017-08-08T17:33:59.234496: step 13848, loss 8.36717e-05, acc 1
2017-08-08T17:33:59.497469: step 13849, loss 0.00216916, acc 1
2017-08-08T17:33:59.698830: step 13850, loss 1.48665e-05, acc 1
2017-08-08T17:33:59.892382: step 13851, loss 9.38607e-05, acc 1
2017-08-08T17:34:00.168501: step 13852, loss 0.00274275, acc 1
2017-08-08T17:34:00.391710: step 13853, loss 0.00506759, acc 1
2017-08-08T17:34:00.588567: step 13854, loss 7.65455e-06, acc 1
2017-08-08T17:34:00.804970: step 13855, loss 6.26194e-05, acc 1
2017-08-08T17:34:01.135869: step 13856, loss 1.32247e-07, acc 1
2017-08-08T17:34:01.476861: step 13857, loss 2.6336e-06, acc 1
2017-08-08T17:34:01.755763: step 13858, loss 0.000188922, acc 1
2017-08-08T17:34:02.018405: step 13859, loss 0.00388488, acc 1
2017-08-08T17:34:02.381446: step 13860, loss 0.000135041, acc 1
2017-08-08T17:34:02.825000: step 13861, loss 1.96113e-05, acc 1
2017-08-08T17:34:03.172594: step 13862, loss 8.04655e-07, acc 1
2017-08-08T17:34:03.436385: step 13863, loss 1.16661e-05, acc 1
2017-08-08T17:34:03.795955: step 13864, loss 1.19406e-05, acc 1
2017-08-08T17:34:04.230795: step 13865, loss 4.34153e-06, acc 1
2017-08-08T17:34:04.543505: step 13866, loss 1.19494e-05, acc 1
2017-08-08T17:34:04.816690: step 13867, loss 0.000357211, acc 1
2017-08-08T17:34:05.047603: step 13868, loss 1.9501e-06, acc 1
2017-08-08T17:34:05.436596: step 13869, loss 3.94121e-05, acc 1
2017-08-08T17:34:05.718407: step 13870, loss 3.74613e-05, acc 1
2017-08-08T17:34:05.989545: step 13871, loss 3.4438e-06, acc 1
2017-08-08T17:34:06.331526: step 13872, loss 4.14234e-06, acc 1
2017-08-08T17:34:06.671613: step 13873, loss 3.88042e-05, acc 1
2017-08-08T17:34:06.949625: step 13874, loss 0.000285717, acc 1
2017-08-08T17:34:07.177769: step 13875, loss 0.0596766, acc 0.984375
2017-08-08T17:34:07.372354: step 13876, loss 7.96267e-05, acc 1
2017-08-08T17:34:07.713306: step 13877, loss 5.7224e-05, acc 1
2017-08-08T17:34:07.915377: step 13878, loss 2.53306e-06, acc 1
2017-08-08T17:34:08.119888: step 13879, loss 6.68681e-07, acc 1
2017-08-08T17:34:08.323793: step 13880, loss 6.01909e-05, acc 1
2017-08-08T17:34:08.682234: step 13881, loss 1.6205e-07, acc 1
2017-08-08T17:34:09.037934: step 13882, loss 9.55662e-06, acc 1
2017-08-08T17:34:09.521999: step 13883, loss 1.11568e-05, acc 1
2017-08-08T17:34:09.752150: step 13884, loss 1.57363e-05, acc 1
2017-08-08T17:34:09.918108: step 13885, loss 0.000113953, acc 1
2017-08-08T17:34:10.207188: step 13886, loss 0.000287855, acc 1
2017-08-08T17:34:10.374057: step 13887, loss 0.000635996, acc 1
2017-08-08T17:34:10.566130: step 13888, loss 3.86851e-06, acc 1
2017-08-08T17:34:10.746211: step 13889, loss 1.05051e-05, acc 1
2017-08-08T17:34:10.988418: step 13890, loss 0.000365068, acc 1
2017-08-08T17:34:11.253333: step 13891, loss 0.000106494, acc 1
2017-08-08T17:34:11.494738: step 13892, loss 1.18981e-05, acc 1
2017-08-08T17:34:11.703740: step 13893, loss 1.2383e-05, acc 1
2017-08-08T17:34:11.871024: step 13894, loss 0.15395, acc 0.984375
2017-08-08T17:34:12.181485: step 13895, loss 0.00016537, acc 1
2017-08-08T17:34:12.365391: step 13896, loss 1.67638e-08, acc 1
2017-08-08T17:34:12.600724: step 13897, loss 0.000629485, acc 1
2017-08-08T17:34:12.847652: step 13898, loss 0.000143258, acc 1
2017-08-08T17:34:13.173372: step 13899, loss 6.77129e-05, acc 1
2017-08-08T17:34:13.485275: step 13900, loss 2.64483e-06, acc 1

Evaluation:
2017-08-08T17:34:14.242758: step 13900, loss 4.85405, acc 0.703565

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-13900

2017-08-08T17:34:14.659539: step 13901, loss 0.000129868, acc 1
2017-08-08T17:34:15.008719: step 13902, loss 5.0477e-07, acc 1
2017-08-08T17:34:15.189146: step 13903, loss 0.00132322, acc 1
2017-08-08T17:34:15.431152: step 13904, loss 1.77243e-05, acc 1
2017-08-08T17:34:15.686893: step 13905, loss 0.0334662, acc 0.984375
2017-08-08T17:34:16.013266: step 13906, loss 0.00156469, acc 1
2017-08-08T17:34:16.343640: step 13907, loss 8.922e-07, acc 1
2017-08-08T17:34:16.613337: step 13908, loss 0.000104188, acc 1
2017-08-08T17:34:16.828827: step 13909, loss 6.48719e-06, acc 1
2017-08-08T17:34:17.031645: step 13910, loss 0.000285291, acc 1
2017-08-08T17:34:17.300733: step 13911, loss 9.51416e-06, acc 1
2017-08-08T17:34:17.555796: step 13912, loss 8.74002e-05, acc 1
2017-08-08T17:34:17.870102: step 13913, loss 0.0423907, acc 0.984375
2017-08-08T17:34:18.113298: step 13914, loss 5.58787e-07, acc 1
2017-08-08T17:34:18.505763: step 13915, loss 4.84254e-06, acc 1
2017-08-08T17:34:18.821901: step 13916, loss 7.33978e-06, acc 1
2017-08-08T17:34:19.257349: step 13917, loss 4.65234e-05, acc 1
2017-08-08T17:34:19.579519: step 13918, loss 1.53823e-05, acc 1
2017-08-08T17:34:19.823186: step 13919, loss 1.25727e-06, acc 1
2017-08-08T17:34:20.178683: step 13920, loss 6.09753e-05, acc 1
2017-08-08T17:34:20.378203: step 13921, loss 2.26772e-05, acc 1
2017-08-08T17:34:20.581918: step 13922, loss 0.00676694, acc 1
2017-08-08T17:34:20.859767: step 13923, loss 0.00178056, acc 1
2017-08-08T17:34:21.149227: step 13924, loss 9.60595e-06, acc 1
2017-08-08T17:34:21.523698: step 13925, loss 0.045373, acc 0.984375
2017-08-08T17:34:21.827771: step 13926, loss 0.000643702, acc 1
2017-08-08T17:34:22.039353: step 13927, loss 5.34866e-06, acc 1
2017-08-08T17:34:22.252485: step 13928, loss 1.74793e-05, acc 1
2017-08-08T17:34:22.633224: step 13929, loss 7.49724e-06, acc 1
2017-08-08T17:34:22.861160: step 13930, loss 3.69839e-05, acc 1
2017-08-08T17:34:23.076002: step 13931, loss 0.000125245, acc 1
2017-08-08T17:34:23.305446: step 13932, loss 0.000169565, acc 1
2017-08-08T17:34:23.677691: step 13933, loss 8.11808e-06, acc 1
2017-08-08T17:34:23.972905: step 13934, loss 4.24846e-06, acc 1
2017-08-08T17:34:24.214007: step 13935, loss 0.000739394, acc 1
2017-08-08T17:34:24.408492: step 13936, loss 1.59808e-06, acc 1
2017-08-08T17:34:24.654744: step 13937, loss 1.20631e-05, acc 1
2017-08-08T17:34:24.992412: step 13938, loss 0.00413904, acc 1
2017-08-08T17:34:25.179138: step 13939, loss 2.77708e-06, acc 1
2017-08-08T17:34:25.364561: step 13940, loss 1.46022e-05, acc 1
2017-08-08T17:34:25.567969: step 13941, loss 7.06487e-05, acc 1
2017-08-08T17:34:25.912454: step 13942, loss 0.000975055, acc 1
2017-08-08T17:34:26.207543: step 13943, loss 4.46438e-06, acc 1
2017-08-08T17:34:26.454486: step 13944, loss 2.19792e-07, acc 1
2017-08-08T17:34:26.682565: step 13945, loss 1.23677e-06, acc 1
2017-08-08T17:34:26.916498: step 13946, loss 0.0055737, acc 1
2017-08-08T17:34:27.251959: step 13947, loss 4.88009e-07, acc 1
2017-08-08T17:34:27.441037: step 13948, loss 2.23517e-07, acc 1
2017-08-08T17:34:27.655186: step 13949, loss 0.000124374, acc 1
2017-08-08T17:34:27.878887: step 13950, loss 1.89551e-05, acc 1
2017-08-08T17:34:28.266169: step 13951, loss 0.000127417, acc 1
2017-08-08T17:34:28.647102: step 13952, loss 4.53515e-06, acc 1
2017-08-08T17:34:28.932230: step 13953, loss 0.000116543, acc 1
2017-08-08T17:34:29.132579: step 13954, loss 0.0011044, acc 1
2017-08-08T17:34:29.474679: step 13955, loss 0.000290519, acc 1
2017-08-08T17:34:29.720897: step 13956, loss 0.0015796, acc 1
2017-08-08T17:34:29.963330: step 13957, loss 6.37683e-06, acc 1
2017-08-08T17:34:30.208588: step 13958, loss 6.70552e-08, acc 1
2017-08-08T17:34:30.569468: step 13959, loss 1.35612e-05, acc 1
2017-08-08T17:34:30.953442: step 13960, loss 4.21299e-06, acc 1
2017-08-08T17:34:31.258341: step 13961, loss 0.00240161, acc 1
2017-08-08T17:34:31.507865: step 13962, loss 0.000174844, acc 1
2017-08-08T17:34:31.741890: step 13963, loss 1.89076e-05, acc 1
2017-08-08T17:34:32.068381: step 13964, loss 3.88128e-06, acc 1
2017-08-08T17:34:32.323556: step 13965, loss 0.000475012, acc 1
2017-08-08T17:34:32.611563: step 13966, loss 1.21548e-05, acc 1
2017-08-08T17:34:32.887744: step 13967, loss 2.05439e-06, acc 1
2017-08-08T17:34:33.174704: step 13968, loss 7.27765e-06, acc 1
2017-08-08T17:34:33.403856: step 13969, loss 0.000173859, acc 1
2017-08-08T17:34:33.678168: step 13970, loss 0.000144121, acc 1
2017-08-08T17:34:34.009295: step 13971, loss 0.00257738, acc 1
2017-08-08T17:34:34.260538: step 13972, loss 0.000102619, acc 1
2017-08-08T17:34:34.594745: step 13973, loss 2.90562e-06, acc 1
2017-08-08T17:34:34.893808: step 13974, loss 5.27353e-05, acc 1
2017-08-08T17:34:35.116316: step 13975, loss 4.90737e-06, acc 1
2017-08-08T17:34:35.359909: step 13976, loss 0.000122618, acc 1
2017-08-08T17:34:35.729267: step 13977, loss 0.011428, acc 0.984375
2017-08-08T17:34:36.174878: step 13978, loss 0.00415701, acc 1
2017-08-08T17:34:36.424491: step 13979, loss 0.000141723, acc 1
2017-08-08T17:34:36.673821: step 13980, loss 1.49011e-07, acc 1
2017-08-08T17:34:36.924363: step 13981, loss 0.000283747, acc 1
2017-08-08T17:34:37.223627: step 13982, loss 4.66956e-05, acc 1
2017-08-08T17:34:37.416285: step 13983, loss 3.89558e-05, acc 1
2017-08-08T17:34:37.631907: step 13984, loss 2.06272e-05, acc 1
2017-08-08T17:34:37.841502: step 13985, loss 0.000678384, acc 1
2017-08-08T17:34:38.152894: step 13986, loss 0.00035003, acc 1
2017-08-08T17:34:38.494718: step 13987, loss 1.79183e-06, acc 1
2017-08-08T17:34:38.766512: step 13988, loss 0.000105541, acc 1
2017-08-08T17:34:38.989188: step 13989, loss 5.51336e-07, acc 1
2017-08-08T17:34:39.273375: step 13990, loss 8.43765e-07, acc 1
2017-08-08T17:34:39.685286: step 13991, loss 5.40164e-07, acc 1
2017-08-08T17:34:39.954667: step 13992, loss 1.40055e-05, acc 1
2017-08-08T17:34:40.236645: step 13993, loss 1.99667e-06, acc 1
2017-08-08T17:34:40.496588: step 13994, loss 3.3655e-06, acc 1
2017-08-08T17:34:40.872309: step 13995, loss 1.88424e-05, acc 1
2017-08-08T17:34:41.175356: step 13996, loss 2.81257e-07, acc 1
2017-08-08T17:34:41.477592: step 13997, loss 1.64468e-06, acc 1
2017-08-08T17:34:41.749000: step 13998, loss 2.07349e-05, acc 1
2017-08-08T17:34:41.973358: step 13999, loss 9.01077e-06, acc 1
2017-08-08T17:34:42.319903: step 14000, loss 4.28408e-08, acc 1

Evaluation:
2017-08-08T17:34:42.809147: step 14000, loss 4.85824, acc 0.706379

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-14000

2017-08-08T17:34:43.509527: step 14001, loss 0.000242598, acc 1
2017-08-08T17:34:43.956024: step 14002, loss 0.000249312, acc 1
2017-08-08T17:34:44.277161: step 14003, loss 4.60175e-05, acc 1
2017-08-08T17:34:44.527192: step 14004, loss 0.000153484, acc 1
2017-08-08T17:34:44.923584: step 14005, loss 1.50874e-07, acc 1
2017-08-08T17:34:45.194494: step 14006, loss 5.08427e-05, acc 1
2017-08-08T17:34:45.488297: step 14007, loss 8.17695e-07, acc 1
2017-08-08T17:34:45.862351: step 14008, loss 0.000355384, acc 1
2017-08-08T17:34:46.206013: step 14009, loss 3.54445e-05, acc 1
2017-08-08T17:34:46.561319: step 14010, loss 0.000172912, acc 1
2017-08-08T17:34:46.837317: step 14011, loss 0.0658157, acc 0.984375
2017-08-08T17:34:47.051534: step 14012, loss 8.97925e-06, acc 1
2017-08-08T17:34:47.345775: step 14013, loss 6.34689e-06, acc 1
2017-08-08T17:34:47.641161: step 14014, loss 1.43004e-05, acc 1
2017-08-08T17:34:48.181554: step 14015, loss 0.000205478, acc 1
2017-08-08T17:34:48.585387: step 14016, loss 6.50345e-06, acc 1
2017-08-08T17:34:48.950543: step 14017, loss 3.0731e-06, acc 1
2017-08-08T17:34:49.196200: step 14018, loss 0.000268059, acc 1
2017-08-08T17:34:49.435859: step 14019, loss 0.00795868, acc 1
2017-08-08T17:34:49.833409: step 14020, loss 3.16255e-06, acc 1
2017-08-08T17:34:50.122067: step 14021, loss 8.81018e-07, acc 1
2017-08-08T17:34:50.393749: step 14022, loss 0.000374942, acc 1
2017-08-08T17:34:50.662818: step 14023, loss 0.000232898, acc 1
2017-08-08T17:34:51.060124: step 14024, loss 1.05795e-06, acc 1
2017-08-08T17:34:51.468639: step 14025, loss 0.200537, acc 0.984375
2017-08-08T17:34:51.844765: step 14026, loss 1.97057e-06, acc 1
2017-08-08T17:34:52.144940: step 14027, loss 3.11423e-05, acc 1
2017-08-08T17:34:52.396708: step 14028, loss 0.000290889, acc 1
2017-08-08T17:34:52.841421: step 14029, loss 9.51216e-05, acc 1
2017-08-08T17:34:53.188495: step 14030, loss 4.44157e-05, acc 1
2017-08-08T17:34:53.481821: step 14031, loss 0.00128264, acc 1
2017-08-08T17:34:53.740247: step 14032, loss 4.85171e-06, acc 1
2017-08-08T17:34:54.165679: step 14033, loss 8.62394e-07, acc 1
2017-08-08T17:34:54.542879: step 14034, loss 3.28188e-06, acc 1
2017-08-08T17:34:54.864761: step 14035, loss 1.08033e-07, acc 1
2017-08-08T17:34:55.146938: step 14036, loss 0.00532948, acc 1
2017-08-08T17:34:55.497418: step 14037, loss 0.00131047, acc 1
2017-08-08T17:34:55.803776: step 14038, loss 0.00122389, acc 1
2017-08-08T17:34:56.091462: step 14039, loss 0.000216302, acc 1
2017-08-08T17:34:56.321469: step 14040, loss 9.88449e-05, acc 1
2017-08-08T17:34:56.634469: step 14041, loss 1.58198e-05, acc 1
2017-08-08T17:34:57.028962: step 14042, loss 2.01165e-07, acc 1
2017-08-08T17:34:57.319203: step 14043, loss 2.17929e-07, acc 1
2017-08-08T17:34:57.497615: step 14044, loss 1.228e-05, acc 1
2017-08-08T17:34:57.725436: step 14045, loss 0.000562186, acc 1
2017-08-08T17:34:57.947433: step 14046, loss 0.0335488, acc 0.984375
2017-08-08T17:34:58.115180: step 14047, loss 0.00141284, acc 1
2017-08-08T17:34:58.306634: step 14048, loss 3.87012e-06, acc 1
2017-08-08T17:34:58.581554: step 14049, loss 0.000362547, acc 1
2017-08-08T17:34:58.874485: step 14050, loss 0.00016162, acc 1
2017-08-08T17:34:59.106006: step 14051, loss 1.27586e-06, acc 1
2017-08-08T17:34:59.266978: step 14052, loss 0.000187956, acc 1
2017-08-08T17:34:59.501356: step 14053, loss 7.87513e-05, acc 1
2017-08-08T17:34:59.826671: step 14054, loss 0.0462074, acc 0.984375
2017-08-08T17:35:00.065150: step 14055, loss 0.00149704, acc 1
2017-08-08T17:35:00.336313: step 14056, loss 0.0263971, acc 0.984375
2017-08-08T17:35:00.720746: step 14057, loss 0.000228437, acc 1
2017-08-08T17:35:01.185388: step 14058, loss 1.19637e-05, acc 1
2017-08-08T17:35:01.522495: step 14059, loss 1.81427e-05, acc 1
2017-08-08T17:35:01.866334: step 14060, loss 0.000122478, acc 1
2017-08-08T17:35:02.180786: step 14061, loss 1.50874e-07, acc 1
2017-08-08T17:35:02.593517: step 14062, loss 0.000281484, acc 1
2017-08-08T17:35:02.833622: step 14063, loss 0.000564444, acc 1
2017-08-08T17:35:03.153991: step 14064, loss 6.33299e-08, acc 1
2017-08-08T17:35:03.420798: step 14065, loss 0.000207332, acc 1
2017-08-08T17:35:03.774287: step 14066, loss 3.02131e-05, acc 1
2017-08-08T17:35:04.077161: step 14067, loss 0.000185358, acc 1
2017-08-08T17:35:04.367447: step 14068, loss 2.05922e-05, acc 1
2017-08-08T17:35:04.614611: step 14069, loss 8.71278e-06, acc 1
2017-08-08T17:35:04.840379: step 14070, loss 3.95012e-05, acc 1
2017-08-08T17:35:05.222313: step 14071, loss 3.77609e-05, acc 1
2017-08-08T17:35:05.438845: step 14072, loss 0.000510426, acc 1
2017-08-08T17:35:05.762678: step 14073, loss 0.000402012, acc 1
2017-08-08T17:35:06.078961: step 14074, loss 8.63513e-06, acc 1
2017-08-08T17:35:06.463831: step 14075, loss 0.000166228, acc 1
2017-08-08T17:35:06.833433: step 14076, loss 0.00047556, acc 1
2017-08-08T17:35:07.296705: step 14077, loss 0.000126452, acc 1
2017-08-08T17:35:07.526723: step 14078, loss 2.22322e-05, acc 1
2017-08-08T17:35:07.825186: step 14079, loss 8.41639e-06, acc 1
2017-08-08T17:35:08.205388: step 14080, loss 1.99668e-06, acc 1
2017-08-08T17:35:08.463907: step 14081, loss 8.7915e-07, acc 1
2017-08-08T17:35:08.755719: step 14082, loss 9.08075e-05, acc 1
2017-08-08T17:35:09.018384: step 14083, loss 1.22934e-07, acc 1
2017-08-08T17:35:09.378508: step 14084, loss 0.000528922, acc 1
2017-08-08T17:35:09.660593: step 14085, loss 0.000962241, acc 1
2017-08-08T17:35:09.914013: step 14086, loss 3.67552e-05, acc 1
2017-08-08T17:35:10.184220: step 14087, loss 5.69204e-06, acc 1
2017-08-08T17:35:10.430328: step 14088, loss 9.69359e-05, acc 1
2017-08-08T17:35:10.786742: step 14089, loss 0.000465387, acc 1
2017-08-08T17:35:11.036361: step 14090, loss 7.57171e-06, acc 1
2017-08-08T17:35:11.270096: step 14091, loss 6.14673e-08, acc 1
2017-08-08T17:35:11.533602: step 14092, loss 1.02445e-07, acc 1
2017-08-08T17:35:11.997876: step 14093, loss 3.61517e-06, acc 1
2017-08-08T17:35:12.422155: step 14094, loss 1.86082e-05, acc 1
2017-08-08T17:35:12.762318: step 14095, loss 2.10724e-05, acc 1
2017-08-08T17:35:12.983408: step 14096, loss 8.61963e-05, acc 1
2017-08-08T17:35:13.377414: step 14097, loss 2.0861e-06, acc 1
2017-08-08T17:35:13.790640: step 14098, loss 0.000825732, acc 1
2017-08-08T17:35:14.109418: step 14099, loss 1.79476e-05, acc 1
2017-08-08T17:35:14.414384: step 14100, loss 0.00329965, acc 1

Evaluation:
2017-08-08T17:35:15.365533: step 14100, loss 4.87417, acc 0.712008

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-14100

2017-08-08T17:35:15.895767: step 14101, loss 2.03947e-05, acc 1
2017-08-08T17:35:16.117593: step 14102, loss 0.000156487, acc 1
2017-08-08T17:35:16.526953: step 14103, loss 0.00113376, acc 1
2017-08-08T17:35:16.792455: step 14104, loss 1.62979e-06, acc 1
2017-08-08T17:35:17.053663: step 14105, loss 0.00288915, acc 1
2017-08-08T17:35:17.314986: step 14106, loss 0.000311735, acc 1
2017-08-08T17:35:17.693385: step 14107, loss 3.63976e-05, acc 1
2017-08-08T17:35:18.108860: step 14108, loss 0.00107917, acc 1
2017-08-08T17:35:18.538100: step 14109, loss 1.73782e-06, acc 1
2017-08-08T17:35:18.852583: step 14110, loss 1.32243e-06, acc 1
2017-08-08T17:35:19.108210: step 14111, loss 0.000156756, acc 1
2017-08-08T17:35:19.352489: step 14112, loss 2.13268e-06, acc 1
2017-08-08T17:35:19.578057: step 14113, loss 5.32709e-07, acc 1
2017-08-08T17:35:19.806770: step 14114, loss 8.36813e-06, acc 1
2017-08-08T17:35:20.101925: step 14115, loss 5.40167e-08, acc 1
2017-08-08T17:35:20.372193: step 14116, loss 0.000688, acc 1
2017-08-08T17:35:20.695081: step 14117, loss 4.55013e-06, acc 1
2017-08-08T17:35:21.053637: step 14118, loss 0.000523492, acc 1
2017-08-08T17:35:21.377315: step 14119, loss 5.55065e-07, acc 1
2017-08-08T17:35:21.648690: step 14120, loss 0.0902178, acc 0.984375
2017-08-08T17:35:22.005350: step 14121, loss 1.23036e-05, acc 1
2017-08-08T17:35:22.314132: step 14122, loss 2.46464e-05, acc 1
2017-08-08T17:35:22.569867: step 14123, loss 0.0129518, acc 0.984375
2017-08-08T17:35:22.865421: step 14124, loss 0.00075881, acc 1
2017-08-08T17:35:23.280456: step 14125, loss 5.21064e-05, acc 1
2017-08-08T17:35:23.729927: step 14126, loss 6.13869e-06, acc 1
2017-08-08T17:35:24.120856: step 14127, loss 4.84283e-07, acc 1
2017-08-08T17:35:24.418790: step 14128, loss 6.39729e-06, acc 1
2017-08-08T17:35:24.680404: step 14129, loss 3.1665e-08, acc 1
2017-08-08T17:35:25.093765: step 14130, loss 1.68379e-06, acc 1
2017-08-08T17:35:25.292338: step 14131, loss 0.000722579, acc 1
2017-08-08T17:35:25.530060: step 14132, loss 3.1868e-06, acc 1
2017-08-08T17:35:25.861456: step 14133, loss 9.86145e-05, acc 1
2017-08-08T17:35:26.287709: step 14134, loss 7.25408e-05, acc 1
2017-08-08T17:35:26.577501: step 14135, loss 1.68937e-06, acc 1
2017-08-08T17:35:26.808019: step 14136, loss 0.000987313, acc 1
2017-08-08T17:35:26.993922: step 14137, loss 3.28931e-06, acc 1
2017-08-08T17:35:27.270211: step 14138, loss 9.26104e-06, acc 1
2017-08-08T17:35:27.522653: step 14139, loss 3.59488e-07, acc 1
2017-08-08T17:35:27.753611: step 14140, loss 0.000862537, acc 1
2017-08-08T17:35:27.965534: step 14141, loss 0.000138417, acc 1
2017-08-08T17:35:28.285806: step 14142, loss 0.000118514, acc 1
2017-08-08T17:35:28.633698: step 14143, loss 0.000183082, acc 1
2017-08-08T17:35:28.873383: step 14144, loss 1.97614e-05, acc 1
2017-08-08T17:35:29.054557: step 14145, loss 7.39465e-07, acc 1
2017-08-08T17:35:29.379750: step 14146, loss 0.0122696, acc 0.984375
2017-08-08T17:35:29.706718: step 14147, loss 3.48109e-06, acc 1
2017-08-08T17:35:30.029922: step 14148, loss 2.05942e-05, acc 1
2017-08-08T17:35:30.328707: step 14149, loss 0.000973846, acc 1
2017-08-08T17:35:30.710864: step 14150, loss 2.65355e-05, acc 1
2017-08-08T17:35:31.075946: step 14151, loss 1.30107e-05, acc 1
2017-08-08T17:35:31.334274: step 14152, loss 8.00936e-08, acc 1
2017-08-08T17:35:31.578004: step 14153, loss 0.00012237, acc 1
2017-08-08T17:35:31.957366: step 14154, loss 8.73884e-05, acc 1
2017-08-08T17:35:32.337836: step 14155, loss 1.72845e-06, acc 1
2017-08-08T17:35:32.591650: step 14156, loss 3.63195e-06, acc 1
2017-08-08T17:35:32.857583: step 14157, loss 6.65075e-05, acc 1
2017-08-08T17:35:33.211911: step 14158, loss 8.71061e-05, acc 1
2017-08-08T17:35:33.641945: step 14159, loss 8.56815e-08, acc 1
2017-08-08T17:35:34.037465: step 14160, loss 8.57124e-06, acc 1
2017-08-08T17:35:34.407499: step 14161, loss 0.00030848, acc 1
2017-08-08T17:35:34.598291: step 14162, loss 6.51018e-05, acc 1
2017-08-08T17:35:34.815974: step 14163, loss 0, acc 1
2017-08-08T17:35:35.147384: step 14164, loss 0.0067793, acc 1
2017-08-08T17:35:35.399355: step 14165, loss 0.000266872, acc 1
2017-08-08T17:35:35.586945: step 14166, loss 0.000850587, acc 1
2017-08-08T17:35:35.759172: step 14167, loss 1.42652e-05, acc 1
2017-08-08T17:35:36.058474: step 14168, loss 6.7691e-05, acc 1
2017-08-08T17:35:36.349291: step 14169, loss 1.25971e-05, acc 1
2017-08-08T17:35:36.675474: step 14170, loss 2.37198e-05, acc 1
2017-08-08T17:35:36.966671: step 14171, loss 0.00227056, acc 1
2017-08-08T17:35:37.214224: step 14172, loss 0.000115253, acc 1
2017-08-08T17:35:37.556269: step 14173, loss 4.30585e-06, acc 1
2017-08-08T17:35:37.842874: step 14174, loss 0.000155106, acc 1
2017-08-08T17:35:38.137675: step 14175, loss 1.81253e-05, acc 1
2017-08-08T17:35:38.379316: step 14176, loss 0.000120836, acc 1
2017-08-08T17:35:38.792334: step 14177, loss 0.00205623, acc 1
2017-08-08T17:35:39.164020: step 14178, loss 6.42601e-07, acc 1
2017-08-08T17:35:39.434861: step 14179, loss 1.92771e-05, acc 1
2017-08-08T17:35:39.615733: step 14180, loss 6.44464e-07, acc 1
2017-08-08T17:35:39.871302: step 14181, loss 2.02428e-05, acc 1
2017-08-08T17:35:40.122656: step 14182, loss 0.0792109, acc 0.984375
2017-08-08T17:35:40.305543: step 14183, loss 0.000560163, acc 1
2017-08-08T17:35:40.495746: step 14184, loss 4.06979e-05, acc 1
2017-08-08T17:35:40.806864: step 14185, loss 1.61454e-05, acc 1
2017-08-08T17:35:41.189516: step 14186, loss 2.84417e-06, acc 1
2017-08-08T17:35:41.545902: step 14187, loss 9.50964e-05, acc 1
2017-08-08T17:35:41.791646: step 14188, loss 0.0455664, acc 0.984375
2017-08-08T17:35:42.015145: step 14189, loss 0.000209794, acc 1
2017-08-08T17:35:42.310898: step 14190, loss 0.000788921, acc 1
2017-08-08T17:35:42.695248: step 14191, loss 0.000606379, acc 1
2017-08-08T17:35:42.977178: step 14192, loss 3.71828e-05, acc 1
2017-08-08T17:35:43.196522: step 14193, loss 0.00502691, acc 1
2017-08-08T17:35:43.469355: step 14194, loss 2.98022e-07, acc 1
2017-08-08T17:35:43.829744: step 14195, loss 1.96414e-05, acc 1
2017-08-08T17:35:44.147221: step 14196, loss 0.00227509, acc 1
2017-08-08T17:35:44.371202: step 14197, loss 0.00228917, acc 1
2017-08-08T17:35:44.552297: step 14198, loss 0.000204575, acc 1
2017-08-08T17:35:44.829552: step 14199, loss 0.000365945, acc 1
2017-08-08T17:35:45.040391: step 14200, loss 0.0253286, acc 0.984375

Evaluation:
2017-08-08T17:35:45.508371: step 14200, loss 5.44347, acc 0.699812

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-14200

2017-08-08T17:35:46.089473: step 14201, loss 1.22371e-06, acc 1
2017-08-08T17:35:46.468076: step 14202, loss 5.12147e-05, acc 1
2017-08-08T17:35:46.757622: step 14203, loss 0.000448702, acc 1
2017-08-08T17:35:47.014925: step 14204, loss 5.03877e-05, acc 1
2017-08-08T17:35:47.346700: step 14205, loss 4.34696e-06, acc 1
2017-08-08T17:35:47.528320: step 14206, loss 3.97096e-06, acc 1
2017-08-08T17:35:47.712264: step 14207, loss 0.00223875, acc 1
2017-08-08T17:35:47.932483: step 14208, loss 0.000409773, acc 1
2017-08-08T17:35:48.285370: step 14209, loss 1.77154e-05, acc 1
2017-08-08T17:35:48.735596: step 14210, loss 0.042036, acc 0.984375
2017-08-08T17:35:49.000632: step 14211, loss 0.00306042, acc 1
2017-08-08T17:35:49.251180: step 14212, loss 0.10126, acc 0.96875
2017-08-08T17:35:49.565369: step 14213, loss 0.00662958, acc 1
2017-08-08T17:35:49.867529: step 14214, loss 0.000163824, acc 1
2017-08-08T17:35:50.141174: step 14215, loss 7.55204e-05, acc 1
2017-08-08T17:35:50.356397: step 14216, loss 2.69735e-05, acc 1
2017-08-08T17:35:50.572498: step 14217, loss 1.02995e-05, acc 1
2017-08-08T17:35:50.911079: step 14218, loss 0.000565402, acc 1
2017-08-08T17:35:51.264518: step 14219, loss 0.000581892, acc 1
2017-08-08T17:35:51.595993: step 14220, loss 0.000354118, acc 1
2017-08-08T17:35:51.849741: step 14221, loss 1.55146e-05, acc 1
2017-08-08T17:35:52.169858: step 14222, loss 0.00842547, acc 1
2017-08-08T17:35:52.555770: step 14223, loss 0.000309922, acc 1
2017-08-08T17:35:52.832732: step 14224, loss 1.48447e-06, acc 1
2017-08-08T17:35:53.097617: step 14225, loss 0.00622793, acc 1
2017-08-08T17:35:53.296799: step 14226, loss 0.00201761, acc 1
2017-08-08T17:35:53.555119: step 14227, loss 0.000184905, acc 1
2017-08-08T17:35:53.813408: step 14228, loss 0.000169846, acc 1
2017-08-08T17:35:54.136864: step 14229, loss 2.51677e-05, acc 1
2017-08-08T17:35:54.501452: step 14230, loss 0.000237681, acc 1
2017-08-08T17:35:54.754010: step 14231, loss 0.0469397, acc 0.984375
2017-08-08T17:35:55.189983: step 14232, loss 5.90766e-05, acc 1
2017-08-08T17:35:55.434569: step 14233, loss 1.36401e-05, acc 1
2017-08-08T17:35:55.649818: step 14234, loss 0.0555449, acc 0.984375
2017-08-08T17:35:55.888759: step 14235, loss 6.96088e-05, acc 1
2017-08-08T17:35:56.068843: step 14236, loss 0.0114987, acc 0.984375
2017-08-08T17:35:56.390837: step 14237, loss 2.19225e-06, acc 1
2017-08-08T17:35:56.738305: step 14238, loss 6.3226e-06, acc 1
2017-08-08T17:35:57.057870: step 14239, loss 0.000101885, acc 1
2017-08-08T17:35:57.308597: step 14240, loss 5.42003e-05, acc 1
2017-08-08T17:35:57.519995: step 14241, loss 0.000101474, acc 1
2017-08-08T17:35:57.904738: step 14242, loss 2.22019e-06, acc 1
2017-08-08T17:35:58.265815: step 14243, loss 0.00013162, acc 1
2017-08-08T17:35:58.536112: step 14244, loss 0.0012231, acc 1
2017-08-08T17:35:58.743106: step 14245, loss 0.000645975, acc 1
2017-08-08T17:35:58.961951: step 14246, loss 0.00211455, acc 1
2017-08-08T17:35:59.331097: step 14247, loss 3.79977e-07, acc 1
2017-08-08T17:35:59.653379: step 14248, loss 0.000106975, acc 1
2017-08-08T17:35:59.897351: step 14249, loss 1.04306e-06, acc 1
2017-08-08T17:36:00.174482: step 14250, loss 2.07097e-05, acc 1
2017-08-08T17:36:00.381475: step 14251, loss 0.000164685, acc 1
2017-08-08T17:36:00.753635: step 14252, loss 1.21235e-05, acc 1
2017-08-08T17:36:00.988618: step 14253, loss 1.02698e-05, acc 1
2017-08-08T17:36:01.214928: step 14254, loss 8.21405e-07, acc 1
2017-08-08T17:36:01.513998: step 14255, loss 5.23271e-05, acc 1
2017-08-08T17:36:01.935271: step 14256, loss 2.39531e-06, acc 1
2017-08-08T17:36:02.337391: step 14257, loss 2.79015e-06, acc 1
2017-08-08T17:36:02.745183: step 14258, loss 4.58871e-05, acc 1
2017-08-08T17:36:03.062811: step 14259, loss 6.9202e-05, acc 1
2017-08-08T17:36:03.285661: step 14260, loss 0.0750719, acc 0.984375
2017-08-08T17:36:03.747794: step 14261, loss 8.46507e-05, acc 1
2017-08-08T17:36:04.079641: step 14262, loss 7.8042e-05, acc 1
2017-08-08T17:36:04.354879: step 14263, loss 2.17685e-05, acc 1
2017-08-08T17:36:04.651828: step 14264, loss 0.00897392, acc 1
2017-08-08T17:36:04.937966: step 14265, loss 1.49348e-05, acc 1
2017-08-08T17:36:05.208701: step 14266, loss 0.0500956, acc 0.984375
2017-08-08T17:36:05.488606: step 14267, loss 1.4684e-05, acc 1
2017-08-08T17:36:05.707903: step 14268, loss 0.000117493, acc 1
2017-08-08T17:36:05.917398: step 14269, loss 1.46166e-05, acc 1
2017-08-08T17:36:06.281591: step 14270, loss 5.01014e-06, acc 1
2017-08-08T17:36:06.480017: step 14271, loss 2.25186e-06, acc 1
2017-08-08T17:36:06.696604: step 14272, loss 2.15501e-06, acc 1
2017-08-08T17:36:06.957570: step 14273, loss 0.000544232, acc 1
2017-08-08T17:36:07.239749: step 14274, loss 0.0080925, acc 1
2017-08-08T17:36:07.569391: step 14275, loss 1.43331e-05, acc 1
2017-08-08T17:36:07.796937: step 14276, loss 2.2761e-06, acc 1
2017-08-08T17:36:07.998828: step 14277, loss 0.0618871, acc 0.984375
2017-08-08T17:36:08.396763: step 14278, loss 7.9539e-05, acc 1
2017-08-08T17:36:08.677368: step 14279, loss 0.00281084, acc 1
2017-08-08T17:36:08.952249: step 14280, loss 0.00111457, acc 1
2017-08-08T17:36:09.207099: step 14281, loss 2.51011e-05, acc 1
2017-08-08T17:36:09.527170: step 14282, loss 0.000374439, acc 1
2017-08-08T17:36:09.888384: step 14283, loss 0.000127055, acc 1
2017-08-08T17:36:10.195339: step 14284, loss 3.50949e-05, acc 1
2017-08-08T17:36:10.531959: step 14285, loss 0.000642044, acc 1
2017-08-08T17:36:10.824239: step 14286, loss 2.52746e-06, acc 1
2017-08-08T17:36:11.235181: step 14287, loss 7.29087e-05, acc 1
2017-08-08T17:36:11.481850: step 14288, loss 0.000671443, acc 1
2017-08-08T17:36:11.712591: step 14289, loss 0.0169693, acc 0.984375
2017-08-08T17:36:11.992803: step 14290, loss 3.59461e-05, acc 1
2017-08-08T17:36:12.339665: step 14291, loss 0.00139327, acc 1
2017-08-08T17:36:12.686688: step 14292, loss 0.000491394, acc 1
2017-08-08T17:36:12.958114: step 14293, loss 6.14672e-08, acc 1
2017-08-08T17:36:13.150583: step 14294, loss 0.000454655, acc 1
2017-08-08T17:36:13.461359: step 14295, loss 7.74841e-07, acc 1
2017-08-08T17:36:13.675106: step 14296, loss 0.000422237, acc 1
2017-08-08T17:36:13.908418: step 14297, loss 0.191196, acc 0.984375
2017-08-08T17:36:14.175882: step 14298, loss 0.00137745, acc 1
2017-08-08T17:36:14.678018: step 14299, loss 0.000629608, acc 1
2017-08-08T17:36:14.920128: step 14300, loss 7.78453e-05, acc 1

Evaluation:
2017-08-08T17:36:15.484948: step 14300, loss 4.98408, acc 0.713884

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-14300

2017-08-08T17:36:16.058827: step 14301, loss 8.84736e-07, acc 1
2017-08-08T17:36:16.248467: step 14302, loss 7.08032e-06, acc 1
2017-08-08T17:36:16.469318: step 14303, loss 1.52916e-06, acc 1
2017-08-08T17:36:16.724251: step 14304, loss 3.67086e-06, acc 1
2017-08-08T17:36:17.031899: step 14305, loss 0.00214688, acc 1
2017-08-08T17:36:17.351602: step 14306, loss 2.07114e-06, acc 1
2017-08-08T17:36:17.562291: step 14307, loss 0.00417228, acc 1
2017-08-08T17:36:17.793338: step 14308, loss 0.000346615, acc 1
2017-08-08T17:36:18.045339: step 14309, loss 0.000188791, acc 1
2017-08-08T17:36:18.220842: step 14310, loss 0.000360142, acc 1
2017-08-08T17:36:18.395419: step 14311, loss 2.42144e-08, acc 1
2017-08-08T17:36:18.597910: step 14312, loss 2.73106e-05, acc 1
2017-08-08T17:36:18.892580: step 14313, loss 1.64052e-05, acc 1
2017-08-08T17:36:19.139698: step 14314, loss 6.79851e-07, acc 1
2017-08-08T17:36:19.352049: step 14315, loss 3.14174e-05, acc 1
2017-08-08T17:36:19.567083: step 14316, loss 0.00170263, acc 1
2017-08-08T17:36:19.837319: step 14317, loss 5.80917e-05, acc 1
2017-08-08T17:36:20.033459: step 14318, loss 0.000772661, acc 1
2017-08-08T17:36:20.219469: step 14319, loss 1.72778e-05, acc 1
2017-08-08T17:36:20.449395: step 14320, loss 0.0067109, acc 1
2017-08-08T17:36:20.751620: step 14321, loss 8.1173e-06, acc 1
2017-08-08T17:36:21.052462: step 14322, loss 9.14798e-05, acc 1
2017-08-08T17:36:21.423064: step 14323, loss 1.03814e-05, acc 1
2017-08-08T17:36:21.689319: step 14324, loss 0.000339315, acc 1
2017-08-08T17:36:21.940342: step 14325, loss 5.50561e-06, acc 1
2017-08-08T17:36:22.305273: step 14326, loss 0.000179158, acc 1
2017-08-08T17:36:22.565585: step 14327, loss 0.000298896, acc 1
2017-08-08T17:36:22.773385: step 14328, loss 0.000252705, acc 1
2017-08-08T17:36:22.998920: step 14329, loss 2.28477e-05, acc 1
2017-08-08T17:36:23.259190: step 14330, loss 1.99302e-07, acc 1
2017-08-08T17:36:23.556070: step 14331, loss 4.47035e-08, acc 1
2017-08-08T17:36:23.832289: step 14332, loss 9.59321e-05, acc 1
2017-08-08T17:36:24.109530: step 14333, loss 2.40281e-07, acc 1
2017-08-08T17:36:24.353713: step 14334, loss 9.38755e-07, acc 1
2017-08-08T17:36:24.791317: step 14335, loss 5.13502e-06, acc 1
2017-08-08T17:36:25.067133: step 14336, loss 2.54538e-05, acc 1
2017-08-08T17:36:25.328727: step 14337, loss 2.56806e-05, acc 1
2017-08-08T17:36:25.597389: step 14338, loss 7.50284e-06, acc 1
2017-08-08T17:36:25.881388: step 14339, loss 0.0124884, acc 0.984375
2017-08-08T17:36:26.177704: step 14340, loss 3.74315e-05, acc 1
2017-08-08T17:36:26.380250: step 14341, loss 1.65582e-06, acc 1
2017-08-08T17:36:26.579251: step 14342, loss 7.0121e-06, acc 1
2017-08-08T17:36:26.884146: step 14343, loss 7.98361e-05, acc 1
2017-08-08T17:36:27.107975: step 14344, loss 6.84712e-05, acc 1
2017-08-08T17:36:27.342346: step 14345, loss 7.54361e-07, acc 1
2017-08-08T17:36:27.644362: step 14346, loss 0.0334018, acc 0.984375
2017-08-08T17:36:28.033662: step 14347, loss 0.000952811, acc 1
2017-08-08T17:36:28.444896: step 14348, loss 4.95425e-06, acc 1
2017-08-08T17:36:28.723043: step 14349, loss 0.00129564, acc 1
2017-08-08T17:36:29.000078: step 14350, loss 0.000370365, acc 1
2017-08-08T17:36:29.296712: step 14351, loss 1.63534e-06, acc 1
2017-08-08T17:36:29.537705: step 14352, loss 9.92769e-07, acc 1
2017-08-08T17:36:29.768235: step 14353, loss 0.00385179, acc 1
2017-08-08T17:36:30.057332: step 14354, loss 1.41583e-05, acc 1
2017-08-08T17:36:30.481324: step 14355, loss 0.000391129, acc 1
2017-08-08T17:36:30.861196: step 14356, loss 3.89929e-05, acc 1
2017-08-08T17:36:31.183111: step 14357, loss 0.00075602, acc 1
2017-08-08T17:36:31.505945: step 14358, loss 1.41561e-07, acc 1
2017-08-08T17:36:31.833301: step 14359, loss 0.000158303, acc 1
2017-08-08T17:36:32.057361: step 14360, loss 9.73796e-06, acc 1
2017-08-08T17:36:32.270903: step 14361, loss 9.85317e-07, acc 1
2017-08-08T17:36:32.598245: step 14362, loss 2.5751e-05, acc 1
2017-08-08T17:36:32.892244: step 14363, loss 1.42677e-06, acc 1
2017-08-08T17:36:33.206793: step 14364, loss 0.00101683, acc 1
2017-08-08T17:36:33.459608: step 14365, loss 2.6077e-07, acc 1
2017-08-08T17:36:33.673507: step 14366, loss 1.75271e-06, acc 1
2017-08-08T17:36:34.045428: step 14367, loss 3.57627e-07, acc 1
2017-08-08T17:36:34.368001: step 14368, loss 0.00186206, acc 1
2017-08-08T17:36:34.714654: step 14369, loss 4.11434e-06, acc 1
2017-08-08T17:36:34.997353: step 14370, loss 1.45654e-06, acc 1
2017-08-08T17:36:35.295078: step 14371, loss 3.14786e-07, acc 1
2017-08-08T17:36:35.590104: step 14372, loss 5.24557e-05, acc 1
2017-08-08T17:36:35.900464: step 14373, loss 2.271e-05, acc 1
2017-08-08T17:36:36.123004: step 14374, loss 7.45056e-08, acc 1
2017-08-08T17:36:36.320096: step 14375, loss 5.42423e-05, acc 1
2017-08-08T17:36:36.556963: step 14376, loss 2.71786e-05, acc 1
2017-08-08T17:36:36.885352: step 14377, loss 0.000433552, acc 1
2017-08-08T17:36:37.106339: step 14378, loss 1.5073e-05, acc 1
2017-08-08T17:36:37.332207: step 14379, loss 0.000353551, acc 1
2017-08-08T17:36:37.550135: step 14380, loss 2.52306e-05, acc 1
2017-08-08T17:36:37.838461: step 14381, loss 1.0455e-05, acc 1
2017-08-08T17:36:38.232331: step 14382, loss 3.88156e-06, acc 1
2017-08-08T17:36:38.574314: step 14383, loss 0.00038212, acc 1
2017-08-08T17:36:38.796106: step 14384, loss 0.000172738, acc 1
2017-08-08T17:36:39.109429: step 14385, loss 8.11648e-05, acc 1
2017-08-08T17:36:39.485258: step 14386, loss 6.97381e-05, acc 1
2017-08-08T17:36:39.791702: step 14387, loss 0.000103448, acc 1
2017-08-08T17:36:40.069189: step 14388, loss 0.00353268, acc 1
2017-08-08T17:36:40.386476: step 14389, loss 8.53414e-05, acc 1
2017-08-08T17:36:40.755601: step 14390, loss 6.47645e-05, acc 1
2017-08-08T17:36:41.138209: step 14391, loss 3.49542e-05, acc 1
2017-08-08T17:36:41.385383: step 14392, loss 2.72668e-06, acc 1
2017-08-08T17:36:41.647847: step 14393, loss 0.0251284, acc 0.984375
2017-08-08T17:36:42.046730: step 14394, loss 0.00113414, acc 1
2017-08-08T17:36:42.300507: step 14395, loss 7.9463e-05, acc 1
2017-08-08T17:36:42.584493: step 14396, loss 2.76216e-06, acc 1
2017-08-08T17:36:42.847128: step 14397, loss 3.03609e-07, acc 1
2017-08-08T17:36:43.281458: step 14398, loss 4.03692e-05, acc 1
2017-08-08T17:36:43.634447: step 14399, loss 9.63898e-05, acc 1
2017-08-08T17:36:43.932477: step 14400, loss 0.000134725, acc 1

Evaluation:
2017-08-08T17:36:44.707769: step 14400, loss 5.07568, acc 0.709193

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-14400

2017-08-08T17:36:45.223158: step 14401, loss 1.28522e-07, acc 1
2017-08-08T17:36:45.522014: step 14402, loss 2.68942e-05, acc 1
2017-08-08T17:36:45.975037: step 14403, loss 1.03399e-05, acc 1
2017-08-08T17:36:46.352676: step 14404, loss 7.66709e-05, acc 1
2017-08-08T17:36:46.600827: step 14405, loss 0.0138372, acc 0.984375
2017-08-08T17:36:46.926831: step 14406, loss 7.28288e-07, acc 1
2017-08-08T17:36:47.307925: step 14407, loss 5.74082e-05, acc 1
2017-08-08T17:36:47.571155: step 14408, loss 0.000106797, acc 1
2017-08-08T17:36:47.812817: step 14409, loss 7.51868e-06, acc 1
2017-08-08T17:36:48.015315: step 14410, loss 1.08033e-07, acc 1
2017-08-08T17:36:48.245412: step 14411, loss 6.70552e-08, acc 1
2017-08-08T17:36:48.668197: step 14412, loss 5.80397e-05, acc 1
2017-08-08T17:36:49.050527: step 14413, loss 5.64372e-07, acc 1
2017-08-08T17:36:49.312088: step 14414, loss 2.89308e-05, acc 1
2017-08-08T17:36:49.563319: step 14415, loss 1.35776e-05, acc 1
2017-08-08T17:36:49.874086: step 14416, loss 0.0774113, acc 0.984375
2017-08-08T17:36:50.091847: step 14417, loss 2.02066e-05, acc 1
2017-08-08T17:36:50.348858: step 14418, loss 3.56867e-06, acc 1
2017-08-08T17:36:50.633395: step 14419, loss 0.0931339, acc 0.984375
2017-08-08T17:36:51.059569: step 14420, loss 0.000154047, acc 1
2017-08-08T17:36:51.470761: step 14421, loss 2.57512e-05, acc 1
2017-08-08T17:36:51.711616: step 14422, loss 1.40475e-05, acc 1
2017-08-08T17:36:52.079769: step 14423, loss 5.96046e-08, acc 1
2017-08-08T17:36:52.398936: step 14424, loss 7.15901e-06, acc 1
2017-08-08T17:36:52.646405: step 14425, loss 3.05826e-05, acc 1
2017-08-08T17:36:52.935701: step 14426, loss 2.43807e-06, acc 1
2017-08-08T17:36:53.353059: step 14427, loss 5.82531e-06, acc 1
2017-08-08T17:36:53.793018: step 14428, loss 0.000737965, acc 1
2017-08-08T17:36:54.210432: step 14429, loss 2.73117e-05, acc 1
2017-08-08T17:36:54.485318: step 14430, loss 7.07805e-08, acc 1
2017-08-08T17:36:54.732171: step 14431, loss 0.000331188, acc 1
2017-08-08T17:36:55.086319: step 14432, loss 3.83701e-07, acc 1
2017-08-08T17:36:55.483993: step 14433, loss 0.000163319, acc 1
2017-08-08T17:36:55.772996: step 14434, loss 0.000313271, acc 1
2017-08-08T17:36:56.062950: step 14435, loss 2.81084e-05, acc 1
2017-08-08T17:36:56.433712: step 14436, loss 0.000140021, acc 1
2017-08-08T17:36:56.915578: step 14437, loss 9.47865e-05, acc 1
2017-08-08T17:36:57.293827: step 14438, loss 0.00110687, acc 1
2017-08-08T17:36:57.614254: step 14439, loss 3.51072e-06, acc 1
2017-08-08T17:36:57.863542: step 14440, loss 0.0745902, acc 0.984375
2017-08-08T17:36:58.153926: step 14441, loss 4.65443e-06, acc 1
2017-08-08T17:36:58.601513: step 14442, loss 3.72529e-09, acc 1
2017-08-08T17:36:58.881646: step 14443, loss 1.59531e-05, acc 1
2017-08-08T17:36:59.197197: step 14444, loss 1.89152e-05, acc 1
2017-08-08T17:36:59.572889: step 14445, loss 4.2885e-05, acc 1
2017-08-08T17:36:59.961962: step 14446, loss 2.75837e-06, acc 1
2017-08-08T17:37:00.346514: step 14447, loss 1.15481e-06, acc 1
2017-08-08T17:37:00.710103: step 14448, loss 6.8424e-06, acc 1
2017-08-08T17:37:00.972266: step 14449, loss 0.000340309, acc 1
2017-08-08T17:37:01.283410: step 14450, loss 2.21643e-06, acc 1
2017-08-08T17:37:01.687863: step 14451, loss 0.0444057, acc 0.984375
2017-08-08T17:37:01.973430: step 14452, loss 0.000258468, acc 1
2017-08-08T17:37:02.263938: step 14453, loss 1.12315e-06, acc 1
2017-08-08T17:37:02.513403: step 14454, loss 2.77356e-05, acc 1
2017-08-08T17:37:03.014768: step 14455, loss 0.000324076, acc 1
2017-08-08T17:37:03.483041: step 14456, loss 6.46282e-06, acc 1
2017-08-08T17:37:03.901698: step 14457, loss 7.90511e-05, acc 1
2017-08-08T17:37:04.169803: step 14458, loss 5.6894e-05, acc 1
2017-08-08T17:37:04.553428: step 14459, loss 8.96486e-05, acc 1
2017-08-08T17:37:04.929296: step 14460, loss 7.11379e-06, acc 1
2017-08-08T17:37:05.204042: step 14461, loss 4.01492e-05, acc 1
2017-08-08T17:37:05.540626: step 14462, loss 1.21072e-07, acc 1
2017-08-08T17:37:05.840120: step 14463, loss 0.000706968, acc 1
2017-08-08T17:37:06.353451: step 14464, loss 0.000371586, acc 1
2017-08-08T17:37:06.731227: step 14465, loss 0.000697136, acc 1
2017-08-08T17:37:07.035536: step 14466, loss 5.12552e-05, acc 1
2017-08-08T17:37:07.325792: step 14467, loss 1.58136e-06, acc 1
2017-08-08T17:37:07.827099: step 14468, loss 2.57398e-06, acc 1
2017-08-08T17:37:08.139747: step 14469, loss 0.00373312, acc 1
2017-08-08T17:37:08.439964: step 14470, loss 0.0612088, acc 0.984375
2017-08-08T17:37:08.710010: step 14471, loss 9.72291e-07, acc 1
2017-08-08T17:37:09.145407: step 14472, loss 1.01858e-05, acc 1
2017-08-08T17:37:09.613141: step 14473, loss 3.87622e-05, acc 1
2017-08-08T17:37:09.909520: step 14474, loss 0.00131781, acc 1
2017-08-08T17:37:10.172776: step 14475, loss 0.00179911, acc 1
2017-08-08T17:37:10.410386: step 14476, loss 0.0033893, acc 1
2017-08-08T17:37:10.765739: step 14477, loss 0.0877693, acc 0.984375
2017-08-08T17:37:10.999735: step 14478, loss 4.11508e-05, acc 1
2017-08-08T17:37:11.274370: step 14479, loss 0.0022603, acc 1
2017-08-08T17:37:11.529376: step 14480, loss 0.000417072, acc 1
2017-08-08T17:37:12.006603: step 14481, loss 0.00749094, acc 1
2017-08-08T17:37:12.417929: step 14482, loss 0.000635434, acc 1
2017-08-08T17:37:12.750106: step 14483, loss 0.000425543, acc 1
2017-08-08T17:37:13.016871: step 14484, loss 0.000283218, acc 1
2017-08-08T17:37:13.215713: step 14485, loss 2.8246e-05, acc 1
2017-08-08T17:37:13.568608: step 14486, loss 0.0135746, acc 0.984375
2017-08-08T17:37:13.854577: step 14487, loss 0.00451807, acc 1
2017-08-08T17:37:14.095605: step 14488, loss 0.000812351, acc 1
2017-08-08T17:37:14.361379: step 14489, loss 5.87048e-06, acc 1
2017-08-08T17:37:14.774557: step 14490, loss 0.000546073, acc 1
2017-08-08T17:37:15.080709: step 14491, loss 6.92325e-05, acc 1
2017-08-08T17:37:15.389654: step 14492, loss 0.000465425, acc 1
2017-08-08T17:37:15.704698: step 14493, loss 9.84919e-05, acc 1
2017-08-08T17:37:15.953005: step 14494, loss 0.0116393, acc 1
2017-08-08T17:37:16.375640: step 14495, loss 4.65661e-08, acc 1
2017-08-08T17:37:16.696179: step 14496, loss 0.000120044, acc 1
2017-08-08T17:37:17.048589: step 14497, loss 0.00255011, acc 1
2017-08-08T17:37:17.412550: step 14498, loss 6.43399e-05, acc 1
2017-08-08T17:37:17.668346: step 14499, loss 0.000751633, acc 1
2017-08-08T17:37:17.900042: step 14500, loss 0.000304935, acc 1

Evaluation:
2017-08-08T17:37:18.457728: step 14500, loss 4.92235, acc 0.72045

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-14500

2017-08-08T17:37:19.023907: step 14501, loss 0.000308325, acc 1
2017-08-08T17:37:19.303580: step 14502, loss 4.32263e-06, acc 1
2017-08-08T17:37:19.606748: step 14503, loss 2.3714e-05, acc 1
2017-08-08T17:37:19.989446: step 14504, loss 4.56621e-05, acc 1
2017-08-08T17:37:20.424435: step 14505, loss 1.35225e-06, acc 1
2017-08-08T17:37:20.765514: step 14506, loss 0.000329222, acc 1
2017-08-08T17:37:21.021169: step 14507, loss 0.00143118, acc 1
2017-08-08T17:37:21.269730: step 14508, loss 9.71955e-06, acc 1
2017-08-08T17:37:21.682286: step 14509, loss 0.000247516, acc 1
2017-08-08T17:37:21.931615: step 14510, loss 1.72927e-05, acc 1
2017-08-08T17:37:22.189595: step 14511, loss 0.00385728, acc 1
2017-08-08T17:37:22.425263: step 14512, loss 0.0346594, acc 0.984375
2017-08-08T17:37:22.783785: step 14513, loss 7.66348e-05, acc 1
2017-08-08T17:37:23.077578: step 14514, loss 7.93694e-06, acc 1
2017-08-08T17:37:23.382950: step 14515, loss 6.50059e-07, acc 1
2017-08-08T17:37:23.674021: step 14516, loss 2.26295e-06, acc 1
2017-08-08T17:37:23.926674: step 14517, loss 0.000260863, acc 1
2017-08-08T17:37:24.370009: step 14518, loss 6.39114e-05, acc 1
2017-08-08T17:37:24.623747: step 14519, loss 0.00334279, acc 1
2017-08-08T17:37:24.911970: step 14520, loss 1.03027e-05, acc 1
2017-08-08T17:37:25.188944: step 14521, loss 0.000262917, acc 1
2017-08-08T17:37:25.502861: step 14522, loss 0.000312932, acc 1
2017-08-08T17:37:25.913282: step 14523, loss 6.1056e-05, acc 1
2017-08-08T17:37:26.326597: step 14524, loss 0.00171494, acc 1
2017-08-08T17:37:26.622264: step 14525, loss 0.00667308, acc 1
2017-08-08T17:37:27.019249: step 14526, loss 2.49077e-05, acc 1
2017-08-08T17:37:27.400534: step 14527, loss 8.49066e-06, acc 1
2017-08-08T17:37:27.707675: step 14528, loss 1.86264e-08, acc 1
2017-08-08T17:37:27.930527: step 14529, loss 2.39507e-05, acc 1
2017-08-08T17:37:28.263427: step 14530, loss 0.0117792, acc 0.984375
2017-08-08T17:37:28.640786: step 14531, loss 0.000620609, acc 1
2017-08-08T17:37:29.040587: step 14532, loss 5.76962e-06, acc 1
2017-08-08T17:37:29.335980: step 14533, loss 7.19837e-05, acc 1
2017-08-08T17:37:29.588025: step 14534, loss 1.03932e-05, acc 1
2017-08-08T17:37:29.914803: step 14535, loss 6.13999e-06, acc 1
2017-08-08T17:37:30.241481: step 14536, loss 0.000132259, acc 1
2017-08-08T17:37:30.536635: step 14537, loss 0.000328312, acc 1
2017-08-08T17:37:30.865605: step 14538, loss 1.6577e-06, acc 1
2017-08-08T17:37:31.122451: step 14539, loss 7.04723e-05, acc 1
2017-08-08T17:37:31.523071: step 14540, loss 0.00303691, acc 1
2017-08-08T17:37:31.795340: step 14541, loss 1.2244e-05, acc 1
2017-08-08T17:37:32.175490: step 14542, loss 0.000133432, acc 1
2017-08-08T17:37:32.528347: step 14543, loss 0.00534808, acc 1
2017-08-08T17:37:32.777109: step 14544, loss 2.34693e-07, acc 1
2017-08-08T17:37:33.143364: step 14545, loss 3.22963e-06, acc 1
2017-08-08T17:37:33.333537: step 14546, loss 2.24811e-06, acc 1
2017-08-08T17:37:33.697406: step 14547, loss 8.07481e-05, acc 1
2017-08-08T17:37:33.913016: step 14548, loss 0.00095701, acc 1
2017-08-08T17:37:34.213387: step 14549, loss 0.000653962, acc 1
2017-08-08T17:37:34.573040: step 14550, loss 5.88207e-06, acc 1
2017-08-08T17:37:34.850199: step 14551, loss 0.00416692, acc 1
2017-08-08T17:37:35.251751: step 14552, loss 0.000858369, acc 1
2017-08-08T17:37:35.626284: step 14553, loss 2.58522e-06, acc 1
2017-08-08T17:37:36.005380: step 14554, loss 0.000773636, acc 1
2017-08-08T17:37:36.371097: step 14555, loss 6.66818e-07, acc 1
2017-08-08T17:37:36.654365: step 14556, loss 5.46605e-05, acc 1
2017-08-08T17:37:37.089257: step 14557, loss 2.98198e-06, acc 1
2017-08-08T17:37:37.380384: step 14558, loss 1.11106e-05, acc 1
2017-08-08T17:37:37.688785: step 14559, loss 0.000642717, acc 1
2017-08-08T17:37:38.048434: step 14560, loss 2.26488e-06, acc 1
2017-08-08T17:37:38.329850: step 14561, loss 1.11746e-05, acc 1
2017-08-08T17:37:38.786949: step 14562, loss 4.82739e-06, acc 1
2017-08-08T17:37:39.076375: step 14563, loss 3.91155e-08, acc 1
2017-08-08T17:37:39.457462: step 14564, loss 0.000688832, acc 1
2017-08-08T17:37:39.774169: step 14565, loss 7.03068e-06, acc 1
2017-08-08T17:37:40.039529: step 14566, loss 1.72505e-05, acc 1
2017-08-08T17:37:40.523637: step 14567, loss 0.000131847, acc 1
2017-08-08T17:37:40.781692: step 14568, loss 0.000395113, acc 1
2017-08-08T17:37:41.168413: step 14569, loss 0.000268899, acc 1
2017-08-08T17:37:41.512481: step 14570, loss 6.62318e-05, acc 1
2017-08-08T17:37:41.811864: step 14571, loss 3.22873e-05, acc 1
2017-08-08T17:37:42.210100: step 14572, loss 1.27961e-06, acc 1
2017-08-08T17:37:42.543376: step 14573, loss 2.18291e-06, acc 1
2017-08-08T17:37:42.812861: step 14574, loss 0.000378076, acc 1
2017-08-08T17:37:43.093441: step 14575, loss 0.000766564, acc 1
2017-08-08T17:37:43.473981: step 14576, loss 6.99863e-06, acc 1
2017-08-08T17:37:43.914710: step 14577, loss 0.00111632, acc 1
2017-08-08T17:37:44.327424: step 14578, loss 1.12502e-06, acc 1
2017-08-08T17:37:44.643893: step 14579, loss 0.00397729, acc 1
2017-08-08T17:37:44.871527: step 14580, loss 1.65154e-05, acc 1
2017-08-08T17:37:45.264149: step 14581, loss 0.0271874, acc 0.984375
2017-08-08T17:37:45.576407: step 14582, loss 0.000167602, acc 1
2017-08-08T17:37:45.831275: step 14583, loss 9.872e-08, acc 1
2017-08-08T17:37:46.120894: step 14584, loss 1.61572e-05, acc 1
2017-08-08T17:37:46.480319: step 14585, loss 1.25928e-05, acc 1
2017-08-08T17:37:46.902415: step 14586, loss 7.45058e-09, acc 1
2017-08-08T17:37:47.304715: step 14587, loss 1.74712e-06, acc 1
2017-08-08T17:37:47.556248: step 14588, loss 6.51925e-08, acc 1
2017-08-08T17:37:47.802384: step 14589, loss 2.51456e-07, acc 1
2017-08-08T17:37:48.113625: step 14590, loss 3.33576e-06, acc 1
2017-08-08T17:37:48.422511: step 14591, loss 6.80782e-05, acc 1
2017-08-08T17:37:48.622349: step 14592, loss 3.74389e-07, acc 1
2017-08-08T17:37:48.800536: step 14593, loss 8.30896e-06, acc 1
2017-08-08T17:37:48.997576: step 14594, loss 5.62273e-06, acc 1
2017-08-08T17:37:49.195821: step 14595, loss 1.25594e-05, acc 1
2017-08-08T17:37:49.491740: step 14596, loss 0.000108806, acc 1
2017-08-08T17:37:49.953530: step 14597, loss 0.000349668, acc 1
2017-08-08T17:37:50.273225: step 14598, loss 0.0145451, acc 0.984375
2017-08-08T17:37:50.607881: step 14599, loss 1.13618e-06, acc 1
2017-08-08T17:37:50.903657: step 14600, loss 2.64494e-07, acc 1

Evaluation:
2017-08-08T17:37:51.731279: step 14600, loss 4.93186, acc 0.716698

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-14600

2017-08-08T17:37:52.346463: step 14601, loss 9.59246e-07, acc 1
2017-08-08T17:37:52.692951: step 14602, loss 9.01766e-06, acc 1
2017-08-08T17:37:53.026275: step 14603, loss 1.01884e-06, acc 1
2017-08-08T17:37:53.272441: step 14604, loss 1.3545e-05, acc 1
2017-08-08T17:37:53.499055: step 14605, loss 4.60305e-05, acc 1
2017-08-08T17:37:53.905590: step 14606, loss 0.000193281, acc 1
2017-08-08T17:37:54.210096: step 14607, loss 0.000567609, acc 1
2017-08-08T17:37:54.499496: step 14608, loss 7.89751e-07, acc 1
2017-08-08T17:37:54.800953: step 14609, loss 5.45746e-07, acc 1
2017-08-08T17:37:55.157393: step 14610, loss 0.0169257, acc 0.984375
2017-08-08T17:37:55.593377: step 14611, loss 1.83839e-06, acc 1
2017-08-08T17:37:56.013283: step 14612, loss 1.38401e-05, acc 1
2017-08-08T17:37:56.347708: step 14613, loss 3.09198e-07, acc 1
2017-08-08T17:37:56.608210: step 14614, loss 0.000353218, acc 1
2017-08-08T17:37:57.041420: step 14615, loss 7.81227e-05, acc 1
2017-08-08T17:37:57.361341: step 14616, loss 1.30385e-08, acc 1
2017-08-08T17:37:57.595952: step 14617, loss 4.65656e-07, acc 1
2017-08-08T17:37:57.818860: step 14618, loss 0.000353422, acc 1
2017-08-08T17:37:58.127374: step 14619, loss 3.61523e-06, acc 1
2017-08-08T17:37:58.560350: step 14620, loss 0.00180637, acc 1
2017-08-08T17:37:58.977375: step 14621, loss 1.39115e-05, acc 1
2017-08-08T17:37:59.238723: step 14622, loss 0.00140812, acc 1
2017-08-08T17:37:59.512667: step 14623, loss 6.5221e-06, acc 1
2017-08-08T17:38:00.091155: step 14624, loss 5.09914e-06, acc 1
2017-08-08T17:38:00.377326: step 14625, loss 0.000340528, acc 1
2017-08-08T17:38:00.749895: step 14626, loss 0.000194054, acc 1
2017-08-08T17:38:01.180951: step 14627, loss 3.61629e-05, acc 1
2017-08-08T17:38:01.586405: step 14628, loss 0.000383475, acc 1
2017-08-08T17:38:01.949349: step 14629, loss 1.00381e-05, acc 1
2017-08-08T17:38:02.300194: step 14630, loss 2.1703e-05, acc 1
2017-08-08T17:38:02.761502: step 14631, loss 7.8231e-08, acc 1
2017-08-08T17:38:03.217833: step 14632, loss 0.077787, acc 0.984375
2017-08-08T17:38:03.516041: step 14633, loss 1.72102e-06, acc 1
2017-08-08T17:38:03.939558: step 14634, loss 1.46626e-05, acc 1
2017-08-08T17:38:04.257719: step 14635, loss 0.00145107, acc 1
2017-08-08T17:38:04.697395: step 14636, loss 7.74847e-07, acc 1
2017-08-08T17:38:05.179881: step 14637, loss 3.70009e-05, acc 1
2017-08-08T17:38:05.556670: step 14638, loss 8.1991e-05, acc 1
2017-08-08T17:38:05.800688: step 14639, loss 1.41357e-05, acc 1
2017-08-08T17:38:06.213291: step 14640, loss 8.10242e-07, acc 1
2017-08-08T17:38:06.561800: step 14641, loss 0.000133389, acc 1
2017-08-08T17:38:06.830860: step 14642, loss 4.00466e-07, acc 1
2017-08-08T17:38:07.070146: step 14643, loss 0.000340165, acc 1
2017-08-08T17:38:07.508933: step 14644, loss 0.00679985, acc 1
2017-08-08T17:38:07.870034: step 14645, loss 0.000112702, acc 1
2017-08-08T17:38:08.271629: step 14646, loss 6.30111e-05, acc 1
2017-08-08T17:38:08.595052: step 14647, loss 0.000635787, acc 1
2017-08-08T17:38:08.848566: step 14648, loss 0.000292519, acc 1
2017-08-08T17:38:09.265873: step 14649, loss 5.79757e-06, acc 1
2017-08-08T17:38:09.566920: step 14650, loss 4.67521e-07, acc 1
2017-08-08T17:38:09.872058: step 14651, loss 0.000691668, acc 1
2017-08-08T17:38:10.157145: step 14652, loss 2.47411e-05, acc 1
2017-08-08T17:38:10.421240: step 14653, loss 0.000326544, acc 1
2017-08-08T17:38:10.741789: step 14654, loss 0.000627419, acc 1
2017-08-08T17:38:11.113435: step 14655, loss 5.16486e-05, acc 1
2017-08-08T17:38:11.483238: step 14656, loss 0.107724, acc 0.984375
2017-08-08T17:38:11.731931: step 14657, loss 2.73406e-05, acc 1
2017-08-08T17:38:11.984389: step 14658, loss 0.000940247, acc 1
2017-08-08T17:38:12.444473: step 14659, loss 1.31654e-05, acc 1
2017-08-08T17:38:12.735366: step 14660, loss 0.000600371, acc 1
2017-08-08T17:38:12.998642: step 14661, loss 0.000161515, acc 1
2017-08-08T17:38:13.262300: step 14662, loss 2.34692e-07, acc 1
2017-08-08T17:38:13.695569: step 14663, loss 0.0101292, acc 1
2017-08-08T17:38:14.197393: step 14664, loss 3.84395e-05, acc 1
2017-08-08T17:38:14.504965: step 14665, loss 1.05254e-05, acc 1
2017-08-08T17:38:14.758766: step 14666, loss 9.31517e-05, acc 1
2017-08-08T17:38:15.019126: step 14667, loss 0.000114757, acc 1
2017-08-08T17:38:15.403509: step 14668, loss 4.33343e-05, acc 1
2017-08-08T17:38:15.625641: step 14669, loss 9.5179e-07, acc 1
2017-08-08T17:38:15.911252: step 14670, loss 2.51253e-06, acc 1
2017-08-08T17:38:16.317383: step 14671, loss 2.65221e-06, acc 1
2017-08-08T17:38:16.714437: step 14672, loss 5.1533e-06, acc 1
2017-08-08T17:38:17.181924: step 14673, loss 2.35805e-06, acc 1
2017-08-08T17:38:17.460632: step 14674, loss 1.695e-07, acc 1
2017-08-08T17:38:17.841953: step 14675, loss 7.42014e-05, acc 1
2017-08-08T17:38:18.227083: step 14676, loss 0.000219458, acc 1
2017-08-08T17:38:18.509150: step 14677, loss 3.37663e-06, acc 1
2017-08-08T17:38:18.802860: step 14678, loss 0.000305313, acc 1
2017-08-08T17:38:19.084119: step 14679, loss 0.00169869, acc 1
2017-08-08T17:38:19.484432: step 14680, loss 7.8629e-05, acc 1
2017-08-08T17:38:19.821376: step 14681, loss 6.38874e-07, acc 1
2017-08-08T17:38:20.208121: step 14682, loss 3.94721e-05, acc 1
2017-08-08T17:38:20.518538: step 14683, loss 8.59729e-06, acc 1
2017-08-08T17:38:20.742683: step 14684, loss 6.50057e-07, acc 1
2017-08-08T17:38:21.137357: step 14685, loss 0.0119107, acc 0.984375
2017-08-08T17:38:21.482783: step 14686, loss 2.17171e-06, acc 1
2017-08-08T17:38:21.754178: step 14687, loss 1.34098e-05, acc 1
2017-08-08T17:38:22.018735: step 14688, loss 3.40457e-05, acc 1
2017-08-08T17:38:22.261426: step 14689, loss 4.71598e-06, acc 1
2017-08-08T17:38:22.722284: step 14690, loss 1.13089e-05, acc 1
2017-08-08T17:38:23.053630: step 14691, loss 4.80921e-05, acc 1
2017-08-08T17:38:23.438304: step 14692, loss 3.1477e-05, acc 1
2017-08-08T17:38:23.689489: step 14693, loss 0.000233428, acc 1
2017-08-08T17:38:24.065382: step 14694, loss 2.55186e-05, acc 1
2017-08-08T17:38:24.400968: step 14695, loss 8.95909e-07, acc 1
2017-08-08T17:38:24.660305: step 14696, loss 0.00160399, acc 1
2017-08-08T17:38:24.973001: step 14697, loss 0.000147252, acc 1
2017-08-08T17:38:25.309962: step 14698, loss 1.00583e-07, acc 1
2017-08-08T17:38:25.725187: step 14699, loss 0.00288178, acc 1
2017-08-08T17:38:26.125646: step 14700, loss 2.78155e-08, acc 1

Evaluation:
2017-08-08T17:38:27.070201: step 14700, loss 4.90027, acc 0.713884

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-14700

2017-08-08T17:38:27.694807: step 14701, loss 1.16208e-05, acc 1
2017-08-08T17:38:27.995462: step 14702, loss 3.59952e-05, acc 1
2017-08-08T17:38:28.361564: step 14703, loss 3.87592e-06, acc 1
2017-08-08T17:38:28.725768: step 14704, loss 4.30151e-05, acc 1
2017-08-08T17:38:29.151684: step 14705, loss 0.0402322, acc 0.984375
2017-08-08T17:38:29.486417: step 14706, loss 3.6337e-05, acc 1
2017-08-08T17:38:29.709059: step 14707, loss 9.61118e-07, acc 1
2017-08-08T17:38:29.952847: step 14708, loss 5.63784e-06, acc 1
2017-08-08T17:38:30.443822: step 14709, loss 2.85146e-06, acc 1
2017-08-08T17:38:30.734216: step 14710, loss 0.0908736, acc 0.984375
2017-08-08T17:38:31.084320: step 14711, loss 1.08485e-05, acc 1
2017-08-08T17:38:31.432223: step 14712, loss 1.40332e-05, acc 1
2017-08-08T17:38:31.861418: step 14713, loss 0.000152033, acc 1
2017-08-08T17:38:32.230867: step 14714, loss 1.67958e-05, acc 1
2017-08-08T17:38:32.552661: step 14715, loss 0.00192421, acc 1
2017-08-08T17:38:32.805524: step 14716, loss 5.61003e-05, acc 1
2017-08-08T17:38:33.193681: step 14717, loss 6.91027e-07, acc 1
2017-08-08T17:38:33.613517: step 14718, loss 6.72825e-05, acc 1
2017-08-08T17:38:33.904819: step 14719, loss 1.0114e-06, acc 1
2017-08-08T17:38:34.184997: step 14720, loss 0.00260933, acc 1
2017-08-08T17:38:34.522186: step 14721, loss 1.24797e-07, acc 1
2017-08-08T17:38:34.870609: step 14722, loss 8.1426e-06, acc 1
2017-08-08T17:38:35.144693: step 14723, loss 2.46694e-05, acc 1
2017-08-08T17:38:35.376484: step 14724, loss 0.000336791, acc 1
2017-08-08T17:38:35.585366: step 14725, loss 1.26471e-06, acc 1
2017-08-08T17:38:35.936679: step 14726, loss 9.82821e-06, acc 1
2017-08-08T17:38:36.204717: step 14727, loss 1.59799e-05, acc 1
2017-08-08T17:38:36.457945: step 14728, loss 7.33868e-07, acc 1
2017-08-08T17:38:36.733873: step 14729, loss 1.43423e-07, acc 1
2017-08-08T17:38:37.195213: step 14730, loss 3.5798e-06, acc 1
2017-08-08T17:38:37.537404: step 14731, loss 2.30424e-05, acc 1
2017-08-08T17:38:37.857850: step 14732, loss 3.76587e-05, acc 1
2017-08-08T17:38:38.086863: step 14733, loss 1.92779e-06, acc 1
2017-08-08T17:38:38.325640: step 14734, loss 1.47859e-05, acc 1
2017-08-08T17:38:38.812879: step 14735, loss 4.70436e-06, acc 1
2017-08-08T17:38:39.107985: step 14736, loss 0.000157608, acc 1
2017-08-08T17:38:39.460327: step 14737, loss 0.000103894, acc 1
2017-08-08T17:38:39.885042: step 14738, loss 1.89611e-06, acc 1
2017-08-08T17:38:40.272740: step 14739, loss 0.00174265, acc 1
2017-08-08T17:38:40.619108: step 14740, loss 1.85141e-06, acc 1
2017-08-08T17:38:40.832957: step 14741, loss 0.000142929, acc 1
2017-08-08T17:38:41.056815: step 14742, loss 0.000313402, acc 1
2017-08-08T17:38:41.439293: step 14743, loss 0.00490879, acc 1
2017-08-08T17:38:41.745490: step 14744, loss 0.000150887, acc 1
2017-08-08T17:38:41.975842: step 14745, loss 2.92041e-06, acc 1
2017-08-08T17:38:42.242450: step 14746, loss 0.000578297, acc 1
2017-08-08T17:38:42.622941: step 14747, loss 6.69866e-05, acc 1
2017-08-08T17:38:43.050931: step 14748, loss 8.21589e-06, acc 1
2017-08-08T17:38:43.355961: step 14749, loss 8.18532e-05, acc 1
2017-08-08T17:38:43.579632: step 14750, loss 1.4537e-05, acc 1
2017-08-08T17:38:43.981393: step 14751, loss 6.23462e-05, acc 1
2017-08-08T17:38:44.288356: step 14752, loss 9.46465e-05, acc 1
2017-08-08T17:38:44.589282: step 14753, loss 8.28111e-06, acc 1
2017-08-08T17:38:44.922241: step 14754, loss 1.117e-05, acc 1
2017-08-08T17:38:45.281325: step 14755, loss 6.61837e-05, acc 1
2017-08-08T17:38:45.627916: step 14756, loss 6.34542e-06, acc 1
2017-08-08T17:38:45.874815: step 14757, loss 0.000384666, acc 1
2017-08-08T17:38:46.100343: step 14758, loss 8.76805e-05, acc 1
2017-08-08T17:38:46.402403: step 14759, loss 4.66442e-05, acc 1
2017-08-08T17:38:46.796748: step 14760, loss 3.22576e-05, acc 1
2017-08-08T17:38:47.088782: step 14761, loss 0.000109858, acc 1
2017-08-08T17:38:47.325013: step 14762, loss 3.35275e-07, acc 1
2017-08-08T17:38:47.777419: step 14763, loss 0.0654215, acc 0.984375
2017-08-08T17:38:48.140827: step 14764, loss 2.11218e-06, acc 1
2017-08-08T17:38:48.398070: step 14765, loss 7.97299e-06, acc 1
2017-08-08T17:38:48.653352: step 14766, loss 3.09005e-06, acc 1
2017-08-08T17:38:49.063914: step 14767, loss 2.64494e-07, acc 1
2017-08-08T17:38:49.397106: step 14768, loss 1.09896e-07, acc 1
2017-08-08T17:38:49.648224: step 14769, loss 4.72226e-05, acc 1
2017-08-08T17:38:49.939204: step 14770, loss 0.000762993, acc 1
2017-08-08T17:38:50.298962: step 14771, loss 0.037412, acc 0.984375
2017-08-08T17:38:50.691554: step 14772, loss 0.0122117, acc 0.984375
2017-08-08T17:38:51.029596: step 14773, loss 3.10287e-05, acc 1
2017-08-08T17:38:51.284723: step 14774, loss 1.99302e-07, acc 1
2017-08-08T17:38:51.539528: step 14775, loss 0.000161272, acc 1
2017-08-08T17:38:51.973366: step 14776, loss 0.000238717, acc 1
2017-08-08T17:38:52.237006: step 14777, loss 7.08675e-05, acc 1
2017-08-08T17:38:52.523836: step 14778, loss 3.0844e-06, acc 1
2017-08-08T17:38:52.833393: step 14779, loss 1.85994e-05, acc 1
2017-08-08T17:38:53.309366: step 14780, loss 8.41826e-05, acc 1
2017-08-08T17:38:53.760870: step 14781, loss 2.62287e-05, acc 1
2017-08-08T17:38:54.099856: step 14782, loss 1.68459e-05, acc 1
2017-08-08T17:38:54.359272: step 14783, loss 0.000325038, acc 1
2017-08-08T17:38:54.811068: step 14784, loss 7.51147e-06, acc 1
2017-08-08T17:38:55.203882: step 14785, loss 2.18102e-06, acc 1
2017-08-08T17:38:55.491576: step 14786, loss 4.0135e-06, acc 1
2017-08-08T17:38:55.756439: step 14787, loss 2.73722e-05, acc 1
2017-08-08T17:38:56.012291: step 14788, loss 1.63348e-06, acc 1
2017-08-08T17:38:56.387646: step 14789, loss 0.00138016, acc 1
2017-08-08T17:38:56.745722: step 14790, loss 0.000238688, acc 1
2017-08-08T17:38:57.029135: step 14791, loss 7.35827e-06, acc 1
2017-08-08T17:38:57.273836: step 14792, loss 0.00115996, acc 1
2017-08-08T17:38:57.558349: step 14793, loss 8.96667e-06, acc 1
2017-08-08T17:38:57.956741: step 14794, loss 0.000224384, acc 1
2017-08-08T17:38:58.218089: step 14795, loss 2.64244e-05, acc 1
2017-08-08T17:38:58.494101: step 14796, loss 7.19085e-06, acc 1
2017-08-08T17:38:58.780125: step 14797, loss 3.69318e-05, acc 1
2017-08-08T17:38:59.160945: step 14798, loss 0.00030479, acc 1
2017-08-08T17:38:59.570741: step 14799, loss 3.4066e-06, acc 1
2017-08-08T17:38:59.951313: step 14800, loss 9.49925e-07, acc 1

Evaluation:
2017-08-08T17:39:00.664819: step 14800, loss 4.93764, acc 0.71576

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-14800

2017-08-08T17:39:01.296093: step 14801, loss 5.2154e-08, acc 1
2017-08-08T17:39:01.686266: step 14802, loss 9.4077e-06, acc 1
2017-08-08T17:39:02.211755: step 14803, loss 6.6159e-05, acc 1
2017-08-08T17:39:03.159528: step 14804, loss 1.06933e-05, acc 1
2017-08-08T17:39:03.705426: step 14805, loss 9.1268e-07, acc 1
2017-08-08T17:39:04.199582: step 14806, loss 2.25379e-07, acc 1
2017-08-08T17:39:04.422529: step 14807, loss 7.6554e-07, acc 1
2017-08-08T17:39:04.731300: step 14808, loss 0.00347411, acc 1
2017-08-08T17:39:05.057906: step 14809, loss 2.13942e-05, acc 1
2017-08-08T17:39:05.383116: step 14810, loss 9.4832e-06, acc 1
2017-08-08T17:39:05.675801: step 14811, loss 0.0133229, acc 0.984375
2017-08-08T17:39:05.935834: step 14812, loss 0.000268681, acc 1
2017-08-08T17:39:06.251707: step 14813, loss 5.99023e-05, acc 1
2017-08-08T17:39:06.642086: step 14814, loss 0.000240663, acc 1
2017-08-08T17:39:07.054277: step 14815, loss 1.21068e-06, acc 1
2017-08-08T17:39:07.319255: step 14816, loss 0.000224171, acc 1
2017-08-08T17:39:07.625638: step 14817, loss 6.21689e-05, acc 1
2017-08-08T17:39:07.928401: step 14818, loss 3.72529e-08, acc 1
2017-08-08T17:39:08.210988: step 14819, loss 0.000487733, acc 1
2017-08-08T17:39:08.563062: step 14820, loss 1.50653e-05, acc 1
2017-08-08T17:39:08.952401: step 14821, loss 1.80107e-06, acc 1
2017-08-08T17:39:09.327285: step 14822, loss 4.88354e-06, acc 1
2017-08-08T17:39:09.642213: step 14823, loss 0.00124495, acc 1
2017-08-08T17:39:09.906736: step 14824, loss 0.0776481, acc 0.984375
2017-08-08T17:39:10.375571: step 14825, loss 2.84831e-05, acc 1
2017-08-08T17:39:10.630747: step 14826, loss 0.000561165, acc 1
2017-08-08T17:39:10.978402: step 14827, loss 7.26418e-07, acc 1
2017-08-08T17:39:11.254807: step 14828, loss 6.91036e-07, acc 1
2017-08-08T17:39:11.637287: step 14829, loss 0.017022, acc 0.984375
2017-08-08T17:39:12.026027: step 14830, loss 5.97813e-06, acc 1
2017-08-08T17:39:12.390894: step 14831, loss 1.0207e-06, acc 1
2017-08-08T17:39:12.660638: step 14832, loss 0.000319324, acc 1
2017-08-08T17:39:13.018791: step 14833, loss 2.5693e-05, acc 1
2017-08-08T17:39:13.475965: step 14834, loss 3.21046e-05, acc 1
2017-08-08T17:39:13.745356: step 14835, loss 0.152745, acc 0.984375
2017-08-08T17:39:14.013211: step 14836, loss 2.62802e-06, acc 1
2017-08-08T17:39:14.305333: step 14837, loss 4.26543e-07, acc 1
2017-08-08T17:39:14.702427: step 14838, loss 3.16649e-08, acc 1
2017-08-08T17:39:15.057374: step 14839, loss 0.000388557, acc 1
2017-08-08T17:39:15.467242: step 14840, loss 0.000301356, acc 1
2017-08-08T17:39:15.837140: step 14841, loss 0.000273052, acc 1
2017-08-08T17:39:16.106297: step 14842, loss 9.12694e-08, acc 1
2017-08-08T17:39:16.437206: step 14843, loss 4.012e-05, acc 1
2017-08-08T17:39:16.808076: step 14844, loss 1.55151e-06, acc 1
2017-08-08T17:39:17.040060: step 14845, loss 7.66047e-05, acc 1
2017-08-08T17:39:17.322062: step 14846, loss 1.49564e-06, acc 1
2017-08-08T17:39:17.740008: step 14847, loss 3.04404e-05, acc 1
2017-08-08T17:39:18.129670: step 14848, loss 1.64349e-05, acc 1
2017-08-08T17:39:18.485827: step 14849, loss 4.86145e-07, acc 1
2017-08-08T17:39:18.745278: step 14850, loss 5.56793e-05, acc 1
2017-08-08T17:39:18.974060: step 14851, loss 0.00186324, acc 1
2017-08-08T17:39:19.239865: step 14852, loss 1.65775e-07, acc 1
2017-08-08T17:39:19.466602: step 14853, loss 3.11932e-05, acc 1
2017-08-08T17:39:19.721378: step 14854, loss 4.1998e-06, acc 1
2017-08-08T17:39:19.943425: step 14855, loss 6.23687e-05, acc 1
2017-08-08T17:39:20.373410: step 14856, loss 3.29566e-05, acc 1
2017-08-08T17:39:20.828332: step 14857, loss 1.09388e-05, acc 1
2017-08-08T17:39:21.128842: step 14858, loss 2.83483e-06, acc 1
2017-08-08T17:39:21.385219: step 14859, loss 0.00115554, acc 1
2017-08-08T17:39:21.717395: step 14860, loss 0.00577645, acc 1
2017-08-08T17:39:22.067772: step 14861, loss 1.38578e-06, acc 1
2017-08-08T17:39:22.343894: step 14862, loss 0.000307661, acc 1
2017-08-08T17:39:22.583386: step 14863, loss 0.000290613, acc 1
2017-08-08T17:39:22.856385: step 14864, loss 1.35973e-07, acc 1
2017-08-08T17:39:23.277386: step 14865, loss 0.000143404, acc 1
2017-08-08T17:39:23.732509: step 14866, loss 2.98023e-08, acc 1
2017-08-08T17:39:24.136926: step 14867, loss 7.45058e-09, acc 1
2017-08-08T17:39:24.372630: step 14868, loss 4.49356e-05, acc 1
2017-08-08T17:39:24.759594: step 14869, loss 1.41727e-05, acc 1
2017-08-08T17:39:25.098469: step 14870, loss 4.04192e-07, acc 1
2017-08-08T17:39:25.357048: step 14871, loss 3.97252e-06, acc 1
2017-08-08T17:39:25.664979: step 14872, loss 2.14508e-05, acc 1
2017-08-08T17:39:25.941233: step 14873, loss 8.20182e-06, acc 1
2017-08-08T17:39:26.409386: step 14874, loss 7.71118e-07, acc 1
2017-08-08T17:39:26.790652: step 14875, loss 1.84637e-05, acc 1
2017-08-08T17:39:27.035313: step 14876, loss 5.30972e-06, acc 1
2017-08-08T17:39:27.278397: step 14877, loss 4.85331e-06, acc 1
2017-08-08T17:39:27.683571: step 14878, loss 8.26281e-05, acc 1
2017-08-08T17:39:28.018094: step 14879, loss 3.57244e-05, acc 1
2017-08-08T17:39:28.373957: step 14880, loss 0.000116009, acc 1
2017-08-08T17:39:28.666309: step 14881, loss 4.11417e-06, acc 1
2017-08-08T17:39:29.076628: step 14882, loss 2.98023e-08, acc 1
2017-08-08T17:39:29.532298: step 14883, loss 5.15857e-05, acc 1
2017-08-08T17:39:29.897405: step 14884, loss 0.000262078, acc 1
2017-08-08T17:39:30.242954: step 14885, loss 3.48022e-05, acc 1
2017-08-08T17:39:30.584133: step 14886, loss 0.00177845, acc 1
2017-08-08T17:39:30.970615: step 14887, loss 1.89234e-06, acc 1
2017-08-08T17:39:31.237284: step 14888, loss 2.93899e-06, acc 1
2017-08-08T17:39:31.514634: step 14889, loss 6.89168e-07, acc 1
2017-08-08T17:39:31.800977: step 14890, loss 3.53902e-08, acc 1
2017-08-08T17:39:32.233386: step 14891, loss 7.12503e-06, acc 1
2017-08-08T17:39:32.506474: step 14892, loss 0.000116263, acc 1
2017-08-08T17:39:32.757273: step 14893, loss 6.82211e-05, acc 1
2017-08-08T17:39:33.001866: step 14894, loss 1.01884e-06, acc 1
2017-08-08T17:39:33.297563: step 14895, loss 2.9671e-06, acc 1
2017-08-08T17:39:33.681420: step 14896, loss 7.84154e-07, acc 1
2017-08-08T17:39:34.008467: step 14897, loss 2.03611e-05, acc 1
2017-08-08T17:39:34.301131: step 14898, loss 1.49378e-06, acc 1
2017-08-08T17:39:34.555136: step 14899, loss 2.84045e-06, acc 1
2017-08-08T17:39:34.847400: step 14900, loss 2.08794e-06, acc 1

Evaluation:
2017-08-08T17:39:35.731504: step 14900, loss 4.95587, acc 0.719512

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-14900

2017-08-08T17:39:36.267852: step 14901, loss 4.82422e-07, acc 1
2017-08-08T17:39:36.495750: step 14902, loss 0.000326305, acc 1
2017-08-08T17:39:36.809716: step 14903, loss 8.79145e-07, acc 1
2017-08-08T17:39:37.151523: step 14904, loss 2.62054e-06, acc 1
2017-08-08T17:39:37.402078: step 14905, loss 1.83535e-05, acc 1
2017-08-08T17:39:37.665532: step 14906, loss 2.27335e-05, acc 1
2017-08-08T17:39:38.089413: step 14907, loss 0.00015744, acc 1
2017-08-08T17:39:38.463080: step 14908, loss 0.000258048, acc 1
2017-08-08T17:39:38.744914: step 14909, loss 5.12223e-07, acc 1
2017-08-08T17:39:38.989224: step 14910, loss 1.20884e-06, acc 1
2017-08-08T17:39:39.391996: step 14911, loss 0.000111624, acc 1
2017-08-08T17:39:39.692988: step 14912, loss 2.80248e-05, acc 1
2017-08-08T17:39:39.961569: step 14913, loss 6.49008e-06, acc 1
2017-08-08T17:39:40.232588: step 14914, loss 1.47149e-07, acc 1
2017-08-08T17:39:40.557417: step 14915, loss 0.000999407, acc 1
2017-08-08T17:39:40.945387: step 14916, loss 6.42501e-06, acc 1
2017-08-08T17:39:41.375387: step 14917, loss 0.00076642, acc 1
2017-08-08T17:39:41.735755: step 14918, loss 9.92699e-05, acc 1
2017-08-08T17:39:41.972720: step 14919, loss 4.55539e-06, acc 1
2017-08-08T17:39:42.217870: step 14920, loss 0.00199653, acc 1
2017-08-08T17:39:42.675896: step 14921, loss 5.58793e-08, acc 1
2017-08-08T17:39:42.972567: step 14922, loss 2.18345e-05, acc 1
2017-08-08T17:39:43.295518: step 14923, loss 1.89062e-05, acc 1
2017-08-08T17:39:43.558158: step 14924, loss 8.77292e-07, acc 1
2017-08-08T17:39:43.935115: step 14925, loss 0.000122502, acc 1
2017-08-08T17:39:44.281341: step 14926, loss 2.39578e-05, acc 1
2017-08-08T17:39:44.607560: step 14927, loss 0.00012811, acc 1
2017-08-08T17:39:44.908345: step 14928, loss 3.95788e-05, acc 1
2017-08-08T17:39:45.183853: step 14929, loss 1.91877e-05, acc 1
2017-08-08T17:39:45.546154: step 14930, loss 4.90415e-05, acc 1
2017-08-08T17:39:45.747897: step 14931, loss 7.65542e-07, acc 1
2017-08-08T17:39:45.978446: step 14932, loss 7.57835e-06, acc 1
2017-08-08T17:39:46.273658: step 14933, loss 2.82729e-06, acc 1
2017-08-08T17:39:46.674841: step 14934, loss 3.27257e-05, acc 1
2017-08-08T17:39:47.047741: step 14935, loss 4.24681e-07, acc 1
2017-08-08T17:39:47.355018: step 14936, loss 8.95388e-05, acc 1
2017-08-08T17:39:47.650274: step 14937, loss 5.90455e-07, acc 1
2017-08-08T17:39:47.921687: step 14938, loss 4.57381e-05, acc 1
2017-08-08T17:39:48.318248: step 14939, loss 2.44005e-07, acc 1
2017-08-08T17:39:48.546444: step 14940, loss 0.0048842, acc 1
2017-08-08T17:39:48.755244: step 14941, loss 6.70551e-08, acc 1
2017-08-08T17:39:48.993353: step 14942, loss 2.25379e-07, acc 1
2017-08-08T17:39:49.411531: step 14943, loss 6.05615e-06, acc 1
2017-08-08T17:39:49.771819: step 14944, loss 2.98023e-08, acc 1
2017-08-08T17:39:50.119689: step 14945, loss 2.56666e-05, acc 1
2017-08-08T17:39:50.381473: step 14946, loss 2.59705e-05, acc 1
2017-08-08T17:39:50.680129: step 14947, loss 2.67838e-05, acc 1
2017-08-08T17:39:50.957919: step 14948, loss 3.37043e-05, acc 1
2017-08-08T17:39:51.210209: step 14949, loss 5.34709e-06, acc 1
2017-08-08T17:39:51.482999: step 14950, loss 3.63214e-07, acc 1
2017-08-08T17:39:51.853603: step 14951, loss 2.6077e-08, acc 1
2017-08-08T17:39:52.249571: step 14952, loss 6.77335e-06, acc 1
2017-08-08T17:39:52.650411: step 14953, loss 2.4773e-07, acc 1
2017-08-08T17:39:52.968922: step 14954, loss 0.0192677, acc 0.984375
2017-08-08T17:39:53.211949: step 14955, loss 0.000141791, acc 1
2017-08-08T17:39:53.606779: step 14956, loss 6.51925e-08, acc 1
2017-08-08T17:39:53.907696: step 14957, loss 3.5199e-05, acc 1
2017-08-08T17:39:54.195142: step 14958, loss 0.000330323, acc 1
2017-08-08T17:39:54.425393: step 14959, loss 0.000127965, acc 1
2017-08-08T17:39:54.830524: step 14960, loss 9.70423e-07, acc 1
2017-08-08T17:39:55.201437: step 14961, loss 4.47035e-08, acc 1
2017-08-08T17:39:55.475882: step 14962, loss 0.000174113, acc 1
2017-08-08T17:39:55.720428: step 14963, loss 0.000165314, acc 1
2017-08-08T17:39:56.064801: step 14964, loss 0.000111989, acc 1
2017-08-08T17:39:56.338489: step 14965, loss 0.00514567, acc 1
2017-08-08T17:39:56.568291: step 14966, loss 0.000613579, acc 1
2017-08-08T17:39:56.793734: step 14967, loss 0.00218286, acc 1
2017-08-08T17:39:57.045463: step 14968, loss 2.01744e-05, acc 1
2017-08-08T17:39:57.326708: step 14969, loss 0.020116, acc 0.984375
2017-08-08T17:39:57.629352: step 14970, loss 0.00229073, acc 1
2017-08-08T17:39:57.909932: step 14971, loss 2.51457e-07, acc 1
2017-08-08T17:39:58.116844: step 14972, loss 0.00044901, acc 1
2017-08-08T17:39:58.416544: step 14973, loss 1.8849e-06, acc 1
2017-08-08T17:39:58.842315: step 14974, loss 0.000223683, acc 1
2017-08-08T17:39:59.137603: step 14975, loss 1.38846e-05, acc 1
2017-08-08T17:39:59.434139: step 14976, loss 3.10196e-05, acc 1
2017-08-08T17:39:59.702928: step 14977, loss 2.04943e-05, acc 1
2017-08-08T17:40:00.105533: step 14978, loss 2.61941e-05, acc 1
2017-08-08T17:40:00.460736: step 14979, loss 0.000812992, acc 1
2017-08-08T17:40:00.843539: step 14980, loss 2.06752e-07, acc 1
2017-08-08T17:40:01.155948: step 14981, loss 5.46423e-06, acc 1
2017-08-08T17:40:01.406496: step 14982, loss 3.66939e-07, acc 1
2017-08-08T17:40:02.107749: step 14983, loss 0.0147078, acc 0.984375
2017-08-08T17:40:02.365348: step 14984, loss 2.33934e-06, acc 1
2017-08-08T17:40:02.659886: step 14985, loss 0.000525948, acc 1
2017-08-08T17:40:02.993503: step 14986, loss 0.000633344, acc 1
2017-08-08T17:40:03.385770: step 14987, loss 1.76014e-06, acc 1
2017-08-08T17:40:03.648567: step 14988, loss 1.11759e-08, acc 1
2017-08-08T17:40:03.938837: step 14989, loss 0.00604468, acc 1
2017-08-08T17:40:04.228624: step 14990, loss 1.79365e-05, acc 1
2017-08-08T17:40:04.609370: step 14991, loss 1.47143e-06, acc 1
2017-08-08T17:40:04.997894: step 14992, loss 8.62042e-05, acc 1
2017-08-08T17:40:05.313858: step 14993, loss 0.00653941, acc 1
2017-08-08T17:40:05.607755: step 14994, loss 3.77612e-05, acc 1
2017-08-08T17:40:06.003762: step 14995, loss 1.54645e-05, acc 1
2017-08-08T17:40:06.392507: step 14996, loss 0.000309882, acc 1
2017-08-08T17:40:06.739875: step 14997, loss 7.15252e-07, acc 1
2017-08-08T17:40:07.137148: step 14998, loss 2.51265e-06, acc 1
2017-08-08T17:40:07.424949: step 14999, loss 0.000885238, acc 1
2017-08-08T17:40:07.830698: step 15000, loss 6.69554e-07, acc 1

Evaluation:
2017-08-08T17:40:08.544262: step 15000, loss 4.99563, acc 0.710131

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-15000

2017-08-08T17:40:09.113655: step 15001, loss 3.60755e-06, acc 1
2017-08-08T17:40:09.476923: step 15002, loss 6.44464e-07, acc 1
2017-08-08T17:40:09.908289: step 15003, loss 5.34573e-07, acc 1
2017-08-08T17:40:10.260850: step 15004, loss 0.00034382, acc 1
2017-08-08T17:40:10.484735: step 15005, loss 1.91871e-05, acc 1
2017-08-08T17:40:10.839557: step 15006, loss 7.26676e-05, acc 1
2017-08-08T17:40:11.182201: step 15007, loss 2.98023e-08, acc 1
2017-08-08T17:40:11.426264: step 15008, loss 0.0136872, acc 0.984375
2017-08-08T17:40:11.722883: step 15009, loss 4.6839e-06, acc 1
2017-08-08T17:40:11.974181: step 15010, loss 1.06171e-07, acc 1
2017-08-08T17:40:12.405724: step 15011, loss 7.71123e-07, acc 1
2017-08-08T17:40:12.803967: step 15012, loss 0.000520602, acc 1
2017-08-08T17:40:13.130115: step 15013, loss 0.000294934, acc 1
2017-08-08T17:40:13.452386: step 15014, loss 6.48547e-06, acc 1
2017-08-08T17:40:13.721619: step 15015, loss 2.38489e-05, acc 1
2017-08-08T17:40:14.119837: step 15016, loss 0.00101737, acc 1
2017-08-08T17:40:14.431051: step 15017, loss 2.50143e-06, acc 1
2017-08-08T17:40:14.743594: step 15018, loss 0.0039891, acc 1
2017-08-08T17:40:15.013241: step 15019, loss 0.00027788, acc 1
2017-08-08T17:40:15.396362: step 15020, loss 9.51797e-07, acc 1
2017-08-08T17:40:15.813393: step 15021, loss 2.33316e-05, acc 1
2017-08-08T17:40:16.172071: step 15022, loss 2.52994e-05, acc 1
2017-08-08T17:40:16.476427: step 15023, loss 1.45095e-06, acc 1
2017-08-08T17:40:16.781428: step 15024, loss 2.26112e-06, acc 1
2017-08-08T17:40:17.208957: step 15025, loss 7.4139e-06, acc 1
2017-08-08T17:40:17.438346: step 15026, loss 8.12388e-05, acc 1
2017-08-08T17:40:17.681836: step 15027, loss 0.000984719, acc 1
2017-08-08T17:40:17.946086: step 15028, loss 2.7607e-05, acc 1
2017-08-08T17:40:18.305384: step 15029, loss 0.00119173, acc 1
2017-08-08T17:40:18.685270: step 15030, loss 3.27824e-07, acc 1
2017-08-08T17:40:19.052923: step 15031, loss 1.01511e-06, acc 1
2017-08-08T17:40:19.296400: step 15032, loss 0.0238479, acc 0.984375
2017-08-08T17:40:19.530007: step 15033, loss 1.47516e-06, acc 1
2017-08-08T17:40:19.850071: step 15034, loss 5.14019e-06, acc 1
2017-08-08T17:40:20.139500: step 15035, loss 1.33362e-06, acc 1
2017-08-08T17:40:20.436632: step 15036, loss 4.81482e-05, acc 1
2017-08-08T17:40:20.762306: step 15037, loss 0, acc 1
2017-08-08T17:40:21.069781: step 15038, loss 0.000202607, acc 1
2017-08-08T17:40:21.390209: step 15039, loss 1.80668e-06, acc 1
2017-08-08T17:40:21.716364: step 15040, loss 5.48477e-06, acc 1
2017-08-08T17:40:21.992785: step 15041, loss 5.60604e-06, acc 1
2017-08-08T17:40:22.178087: step 15042, loss 1.3999e-05, acc 1
2017-08-08T17:40:22.392834: step 15043, loss 6.87307e-07, acc 1
2017-08-08T17:40:22.745460: step 15044, loss 0.000360489, acc 1
2017-08-08T17:40:22.989872: step 15045, loss 4.44004e-06, acc 1
2017-08-08T17:40:23.318069: step 15046, loss 0.000262711, acc 1
2017-08-08T17:40:23.647662: step 15047, loss 4.08867e-05, acc 1
2017-08-08T17:40:23.986150: step 15048, loss 6.29136e-06, acc 1
2017-08-08T17:40:24.397375: step 15049, loss 0.000928986, acc 1
2017-08-08T17:40:24.837967: step 15050, loss 2.5032e-06, acc 1
2017-08-08T17:40:25.180181: step 15051, loss 5.81068e-06, acc 1
2017-08-08T17:40:25.626939: step 15052, loss 2.39895e-06, acc 1
2017-08-08T17:40:25.966686: step 15053, loss 3.56672e-06, acc 1
2017-08-08T17:40:26.237547: step 15054, loss 1.19316e-05, acc 1
2017-08-08T17:40:26.514403: step 15055, loss 1.02445e-07, acc 1
2017-08-08T17:40:26.883051: step 15056, loss 0.0817733, acc 0.984375
2017-08-08T17:40:27.327338: step 15057, loss 1.46874e-05, acc 1
2017-08-08T17:40:27.731968: step 15058, loss 4.52881e-05, acc 1
2017-08-08T17:40:28.019369: step 15059, loss 3.20372e-07, acc 1
2017-08-08T17:40:28.241012: step 15060, loss 0.000808847, acc 1
2017-08-08T17:40:28.486992: step 15061, loss 4.84696e-05, acc 1
2017-08-08T17:40:28.894606: step 15062, loss 5.64828e-05, acc 1
2017-08-08T17:40:29.146557: step 15063, loss 7.89749e-07, acc 1
2017-08-08T17:40:29.415795: step 15064, loss 1.70798e-06, acc 1
2017-08-08T17:40:29.665474: step 15065, loss 8.47491e-07, acc 1
2017-08-08T17:40:29.969448: step 15066, loss 3.91155e-08, acc 1
2017-08-08T17:40:30.324090: step 15067, loss 0.000157872, acc 1
2017-08-08T17:40:30.663817: step 15068, loss 0.000306284, acc 1
2017-08-08T17:40:30.910132: step 15069, loss 4.71452e-05, acc 1
2017-08-08T17:40:31.255259: step 15070, loss 0.000771363, acc 1
2017-08-08T17:40:31.591597: step 15071, loss 7.84046e-06, acc 1
2017-08-08T17:40:31.857183: step 15072, loss 0.00062897, acc 1
2017-08-08T17:40:32.144421: step 15073, loss 6.68221e-05, acc 1
2017-08-08T17:40:32.437283: step 15074, loss 5.8357e-05, acc 1
2017-08-08T17:40:32.876669: step 15075, loss 0.000121799, acc 1
2017-08-08T17:40:33.219271: step 15076, loss 1.10268e-05, acc 1
2017-08-08T17:40:33.582020: step 15077, loss 2.75669e-07, acc 1
2017-08-08T17:40:33.872223: step 15078, loss 0.00685609, acc 1
2017-08-08T17:40:34.105047: step 15079, loss 1.71363e-07, acc 1
2017-08-08T17:40:34.386360: step 15080, loss 5.91692e-05, acc 1
2017-08-08T17:40:34.753067: step 15081, loss 0.000377483, acc 1
2017-08-08T17:40:35.038997: step 15082, loss 1.64091e-06, acc 1
2017-08-08T17:40:35.347123: step 15083, loss 4.5448e-07, acc 1
2017-08-08T17:40:35.777381: step 15084, loss 1.65527e-05, acc 1
2017-08-08T17:40:36.171371: step 15085, loss 1.44722e-06, acc 1
2017-08-08T17:40:36.558703: step 15086, loss 6.11085e-06, acc 1
2017-08-08T17:40:36.887929: step 15087, loss 0.000132186, acc 1
2017-08-08T17:40:37.161787: step 15088, loss 1.46429e-05, acc 1
2017-08-08T17:40:37.587336: step 15089, loss 1.18538e-05, acc 1
2017-08-08T17:40:37.895900: step 15090, loss 2.91664e-06, acc 1
2017-08-08T17:40:38.187265: step 15091, loss 0.000196523, acc 1
2017-08-08T17:40:38.510391: step 15092, loss 5.71818e-05, acc 1
2017-08-08T17:40:38.969990: step 15093, loss 1.80549e-05, acc 1
2017-08-08T17:40:39.389434: step 15094, loss 0.0377224, acc 0.984375
2017-08-08T17:40:39.786803: step 15095, loss 0.000919312, acc 1
2017-08-08T17:40:40.030478: step 15096, loss 2.46415e-05, acc 1
2017-08-08T17:40:40.416881: step 15097, loss 7.70412e-05, acc 1
2017-08-08T17:40:40.776767: step 15098, loss 0.00431818, acc 1
2017-08-08T17:40:41.051116: step 15099, loss 0.000291203, acc 1
2017-08-08T17:40:41.326664: step 15100, loss 1.44441e-05, acc 1

Evaluation:
2017-08-08T17:40:42.799787: step 15100, loss 4.99842, acc 0.712008

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-15100

2017-08-08T17:40:43.257161: step 15101, loss 3.75553e-05, acc 1
2017-08-08T17:40:43.668856: step 15102, loss 0.000325623, acc 1
2017-08-08T17:40:44.019851: step 15103, loss 0.000638944, acc 1
2017-08-08T17:40:44.312096: step 15104, loss 6.00327e-05, acc 1
2017-08-08T17:40:44.597489: step 15105, loss 0.00047265, acc 1
2017-08-08T17:40:44.981191: step 15106, loss 6.33287e-07, acc 1
2017-08-08T17:40:45.429569: step 15107, loss 9.87201e-08, acc 1
2017-08-08T17:40:45.804874: step 15108, loss 2.76035e-06, acc 1
2017-08-08T17:40:46.125226: step 15109, loss 9.84489e-06, acc 1
2017-08-08T17:40:46.424370: step 15110, loss 3.30641e-05, acc 1
2017-08-08T17:40:46.810722: step 15111, loss 0.0316903, acc 0.984375
2017-08-08T17:40:47.235275: step 15112, loss 0.0234837, acc 0.984375
2017-08-08T17:40:47.592731: step 15113, loss 1.60013e-05, acc 1
2017-08-08T17:40:47.963542: step 15114, loss 6.07704e-05, acc 1
2017-08-08T17:40:48.292125: step 15115, loss 6.84169e-05, acc 1
2017-08-08T17:40:48.728385: step 15116, loss 7.31305e-06, acc 1
2017-08-08T17:40:49.178607: step 15117, loss 1.94381e-05, acc 1
2017-08-08T17:40:49.561866: step 15118, loss 2.35324e-05, acc 1
2017-08-08T17:40:49.811302: step 15119, loss 6.22597e-06, acc 1
2017-08-08T17:40:50.013813: step 15120, loss 2.83121e-07, acc 1
2017-08-08T17:40:50.351230: step 15121, loss 0.000101425, acc 1
2017-08-08T17:40:50.614687: step 15122, loss 4.19467e-05, acc 1
2017-08-08T17:40:50.882968: step 15123, loss 0.0158065, acc 0.984375
2017-08-08T17:40:51.196272: step 15124, loss 6.33299e-08, acc 1
2017-08-08T17:40:51.679533: step 15125, loss 7.43072e-05, acc 1
2017-08-08T17:40:52.182089: step 15126, loss 9.69485e-05, acc 1
2017-08-08T17:40:52.603649: step 15127, loss 4.34633e-05, acc 1
2017-08-08T17:40:53.075776: step 15128, loss 0.000313577, acc 1
2017-08-08T17:40:53.411710: step 15129, loss 7.5837e-06, acc 1
2017-08-08T17:40:53.867531: step 15130, loss 0.000462208, acc 1
2017-08-08T17:40:54.284209: step 15131, loss 4.4384e-06, acc 1
2017-08-08T17:40:54.593967: step 15132, loss 1.35781e-06, acc 1
2017-08-08T17:40:54.935018: step 15133, loss 1.41389e-05, acc 1
2017-08-08T17:40:55.252222: step 15134, loss 2.2611e-06, acc 1
2017-08-08T17:40:55.762852: step 15135, loss 3.87395e-06, acc 1
2017-08-08T17:40:56.213457: step 15136, loss 0.00016802, acc 1
2017-08-08T17:40:56.688807: step 15137, loss 0.000118772, acc 1
2017-08-08T17:40:56.971199: step 15138, loss 0.00443428, acc 1
2017-08-08T17:40:57.255567: step 15139, loss 0.0702486, acc 0.984375
2017-08-08T17:40:57.618816: step 15140, loss 2.04135e-06, acc 1
2017-08-08T17:40:57.944899: step 15141, loss 0.000106132, acc 1
2017-08-08T17:40:58.250207: step 15142, loss 2.76724e-05, acc 1
2017-08-08T17:40:58.571221: step 15143, loss 2.44709e-05, acc 1
2017-08-08T17:40:58.882385: step 15144, loss 3.89499e-05, acc 1
2017-08-08T17:40:59.383201: step 15145, loss 1.33549e-06, acc 1
2017-08-08T17:40:59.766210: step 15146, loss 0.000371203, acc 1
2017-08-08T17:41:00.027060: step 15147, loss 6.57022e-06, acc 1
2017-08-08T17:41:00.267720: step 15148, loss 1.2244e-05, acc 1
2017-08-08T17:41:00.665593: step 15149, loss 0.000283437, acc 1
2017-08-08T17:41:00.949454: step 15150, loss 0.0374789, acc 0.983333
2017-08-08T17:41:01.227886: step 15151, loss 2.91945e-05, acc 1
2017-08-08T17:41:01.497143: step 15152, loss 3.51216e-05, acc 1
2017-08-08T17:41:01.905561: step 15153, loss 2.18476e-06, acc 1
2017-08-08T17:41:02.309409: step 15154, loss 0.00117164, acc 1
2017-08-08T17:41:02.814955: step 15155, loss 7.1253e-06, acc 1
2017-08-08T17:41:03.128565: step 15156, loss 1.80317e-05, acc 1
2017-08-08T17:41:03.410860: step 15157, loss 5.39606e-05, acc 1
2017-08-08T17:41:03.813229: step 15158, loss 0.000178924, acc 1
2017-08-08T17:41:04.115050: step 15159, loss 8.31902e-05, acc 1
2017-08-08T17:41:04.430857: step 15160, loss 2.57043e-07, acc 1
2017-08-08T17:41:04.743641: step 15161, loss 5.7742e-08, acc 1
2017-08-08T17:41:05.096762: step 15162, loss 0.000468723, acc 1
2017-08-08T17:41:05.463921: step 15163, loss 1.45504e-05, acc 1
2017-08-08T17:41:05.995748: step 15164, loss 0.000207844, acc 1
2017-08-08T17:41:06.369941: step 15165, loss 3.1851e-07, acc 1
2017-08-08T17:41:06.628311: step 15166, loss 2.67197e-05, acc 1
2017-08-08T17:41:07.126635: step 15167, loss 1.86265e-09, acc 1
2017-08-08T17:41:07.507891: step 15168, loss 2.42499e-05, acc 1
2017-08-08T17:41:07.738321: step 15169, loss 0.000299527, acc 1
2017-08-08T17:41:07.971709: step 15170, loss 1.85441e-05, acc 1
2017-08-08T17:41:08.256628: step 15171, loss 3.52836e-05, acc 1
2017-08-08T17:41:08.717748: step 15172, loss 4.70455e-06, acc 1
2017-08-08T17:41:09.216301: step 15173, loss 7.32426e-05, acc 1
2017-08-08T17:41:09.737338: step 15174, loss 4.26541e-07, acc 1
2017-08-08T17:41:10.070610: step 15175, loss 0.00207396, acc 1
2017-08-08T17:41:10.497418: step 15176, loss 1.75088e-07, acc 1
2017-08-08T17:41:10.911908: step 15177, loss 4.50758e-07, acc 1
2017-08-08T17:41:11.260291: step 15178, loss 2.90938e-05, acc 1
2017-08-08T17:41:11.591715: step 15179, loss 1.92399e-06, acc 1
2017-08-08T17:41:11.845538: step 15180, loss 4.28408e-08, acc 1
2017-08-08T17:41:12.289102: step 15181, loss 0.000943057, acc 1
2017-08-08T17:41:12.614513: step 15182, loss 0.000516395, acc 1
2017-08-08T17:41:12.895228: step 15183, loss 0.00350095, acc 1
2017-08-08T17:41:13.150941: step 15184, loss 0.00018496, acc 1
2017-08-08T17:41:13.403410: step 15185, loss 8.63859e-06, acc 1
2017-08-08T17:41:13.741390: step 15186, loss 1.00583e-07, acc 1
2017-08-08T17:41:14.020585: step 15187, loss 1.47891e-06, acc 1
2017-08-08T17:41:14.296244: step 15188, loss 0.000137359, acc 1
2017-08-08T17:41:14.682610: step 15189, loss 1.7732e-06, acc 1
2017-08-08T17:41:15.152155: step 15190, loss 3.42323e-06, acc 1
2017-08-08T17:41:15.538504: step 15191, loss 4.14842e-05, acc 1
2017-08-08T17:41:15.795679: step 15192, loss 0.0005859, acc 1
2017-08-08T17:41:15.969200: step 15193, loss 2.94297e-07, acc 1
2017-08-08T17:41:16.284354: step 15194, loss 0.000520118, acc 1
2017-08-08T17:41:16.549833: step 15195, loss 3.29899e-05, acc 1
2017-08-08T17:41:16.804994: step 15196, loss 0.000298379, acc 1
2017-08-08T17:41:17.119035: step 15197, loss 0.000246075, acc 1
2017-08-08T17:41:17.433431: step 15198, loss 5.2159e-05, acc 1
2017-08-08T17:41:17.796630: step 15199, loss 6.14673e-08, acc 1
2017-08-08T17:41:18.138787: step 15200, loss 2.77532e-07, acc 1

Evaluation:
2017-08-08T17:41:18.910452: step 15200, loss 5.09281, acc 0.708255

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-15200

2017-08-08T17:41:19.576814: step 15201, loss 3.02343e-05, acc 1
2017-08-08T17:41:19.851684: step 15202, loss 5.72379e-05, acc 1
2017-08-08T17:41:20.129521: step 15203, loss 2.70083e-07, acc 1
2017-08-08T17:41:20.426882: step 15204, loss 1.2796e-05, acc 1
2017-08-08T17:41:20.807627: step 15205, loss 1.50592e-05, acc 1
2017-08-08T17:41:21.161364: step 15206, loss 1.35597e-06, acc 1
2017-08-08T17:41:21.441366: step 15207, loss 0.000947572, acc 1
2017-08-08T17:41:21.656383: step 15208, loss 0.0001208, acc 1
2017-08-08T17:41:21.883274: step 15209, loss 7.01297e-05, acc 1
2017-08-08T17:41:22.294455: step 15210, loss 0.000207218, acc 1
2017-08-08T17:41:22.653369: step 15211, loss 2.57742e-05, acc 1
2017-08-08T17:41:23.108071: step 15212, loss 0.000847305, acc 1
2017-08-08T17:41:23.388004: step 15213, loss 0.000319707, acc 1
2017-08-08T17:41:23.685182: step 15214, loss 0.000502282, acc 1
2017-08-08T17:41:24.074838: step 15215, loss 9.58421e-05, acc 1
2017-08-08T17:41:24.340459: step 15216, loss 1.45286e-07, acc 1
2017-08-08T17:41:24.660367: step 15217, loss 2.44768e-05, acc 1
2017-08-08T17:41:24.948372: step 15218, loss 2.21654e-07, acc 1
2017-08-08T17:41:25.230624: step 15219, loss 2.03028e-07, acc 1
2017-08-08T17:41:25.505914: step 15220, loss 1.30385e-08, acc 1
2017-08-08T17:41:25.689370: step 15221, loss 0.000158905, acc 1
2017-08-08T17:41:25.921337: step 15222, loss 2.55498e-05, acc 1
2017-08-08T17:41:26.273767: step 15223, loss 2.10102e-06, acc 1
2017-08-08T17:41:26.559828: step 15224, loss 2.49956e-06, acc 1
2017-08-08T17:41:26.851448: step 15225, loss 8.37627e-06, acc 1
2017-08-08T17:41:27.113183: step 15226, loss 0.0337905, acc 0.984375
2017-08-08T17:41:27.525612: step 15227, loss 1.97992e-06, acc 1
2017-08-08T17:41:27.888823: step 15228, loss 0.00924349, acc 1
2017-08-08T17:41:28.281019: step 15229, loss 9.78162e-05, acc 1
2017-08-08T17:41:28.529252: step 15230, loss 2.8497e-05, acc 1
2017-08-08T17:41:28.783148: step 15231, loss 1.40328e-05, acc 1
2017-08-08T17:41:29.116790: step 15232, loss 2.23882e-06, acc 1
2017-08-08T17:41:29.469777: step 15233, loss 0.000578776, acc 1
2017-08-08T17:41:29.700447: step 15234, loss 7.87392e-05, acc 1
2017-08-08T17:41:29.967796: step 15235, loss 0.0484345, acc 0.984375
2017-08-08T17:41:30.233719: step 15236, loss 1.37835e-07, acc 1
2017-08-08T17:41:30.611541: step 15237, loss 3.62125e-05, acc 1
2017-08-08T17:41:31.089370: step 15238, loss 9.16437e-06, acc 1
2017-08-08T17:41:31.430171: step 15239, loss 4.51464e-06, acc 1
2017-08-08T17:41:31.648171: step 15240, loss 5.79277e-07, acc 1
2017-08-08T17:41:31.852099: step 15241, loss 0.0183469, acc 0.984375
2017-08-08T17:41:32.152950: step 15242, loss 1.30385e-08, acc 1
2017-08-08T17:41:32.392040: step 15243, loss 1.18964e-05, acc 1
2017-08-08T17:41:32.590935: step 15244, loss 5.54769e-05, acc 1
2017-08-08T17:41:32.813318: step 15245, loss 8.30906e-05, acc 1
2017-08-08T17:41:33.159662: step 15246, loss 3.67611e-05, acc 1
2017-08-08T17:41:33.564851: step 15247, loss 0.000124959, acc 1
2017-08-08T17:41:33.872768: step 15248, loss 1.75638e-05, acc 1
2017-08-08T17:41:34.104907: step 15249, loss 0.000156519, acc 1
2017-08-08T17:41:34.311114: step 15250, loss 6.51926e-08, acc 1
2017-08-08T17:41:34.681361: step 15251, loss 1.40995e-05, acc 1
2017-08-08T17:41:34.953312: step 15252, loss 0.00479837, acc 1
2017-08-08T17:41:35.218244: step 15253, loss 3.53902e-08, acc 1
2017-08-08T17:41:35.426314: step 15254, loss 3.01717e-05, acc 1
2017-08-08T17:41:35.664404: step 15255, loss 7.81955e-06, acc 1
2017-08-08T17:41:35.954432: step 15256, loss 0.000381216, acc 1
2017-08-08T17:41:36.308610: step 15257, loss 0.000285798, acc 1
2017-08-08T17:41:36.681267: step 15258, loss 0.000396885, acc 1
2017-08-08T17:41:36.998714: step 15259, loss 1.22833e-05, acc 1
2017-08-08T17:41:37.311316: step 15260, loss 0.01826, acc 0.984375
2017-08-08T17:41:37.690361: step 15261, loss 2.2247e-05, acc 1
2017-08-08T17:41:37.925360: step 15262, loss 4.19359e-05, acc 1
2017-08-08T17:41:38.224562: step 15263, loss 2.76951e-06, acc 1
2017-08-08T17:41:38.562070: step 15264, loss 1.14865e-05, acc 1
2017-08-08T17:41:38.933362: step 15265, loss 0.000267359, acc 1
2017-08-08T17:41:39.403544: step 15266, loss 0.000728228, acc 1
2017-08-08T17:41:39.638586: step 15267, loss 9.88066e-05, acc 1
2017-08-08T17:41:39.867568: step 15268, loss 6.42608e-07, acc 1
2017-08-08T17:41:40.149371: step 15269, loss 2.35936e-05, acc 1
2017-08-08T17:41:40.439416: step 15270, loss 0.000233693, acc 1
2017-08-08T17:41:40.715951: step 15271, loss 0.00052522, acc 1
2017-08-08T17:41:41.034184: step 15272, loss 2.86191e-05, acc 1
2017-08-08T17:41:41.461394: step 15273, loss 3.31718e-06, acc 1
2017-08-08T17:41:41.861697: step 15274, loss 0.00165996, acc 1
2017-08-08T17:41:42.222573: step 15275, loss 2.51456e-07, acc 1
2017-08-08T17:41:42.511704: step 15276, loss 7.28284e-07, acc 1
2017-08-08T17:41:42.769023: step 15277, loss 3.28508e-05, acc 1
2017-08-08T17:41:43.059957: step 15278, loss 0.00116309, acc 1
2017-08-08T17:41:43.490713: step 15279, loss 4.20725e-06, acc 1
2017-08-08T17:41:43.780752: step 15280, loss 0.000490378, acc 1
2017-08-08T17:41:44.071667: step 15281, loss 2.77714e-05, acc 1
2017-08-08T17:41:44.504577: step 15282, loss 0.000728086, acc 1
2017-08-08T17:41:44.927510: step 15283, loss 1.08032e-06, acc 1
2017-08-08T17:41:45.255217: step 15284, loss 1.69987e-05, acc 1
2017-08-08T17:41:45.504444: step 15285, loss 0.00015027, acc 1
2017-08-08T17:41:45.702824: step 15286, loss 5.51334e-07, acc 1
2017-08-08T17:41:46.082580: step 15287, loss 4.33994e-07, acc 1
2017-08-08T17:41:46.346888: step 15288, loss 0.000790688, acc 1
2017-08-08T17:41:46.579721: step 15289, loss 0.000801896, acc 1
2017-08-08T17:41:46.803398: step 15290, loss 2.13644e-05, acc 1
2017-08-08T17:41:47.169422: step 15291, loss 0.00168956, acc 1
2017-08-08T17:41:47.449428: step 15292, loss 7.95565e-06, acc 1
2017-08-08T17:41:47.822060: step 15293, loss 5.18702e-06, acc 1
2017-08-08T17:41:48.121943: step 15294, loss 2.04891e-08, acc 1
2017-08-08T17:41:48.355901: step 15295, loss 3.15433e-05, acc 1
2017-08-08T17:41:48.731013: step 15296, loss 0.000142741, acc 1
2017-08-08T17:41:49.069413: step 15297, loss 0.000534039, acc 1
2017-08-08T17:41:49.281817: step 15298, loss 0.000486902, acc 1
2017-08-08T17:41:49.556193: step 15299, loss 0.176576, acc 0.984375
2017-08-08T17:41:50.002026: step 15300, loss 0.000413782, acc 1

Evaluation:
2017-08-08T17:41:50.954477: step 15300, loss 5.2129, acc 0.706379

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-15300

2017-08-08T17:41:51.480560: step 15301, loss 1.26333e-05, acc 1
2017-08-08T17:41:51.786623: step 15302, loss 8.56816e-08, acc 1
2017-08-08T17:41:52.057619: step 15303, loss 3.8184e-07, acc 1
2017-08-08T17:41:52.326667: step 15304, loss 3.23969e-05, acc 1
2017-08-08T17:41:52.573325: step 15305, loss 3.47585e-05, acc 1
2017-08-08T17:41:52.969555: step 15306, loss 1.40886e-05, acc 1
2017-08-08T17:41:53.393512: step 15307, loss 1.361e-05, acc 1
2017-08-08T17:41:53.682230: step 15308, loss 2.27242e-07, acc 1
2017-08-08T17:41:53.937406: step 15309, loss 3.52581e-05, acc 1
2017-08-08T17:41:54.270485: step 15310, loss 0.000182054, acc 1
2017-08-08T17:41:54.554442: step 15311, loss 3.35276e-08, acc 1
2017-08-08T17:41:54.793498: step 15312, loss 0.000456955, acc 1
2017-08-08T17:41:55.080625: step 15313, loss 1.31184e-05, acc 1
2017-08-08T17:41:55.447397: step 15314, loss 5.99761e-07, acc 1
2017-08-08T17:41:55.777447: step 15315, loss 8.94068e-08, acc 1
2017-08-08T17:41:56.070832: step 15316, loss 9.68561e-07, acc 1
2017-08-08T17:41:56.292894: step 15317, loss 1.86265e-09, acc 1
2017-08-08T17:41:56.504783: step 15318, loss 3.22591e-06, acc 1
2017-08-08T17:41:56.789350: step 15319, loss 0.000339207, acc 1
2017-08-08T17:41:56.996404: step 15320, loss 3.65277e-05, acc 1
2017-08-08T17:41:57.245002: step 15321, loss 3.37136e-07, acc 1
2017-08-08T17:41:57.516426: step 15322, loss 6.87644e-05, acc 1
2017-08-08T17:41:57.931131: step 15323, loss 8.45206e-05, acc 1
2017-08-08T17:41:58.291634: step 15324, loss 0.00110837, acc 1
2017-08-08T17:41:58.787102: step 15325, loss 0.000180254, acc 1
2017-08-08T17:41:59.072262: step 15326, loss 0.000473664, acc 1
2017-08-08T17:41:59.312181: step 15327, loss 9.15975e-06, acc 1
2017-08-08T17:41:59.720593: step 15328, loss 4.06578e-06, acc 1
2017-08-08T17:42:00.017993: step 15329, loss 0.000134221, acc 1
2017-08-08T17:42:00.320742: step 15330, loss 0.000969588, acc 1
2017-08-08T17:42:00.610759: step 15331, loss 0.000139338, acc 1
2017-08-08T17:42:01.038782: step 15332, loss 3.44113e-05, acc 1
2017-08-08T17:42:01.395089: step 15333, loss 0.000244074, acc 1
2017-08-08T17:42:01.762948: step 15334, loss 0.000106433, acc 1
2017-08-08T17:42:02.100964: step 15335, loss 2.06004e-06, acc 1
2017-08-08T17:42:02.409044: step 15336, loss 0.035999, acc 0.984375
2017-08-08T17:42:02.683604: step 15337, loss 0.00021313, acc 1
2017-08-08T17:42:03.215979: step 15338, loss 2.65135e-05, acc 1
2017-08-08T17:42:03.458554: step 15339, loss 1.16041e-06, acc 1
2017-08-08T17:42:03.756874: step 15340, loss 0.000307798, acc 1
2017-08-08T17:42:04.092117: step 15341, loss 4.02758e-05, acc 1
2017-08-08T17:42:04.557410: step 15342, loss 4.5518e-05, acc 1
2017-08-08T17:42:04.921383: step 15343, loss 1.56146e-05, acc 1
2017-08-08T17:42:05.241157: step 15344, loss 0.000514535, acc 1
2017-08-08T17:42:05.503882: step 15345, loss 2.49593e-07, acc 1
2017-08-08T17:42:05.848765: step 15346, loss 4.28408e-08, acc 1
2017-08-08T17:42:06.181658: step 15347, loss 4.71396e-05, acc 1
2017-08-08T17:42:06.515837: step 15348, loss 0.000289395, acc 1
2017-08-08T17:42:06.730008: step 15349, loss 0.000157525, acc 1
2017-08-08T17:42:07.013357: step 15350, loss 0.234061, acc 0.984375
2017-08-08T17:42:07.273058: step 15351, loss 5.47743e-05, acc 1
2017-08-08T17:42:07.640051: step 15352, loss 2.90671e-05, acc 1
2017-08-08T17:42:08.000547: step 15353, loss 9.59702e-06, acc 1
2017-08-08T17:42:08.245284: step 15354, loss 2.06137e-05, acc 1
2017-08-08T17:42:08.636107: step 15355, loss 0.000635566, acc 1
2017-08-08T17:42:09.004333: step 15356, loss 0.0013852, acc 1
2017-08-08T17:42:09.277729: step 15357, loss 4.22815e-07, acc 1
2017-08-08T17:42:09.568705: step 15358, loss 1.18832e-06, acc 1
2017-08-08T17:42:09.845423: step 15359, loss 0.000826678, acc 1
2017-08-08T17:42:10.325357: step 15360, loss 0.000278681, acc 1
2017-08-08T17:42:10.719097: step 15361, loss 6.79856e-07, acc 1
2017-08-08T17:42:11.119887: step 15362, loss 1.54599e-07, acc 1
2017-08-08T17:42:11.403821: step 15363, loss 0.0037771, acc 1
2017-08-08T17:42:11.874710: step 15364, loss 0.000163973, acc 1
2017-08-08T17:42:12.162650: step 15365, loss 0.00051554, acc 1
2017-08-08T17:42:12.392913: step 15366, loss 4.0939e-05, acc 1
2017-08-08T17:42:12.637348: step 15367, loss 3.57423e-06, acc 1
2017-08-08T17:42:13.009496: step 15368, loss 8.06508e-07, acc 1
2017-08-08T17:42:13.336352: step 15369, loss 1.14923e-06, acc 1
2017-08-08T17:42:13.618779: step 15370, loss 1.53426e-05, acc 1
2017-08-08T17:42:13.836870: step 15371, loss 0.006898, acc 1
2017-08-08T17:42:14.106148: step 15372, loss 1.10504e-05, acc 1
2017-08-08T17:42:14.407988: step 15373, loss 1.72476e-06, acc 1
2017-08-08T17:42:14.659421: step 15374, loss 9.39238e-05, acc 1
2017-08-08T17:42:14.937736: step 15375, loss 0.000112131, acc 1
2017-08-08T17:42:15.217379: step 15376, loss 0.000597329, acc 1
2017-08-08T17:42:15.584008: step 15377, loss 5.17089e-05, acc 1
2017-08-08T17:42:16.025034: step 15378, loss 3.05222e-05, acc 1
2017-08-08T17:42:16.430997: step 15379, loss 8.6053e-07, acc 1
2017-08-08T17:42:16.705131: step 15380, loss 1.05039e-05, acc 1
2017-08-08T17:42:17.086901: step 15381, loss 0.000401813, acc 1
2017-08-08T17:42:17.448874: step 15382, loss 0.000145457, acc 1
2017-08-08T17:42:17.732177: step 15383, loss 0.00132894, acc 1
2017-08-08T17:42:17.988171: step 15384, loss 0.000181311, acc 1
2017-08-08T17:42:18.262783: step 15385, loss 1.65702e-05, acc 1
2017-08-08T17:42:18.609337: step 15386, loss 1.64841e-06, acc 1
2017-08-08T17:42:18.953359: step 15387, loss 7.26492e-05, acc 1
2017-08-08T17:42:19.289151: step 15388, loss 4.94859e-06, acc 1
2017-08-08T17:42:19.509596: step 15389, loss 4.97847e-05, acc 1
2017-08-08T17:42:19.776783: step 15390, loss 6.59367e-07, acc 1
2017-08-08T17:42:19.989770: step 15391, loss 2.71553e-06, acc 1
2017-08-08T17:42:20.165350: step 15392, loss 0.00036783, acc 1
2017-08-08T17:42:20.435580: step 15393, loss 2.60693e-05, acc 1
2017-08-08T17:42:20.698499: step 15394, loss 2.489e-05, acc 1
2017-08-08T17:42:21.115929: step 15395, loss 4.9811e-05, acc 1
2017-08-08T17:42:21.509446: step 15396, loss 1.86264e-07, acc 1
2017-08-08T17:42:21.828090: step 15397, loss 0.00060803, acc 1
2017-08-08T17:42:22.055032: step 15398, loss 9.49947e-08, acc 1
2017-08-08T17:42:22.278376: step 15399, loss 6.58317e-05, acc 1
2017-08-08T17:42:22.648965: step 15400, loss 4.36948e-06, acc 1

Evaluation:
2017-08-08T17:42:23.388538: step 15400, loss 5.21702, acc 0.698874

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-15400

2017-08-08T17:42:23.989363: step 15401, loss 1.6205e-07, acc 1
2017-08-08T17:42:24.338634: step 15402, loss 3.74388e-07, acc 1
2017-08-08T17:42:24.775845: step 15403, loss 1.38646e-05, acc 1
2017-08-08T17:42:25.028877: step 15404, loss 0.000104535, acc 1
2017-08-08T17:42:25.273601: step 15405, loss 3.55029e-05, acc 1
2017-08-08T17:42:25.636554: step 15406, loss 1.52362e-06, acc 1
2017-08-08T17:42:25.863373: step 15407, loss 0.000410406, acc 1
2017-08-08T17:42:26.125423: step 15408, loss 3.67854e-06, acc 1
2017-08-08T17:42:26.349522: step 15409, loss 0.000193055, acc 1
2017-08-08T17:42:26.722158: step 15410, loss 2.62632e-07, acc 1
2017-08-08T17:42:27.129223: step 15411, loss 1.17346e-07, acc 1
2017-08-08T17:42:27.412058: step 15412, loss 0.0332483, acc 0.984375
2017-08-08T17:42:27.627329: step 15413, loss 3.67113e-06, acc 1
2017-08-08T17:42:27.873431: step 15414, loss 0.000471019, acc 1
2017-08-08T17:42:28.258043: step 15415, loss 1.44267e-05, acc 1
2017-08-08T17:42:28.458663: step 15416, loss 0.00011338, acc 1
2017-08-08T17:42:28.656989: step 15417, loss 8.19785e-05, acc 1
2017-08-08T17:42:28.897377: step 15418, loss 0.000201926, acc 1
2017-08-08T17:42:29.385374: step 15419, loss 0.000359722, acc 1
2017-08-08T17:42:29.823576: step 15420, loss 0.000147476, acc 1
2017-08-08T17:42:30.092139: step 15421, loss 6.70551e-08, acc 1
2017-08-08T17:42:30.425403: step 15422, loss 0.000275713, acc 1
2017-08-08T17:42:30.885300: step 15423, loss 1.88127e-07, acc 1
2017-08-08T17:42:31.192304: step 15424, loss 3.0081e-06, acc 1
2017-08-08T17:42:31.498966: step 15425, loss 7.59465e-05, acc 1
2017-08-08T17:42:31.834667: step 15426, loss 3.35274e-07, acc 1
2017-08-08T17:42:32.307916: step 15427, loss 1.28522e-07, acc 1
2017-08-08T17:42:32.655248: step 15428, loss 4.016e-05, acc 1
2017-08-08T17:42:32.894589: step 15429, loss 0.00807033, acc 1
2017-08-08T17:42:33.174573: step 15430, loss 0.000389022, acc 1
2017-08-08T17:42:33.591353: step 15431, loss 1.5087e-06, acc 1
2017-08-08T17:42:33.874223: step 15432, loss 1.58277e-05, acc 1
2017-08-08T17:42:34.110520: step 15433, loss 0.000601013, acc 1
2017-08-08T17:42:34.500841: step 15434, loss 0.000374691, acc 1
2017-08-08T17:42:34.745952: step 15435, loss 1.77183e-05, acc 1
2017-08-08T17:42:35.113707: step 15436, loss 1.1481e-05, acc 1
2017-08-08T17:42:35.421860: step 15437, loss 0.00549698, acc 1
2017-08-08T17:42:35.709557: step 15438, loss 8.94053e-07, acc 1
2017-08-08T17:42:36.075458: step 15439, loss 2.85429e-05, acc 1
2017-08-08T17:42:36.280117: step 15440, loss 3.11051e-06, acc 1
2017-08-08T17:42:36.557370: step 15441, loss 0.000162744, acc 1
2017-08-08T17:42:36.804477: step 15442, loss 7.14043e-05, acc 1
2017-08-08T17:42:37.097557: step 15443, loss 0.0102461, acc 1
2017-08-08T17:42:37.414715: step 15444, loss 4.02838e-05, acc 1
2017-08-08T17:42:37.669158: step 15445, loss 1.1157e-06, acc 1
2017-08-08T17:42:38.111812: step 15446, loss 1.35658e-05, acc 1
2017-08-08T17:42:38.423934: step 15447, loss 0.000481306, acc 1
2017-08-08T17:42:38.841743: step 15448, loss 0.0010357, acc 1
2017-08-08T17:42:39.188841: step 15449, loss 1.27234e-05, acc 1
2017-08-08T17:42:39.440567: step 15450, loss 7.39237e-06, acc 1
2017-08-08T17:42:39.894257: step 15451, loss 0.000129285, acc 1
2017-08-08T17:42:40.167884: step 15452, loss 0.00288644, acc 1
2017-08-08T17:42:40.570171: step 15453, loss 3.75513e-05, acc 1
2017-08-08T17:42:41.093383: step 15454, loss 2.60768e-07, acc 1
2017-08-08T17:42:41.484925: step 15455, loss 6.07139e-06, acc 1
2017-08-08T17:42:42.018836: step 15456, loss 8.67976e-07, acc 1
2017-08-08T17:42:42.421316: step 15457, loss 1.68521e-05, acc 1
2017-08-08T17:42:42.757033: step 15458, loss 2.21909e-05, acc 1
2017-08-08T17:42:43.085601: step 15459, loss 1.69948e-05, acc 1
2017-08-08T17:42:43.344838: step 15460, loss 3.6627e-05, acc 1
2017-08-08T17:42:43.735644: step 15461, loss 3.27062e-06, acc 1
2017-08-08T17:42:44.045279: step 15462, loss 8.74541e-05, acc 1
2017-08-08T17:42:44.495104: step 15463, loss 1.21072e-07, acc 1
2017-08-08T17:42:44.920639: step 15464, loss 0.175263, acc 0.984375
2017-08-08T17:42:45.289674: step 15465, loss 1.14147e-05, acc 1
2017-08-08T17:42:45.660255: step 15466, loss 4.10219e-05, acc 1
2017-08-08T17:42:45.893368: step 15467, loss 4.01633e-05, acc 1
2017-08-08T17:42:46.122592: step 15468, loss 5.83084e-05, acc 1
2017-08-08T17:42:46.473774: step 15469, loss 2.97455e-05, acc 1
2017-08-08T17:42:46.705515: step 15470, loss 2.98174e-05, acc 1
2017-08-08T17:42:46.986706: step 15471, loss 0.000375361, acc 1
2017-08-08T17:42:47.327071: step 15472, loss 2.6959e-05, acc 1
2017-08-08T17:42:47.793372: step 15473, loss 0.000192452, acc 1
2017-08-08T17:42:48.147863: step 15474, loss 9.28771e-06, acc 1
2017-08-08T17:42:48.391376: step 15475, loss 1.16377e-05, acc 1
2017-08-08T17:42:48.612964: step 15476, loss 3.43081e-06, acc 1
2017-08-08T17:42:49.016275: step 15477, loss 4.03224e-06, acc 1
2017-08-08T17:42:49.253706: step 15478, loss 1.6871e-05, acc 1
2017-08-08T17:42:49.452261: step 15479, loss 7.91613e-07, acc 1
2017-08-08T17:42:49.717835: step 15480, loss 4.41444e-07, acc 1
2017-08-08T17:42:50.066390: step 15481, loss 0.000176473, acc 1
2017-08-08T17:42:50.405401: step 15482, loss 0.000128673, acc 1
2017-08-08T17:42:50.758787: step 15483, loss 0.000213955, acc 1
2017-08-08T17:42:50.993496: step 15484, loss 1.86265e-09, acc 1
2017-08-08T17:42:51.326297: step 15485, loss 5.84567e-05, acc 1
2017-08-08T17:42:51.589306: step 15486, loss 7.63025e-06, acc 1
2017-08-08T17:42:51.805375: step 15487, loss 2.49173e-05, acc 1
2017-08-08T17:42:52.055715: step 15488, loss 4.61324e-06, acc 1
2017-08-08T17:42:52.439034: step 15489, loss 6.91525e-05, acc 1
2017-08-08T17:42:52.861140: step 15490, loss 1.70608e-05, acc 1
2017-08-08T17:42:53.142283: step 15491, loss 3.27242e-05, acc 1
2017-08-08T17:42:53.365961: step 15492, loss 4.13646e-06, acc 1
2017-08-08T17:42:53.648980: step 15493, loss 3.01889e-05, acc 1
2017-08-08T17:42:53.889806: step 15494, loss 0.000415265, acc 1
2017-08-08T17:42:54.107764: step 15495, loss 6.29876e-06, acc 1
2017-08-08T17:42:54.336372: step 15496, loss 0.0188778, acc 0.984375
2017-08-08T17:42:54.717473: step 15497, loss 5.93525e-06, acc 1
2017-08-08T17:42:55.050628: step 15498, loss 0.000158515, acc 1
2017-08-08T17:42:55.409927: step 15499, loss 2.55312e-05, acc 1
2017-08-08T17:42:55.686413: step 15500, loss 0.0435863, acc 0.984375

Evaluation:
2017-08-08T17:42:56.601349: step 15500, loss 5.24341, acc 0.712946

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-15500

2017-08-08T17:42:56.980236: step 15501, loss 2.38599e-06, acc 1
2017-08-08T17:42:57.285328: step 15502, loss 0.000192288, acc 1
2017-08-08T17:42:57.666250: step 15503, loss 0.0881142, acc 0.984375
2017-08-08T17:42:57.920986: step 15504, loss 5.71347e-05, acc 1
2017-08-08T17:42:58.147150: step 15505, loss 0.000358469, acc 1
2017-08-08T17:42:58.386353: step 15506, loss 0.000470577, acc 1
2017-08-08T17:42:58.811469: step 15507, loss 0.00223111, acc 1
2017-08-08T17:42:59.022918: step 15508, loss 6.17162e-06, acc 1
2017-08-08T17:42:59.312990: step 15509, loss 7.23909e-06, acc 1
2017-08-08T17:42:59.589923: step 15510, loss 0.00605723, acc 1
2017-08-08T17:43:00.053637: step 15511, loss 2.97622e-06, acc 1
2017-08-08T17:43:00.469376: step 15512, loss 0.000640792, acc 1
2017-08-08T17:43:00.867365: step 15513, loss 3.86826e-06, acc 1
2017-08-08T17:43:01.133533: step 15514, loss 6.51925e-08, acc 1
2017-08-08T17:43:01.477432: step 15515, loss 2.02092e-06, acc 1
2017-08-08T17:43:01.892564: step 15516, loss 0.0455436, acc 0.984375
2017-08-08T17:43:02.167824: step 15517, loss 0.00323077, acc 1
2017-08-08T17:43:02.456562: step 15518, loss 3.48782e-05, acc 1
2017-08-08T17:43:02.749188: step 15519, loss 7.24299e-05, acc 1
2017-08-08T17:43:03.184475: step 15520, loss 3.03402e-05, acc 1
2017-08-08T17:43:03.620226: step 15521, loss 9.83463e-07, acc 1
2017-08-08T17:43:03.999245: step 15522, loss 1.09709e-06, acc 1
2017-08-08T17:43:04.261580: step 15523, loss 0.000154301, acc 1
2017-08-08T17:43:04.700630: step 15524, loss 5.70469e-06, acc 1
2017-08-08T17:43:05.104144: step 15525, loss 0.0120371, acc 0.984375
2017-08-08T17:43:05.397319: step 15526, loss 7.98075e-06, acc 1
2017-08-08T17:43:05.672936: step 15527, loss 5.55254e-05, acc 1
2017-08-08T17:43:05.931604: step 15528, loss 3.44586e-07, acc 1
2017-08-08T17:43:06.204853: step 15529, loss 6.48524e-06, acc 1
2017-08-08T17:43:06.622855: step 15530, loss 6.5937e-07, acc 1
2017-08-08T17:43:06.908429: step 15531, loss 7.19843e-06, acc 1
2017-08-08T17:43:07.189756: step 15532, loss 0.00095068, acc 1
2017-08-08T17:43:07.408340: step 15533, loss 0.000130532, acc 1
2017-08-08T17:43:07.789769: step 15534, loss 8.81569e-05, acc 1
2017-08-08T17:43:08.008045: step 15535, loss 2.44005e-07, acc 1
2017-08-08T17:43:08.223970: step 15536, loss 9.3132e-08, acc 1
2017-08-08T17:43:08.427663: step 15537, loss 0.000345724, acc 1
2017-08-08T17:43:08.761369: step 15538, loss 5.99537e-05, acc 1
2017-08-08T17:43:09.033376: step 15539, loss 2.38328e-05, acc 1
2017-08-08T17:43:09.321388: step 15540, loss 5.0004e-06, acc 1
2017-08-08T17:43:09.612394: step 15541, loss 0.00120987, acc 1
2017-08-08T17:43:09.907945: step 15542, loss 0.000440063, acc 1
2017-08-08T17:43:10.378913: step 15543, loss 1.7994e-05, acc 1
2017-08-08T17:43:10.649195: step 15544, loss 1.75017e-05, acc 1
2017-08-08T17:43:10.942068: step 15545, loss 6.11599e-06, acc 1
2017-08-08T17:43:11.321411: step 15546, loss 5.32501e-06, acc 1
2017-08-08T17:43:11.762363: step 15547, loss 2.06753e-07, acc 1
2017-08-08T17:43:12.077021: step 15548, loss 4.15367e-07, acc 1
2017-08-08T17:43:12.346860: step 15549, loss 0.0165441, acc 0.984375
2017-08-08T17:43:12.635602: step 15550, loss 1.87434e-05, acc 1
2017-08-08T17:43:13.069370: step 15551, loss 4.68385e-06, acc 1
2017-08-08T17:43:13.312957: step 15552, loss 0.0188712, acc 0.984375
2017-08-08T17:43:13.580000: step 15553, loss 1.93268e-05, acc 1
2017-08-08T17:43:13.835035: step 15554, loss 0.000180178, acc 1
2017-08-08T17:43:14.117363: step 15555, loss 4.32088e-06, acc 1
2017-08-08T17:43:14.566966: step 15556, loss 0.0112552, acc 0.984375
2017-08-08T17:43:14.920416: step 15557, loss 5.80486e-06, acc 1
2017-08-08T17:43:15.158451: step 15558, loss 0.00498664, acc 1
2017-08-08T17:43:15.382895: step 15559, loss 1.44649e-05, acc 1
2017-08-08T17:43:15.759554: step 15560, loss 1.67638e-08, acc 1
2017-08-08T17:43:16.132232: step 15561, loss 0.000965039, acc 1
2017-08-08T17:43:16.405428: step 15562, loss 0.00213721, acc 1
2017-08-08T17:43:16.690610: step 15563, loss 5.00276e-05, acc 1
2017-08-08T17:43:17.056565: step 15564, loss 1.75085e-06, acc 1
2017-08-08T17:43:17.393377: step 15565, loss 2.55969e-05, acc 1
2017-08-08T17:43:17.829340: step 15566, loss 0.000178975, acc 1
2017-08-08T17:43:18.147305: step 15567, loss 2.12238e-05, acc 1
2017-08-08T17:43:18.360546: step 15568, loss 8.5307e-07, acc 1
2017-08-08T17:43:18.810789: step 15569, loss 0.000744904, acc 1
2017-08-08T17:43:19.044012: step 15570, loss 2.01165e-07, acc 1
2017-08-08T17:43:19.328886: step 15571, loss 8.38189e-08, acc 1
2017-08-08T17:43:19.575491: step 15572, loss 5.92476e-05, acc 1
2017-08-08T17:43:19.962393: step 15573, loss 0.0123758, acc 1
2017-08-08T17:43:20.353307: step 15574, loss 2.9447e-06, acc 1
2017-08-08T17:43:20.619405: step 15575, loss 3.04122e-05, acc 1
2017-08-08T17:43:20.823514: step 15576, loss 0.000530373, acc 1
2017-08-08T17:43:21.032718: step 15577, loss 3.29477e-06, acc 1
2017-08-08T17:43:21.461328: step 15578, loss 4.28406e-07, acc 1
2017-08-08T17:43:21.758024: step 15579, loss 0.000133099, acc 1
2017-08-08T17:43:22.029812: step 15580, loss 9.85013e-06, acc 1
2017-08-08T17:43:22.313355: step 15581, loss 3.7439e-07, acc 1
2017-08-08T17:43:22.601348: step 15582, loss 3.28363e-06, acc 1
2017-08-08T17:43:23.029981: step 15583, loss 3.76614e-06, acc 1
2017-08-08T17:43:23.255223: step 15584, loss 0.000949548, acc 1
2017-08-08T17:43:23.487141: step 15585, loss 2.89802e-06, acc 1
2017-08-08T17:43:23.881372: step 15586, loss 0.000601533, acc 1
2017-08-08T17:43:24.155857: step 15587, loss 0.00338279, acc 1
2017-08-08T17:43:24.444918: step 15588, loss 0.000812673, acc 1
2017-08-08T17:43:24.719621: step 15589, loss 0.000175526, acc 1
2017-08-08T17:43:25.106192: step 15590, loss 0.000309108, acc 1
2017-08-08T17:43:25.512580: step 15591, loss 0.000669514, acc 1
2017-08-08T17:43:25.903033: step 15592, loss 1.80732e-05, acc 1
2017-08-08T17:43:26.173658: step 15593, loss 5.54572e-05, acc 1
2017-08-08T17:43:26.359124: step 15594, loss 4.11371e-05, acc 1
2017-08-08T17:43:26.717697: step 15595, loss 0.000110869, acc 1
2017-08-08T17:43:27.045035: step 15596, loss 0.0158877, acc 0.984375
2017-08-08T17:43:27.325348: step 15597, loss 4.50536e-06, acc 1
2017-08-08T17:43:27.589714: step 15598, loss 0.000197991, acc 1
2017-08-08T17:43:27.881134: step 15599, loss 5.84208e-05, acc 1
2017-08-08T17:43:28.248783: step 15600, loss 1.82898e-05, acc 1

Evaluation:
2017-08-08T17:43:29.125946: step 15600, loss 5.24513, acc 0.713884

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-15600

2017-08-08T17:43:29.587037: step 15601, loss 0.000170728, acc 1
2017-08-08T17:43:30.037406: step 15602, loss 1.16785e-06, acc 1
2017-08-08T17:43:30.320755: step 15603, loss 1.2944e-05, acc 1
2017-08-08T17:43:30.610064: step 15604, loss 0.000193285, acc 1
2017-08-08T17:43:30.909301: step 15605, loss 0.000145605, acc 1
2017-08-08T17:43:31.311936: step 15606, loss 4.07918e-07, acc 1
2017-08-08T17:43:31.662087: step 15607, loss 9.9276e-07, acc 1
2017-08-08T17:43:32.082020: step 15608, loss 1.04307e-06, acc 1
2017-08-08T17:43:32.348836: step 15609, loss 4.78676e-06, acc 1
2017-08-08T17:43:32.749373: step 15610, loss 0.00229696, acc 1
2017-08-08T17:43:33.095054: step 15611, loss 0.000297978, acc 1
2017-08-08T17:43:33.411557: step 15612, loss 0.00384421, acc 1
2017-08-08T17:43:33.683309: step 15613, loss 1.30194e-06, acc 1
2017-08-08T17:43:34.103768: step 15614, loss 0.0840745, acc 0.984375
2017-08-08T17:43:34.503095: step 15615, loss 1.86264e-08, acc 1
2017-08-08T17:43:34.854133: step 15616, loss 0.00569644, acc 1
2017-08-08T17:43:35.097723: step 15617, loss 3.94907e-05, acc 1
2017-08-08T17:43:35.294374: step 15618, loss 1.74014e-05, acc 1
2017-08-08T17:43:35.767957: step 15619, loss 1.95202e-06, acc 1
2017-08-08T17:43:36.103409: step 15620, loss 0.0322497, acc 0.984375
2017-08-08T17:43:36.427397: step 15621, loss 7.26896e-05, acc 1
2017-08-08T17:43:36.691787: step 15622, loss 0.000126077, acc 1
2017-08-08T17:43:37.077686: step 15623, loss 5.55682e-05, acc 1
2017-08-08T17:43:37.474520: step 15624, loss 5.41983e-05, acc 1
2017-08-08T17:43:37.845414: step 15625, loss 0.000351129, acc 1
2017-08-08T17:43:38.033188: step 15626, loss 0.0192495, acc 0.984375
2017-08-08T17:43:38.325165: step 15627, loss 2.71149e-05, acc 1
2017-08-08T17:43:38.719591: step 15628, loss 0.000381707, acc 1
2017-08-08T17:43:38.991420: step 15629, loss 0.000777149, acc 1
2017-08-08T17:43:39.177378: step 15630, loss 0.00214887, acc 1
2017-08-08T17:43:39.463999: step 15631, loss 0.000786624, acc 1
2017-08-08T17:43:39.801786: step 15632, loss 5.21261e-05, acc 1
2017-08-08T17:43:40.255142: step 15633, loss 0.000300971, acc 1
2017-08-08T17:43:40.682380: step 15634, loss 9.87185e-07, acc 1
2017-08-08T17:43:40.974543: step 15635, loss 3.05136e-05, acc 1
2017-08-08T17:43:41.151845: step 15636, loss 7.16852e-06, acc 1
2017-08-08T17:43:41.470103: step 15637, loss 9.12999e-05, acc 1
2017-08-08T17:43:41.690614: step 15638, loss 0.000616965, acc 1
2017-08-08T17:43:41.884964: step 15639, loss 9.71561e-05, acc 1
2017-08-08T17:43:42.102251: step 15640, loss 0.000407002, acc 1
2017-08-08T17:43:42.338501: step 15641, loss 0.00224512, acc 1
2017-08-08T17:43:42.674448: step 15642, loss 0.000119197, acc 1
2017-08-08T17:43:42.991115: step 15643, loss 0.00220998, acc 1
2017-08-08T17:43:43.301826: step 15644, loss 6.77334e-06, acc 1
2017-08-08T17:43:43.584033: step 15645, loss 8.56815e-08, acc 1
2017-08-08T17:43:43.874386: step 15646, loss 2.72482e-06, acc 1
2017-08-08T17:43:44.221571: step 15647, loss 6.21448e-06, acc 1
2017-08-08T17:43:44.540202: step 15648, loss 2.58109e-05, acc 1
2017-08-08T17:43:44.848992: step 15649, loss 3.7194e-06, acc 1
2017-08-08T17:43:45.178745: step 15650, loss 2.16053e-06, acc 1
2017-08-08T17:43:45.577871: step 15651, loss 0.00531068, acc 1
2017-08-08T17:43:46.005497: step 15652, loss 0.00929831, acc 1
2017-08-08T17:43:46.302062: step 15653, loss 7.62943e-06, acc 1
2017-08-08T17:43:46.538180: step 15654, loss 4.41442e-07, acc 1
2017-08-08T17:43:46.906045: step 15655, loss 0.000100508, acc 1
2017-08-08T17:43:47.203059: step 15656, loss 0.000385812, acc 1
2017-08-08T17:43:47.435804: step 15657, loss 0.0697141, acc 0.984375
2017-08-08T17:43:47.653253: step 15658, loss 0.000137062, acc 1
2017-08-08T17:43:47.873986: step 15659, loss 6.66821e-07, acc 1
2017-08-08T17:43:48.260065: step 15660, loss 8.48489e-06, acc 1
2017-08-08T17:43:48.606563: step 15661, loss 3.62423e-05, acc 1
2017-08-08T17:43:48.875271: step 15662, loss 1.43488e-05, acc 1
2017-08-08T17:43:49.108793: step 15663, loss 1.59848e-05, acc 1
2017-08-08T17:43:49.365245: step 15664, loss 5.09372e-05, acc 1
2017-08-08T17:43:49.814350: step 15665, loss 7.17103e-07, acc 1
2017-08-08T17:43:50.010527: step 15666, loss 7.00342e-07, acc 1
2017-08-08T17:43:50.300977: step 15667, loss 5.39365e-06, acc 1
2017-08-08T17:43:50.703369: step 15668, loss 5.47185e-06, acc 1
2017-08-08T17:43:51.105384: step 15669, loss 8.20373e-05, acc 1
2017-08-08T17:43:51.490197: step 15670, loss 4.38225e-06, acc 1
2017-08-08T17:43:51.816528: step 15671, loss 3.92442e-06, acc 1
2017-08-08T17:43:52.049997: step 15672, loss 4.25932e-06, acc 1
2017-08-08T17:43:52.357454: step 15673, loss 0.0236411, acc 0.984375
2017-08-08T17:43:52.730102: step 15674, loss 2.51456e-07, acc 1
2017-08-08T17:43:52.965709: step 15675, loss 1.67638e-08, acc 1
2017-08-08T17:43:53.194799: step 15676, loss 1.5281e-05, acc 1
2017-08-08T17:43:53.504838: step 15677, loss 7.60715e-06, acc 1
2017-08-08T17:43:53.833523: step 15678, loss 0.000147725, acc 1
2017-08-08T17:43:54.156952: step 15679, loss 2.81061e-06, acc 1
2017-08-08T17:43:54.470140: step 15680, loss 7.99924e-05, acc 1
2017-08-08T17:43:54.651217: step 15681, loss 0.00144294, acc 1
2017-08-08T17:43:54.950557: step 15682, loss 0.000167725, acc 1
2017-08-08T17:43:55.285123: step 15683, loss 2.21654e-07, acc 1
2017-08-08T17:43:55.550597: step 15684, loss 0.000196091, acc 1
2017-08-08T17:43:55.858260: step 15685, loss 3.53903e-08, acc 1
2017-08-08T17:43:56.145709: step 15686, loss 0.000747408, acc 1
2017-08-08T17:43:56.553653: step 15687, loss 2.13129e-05, acc 1
2017-08-08T17:43:56.916920: step 15688, loss 3.54259e-06, acc 1
2017-08-08T17:43:57.292262: step 15689, loss 0.000184102, acc 1
2017-08-08T17:43:57.547199: step 15690, loss 3.60889e-05, acc 1
2017-08-08T17:43:57.884962: step 15691, loss 0.000200006, acc 1
2017-08-08T17:43:58.191723: step 15692, loss 0.00189174, acc 1
2017-08-08T17:43:58.396921: step 15693, loss 7.89707e-05, acc 1
2017-08-08T17:43:58.638497: step 15694, loss 0.000394086, acc 1
2017-08-08T17:43:58.960207: step 15695, loss 2.38966e-06, acc 1
2017-08-08T17:43:59.307641: step 15696, loss 6.92974e-06, acc 1
2017-08-08T17:43:59.624749: step 15697, loss 0.00152248, acc 1
2017-08-08T17:43:59.931404: step 15698, loss 0.107016, acc 0.984375
2017-08-08T17:44:00.373957: step 15699, loss 1.60155e-05, acc 1
2017-08-08T17:44:00.664789: step 15700, loss 0.000220719, acc 1

Evaluation:
2017-08-08T17:44:01.545346: step 15700, loss 5.25408, acc 0.710131

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-15700

2017-08-08T17:44:02.321375: step 15701, loss 0.000974284, acc 1
2017-08-08T17:44:02.575611: step 15702, loss 7.64178e-05, acc 1
2017-08-08T17:44:02.774951: step 15703, loss 0.00104896, acc 1
2017-08-08T17:44:03.168508: step 15704, loss 0.002106, acc 1
2017-08-08T17:44:03.487905: step 15705, loss 2.57043e-07, acc 1
2017-08-08T17:44:03.743943: step 15706, loss 2.42126e-06, acc 1
2017-08-08T17:44:03.991176: step 15707, loss 1.13257e-05, acc 1
2017-08-08T17:44:04.303579: step 15708, loss 0.00107858, acc 1
2017-08-08T17:44:04.771998: step 15709, loss 2.06026e-05, acc 1
2017-08-08T17:44:05.195939: step 15710, loss 3.52954e-06, acc 1
2017-08-08T17:44:05.464618: step 15711, loss 6.33299e-08, acc 1
2017-08-08T17:44:05.671850: step 15712, loss 7.51965e-05, acc 1
2017-08-08T17:44:06.054167: step 15713, loss 8.95909e-07, acc 1
2017-08-08T17:44:06.375246: step 15714, loss 8.27006e-07, acc 1
2017-08-08T17:44:06.682137: step 15715, loss 1.59097e-05, acc 1
2017-08-08T17:44:06.951818: step 15716, loss 4.68459e-05, acc 1
2017-08-08T17:44:07.308812: step 15717, loss 6.67904e-06, acc 1
2017-08-08T17:44:07.601274: step 15718, loss 4.79631e-05, acc 1
2017-08-08T17:44:07.992164: step 15719, loss 0.000980167, acc 1
2017-08-08T17:44:08.306553: step 15720, loss 1.41957e-05, acc 1
2017-08-08T17:44:08.581377: step 15721, loss 0.000501854, acc 1
2017-08-08T17:44:09.019970: step 15722, loss 3.37104e-06, acc 1
2017-08-08T17:44:09.250947: step 15723, loss 9.00704e-06, acc 1
2017-08-08T17:44:09.493933: step 15724, loss 0.000340572, acc 1
2017-08-08T17:44:09.781231: step 15725, loss 0.000150121, acc 1
2017-08-08T17:44:10.056652: step 15726, loss 0.0001267, acc 1
2017-08-08T17:44:10.380924: step 15727, loss 0.00176512, acc 1
2017-08-08T17:44:10.730530: step 15728, loss 2.88462e-05, acc 1
2017-08-08T17:44:11.015485: step 15729, loss 4.76439e-06, acc 1
2017-08-08T17:44:11.276389: step 15730, loss 0.00374663, acc 1
2017-08-08T17:44:11.508129: step 15731, loss 1.5954e-05, acc 1
2017-08-08T17:44:11.898816: step 15732, loss 7.84732e-06, acc 1
2017-08-08T17:44:12.151960: step 15733, loss 7.93961e-05, acc 1
2017-08-08T17:44:12.426003: step 15734, loss 3.96858e-05, acc 1
2017-08-08T17:44:12.850801: step 15735, loss 0.000355273, acc 1
2017-08-08T17:44:13.240473: step 15736, loss 4.47035e-08, acc 1
2017-08-08T17:44:13.568903: step 15737, loss 5.70049e-05, acc 1
2017-08-08T17:44:13.844956: step 15738, loss 4.40012e-05, acc 1
2017-08-08T17:44:14.097529: step 15739, loss 0.000117711, acc 1
2017-08-08T17:44:14.493119: step 15740, loss 1.41561e-07, acc 1
2017-08-08T17:44:14.822266: step 15741, loss 9.7484e-05, acc 1
2017-08-08T17:44:15.100465: step 15742, loss 5.22515e-05, acc 1
2017-08-08T17:44:15.439781: step 15743, loss 1.10267e-06, acc 1
2017-08-08T17:44:15.853706: step 15744, loss 5.19674e-07, acc 1
2017-08-08T17:44:16.268884: step 15745, loss 3.03609e-07, acc 1
2017-08-08T17:44:16.710535: step 15746, loss 1.53847e-06, acc 1
2017-08-08T17:44:16.967898: step 15747, loss 1.14139e-05, acc 1
2017-08-08T17:44:17.234219: step 15748, loss 0.000146039, acc 1
2017-08-08T17:44:17.722395: step 15749, loss 1.03873e-05, acc 1
2017-08-08T17:44:17.997679: step 15750, loss 8.64245e-07, acc 1
2017-08-08T17:44:18.307352: step 15751, loss 1.93145e-06, acc 1
2017-08-08T17:44:18.705926: step 15752, loss 6.45854e-06, acc 1
2017-08-08T17:44:19.122590: step 15753, loss 2.3002e-05, acc 1
2017-08-08T17:44:19.505463: step 15754, loss 1.14189e-05, acc 1
2017-08-08T17:44:19.789336: step 15755, loss 1.90909e-06, acc 1
2017-08-08T17:44:20.052604: step 15756, loss 2.13525e-05, acc 1
2017-08-08T17:44:20.364255: step 15757, loss 0.000676395, acc 1
2017-08-08T17:44:20.570998: step 15758, loss 7.70817e-05, acc 1
2017-08-08T17:44:20.853450: step 15759, loss 0.000110038, acc 1
2017-08-08T17:44:21.105379: step 15760, loss 1.30836e-05, acc 1
2017-08-08T17:44:21.559553: step 15761, loss 6.97889e-06, acc 1
2017-08-08T17:44:21.969346: step 15762, loss 0.000179572, acc 1
2017-08-08T17:44:22.323932: step 15763, loss 4.8223e-05, acc 1
2017-08-08T17:44:22.547400: step 15764, loss 0.00229334, acc 1
2017-08-08T17:44:22.883119: step 15765, loss 4.13301e-06, acc 1
2017-08-08T17:44:23.248060: step 15766, loss 8.69362e-05, acc 1
2017-08-08T17:44:23.513406: step 15767, loss 4.50417e-05, acc 1
2017-08-08T17:44:23.808853: step 15768, loss 1.52182e-05, acc 1
2017-08-08T17:44:24.245964: step 15769, loss 2.03028e-07, acc 1
2017-08-08T17:44:24.626649: step 15770, loss 0.000102003, acc 1
2017-08-08T17:44:25.030846: step 15771, loss 7.16478e-06, acc 1
2017-08-08T17:44:25.275284: step 15772, loss 3.61974e-05, acc 1
2017-08-08T17:44:25.490055: step 15773, loss 0.00329932, acc 1
2017-08-08T17:44:25.840276: step 15774, loss 3.48235e-05, acc 1
2017-08-08T17:44:26.088053: step 15775, loss 9.85235e-05, acc 1
2017-08-08T17:44:26.362306: step 15776, loss 7.44442e-06, acc 1
2017-08-08T17:44:26.628608: step 15777, loss 9.03363e-07, acc 1
2017-08-08T17:44:27.065363: step 15778, loss 7.24553e-07, acc 1
2017-08-08T17:44:27.488813: step 15779, loss 2.42144e-08, acc 1
2017-08-08T17:44:27.868781: step 15780, loss 1.31498e-06, acc 1
2017-08-08T17:44:28.133317: step 15781, loss 1.44535e-06, acc 1
2017-08-08T17:44:28.365862: step 15782, loss 0.00108055, acc 1
2017-08-08T17:44:28.766448: step 15783, loss 4.50755e-07, acc 1
2017-08-08T17:44:29.056378: step 15784, loss 0.0863535, acc 0.984375
2017-08-08T17:44:29.336742: step 15785, loss 0.000372285, acc 1
2017-08-08T17:44:29.601011: step 15786, loss 8.68586e-06, acc 1
2017-08-08T17:44:29.867085: step 15787, loss 6.24855e-06, acc 1
2017-08-08T17:44:30.293609: step 15788, loss 8.86092e-05, acc 1
2017-08-08T17:44:30.714590: step 15789, loss 1.63478e-05, acc 1
2017-08-08T17:44:30.976939: step 15790, loss 1.30637e-05, acc 1
2017-08-08T17:44:31.224041: step 15791, loss 8.64446e-06, acc 1
2017-08-08T17:44:31.571315: step 15792, loss 7.53657e-06, acc 1
2017-08-08T17:44:31.884101: step 15793, loss 2.07857e-05, acc 1
2017-08-08T17:44:32.149542: step 15794, loss 3.29104e-06, acc 1
2017-08-08T17:44:32.404733: step 15795, loss 0.000175494, acc 1
2017-08-08T17:44:32.869492: step 15796, loss 3.50701e-06, acc 1
2017-08-08T17:44:33.276729: step 15797, loss 2.07489e-06, acc 1
2017-08-08T17:44:33.667562: step 15798, loss 3.78115e-07, acc 1
2017-08-08T17:44:33.911367: step 15799, loss 0.000172208, acc 1
2017-08-08T17:44:34.143564: step 15800, loss 5.53713e-06, acc 1

Evaluation:
2017-08-08T17:44:34.979907: step 15800, loss 5.35731, acc 0.714822

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-15800

2017-08-08T17:44:35.437564: step 15801, loss 8.48657e-06, acc 1
2017-08-08T17:44:35.905378: step 15802, loss 0.000341552, acc 1
2017-08-08T17:44:36.209695: step 15803, loss 0.000222724, acc 1
2017-08-08T17:44:36.481932: step 15804, loss 2.89299e-05, acc 1
2017-08-08T17:44:36.671043: step 15805, loss 1.36154e-06, acc 1
2017-08-08T17:44:36.953361: step 15806, loss 7.49985e-05, acc 1
2017-08-08T17:44:37.243869: step 15807, loss 1.58877e-06, acc 1
2017-08-08T17:44:37.468346: step 15808, loss 6.02889e-06, acc 1
2017-08-08T17:44:37.752036: step 15809, loss 2.99883e-07, acc 1
2017-08-08T17:44:38.177683: step 15810, loss 0.000404149, acc 1
2017-08-08T17:44:38.564668: step 15811, loss 4.02753e-05, acc 1
2017-08-08T17:44:38.953325: step 15812, loss 9.24757e-05, acc 1
2017-08-08T17:44:39.257844: step 15813, loss 1.27391e-05, acc 1
2017-08-08T17:44:39.502315: step 15814, loss 0.00914422, acc 1
2017-08-08T17:44:39.922737: step 15815, loss 0.000142619, acc 1
2017-08-08T17:44:40.281152: step 15816, loss 6.51925e-08, acc 1
2017-08-08T17:44:40.521595: step 15817, loss 0.00518249, acc 1
2017-08-08T17:44:40.783615: step 15818, loss 3.04333e-05, acc 1
2017-08-08T17:44:41.173472: step 15819, loss 0.000966935, acc 1
2017-08-08T17:44:41.549968: step 15820, loss 3.00994e-05, acc 1
2017-08-08T17:44:41.909747: step 15821, loss 4.04734e-06, acc 1
2017-08-08T17:44:42.110061: step 15822, loss 1.07844e-06, acc 1
2017-08-08T17:44:42.299585: step 15823, loss 0.000871985, acc 1
2017-08-08T17:44:42.577448: step 15824, loss 2.54608e-06, acc 1
2017-08-08T17:44:42.757744: step 15825, loss 0.0574976, acc 0.984375
2017-08-08T17:44:42.943237: step 15826, loss 1.8611e-05, acc 1
2017-08-08T17:44:43.232928: step 15827, loss 9.92759e-07, acc 1
2017-08-08T17:44:43.505372: step 15828, loss 0.00197566, acc 1
2017-08-08T17:44:43.847640: step 15829, loss 3.43592e-05, acc 1
2017-08-08T17:44:44.187761: step 15830, loss 6.4942e-05, acc 1
2017-08-08T17:44:44.440932: step 15831, loss 5.33579e-06, acc 1
2017-08-08T17:44:44.705325: step 15832, loss 1.34109e-06, acc 1
2017-08-08T17:44:45.106907: step 15833, loss 2.01721e-06, acc 1
2017-08-08T17:44:45.432560: step 15834, loss 4.65661e-08, acc 1
2017-08-08T17:44:45.711423: step 15835, loss 1.14363e-06, acc 1
2017-08-08T17:44:46.009129: step 15836, loss 2.4288e-05, acc 1
2017-08-08T17:44:46.412094: step 15837, loss 4.78697e-07, acc 1
2017-08-08T17:44:46.795650: step 15838, loss 3.35057e-06, acc 1
2017-08-08T17:44:47.134171: step 15839, loss 1.0267e-05, acc 1
2017-08-08T17:44:47.382569: step 15840, loss 0.000686996, acc 1
2017-08-08T17:44:47.539398: step 15841, loss 2.98023e-08, acc 1
2017-08-08T17:44:47.828994: step 15842, loss 1.87829e-05, acc 1
2017-08-08T17:44:48.168759: step 15843, loss 0.00307226, acc 1
2017-08-08T17:44:48.411288: step 15844, loss 0.000106241, acc 1
2017-08-08T17:44:48.626092: step 15845, loss 3.72529e-08, acc 1
2017-08-08T17:44:48.842909: step 15846, loss 2.97825e-06, acc 1
2017-08-08T17:44:49.157331: step 15847, loss 0.000352451, acc 1
2017-08-08T17:44:49.535995: step 15848, loss 0.000851243, acc 1
2017-08-08T17:44:49.870517: step 15849, loss 3.45564e-05, acc 1
2017-08-08T17:44:50.119097: step 15850, loss 1.81417e-06, acc 1
2017-08-08T17:44:50.326672: step 15851, loss 2.66358e-07, acc 1
2017-08-08T17:44:50.552335: step 15852, loss 0.000323017, acc 1
2017-08-08T17:44:50.874167: step 15853, loss 0.000116334, acc 1
2017-08-08T17:44:51.065392: step 15854, loss 2.11531e-05, acc 1
2017-08-08T17:44:51.293669: step 15855, loss 0.00255211, acc 1
2017-08-08T17:44:51.544444: step 15856, loss 0.000266901, acc 1
2017-08-08T17:44:51.970131: step 15857, loss 5.51667e-06, acc 1
2017-08-08T17:44:52.277679: step 15858, loss 7.07804e-08, acc 1
2017-08-08T17:44:52.562479: step 15859, loss 2.91119e-06, acc 1
2017-08-08T17:44:52.784164: step 15860, loss 0.000185639, acc 1
2017-08-08T17:44:53.170263: step 15861, loss 2.60965e-05, acc 1
2017-08-08T17:44:53.411048: step 15862, loss 3.1606e-05, acc 1
2017-08-08T17:44:53.701508: step 15863, loss 2.22019e-06, acc 1
2017-08-08T17:44:53.947823: step 15864, loss 8.77759e-06, acc 1
2017-08-08T17:44:54.269610: step 15865, loss 4.66729e-06, acc 1
2017-08-08T17:44:54.641543: step 15866, loss 6.78208e-05, acc 1
2017-08-08T17:44:54.937561: step 15867, loss 7.33051e-06, acc 1
2017-08-08T17:44:55.259836: step 15868, loss 8.40036e-07, acc 1
2017-08-08T17:44:55.497270: step 15869, loss 0.000952524, acc 1
2017-08-08T17:44:55.872475: step 15870, loss 2.06753e-07, acc 1
2017-08-08T17:44:56.271113: step 15871, loss 2.98023e-08, acc 1
2017-08-08T17:44:56.526478: step 15872, loss 0.093796, acc 0.984375
2017-08-08T17:44:56.770736: step 15873, loss 3.67341e-05, acc 1
2017-08-08T17:44:57.189371: step 15874, loss 3.53902e-07, acc 1
2017-08-08T17:44:57.631851: step 15875, loss 1.86265e-09, acc 1
2017-08-08T17:44:57.982118: step 15876, loss 0.000558795, acc 1
2017-08-08T17:44:58.318872: step 15877, loss 1.2796e-06, acc 1
2017-08-08T17:44:58.563821: step 15878, loss 0.000107647, acc 1
2017-08-08T17:44:58.887765: step 15879, loss 2.47353e-06, acc 1
2017-08-08T17:44:59.279763: step 15880, loss 3.2596e-07, acc 1
2017-08-08T17:44:59.590770: step 15881, loss 1.63218e-05, acc 1
2017-08-08T17:44:59.885760: step 15882, loss 9.76008e-07, acc 1
2017-08-08T17:45:00.190872: step 15883, loss 4.82969e-05, acc 1
2017-08-08T17:45:00.547656: step 15884, loss 2.23517e-07, acc 1
2017-08-08T17:45:01.011266: step 15885, loss 5.96733e-05, acc 1
2017-08-08T17:45:01.395609: step 15886, loss 2.6935e-05, acc 1
2017-08-08T17:45:01.759095: step 15887, loss 7.16955e-06, acc 1
2017-08-08T17:45:02.158056: step 15888, loss 2.14204e-07, acc 1
2017-08-08T17:45:02.664486: step 15889, loss 0.000247219, acc 1
2017-08-08T17:45:03.001126: step 15890, loss 9.31322e-09, acc 1
2017-08-08T17:45:03.324823: step 15891, loss 2.83657e-06, acc 1
2017-08-08T17:45:03.681392: step 15892, loss 6.0388e-05, acc 1
2017-08-08T17:45:04.086568: step 15893, loss 3.73246e-06, acc 1
2017-08-08T17:45:04.486287: step 15894, loss 1.22557e-06, acc 1
2017-08-08T17:45:04.806752: step 15895, loss 2.25186e-06, acc 1
2017-08-08T17:45:05.081932: step 15896, loss 5.63853e-05, acc 1
2017-08-08T17:45:05.542248: step 15897, loss 0.00884663, acc 1
2017-08-08T17:45:05.844526: step 15898, loss 0.000115104, acc 1
2017-08-08T17:45:06.114690: step 15899, loss 0.000403782, acc 1
2017-08-08T17:45:06.358545: step 15900, loss 1.15235e-07, acc 1

Evaluation:
2017-08-08T17:45:07.195251: step 15900, loss 5.31862, acc 0.718574

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-15900

2017-08-08T17:45:07.743415: step 15901, loss 2.9484e-06, acc 1
2017-08-08T17:45:07.992791: step 15902, loss 2.01164e-07, acc 1
2017-08-08T17:45:08.381766: step 15903, loss 9.33756e-06, acc 1
2017-08-08T17:45:08.706189: step 15904, loss 0.000319586, acc 1
2017-08-08T17:45:08.997370: step 15905, loss 1.03551e-05, acc 1
2017-08-08T17:45:09.282646: step 15906, loss 0.000585206, acc 1
2017-08-08T17:45:09.613370: step 15907, loss 0.00117323, acc 1
2017-08-08T17:45:10.093030: step 15908, loss 4.34126e-05, acc 1
2017-08-08T17:45:10.471435: step 15909, loss 1.14176e-06, acc 1
2017-08-08T17:45:10.829765: step 15910, loss 5.02913e-08, acc 1
2017-08-08T17:45:11.082796: step 15911, loss 1.81725e-05, acc 1
2017-08-08T17:45:11.356054: step 15912, loss 4.63585e-06, acc 1
2017-08-08T17:45:11.750858: step 15913, loss 1.21939e-05, acc 1
2017-08-08T17:45:12.038559: step 15914, loss 0.000338783, acc 1
2017-08-08T17:45:12.339569: step 15915, loss 0.000136048, acc 1
2017-08-08T17:45:12.694826: step 15916, loss 7.10107e-05, acc 1
2017-08-08T17:45:13.097585: step 15917, loss 0.000415278, acc 1
2017-08-08T17:45:13.447123: step 15918, loss 5.97471e-05, acc 1
2017-08-08T17:45:13.717555: step 15919, loss 3.20034e-05, acc 1
2017-08-08T17:45:13.989115: step 15920, loss 2.8478e-06, acc 1
2017-08-08T17:45:14.365578: step 15921, loss 2.10096e-06, acc 1
2017-08-08T17:45:14.611243: step 15922, loss 1.67631e-06, acc 1
2017-08-08T17:45:14.891088: step 15923, loss 3.36539e-05, acc 1
2017-08-08T17:45:15.262003: step 15924, loss 2.42319e-06, acc 1
2017-08-08T17:45:15.661417: step 15925, loss 0.000169438, acc 1
2017-08-08T17:45:16.046145: step 15926, loss 4.41444e-07, acc 1
2017-08-08T17:45:16.392171: step 15927, loss 3.27978e-06, acc 1
2017-08-08T17:45:16.674209: step 15928, loss 0.000145388, acc 1
2017-08-08T17:45:17.058945: step 15929, loss 3.05758e-05, acc 1
2017-08-08T17:45:17.358449: step 15930, loss 1.77458e-05, acc 1
2017-08-08T17:45:17.614763: step 15931, loss 0.000298333, acc 1
2017-08-08T17:45:17.854687: step 15932, loss 0.000150246, acc 1
2017-08-08T17:45:18.060603: step 15933, loss 0.0175649, acc 0.984375
2017-08-08T17:45:18.644594: step 15934, loss 2.42129e-06, acc 1
2017-08-08T17:45:19.074562: step 15935, loss 2.5201e-06, acc 1
2017-08-08T17:45:19.405164: step 15936, loss 0.000389663, acc 1
2017-08-08T17:45:19.651350: step 15937, loss 0.000134457, acc 1
2017-08-08T17:45:20.068193: step 15938, loss 5.81565e-05, acc 1
2017-08-08T17:45:20.372053: step 15939, loss 0.000238798, acc 1
2017-08-08T17:45:20.631822: step 15940, loss 2.14004e-06, acc 1
2017-08-08T17:45:20.902497: step 15941, loss 1.3411e-07, acc 1
2017-08-08T17:45:21.296883: step 15942, loss 3.27061e-06, acc 1
2017-08-08T17:45:21.687119: step 15943, loss 0.000190552, acc 1
2017-08-08T17:45:22.072710: step 15944, loss 7.99308e-06, acc 1
2017-08-08T17:45:22.367518: step 15945, loss 0.0016972, acc 1
2017-08-08T17:45:22.616100: step 15946, loss 2.8173e-05, acc 1
2017-08-08T17:45:23.100578: step 15947, loss 7.02508e-06, acc 1
2017-08-08T17:45:23.515436: step 15948, loss 1.42409e-05, acc 1
2017-08-08T17:45:23.807844: step 15949, loss 2.47759e-05, acc 1
2017-08-08T17:45:24.105308: step 15950, loss 3.5628e-05, acc 1
2017-08-08T17:45:24.541387: step 15951, loss 2.04032e-05, acc 1
2017-08-08T17:45:24.907072: step 15952, loss 1.08225e-05, acc 1
2017-08-08T17:45:25.281062: step 15953, loss 2.27e-05, acc 1
2017-08-08T17:45:25.559261: step 15954, loss 7.30148e-07, acc 1
2017-08-08T17:45:25.824251: step 15955, loss 3.33411e-07, acc 1
2017-08-08T17:45:26.245370: step 15956, loss 2.28731e-05, acc 1
2017-08-08T17:45:26.527621: step 15957, loss 0.00207967, acc 1
2017-08-08T17:45:26.811623: step 15958, loss 0.000162848, acc 1
2017-08-08T17:45:27.074785: step 15959, loss 3.10301e-06, acc 1
2017-08-08T17:45:27.349445: step 15960, loss 1.80676e-07, acc 1
2017-08-08T17:45:27.577472: step 15961, loss 2.08046e-06, acc 1
2017-08-08T17:45:27.877341: step 15962, loss 2.56661e-06, acc 1
2017-08-08T17:45:28.098727: step 15963, loss 5.78566e-05, acc 1
2017-08-08T17:45:28.311725: step 15964, loss 2.00916e-05, acc 1
2017-08-08T17:45:28.582050: step 15965, loss 3.9846e-05, acc 1
2017-08-08T17:45:28.815339: step 15966, loss 1.2703e-06, acc 1
2017-08-08T17:45:29.024527: step 15967, loss 7.75489e-06, acc 1
2017-08-08T17:45:29.234124: step 15968, loss 3.53902e-08, acc 1
2017-08-08T17:45:29.595419: step 15969, loss 6.8544e-07, acc 1
2017-08-08T17:45:29.941788: step 15970, loss 7.2269e-07, acc 1
2017-08-08T17:45:30.209669: step 15971, loss 0.000328163, acc 1
2017-08-08T17:45:30.449350: step 15972, loss 0.000152751, acc 1
2017-08-08T17:45:30.650360: step 15973, loss 0.000493763, acc 1
2017-08-08T17:45:30.949373: step 15974, loss 0.000998046, acc 1
2017-08-08T17:45:31.277497: step 15975, loss 0.000184697, acc 1
2017-08-08T17:45:31.598734: step 15976, loss 1.92592e-06, acc 1
2017-08-08T17:45:31.915965: step 15977, loss 2.53629e-05, acc 1
2017-08-08T17:45:32.347056: step 15978, loss 0.000175023, acc 1
2017-08-08T17:45:32.757541: step 15979, loss 0.000144374, acc 1
2017-08-08T17:45:33.134732: step 15980, loss 1.95924e-05, acc 1
2017-08-08T17:45:33.526087: step 15981, loss 1.71616e-05, acc 1
2017-08-08T17:45:33.869122: step 15982, loss 0.000279683, acc 1
2017-08-08T17:45:34.299768: step 15983, loss 0.00291559, acc 1
2017-08-08T17:45:34.575888: step 15984, loss 5.44041e-06, acc 1
2017-08-08T17:45:34.822648: step 15985, loss 1.11759e-08, acc 1
2017-08-08T17:45:35.081308: step 15986, loss 3.94462e-05, acc 1
2017-08-08T17:45:35.320650: step 15987, loss 9.52567e-06, acc 1
2017-08-08T17:45:35.721382: step 15988, loss 7.22532e-05, acc 1
2017-08-08T17:45:36.081182: step 15989, loss 8.77296e-07, acc 1
2017-08-08T17:45:36.360667: step 15990, loss 1.30722e-05, acc 1
2017-08-08T17:45:36.585632: step 15991, loss 1.03764e-05, acc 1
2017-08-08T17:45:36.964950: step 15992, loss 4.28405e-07, acc 1
2017-08-08T17:45:37.353122: step 15993, loss 7.54353e-07, acc 1
2017-08-08T17:45:37.627095: step 15994, loss 5.96197e-06, acc 1
2017-08-08T17:45:37.941148: step 15995, loss 6.28026e-06, acc 1
2017-08-08T17:45:38.212729: step 15996, loss 4.98933e-06, acc 1
2017-08-08T17:45:38.659603: step 15997, loss 8.00936e-08, acc 1
2017-08-08T17:45:39.139738: step 15998, loss 3.25961e-07, acc 1
2017-08-08T17:45:39.578299: step 15999, loss 1.35973e-07, acc 1
2017-08-08T17:45:39.944000: step 16000, loss 0.00216743, acc 1

Evaluation:
2017-08-08T17:45:41.071759: step 16000, loss 5.31947, acc 0.719512

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-16000

2017-08-08T17:45:41.585135: step 16001, loss 8.89441e-06, acc 1
2017-08-08T17:45:41.906993: step 16002, loss 5.01554e-06, acc 1
2017-08-08T17:45:42.225230: step 16003, loss 0.000221203, acc 1
2017-08-08T17:45:42.661933: step 16004, loss 2.79395e-07, acc 1
2017-08-08T17:45:43.098733: step 16005, loss 0.000406543, acc 1
2017-08-08T17:45:43.520047: step 16006, loss 8.28225e-05, acc 1
2017-08-08T17:45:43.875661: step 16007, loss 0.000460429, acc 1
2017-08-08T17:45:44.297255: step 16008, loss 0.000153317, acc 1
2017-08-08T17:45:44.679670: step 16009, loss 0.000585018, acc 1
2017-08-08T17:45:44.898478: step 16010, loss 0.00789021, acc 1
2017-08-08T17:45:45.132225: step 16011, loss 2.78453e-06, acc 1
2017-08-08T17:45:45.370571: step 16012, loss 6.07217e-07, acc 1
2017-08-08T17:45:45.702525: step 16013, loss 1.09333e-06, acc 1
2017-08-08T17:45:46.047445: step 16014, loss 0.000446098, acc 1
2017-08-08T17:45:46.411562: step 16015, loss 1.30385e-07, acc 1
2017-08-08T17:45:46.701662: step 16016, loss 7.76705e-07, acc 1
2017-08-08T17:45:46.947925: step 16017, loss 3.14784e-07, acc 1
2017-08-08T17:45:47.425941: step 16018, loss 1.44417e-05, acc 1
2017-08-08T17:45:47.688624: step 16019, loss 7.0779e-07, acc 1
2017-08-08T17:45:47.928496: step 16020, loss 0.000283409, acc 1
2017-08-08T17:45:48.261375: step 16021, loss 0.00572445, acc 1
2017-08-08T17:45:48.585370: step 16022, loss 0.000595434, acc 1
2017-08-08T17:45:48.879684: step 16023, loss 1.67443e-06, acc 1
2017-08-08T17:45:49.094773: step 16024, loss 8.19563e-08, acc 1
2017-08-08T17:45:49.477458: step 16025, loss 2.04891e-08, acc 1
2017-08-08T17:45:49.706441: step 16026, loss 4.28408e-08, acc 1
2017-08-08T17:45:49.948286: step 16027, loss 3.20913e-05, acc 1
2017-08-08T17:45:50.266755: step 16028, loss 1.20882e-05, acc 1
2017-08-08T17:45:50.813039: step 16029, loss 6.46534e-05, acc 1
2017-08-08T17:45:51.232633: step 16030, loss 2.64846e-06, acc 1
2017-08-08T17:45:51.545514: step 16031, loss 0.000208376, acc 1
2017-08-08T17:45:51.806109: step 16032, loss 0.000258519, acc 1
2017-08-08T17:45:52.132499: step 16033, loss 0.0366983, acc 0.984375
2017-08-08T17:45:52.527457: step 16034, loss 8.70964e-06, acc 1
2017-08-08T17:45:52.785863: step 16035, loss 1.10638e-06, acc 1
2017-08-08T17:45:53.038313: step 16036, loss 0.00936662, acc 1
2017-08-08T17:45:53.312240: step 16037, loss 1.92964e-06, acc 1
2017-08-08T17:45:53.626236: step 16038, loss 6.4633e-07, acc 1
2017-08-08T17:45:53.929477: step 16039, loss 0.0198553, acc 0.984375
2017-08-08T17:45:54.246536: step 16040, loss 4.29557e-05, acc 1
2017-08-08T17:45:54.494136: step 16041, loss 4.65019e-05, acc 1
2017-08-08T17:45:54.937430: step 16042, loss 5.9231e-07, acc 1
2017-08-08T17:45:55.218654: step 16043, loss 0.000248441, acc 1
2017-08-08T17:45:55.458862: step 16044, loss 0.000185807, acc 1
2017-08-08T17:45:55.719809: step 16045, loss 1.49378e-06, acc 1
2017-08-08T17:45:55.973912: step 16046, loss 2.16633e-05, acc 1
2017-08-08T17:45:56.363647: step 16047, loss 3.4949e-05, acc 1
2017-08-08T17:45:56.714915: step 16048, loss 0.00274154, acc 1
2017-08-08T17:45:57.067025: step 16049, loss 4.96728e-06, acc 1
2017-08-08T17:45:57.291569: step 16050, loss 4.90741e-07, acc 1
2017-08-08T17:45:57.532444: step 16051, loss 5.58794e-09, acc 1
2017-08-08T17:45:57.865465: step 16052, loss 2.41814e-05, acc 1
2017-08-08T17:45:58.058394: step 16053, loss 2.22954e-06, acc 1
2017-08-08T17:45:58.309388: step 16054, loss 1.85105e-05, acc 1
2017-08-08T17:45:58.645890: step 16055, loss 5.05854e-06, acc 1
2017-08-08T17:45:59.099384: step 16056, loss 0.00133812, acc 1
2017-08-08T17:45:59.469177: step 16057, loss 0.000695398, acc 1
2017-08-08T17:45:59.759519: step 16058, loss 2.09169e-06, acc 1
2017-08-08T17:45:59.982481: step 16059, loss 0, acc 1
2017-08-08T17:46:00.349233: step 16060, loss 1.78813e-07, acc 1
2017-08-08T17:46:00.692774: step 16061, loss 1.50872e-06, acc 1
2017-08-08T17:46:00.975357: step 16062, loss 0.000738044, acc 1
2017-08-08T17:46:01.217402: step 16063, loss 0.000366632, acc 1
2017-08-08T17:46:01.633328: step 16064, loss 3.27176e-05, acc 1
2017-08-08T17:46:02.046988: step 16065, loss 2.03028e-07, acc 1
2017-08-08T17:46:02.330031: step 16066, loss 1.30196e-06, acc 1
2017-08-08T17:46:02.609985: step 16067, loss 3.12354e-06, acc 1
2017-08-08T17:46:03.115033: step 16068, loss 4.09782e-08, acc 1
2017-08-08T17:46:03.432638: step 16069, loss 1.90728e-06, acc 1
2017-08-08T17:46:03.757416: step 16070, loss 0.000713941, acc 1
2017-08-08T17:46:04.058958: step 16071, loss 7.63738e-05, acc 1
2017-08-08T17:46:04.469523: step 16072, loss 5.01048e-07, acc 1
2017-08-08T17:46:04.814594: step 16073, loss 0.000121181, acc 1
2017-08-08T17:46:05.210671: step 16074, loss 9.23489e-06, acc 1
2017-08-08T17:46:05.522970: step 16075, loss 0.00037047, acc 1
2017-08-08T17:46:05.780217: step 16076, loss 4.50757e-07, acc 1
2017-08-08T17:46:06.215727: step 16077, loss 0.00207632, acc 1
2017-08-08T17:46:06.449930: step 16078, loss 0.00944442, acc 1
2017-08-08T17:46:06.698146: step 16079, loss 6.31257e-05, acc 1
2017-08-08T17:46:06.957651: step 16080, loss 6.30306e-05, acc 1
2017-08-08T17:46:07.397465: step 16081, loss 3.94878e-07, acc 1
2017-08-08T17:46:07.706703: step 16082, loss 4.69888e-06, acc 1
2017-08-08T17:46:08.002116: step 16083, loss 0.000197309, acc 1
2017-08-08T17:46:08.250954: step 16084, loss 1.16041e-06, acc 1
2017-08-08T17:46:08.588227: step 16085, loss 0.00489234, acc 1
2017-08-08T17:46:09.036309: step 16086, loss 7.33204e-05, acc 1
2017-08-08T17:46:09.333990: step 16087, loss 1.49012e-08, acc 1
2017-08-08T17:46:09.620331: step 16088, loss 0.000108283, acc 1
2017-08-08T17:46:09.963342: step 16089, loss 6.91164e-05, acc 1
2017-08-08T17:46:10.432361: step 16090, loss 0.000121482, acc 1
2017-08-08T17:46:10.817821: step 16091, loss 0.000516041, acc 1
2017-08-08T17:46:11.139264: step 16092, loss 8.86267e-06, acc 1
2017-08-08T17:46:11.405587: step 16093, loss 7.54586e-05, acc 1
2017-08-08T17:46:11.827930: step 16094, loss 3.45809e-05, acc 1
2017-08-08T17:46:12.083538: step 16095, loss 5.3487e-06, acc 1
2017-08-08T17:46:12.345708: step 16096, loss 7.59946e-07, acc 1
2017-08-08T17:46:12.609315: step 16097, loss 5.77419e-08, acc 1
2017-08-08T17:46:12.949323: step 16098, loss 3.91151e-07, acc 1
2017-08-08T17:46:13.227117: step 16099, loss 5.4575e-07, acc 1
2017-08-08T17:46:13.572615: step 16100, loss 7.96323e-05, acc 1

Evaluation:
2017-08-08T17:46:14.376865: step 16100, loss 5.38415, acc 0.713884

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-16100

2017-08-08T17:46:14.923718: step 16101, loss 6.68679e-07, acc 1
2017-08-08T17:46:15.212627: step 16102, loss 0.0339715, acc 0.984375
2017-08-08T17:46:15.593064: step 16103, loss 2.19792e-07, acc 1
2017-08-08T17:46:16.035046: step 16104, loss 8.19011e-05, acc 1
2017-08-08T17:46:16.384963: step 16105, loss 7.30809e-05, acc 1
2017-08-08T17:46:16.724383: step 16106, loss 0.00113152, acc 1
2017-08-08T17:46:16.991008: step 16107, loss 1.04864e-06, acc 1
2017-08-08T17:46:17.460196: step 16108, loss 5.12223e-07, acc 1
2017-08-08T17:46:17.729675: step 16109, loss 5.2154e-08, acc 1
2017-08-08T17:46:18.006201: step 16110, loss 3.16991e-06, acc 1
2017-08-08T17:46:18.269109: step 16111, loss 0.000436927, acc 1
2017-08-08T17:46:18.620670: step 16112, loss 0.000703482, acc 1
2017-08-08T17:46:19.088083: step 16113, loss 5.51336e-07, acc 1
2017-08-08T17:46:19.429370: step 16114, loss 2.01059e-05, acc 1
2017-08-08T17:46:19.781341: step 16115, loss 1.49012e-08, acc 1
2017-08-08T17:46:20.025954: step 16116, loss 3.43114e-05, acc 1
2017-08-08T17:46:20.409387: step 16117, loss 3.56135e-05, acc 1
2017-08-08T17:46:20.782698: step 16118, loss 6.00104e-06, acc 1
2017-08-08T17:46:21.045176: step 16119, loss 9.0272e-05, acc 1
2017-08-08T17:46:21.361639: step 16120, loss 5.99918e-05, acc 1
2017-08-08T17:46:21.621039: step 16121, loss 0.0115598, acc 0.984375
2017-08-08T17:46:21.894701: step 16122, loss 0.0215935, acc 0.984375
2017-08-08T17:46:22.097031: step 16123, loss 2.35683e-05, acc 1
2017-08-08T17:46:22.318211: step 16124, loss 6.12798e-07, acc 1
2017-08-08T17:46:22.591959: step 16125, loss 7.45058e-08, acc 1
2017-08-08T17:46:22.835722: step 16126, loss 1.70731e-05, acc 1
2017-08-08T17:46:23.104109: step 16127, loss 1.45362e-05, acc 1
2017-08-08T17:46:23.479158: step 16128, loss 4.88006e-07, acc 1
2017-08-08T17:46:23.901977: step 16129, loss 7.31317e-05, acc 1
2017-08-08T17:46:24.240758: step 16130, loss 2.04886e-06, acc 1
2017-08-08T17:46:24.592486: step 16131, loss 2.98021e-07, acc 1
2017-08-08T17:46:24.850083: step 16132, loss 2.83847e-06, acc 1
2017-08-08T17:46:25.117302: step 16133, loss 6.09082e-07, acc 1
2017-08-08T17:46:25.402513: step 16134, loss 2.62482e-05, acc 1
2017-08-08T17:46:25.613606: step 16135, loss 6.3143e-07, acc 1
2017-08-08T17:46:25.834233: step 16136, loss 6.0939e-05, acc 1
2017-08-08T17:46:26.169029: step 16137, loss 2.11433e-05, acc 1
2017-08-08T17:46:26.626632: step 16138, loss 0.00961284, acc 1
2017-08-08T17:46:26.947092: step 16139, loss 1.21913e-05, acc 1
2017-08-08T17:46:27.134402: step 16140, loss 0.0021646, acc 1
2017-08-08T17:46:27.336913: step 16141, loss 1.24717e-05, acc 1
2017-08-08T17:46:27.781855: step 16142, loss 5.4537e-05, acc 1
2017-08-08T17:46:28.049413: step 16143, loss 3.71215e-05, acc 1
2017-08-08T17:46:28.367937: step 16144, loss 8.30632e-05, acc 1
2017-08-08T17:46:28.737665: step 16145, loss 3.19426e-06, acc 1
2017-08-08T17:46:29.134453: step 16146, loss 2.449e-05, acc 1
2017-08-08T17:46:29.506673: step 16147, loss 1.24129e-05, acc 1
2017-08-08T17:46:29.802840: step 16148, loss 0.000173532, acc 1
2017-08-08T17:46:30.011222: step 16149, loss 4.8655e-05, acc 1
2017-08-08T17:46:30.353334: step 16150, loss 1.19175e-05, acc 1
2017-08-08T17:46:30.684148: step 16151, loss 1.93519e-06, acc 1
2017-08-08T17:46:30.952898: step 16152, loss 1.43423e-07, acc 1
2017-08-08T17:46:31.223184: step 16153, loss 1.14642e-05, acc 1
2017-08-08T17:46:31.500132: step 16154, loss 0.0448752, acc 0.984375
2017-08-08T17:46:31.865284: step 16155, loss 1.78202e-05, acc 1
2017-08-08T17:46:32.333582: step 16156, loss 0.0216532, acc 0.984375
2017-08-08T17:46:32.689199: step 16157, loss 3.29854e-06, acc 1
2017-08-08T17:46:32.965695: step 16158, loss 2.73808e-07, acc 1
2017-08-08T17:46:33.157683: step 16159, loss 0.000104192, acc 1
2017-08-08T17:46:33.495951: step 16160, loss 0.0129537, acc 0.984375
2017-08-08T17:46:33.701308: step 16161, loss 3.5593e-06, acc 1
2017-08-08T17:46:33.963976: step 16162, loss 0.0282545, acc 0.984375
2017-08-08T17:46:34.213937: step 16163, loss 4.36015e-06, acc 1
2017-08-08T17:46:34.623936: step 16164, loss 3.62687e-05, acc 1
2017-08-08T17:46:35.070788: step 16165, loss 4.53311e-06, acc 1
2017-08-08T17:46:35.480443: step 16166, loss 1.98364e-06, acc 1
2017-08-08T17:46:35.713188: step 16167, loss 0.00464851, acc 1
2017-08-08T17:46:35.964885: step 16168, loss 7.41325e-07, acc 1
2017-08-08T17:46:36.363038: step 16169, loss 8.22955e-06, acc 1
2017-08-08T17:46:36.626849: step 16170, loss 4.00465e-07, acc 1
2017-08-08T17:46:36.912780: step 16171, loss 1.24236e-06, acc 1
2017-08-08T17:46:37.155946: step 16172, loss 0.0536476, acc 0.984375
2017-08-08T17:46:37.582101: step 16173, loss 6.65209e-06, acc 1
2017-08-08T17:46:37.924087: step 16174, loss 2.53172e-05, acc 1
2017-08-08T17:46:38.286492: step 16175, loss 3.85367e-06, acc 1
2017-08-08T17:46:38.506534: step 16176, loss 1.38017e-06, acc 1
2017-08-08T17:46:38.789306: step 16177, loss 3.6952e-06, acc 1
2017-08-08T17:46:39.261018: step 16178, loss 0.000118742, acc 1
2017-08-08T17:46:39.515970: step 16179, loss 4.44606e-05, acc 1
2017-08-08T17:46:39.795311: step 16180, loss 1.34361e-05, acc 1
2017-08-08T17:46:40.020557: step 16181, loss 4.21107e-05, acc 1
2017-08-08T17:46:40.273995: step 16182, loss 1.58488e-05, acc 1
2017-08-08T17:46:40.743388: step 16183, loss 0.000522291, acc 1
2017-08-08T17:46:41.123673: step 16184, loss 0.000183517, acc 1
2017-08-08T17:46:41.394915: step 16185, loss 0.000358234, acc 1
2017-08-08T17:46:41.629269: step 16186, loss 1.1703e-05, acc 1
2017-08-08T17:46:41.993380: step 16187, loss 8.27727e-06, acc 1
2017-08-08T17:46:42.225522: step 16188, loss 0.000413904, acc 1
2017-08-08T17:46:42.447301: step 16189, loss 1.13564e-05, acc 1
2017-08-08T17:46:42.678398: step 16190, loss 0.000177255, acc 1
2017-08-08T17:46:42.939131: step 16191, loss 4.84287e-08, acc 1
2017-08-08T17:46:43.372622: step 16192, loss 5.69965e-07, acc 1
2017-08-08T17:46:43.697399: step 16193, loss 0.000133767, acc 1
2017-08-08T17:46:44.024347: step 16194, loss 0.00625247, acc 1
2017-08-08T17:46:44.268941: step 16195, loss 5.40165e-07, acc 1
2017-08-08T17:46:44.604433: step 16196, loss 2.32862e-05, acc 1
2017-08-08T17:46:44.956942: step 16197, loss 0.000619425, acc 1
2017-08-08T17:46:45.243220: step 16198, loss 3.96694e-06, acc 1
2017-08-08T17:46:45.577964: step 16199, loss 0.000104831, acc 1
2017-08-08T17:46:45.926844: step 16200, loss 2.80141e-07, acc 1

Evaluation:
2017-08-08T17:46:46.828706: step 16200, loss 5.43197, acc 0.712008

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-16200

2017-08-08T17:46:47.277840: step 16201, loss 1.62749e-05, acc 1
2017-08-08T17:46:47.492937: step 16202, loss 0.000303708, acc 1
2017-08-08T17:46:47.870363: step 16203, loss 3.08399e-05, acc 1
2017-08-08T17:46:48.190509: step 16204, loss 3.21848e-06, acc 1
2017-08-08T17:46:48.477191: step 16205, loss 4.92407e-06, acc 1
2017-08-08T17:46:48.760125: step 16206, loss 0.000122611, acc 1
2017-08-08T17:46:49.173355: step 16207, loss 1.16599e-06, acc 1
2017-08-08T17:46:49.476109: step 16208, loss 2.51652e-05, acc 1
2017-08-08T17:46:49.824588: step 16209, loss 5.36437e-07, acc 1
2017-08-08T17:46:50.129661: step 16210, loss 5.36438e-07, acc 1
2017-08-08T17:46:50.341203: step 16211, loss 5.86941e-05, acc 1
2017-08-08T17:46:50.724348: step 16212, loss 2.2667e-05, acc 1
2017-08-08T17:46:50.946764: step 16213, loss 0.000691732, acc 1
2017-08-08T17:46:51.182913: step 16214, loss 1.0932e-05, acc 1
2017-08-08T17:46:51.472224: step 16215, loss 9.30066e-05, acc 1
2017-08-08T17:46:51.765311: step 16216, loss 0.000201866, acc 1
2017-08-08T17:46:52.043254: step 16217, loss 2.1829e-06, acc 1
2017-08-08T17:46:52.301361: step 16218, loss 2.57043e-07, acc 1
2017-08-08T17:46:52.483931: step 16219, loss 2.81258e-07, acc 1
2017-08-08T17:46:52.719291: step 16220, loss 2.23505e-06, acc 1
2017-08-08T17:46:52.950625: step 16221, loss 4.87575e-05, acc 1
2017-08-08T17:46:53.172482: step 16222, loss 3.19414e-06, acc 1
2017-08-08T17:46:53.412083: step 16223, loss 1.91841e-06, acc 1
2017-08-08T17:46:53.621482: step 16224, loss 9.50413e-06, acc 1
2017-08-08T17:46:53.848390: step 16225, loss 3.72529e-08, acc 1
2017-08-08T17:46:54.139400: step 16226, loss 0.00114349, acc 1
2017-08-08T17:46:54.416236: step 16227, loss 4.26541e-07, acc 1
2017-08-08T17:46:54.622103: step 16228, loss 3.40862e-07, acc 1
2017-08-08T17:46:54.803295: step 16229, loss 1.44683e-05, acc 1
2017-08-08T17:46:55.157653: step 16230, loss 0.000231004, acc 1
2017-08-08T17:46:55.365879: step 16231, loss 2.72671e-06, acc 1
2017-08-08T17:46:55.619532: step 16232, loss 0.000235454, acc 1
2017-08-08T17:46:55.888490: step 16233, loss 0.000128787, acc 1
2017-08-08T17:46:56.169327: step 16234, loss 6.07559e-06, acc 1
2017-08-08T17:46:56.429319: step 16235, loss 4.41198e-05, acc 1
2017-08-08T17:46:56.655245: step 16236, loss 0.00326608, acc 1
2017-08-08T17:46:56.850424: step 16237, loss 5.49477e-07, acc 1
2017-08-08T17:46:57.036195: step 16238, loss 2.39716e-06, acc 1
2017-08-08T17:46:57.316068: step 16239, loss 0.000163907, acc 1
2017-08-08T17:46:57.494736: step 16240, loss 0.00229247, acc 1
2017-08-08T17:46:57.676506: step 16241, loss 3.72529e-09, acc 1
2017-08-08T17:46:57.873354: step 16242, loss 0.000625016, acc 1
2017-08-08T17:46:58.220459: step 16243, loss 8.3445e-07, acc 1
2017-08-08T17:46:58.422498: step 16244, loss 4.82422e-07, acc 1
2017-08-08T17:46:58.629837: step 16245, loss 0.008072, acc 1
2017-08-08T17:46:58.907880: step 16246, loss 0.0171383, acc 0.984375
2017-08-08T17:46:59.180734: step 16247, loss 1.62044e-06, acc 1
2017-08-08T17:46:59.351728: step 16248, loss 2.93203e-05, acc 1
2017-08-08T17:46:59.535736: step 16249, loss 1.97439e-07, acc 1
2017-08-08T17:46:59.779417: step 16250, loss 4.81727e-05, acc 1
2017-08-08T17:47:00.103680: step 16251, loss 7.45058e-09, acc 1
2017-08-08T17:47:00.422596: step 16252, loss 1.03948e-05, acc 1
2017-08-08T17:47:00.746874: step 16253, loss 2.27789e-06, acc 1
2017-08-08T17:47:00.930038: step 16254, loss 4.30268e-07, acc 1
2017-08-08T17:47:01.189362: step 16255, loss 4.86427e-05, acc 1
2017-08-08T17:47:01.386331: step 16256, loss 0.00012192, acc 1
2017-08-08T17:47:01.576229: step 16257, loss 5.43885e-07, acc 1
2017-08-08T17:47:01.809452: step 16258, loss 4.44029e-05, acc 1
2017-08-08T17:47:02.254090: step 16259, loss 0.000401724, acc 1
2017-08-08T17:47:02.613421: step 16260, loss 1.53147e-05, acc 1
2017-08-08T17:47:03.008630: step 16261, loss 3.07497e-06, acc 1
2017-08-08T17:47:03.312895: step 16262, loss 0.0015009, acc 1
2017-08-08T17:47:03.741423: step 16263, loss 7.64699e-06, acc 1
2017-08-08T17:47:04.129241: step 16264, loss 7.78566e-07, acc 1
2017-08-08T17:47:04.399592: step 16265, loss 2.25378e-07, acc 1
2017-08-08T17:47:04.689572: step 16266, loss 0.0773781, acc 0.96875
2017-08-08T17:47:05.067797: step 16267, loss 2.77514e-06, acc 1
2017-08-08T17:47:05.513386: step 16268, loss 2.98325e-05, acc 1
2017-08-08T17:47:05.873680: step 16269, loss 3.61628e-05, acc 1
2017-08-08T17:47:06.161538: step 16270, loss 4.65661e-08, acc 1
2017-08-08T17:47:06.437896: step 16271, loss 1.86265e-09, acc 1
2017-08-08T17:47:06.897595: step 16272, loss 2.30966e-07, acc 1
2017-08-08T17:47:07.169209: step 16273, loss 1.87561e-06, acc 1
2017-08-08T17:47:07.465301: step 16274, loss 2.81997e-05, acc 1
2017-08-08T17:47:07.835284: step 16275, loss 0.00236316, acc 1
2017-08-08T17:47:08.174874: step 16276, loss 1.0114e-06, acc 1
2017-08-08T17:47:08.562282: step 16277, loss 1.40625e-06, acc 1
2017-08-08T17:47:08.803743: step 16278, loss 0.000100386, acc 1
2017-08-08T17:47:09.032501: step 16279, loss 0.000409952, acc 1
2017-08-08T17:47:09.403936: step 16280, loss 2.80503e-06, acc 1
2017-08-08T17:47:09.755635: step 16281, loss 8.06145e-05, acc 1
2017-08-08T17:47:09.976597: step 16282, loss 6.53665e-06, acc 1
2017-08-08T17:47:10.196248: step 16283, loss 0.00250093, acc 1
2017-08-08T17:47:10.519322: step 16284, loss 0.00179916, acc 1
2017-08-08T17:47:10.887515: step 16285, loss 1.51615e-06, acc 1
2017-08-08T17:47:11.343100: step 16286, loss 4.14532e-05, acc 1
2017-08-08T17:47:11.648254: step 16287, loss 1.40439e-06, acc 1
2017-08-08T17:47:11.860653: step 16288, loss 0.000228812, acc 1
2017-08-08T17:47:12.090277: step 16289, loss 6.37615e-05, acc 1
2017-08-08T17:47:12.361482: step 16290, loss 0.0014629, acc 1
2017-08-08T17:47:12.557439: step 16291, loss 1.34667e-06, acc 1
2017-08-08T17:47:12.736522: step 16292, loss 0.00372397, acc 1
2017-08-08T17:47:12.943766: step 16293, loss 1.35901e-05, acc 1
2017-08-08T17:47:13.353354: step 16294, loss 2.2688e-05, acc 1
2017-08-08T17:47:13.653376: step 16295, loss 0.000627695, acc 1
2017-08-08T17:47:13.913326: step 16296, loss 3.62263e-06, acc 1
2017-08-08T17:47:14.136855: step 16297, loss 3.71218e-05, acc 1
2017-08-08T17:47:14.330417: step 16298, loss 0.0014274, acc 1
2017-08-08T17:47:14.668191: step 16299, loss 3.57624e-07, acc 1
2017-08-08T17:47:14.863196: step 16300, loss 2.88447e-05, acc 1

Evaluation:
2017-08-08T17:47:15.342612: step 16300, loss 5.70325, acc 0.711069

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-16300

2017-08-08T17:47:15.885311: step 16301, loss 0.000272432, acc 1
2017-08-08T17:47:16.104962: step 16302, loss 0.000193057, acc 1
2017-08-08T17:47:16.324288: step 16303, loss 0.000200246, acc 1
2017-08-08T17:47:16.608526: step 16304, loss 0.0121541, acc 1
2017-08-08T17:47:16.890307: step 16305, loss 1.18319e-05, acc 1
2017-08-08T17:47:17.092475: step 16306, loss 0.000474575, acc 1
2017-08-08T17:47:17.313666: step 16307, loss 1.11759e-08, acc 1
2017-08-08T17:47:17.543258: step 16308, loss 3.27814e-06, acc 1
2017-08-08T17:47:17.920490: step 16309, loss 0.000111686, acc 1
2017-08-08T17:47:18.346375: step 16310, loss 1.89857e-05, acc 1
2017-08-08T17:47:18.702462: step 16311, loss 4.07914e-07, acc 1
2017-08-08T17:47:18.953327: step 16312, loss 3.3152e-06, acc 1
2017-08-08T17:47:19.403492: step 16313, loss 0.0112014, acc 0.984375
2017-08-08T17:47:19.669344: step 16314, loss 0.00120499, acc 1
2017-08-08T17:47:19.864481: step 16315, loss 4.48603e-05, acc 1
2017-08-08T17:47:20.086007: step 16316, loss 2.15631e-05, acc 1
2017-08-08T17:47:20.416131: step 16317, loss 3.14944e-06, acc 1
2017-08-08T17:47:20.830046: step 16318, loss 0.000247523, acc 1
2017-08-08T17:47:21.111270: step 16319, loss 1.17901e-06, acc 1
2017-08-08T17:47:21.385456: step 16320, loss 3.33567e-06, acc 1
2017-08-08T17:47:21.585651: step 16321, loss 0.0367838, acc 0.984375
2017-08-08T17:47:21.921685: step 16322, loss 4.81638e-06, acc 1
2017-08-08T17:47:22.178769: step 16323, loss 1.67638e-08, acc 1
2017-08-08T17:47:22.369355: step 16324, loss 1.39694e-06, acc 1
2017-08-08T17:47:22.643986: step 16325, loss 1.13621e-07, acc 1
2017-08-08T17:47:22.851220: step 16326, loss 7.38469e-05, acc 1
2017-08-08T17:47:23.089529: step 16327, loss 5.63e-06, acc 1
2017-08-08T17:47:23.348862: step 16328, loss 4.79779e-05, acc 1
2017-08-08T17:47:23.592862: step 16329, loss 5.04001e-05, acc 1
2017-08-08T17:47:23.995351: step 16330, loss 0.00256306, acc 1
2017-08-08T17:47:24.252652: step 16331, loss 0.00195667, acc 1
2017-08-08T17:47:24.497733: step 16332, loss 3.1436e-05, acc 1
2017-08-08T17:47:24.878521: step 16333, loss 0.000455086, acc 1
2017-08-08T17:47:25.100425: step 16334, loss 4.25901e-05, acc 1
2017-08-08T17:47:25.371934: step 16335, loss 1.73033e-06, acc 1
2017-08-08T17:47:25.643475: step 16336, loss 1.60295e-05, acc 1
2017-08-08T17:47:25.853214: step 16337, loss 0.00238132, acc 1
2017-08-08T17:47:26.194479: step 16338, loss 0.00246365, acc 1
2017-08-08T17:47:26.476549: step 16339, loss 2.09266e-05, acc 1
2017-08-08T17:47:26.835664: step 16340, loss 0.000118501, acc 1
2017-08-08T17:47:27.177798: step 16341, loss 1.00826e-05, acc 1
2017-08-08T17:47:27.458155: step 16342, loss 0.00718653, acc 1
2017-08-08T17:47:27.891549: step 16343, loss 1.10452e-06, acc 1
2017-08-08T17:47:28.152303: step 16344, loss 3.74786e-05, acc 1
2017-08-08T17:47:28.513315: step 16345, loss 0.000341407, acc 1
2017-08-08T17:47:28.846172: step 16346, loss 4.77107e-05, acc 1
2017-08-08T17:47:29.115636: step 16347, loss 0.000663984, acc 1
2017-08-08T17:47:29.453355: step 16348, loss 0.00016076, acc 1
2017-08-08T17:47:29.692869: step 16349, loss 0.0291363, acc 0.984375
2017-08-08T17:47:30.058322: step 16350, loss 0.000353457, acc 1
2017-08-08T17:47:30.308426: step 16351, loss 9.3565e-05, acc 1
2017-08-08T17:47:30.548141: step 16352, loss 0.00013453, acc 1
2017-08-08T17:47:30.869855: step 16353, loss 3.59486e-07, acc 1
2017-08-08T17:47:31.201912: step 16354, loss 2.89005e-05, acc 1
2017-08-08T17:47:31.459643: step 16355, loss 2.42371e-05, acc 1
2017-08-08T17:47:31.717377: step 16356, loss 2.8198e-06, acc 1
2017-08-08T17:47:31.984041: step 16357, loss 3.48312e-07, acc 1
2017-08-08T17:47:32.186167: step 16358, loss 5.78058e-06, acc 1
2017-08-08T17:47:32.504063: step 16359, loss 4.8631e-05, acc 1
2017-08-08T17:47:32.721177: step 16360, loss 0.000911637, acc 1
2017-08-08T17:47:33.004608: step 16361, loss 7.70814e-05, acc 1
2017-08-08T17:47:33.338854: step 16362, loss 8.09709e-06, acc 1
2017-08-08T17:47:33.554404: step 16363, loss 0.000525359, acc 1
2017-08-08T17:47:33.757834: step 16364, loss 4.72373e-05, acc 1
2017-08-08T17:47:34.065306: step 16365, loss 1.44535e-06, acc 1
2017-08-08T17:47:34.278613: step 16366, loss 2.65224e-05, acc 1
2017-08-08T17:47:34.650983: step 16367, loss 8.41719e-06, acc 1
2017-08-08T17:47:34.933774: step 16368, loss 3.91155e-08, acc 1
2017-08-08T17:47:35.149378: step 16369, loss 2.42144e-08, acc 1
2017-08-08T17:47:35.416572: step 16370, loss 0.000118269, acc 1
2017-08-08T17:47:35.706280: step 16371, loss 6.22368e-05, acc 1
2017-08-08T17:47:35.898094: step 16372, loss 6.55644e-07, acc 1
2017-08-08T17:47:36.118630: step 16373, loss 2.34493e-06, acc 1
2017-08-08T17:47:36.434149: step 16374, loss 3.53509e-06, acc 1
2017-08-08T17:47:36.817371: step 16375, loss 0.00241922, acc 1
2017-08-08T17:47:37.134089: step 16376, loss 1.36815e-05, acc 1
2017-08-08T17:47:37.359272: step 16377, loss 7.05301e-06, acc 1
2017-08-08T17:47:37.715413: step 16378, loss 8.49512e-06, acc 1
2017-08-08T17:47:37.997161: step 16379, loss 1.89611e-05, acc 1
2017-08-08T17:47:38.259392: step 16380, loss 2.80995e-05, acc 1
2017-08-08T17:47:38.441361: step 16381, loss 4.20903e-06, acc 1
2017-08-08T17:47:38.659925: step 16382, loss 6.35157e-07, acc 1
2017-08-08T17:47:38.991967: step 16383, loss 6.94103e-06, acc 1
2017-08-08T17:47:39.255969: step 16384, loss 3.35276e-08, acc 1
2017-08-08T17:47:39.484871: step 16385, loss 1.99302e-07, acc 1
2017-08-08T17:47:39.672637: step 16386, loss 1.16922e-05, acc 1
2017-08-08T17:47:39.909405: step 16387, loss 3.712e-06, acc 1
2017-08-08T17:47:40.144243: step 16388, loss 1.32247e-07, acc 1
2017-08-08T17:47:40.400549: step 16389, loss 0.00276998, acc 1
2017-08-08T17:47:40.589913: step 16390, loss 2.23517e-08, acc 1
2017-08-08T17:47:40.800016: step 16391, loss 2.7174e-06, acc 1
2017-08-08T17:47:41.163306: step 16392, loss 2.56811e-05, acc 1
2017-08-08T17:47:41.508569: step 16393, loss 1.3411e-07, acc 1
2017-08-08T17:47:41.773548: step 16394, loss 0.000677808, acc 1
2017-08-08T17:47:41.995993: step 16395, loss 7.79568e-06, acc 1
2017-08-08T17:47:42.417545: step 16396, loss 7.84155e-07, acc 1
2017-08-08T17:47:42.734258: step 16397, loss 0.000595902, acc 1
2017-08-08T17:47:43.004838: step 16398, loss 0.000386961, acc 1
2017-08-08T17:47:43.301170: step 16399, loss 4.78694e-07, acc 1
2017-08-08T17:47:43.661935: step 16400, loss 5.47226e-05, acc 1

Evaluation:
2017-08-08T17:47:44.535419: step 16400, loss 5.67681, acc 0.704503

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-16400

2017-08-08T17:47:44.931173: step 16401, loss 0.000148762, acc 1
2017-08-08T17:47:45.370183: step 16402, loss 1.74337e-06, acc 1
2017-08-08T17:47:45.648455: step 16403, loss 1.86442e-06, acc 1
2017-08-08T17:47:45.890303: step 16404, loss 1.77136e-05, acc 1
2017-08-08T17:47:46.228453: step 16405, loss 2.87626e-05, acc 1
2017-08-08T17:47:46.559199: step 16406, loss 1.22177e-05, acc 1
2017-08-08T17:47:46.806581: step 16407, loss 9.08737e-05, acc 1
2017-08-08T17:47:46.998219: step 16408, loss 4.13503e-07, acc 1
2017-08-08T17:47:47.285303: step 16409, loss 9.0104e-05, acc 1
2017-08-08T17:47:47.502984: step 16410, loss 0.000408962, acc 1
2017-08-08T17:47:47.756118: step 16411, loss 0.000499683, acc 1
2017-08-08T17:47:48.112273: step 16412, loss 0.000116699, acc 1
2017-08-08T17:47:48.409314: step 16413, loss 1.48436e-05, acc 1
2017-08-08T17:47:48.686951: step 16414, loss 0.000145772, acc 1
2017-08-08T17:47:48.889165: step 16415, loss 3.52036e-07, acc 1
2017-08-08T17:47:49.281339: step 16416, loss 3.43808e-06, acc 1
2017-08-08T17:47:49.548761: step 16417, loss 9.29434e-07, acc 1
2017-08-08T17:47:49.779714: step 16418, loss 1.50684e-06, acc 1
2017-08-08T17:47:50.042955: step 16419, loss 5.14087e-07, acc 1
2017-08-08T17:47:50.318127: step 16420, loss 2.96424e-05, acc 1
2017-08-08T17:47:50.636559: step 16421, loss 3.58845e-05, acc 1
2017-08-08T17:47:50.862626: step 16422, loss 0.00304261, acc 1
2017-08-08T17:47:51.094185: step 16423, loss 1.0724e-05, acc 1
2017-08-08T17:47:51.272009: step 16424, loss 6.18642e-05, acc 1
2017-08-08T17:47:51.655743: step 16425, loss 8.64259e-07, acc 1
2017-08-08T17:47:51.958416: step 16426, loss 5.75081e-06, acc 1
2017-08-08T17:47:52.233931: step 16427, loss 0.000101242, acc 1
2017-08-08T17:47:52.439010: step 16428, loss 4.89427e-06, acc 1
2017-08-08T17:47:52.770115: step 16429, loss 4.31734e-06, acc 1
2017-08-08T17:47:53.141367: step 16430, loss 8.17817e-05, acc 1
2017-08-08T17:47:53.498155: step 16431, loss 8.83748e-06, acc 1
2017-08-08T17:47:53.739222: step 16432, loss 0.000435589, acc 1
2017-08-08T17:47:53.991775: step 16433, loss 1.37924e-05, acc 1
2017-08-08T17:47:54.386046: step 16434, loss 4.57402e-05, acc 1
2017-08-08T17:47:54.569782: step 16435, loss 2.78627e-05, acc 1
2017-08-08T17:47:54.759203: step 16436, loss 0.00567232, acc 1
2017-08-08T17:47:54.935281: step 16437, loss 2.42142e-07, acc 1
2017-08-08T17:47:55.157359: step 16438, loss 5.06374e-06, acc 1
2017-08-08T17:47:55.445306: step 16439, loss 0.000712716, acc 1
2017-08-08T17:47:55.725585: step 16440, loss 2.12242e-05, acc 1
2017-08-08T17:47:55.926937: step 16441, loss 1.88431e-05, acc 1
2017-08-08T17:47:56.173366: step 16442, loss 1.04308e-07, acc 1
2017-08-08T17:47:56.523609: step 16443, loss 9.81429e-05, acc 1
2017-08-08T17:47:56.703758: step 16444, loss 7.71613e-05, acc 1
2017-08-08T17:47:56.990282: step 16445, loss 9.0522e-07, acc 1
2017-08-08T17:47:57.249885: step 16446, loss 0.00296632, acc 1
2017-08-08T17:47:57.646356: step 16447, loss 6.51918e-07, acc 1
2017-08-08T17:47:58.076043: step 16448, loss 1.25488e-05, acc 1
2017-08-08T17:47:58.345191: step 16449, loss 8.89932e-06, acc 1
2017-08-08T17:47:58.592572: step 16450, loss 9.05012e-06, acc 1
2017-08-08T17:47:58.924148: step 16451, loss 1.26399e-05, acc 1
2017-08-08T17:47:59.099019: step 16452, loss 2.8757e-06, acc 1
2017-08-08T17:47:59.386579: step 16453, loss 4.17228e-07, acc 1
2017-08-08T17:47:59.789518: step 16454, loss 8.34444e-07, acc 1
2017-08-08T17:48:00.096856: step 16455, loss 1.00953e-06, acc 1
2017-08-08T17:48:00.332140: step 16456, loss 1.99493e-05, acc 1
2017-08-08T17:48:00.789293: step 16457, loss 1.0859e-06, acc 1
2017-08-08T17:48:01.081926: step 16458, loss 3.72529e-09, acc 1
2017-08-08T17:48:01.359939: step 16459, loss 9.24801e-06, acc 1
2017-08-08T17:48:01.617375: step 16460, loss 6.14663e-07, acc 1
2017-08-08T17:48:02.057452: step 16461, loss 2.77533e-07, acc 1
2017-08-08T17:48:02.581392: step 16462, loss 0.0417906, acc 0.984375
2017-08-08T17:48:02.989667: step 16463, loss 2.91955e-05, acc 1
2017-08-08T17:48:03.313821: step 16464, loss 9.66634e-06, acc 1
2017-08-08T17:48:03.569373: step 16465, loss 2.54184e-05, acc 1
2017-08-08T17:48:03.970204: step 16466, loss 0.000174593, acc 1
2017-08-08T17:48:04.266754: step 16467, loss 3.99688e-06, acc 1
2017-08-08T17:48:04.496001: step 16468, loss 0.000166896, acc 1
2017-08-08T17:48:04.771849: step 16469, loss 7.01893e-05, acc 1
2017-08-08T17:48:05.246159: step 16470, loss 1.16971e-06, acc 1
2017-08-08T17:48:05.630860: step 16471, loss 0.000768927, acc 1
2017-08-08T17:48:05.952325: step 16472, loss 0.00113545, acc 1
2017-08-08T17:48:06.154904: step 16473, loss 1.84357e-05, acc 1
2017-08-08T17:48:06.397398: step 16474, loss 3.99138e-06, acc 1
2017-08-08T17:48:06.740165: step 16475, loss 5.05111e-06, acc 1
2017-08-08T17:48:06.982333: step 16476, loss 0.0091099, acc 1
2017-08-08T17:48:07.214211: step 16477, loss 5.99285e-06, acc 1
2017-08-08T17:48:07.453467: step 16478, loss 5.17217e-06, acc 1
2017-08-08T17:48:07.787757: step 16479, loss 3.73254e-06, acc 1
2017-08-08T17:48:08.126532: step 16480, loss 1.46211e-06, acc 1
2017-08-08T17:48:08.442514: step 16481, loss 8.16097e-05, acc 1
2017-08-08T17:48:08.686904: step 16482, loss 2.05863e-05, acc 1
2017-08-08T17:48:09.169395: step 16483, loss 0.000111402, acc 1
2017-08-08T17:48:09.492207: step 16484, loss 9.68573e-08, acc 1
2017-08-08T17:48:09.775942: step 16485, loss 2.17357e-06, acc 1
2017-08-08T17:48:10.057658: step 16486, loss 0.0367845, acc 0.984375
2017-08-08T17:48:10.506895: step 16487, loss 2.40263e-06, acc 1
2017-08-08T17:48:10.849341: step 16488, loss 0.00143729, acc 1
2017-08-08T17:48:11.117939: step 16489, loss 0.000687076, acc 1
2017-08-08T17:48:11.444582: step 16490, loss 1.23172e-05, acc 1
2017-08-08T17:48:11.804072: step 16491, loss 6.94762e-07, acc 1
2017-08-08T17:48:12.170003: step 16492, loss 0.0787787, acc 0.984375
2017-08-08T17:48:12.435270: step 16493, loss 1.43423e-07, acc 1
2017-08-08T17:48:12.712409: step 16494, loss 2.58892e-06, acc 1
2017-08-08T17:48:13.090377: step 16495, loss 1.82534e-06, acc 1
2017-08-08T17:48:13.501342: step 16496, loss 2.10478e-07, acc 1
2017-08-08T17:48:13.913414: step 16497, loss 6.4084e-06, acc 1
2017-08-08T17:48:14.229655: step 16498, loss 3.18174e-05, acc 1
2017-08-08T17:48:14.624048: step 16499, loss 1.60187e-07, acc 1
2017-08-08T17:48:14.977598: step 16500, loss 1.07847e-05, acc 1

Evaluation:
2017-08-08T17:48:15.542130: step 16500, loss 5.56099, acc 0.709193

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-16500

2017-08-08T17:48:16.049426: step 16501, loss 1.86384e-05, acc 1
2017-08-08T17:48:16.385992: step 16502, loss 2.31144e-06, acc 1
2017-08-08T17:48:16.629587: step 16503, loss 7.83277e-06, acc 1
2017-08-08T17:48:16.849856: step 16504, loss 0.000465458, acc 1
2017-08-08T17:48:17.151089: step 16505, loss 1.65409e-05, acc 1
2017-08-08T17:48:17.491253: step 16506, loss 2.23516e-07, acc 1
2017-08-08T17:48:17.764092: step 16507, loss 3.18968e-05, acc 1
2017-08-08T17:48:17.983573: step 16508, loss 1.98364e-06, acc 1
2017-08-08T17:48:18.369329: step 16509, loss 8.94068e-08, acc 1
2017-08-08T17:48:18.706924: step 16510, loss 1.41635e-05, acc 1
2017-08-08T17:48:18.947537: step 16511, loss 3.78288e-06, acc 1
2017-08-08T17:48:19.130193: step 16512, loss 6.93911e-06, acc 1
2017-08-08T17:48:19.453209: step 16513, loss 2.35678e-05, acc 1
2017-08-08T17:48:19.701395: step 16514, loss 0.000178371, acc 1
2017-08-08T17:48:20.005267: step 16515, loss 1.65138e-05, acc 1
2017-08-08T17:48:20.289207: step 16516, loss 4.85819e-05, acc 1
2017-08-08T17:48:20.754756: step 16517, loss 9.43055e-05, acc 1
2017-08-08T17:48:21.108903: step 16518, loss 0.000253326, acc 1
2017-08-08T17:48:21.520449: step 16519, loss 0.000584093, acc 1
2017-08-08T17:48:21.811716: step 16520, loss 3.07335e-07, acc 1
2017-08-08T17:48:22.069315: step 16521, loss 1.43423e-07, acc 1
2017-08-08T17:48:22.530403: step 16522, loss 0.000163931, acc 1
2017-08-08T17:48:22.830218: step 16523, loss 3.06378e-06, acc 1
2017-08-08T17:48:23.126961: step 16524, loss 0, acc 1
2017-08-08T17:48:23.413943: step 16525, loss 1.54597e-05, acc 1
2017-08-08T17:48:23.852955: step 16526, loss 1.32386e-05, acc 1
2017-08-08T17:48:24.168277: step 16527, loss 6.51925e-08, acc 1
2017-08-08T17:48:24.551880: step 16528, loss 4.13951e-05, acc 1
2017-08-08T17:48:24.764997: step 16529, loss 7.97784e-05, acc 1
2017-08-08T17:48:24.952097: step 16530, loss 1.99626e-05, acc 1
2017-08-08T17:48:25.247388: step 16531, loss 5.33371e-06, acc 1
2017-08-08T17:48:25.436375: step 16532, loss 1.70076e-05, acc 1
2017-08-08T17:48:25.688748: step 16533, loss 1.86264e-08, acc 1
2017-08-08T17:48:26.090730: step 16534, loss 3.33239e-05, acc 1
2017-08-08T17:48:26.493427: step 16535, loss 0.00528611, acc 1
2017-08-08T17:48:26.887163: step 16536, loss 5.56461e-06, acc 1
2017-08-08T17:48:27.158387: step 16537, loss 0.00121417, acc 1
2017-08-08T17:48:27.419212: step 16538, loss 0.0011435, acc 1
2017-08-08T17:48:27.883379: step 16539, loss 1.11759e-08, acc 1
2017-08-08T17:48:28.176932: step 16540, loss 7.64502e-06, acc 1
2017-08-08T17:48:28.440109: step 16541, loss 7.63683e-08, acc 1
2017-08-08T17:48:28.737868: step 16542, loss 0.00035348, acc 1
2017-08-08T17:48:29.121717: step 16543, loss 7.33092e-05, acc 1
2017-08-08T17:48:29.484990: step 16544, loss 2.222e-06, acc 1
2017-08-08T17:48:29.800481: step 16545, loss 2.79397e-08, acc 1
2017-08-08T17:48:30.037690: step 16546, loss 2.42144e-08, acc 1
2017-08-08T17:48:30.384609: step 16547, loss 0.000565926, acc 1
2017-08-08T17:48:30.730537: step 16548, loss 0.000110823, acc 1
2017-08-08T17:48:31.076542: step 16549, loss 0.00235777, acc 1
2017-08-08T17:48:31.330261: step 16550, loss 4.82169e-06, acc 1
2017-08-08T17:48:31.584959: step 16551, loss 1.27027e-06, acc 1
2017-08-08T17:48:32.055920: step 16552, loss 7.3573e-07, acc 1
2017-08-08T17:48:32.395530: step 16553, loss 4.64477e-06, acc 1
2017-08-08T17:48:32.686590: step 16554, loss 8.07711e-06, acc 1
2017-08-08T17:48:32.918274: step 16555, loss 1.30385e-08, acc 1
2017-08-08T17:48:33.202848: step 16556, loss 0.00123256, acc 1
2017-08-08T17:48:33.472343: step 16557, loss 2.6077e-08, acc 1
2017-08-08T17:48:33.660547: step 16558, loss 2.1382e-06, acc 1
2017-08-08T17:48:33.876200: step 16559, loss 0.0441517, acc 0.984375
2017-08-08T17:48:34.114692: step 16560, loss 2.51242e-05, acc 1
2017-08-08T17:48:34.529193: step 16561, loss 8.43769e-07, acc 1
2017-08-08T17:48:34.897378: step 16562, loss 1.00021e-06, acc 1
2017-08-08T17:48:35.242457: step 16563, loss 4.19807e-06, acc 1
2017-08-08T17:48:35.475796: step 16564, loss 1.08402e-06, acc 1
2017-08-08T17:48:35.713359: step 16565, loss 2.22967e-05, acc 1
2017-08-08T17:48:36.186503: step 16566, loss 0.00219826, acc 1
2017-08-08T17:48:36.397763: step 16567, loss 4.20158e-06, acc 1
2017-08-08T17:48:36.644627: step 16568, loss 3.72529e-09, acc 1
2017-08-08T17:48:37.065850: step 16569, loss 1.45103e-05, acc 1
2017-08-08T17:48:37.554800: step 16570, loss 5.66702e-06, acc 1
2017-08-08T17:48:37.962576: step 16571, loss 1.78348e-05, acc 1
2017-08-08T17:48:38.206022: step 16572, loss 2.03299e-05, acc 1
2017-08-08T17:48:38.566663: step 16573, loss 1.42754e-05, acc 1
2017-08-08T17:48:38.785549: step 16574, loss 0.000209994, acc 1
2017-08-08T17:48:39.060314: step 16575, loss 2.21653e-07, acc 1
2017-08-08T17:48:39.360977: step 16576, loss 0.00174043, acc 1
2017-08-08T17:48:39.785460: step 16577, loss 1.39134e-05, acc 1
2017-08-08T17:48:40.211357: step 16578, loss 2.93339e-06, acc 1
2017-08-08T17:48:40.578930: step 16579, loss 1.25451e-05, acc 1
2017-08-08T17:48:40.833561: step 16580, loss 6.50739e-06, acc 1
2017-08-08T17:48:41.193493: step 16581, loss 3.86854e-06, acc 1
2017-08-08T17:48:41.468582: step 16582, loss 3.65787e-06, acc 1
2017-08-08T17:48:41.712812: step 16583, loss 0.00145481, acc 1
2017-08-08T17:48:41.945628: step 16584, loss 0.000367083, acc 1
2017-08-08T17:48:42.237194: step 16585, loss 0.00114754, acc 1
2017-08-08T17:48:42.646170: step 16586, loss 5.68099e-07, acc 1
2017-08-08T17:48:42.969043: step 16587, loss 1.97329e-05, acc 1
2017-08-08T17:48:43.353481: step 16588, loss 6.5564e-07, acc 1
2017-08-08T17:48:43.650475: step 16589, loss 0.000376405, acc 1
2017-08-08T17:48:44.065442: step 16590, loss 0.000319778, acc 1
2017-08-08T17:48:44.435279: step 16591, loss 5.58793e-09, acc 1
2017-08-08T17:48:44.739716: step 16592, loss 5.33004e-06, acc 1
2017-08-08T17:48:45.025752: step 16593, loss 1.29635e-06, acc 1
2017-08-08T17:48:45.341400: step 16594, loss 4.67453e-05, acc 1
2017-08-08T17:48:45.775802: step 16595, loss 1.53848e-06, acc 1
2017-08-08T17:48:46.215376: step 16596, loss 2.72668e-06, acc 1
2017-08-08T17:48:46.598474: step 16597, loss 2.79966e-05, acc 1
2017-08-08T17:48:46.871545: step 16598, loss 1.35763e-05, acc 1
2017-08-08T17:48:47.210324: step 16599, loss 2.42422e-05, acc 1
2017-08-08T17:48:47.442881: step 16600, loss 0.000109264, acc 1

Evaluation:
2017-08-08T17:48:48.070894: step 16600, loss 5.60954, acc 0.708255

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-16600

2017-08-08T17:48:48.691639: step 16601, loss 1.20523e-05, acc 1
2017-08-08T17:48:48.957946: step 16602, loss 2.95662e-05, acc 1
2017-08-08T17:48:49.213549: step 16603, loss 1.49011e-07, acc 1
2017-08-08T17:48:49.436878: step 16604, loss 1.86264e-08, acc 1
2017-08-08T17:48:49.819110: step 16605, loss 1.14551e-06, acc 1
2017-08-08T17:48:50.129949: step 16606, loss 0.000142386, acc 1
2017-08-08T17:48:50.441440: step 16607, loss 3.11059e-07, acc 1
2017-08-08T17:48:50.725769: step 16608, loss 2.48099e-06, acc 1
2017-08-08T17:48:51.095946: step 16609, loss 0.000153546, acc 1
2017-08-08T17:48:51.505801: step 16610, loss 1.06124e-05, acc 1
2017-08-08T17:48:51.816975: step 16611, loss 2.62801e-06, acc 1
2017-08-08T17:48:52.062510: step 16612, loss 1.19098e-05, acc 1
2017-08-08T17:48:52.343858: step 16613, loss 7.71504e-06, acc 1
2017-08-08T17:48:52.630642: step 16614, loss 6.61298e-06, acc 1
2017-08-08T17:48:52.807787: step 16615, loss 1.55339e-06, acc 1
2017-08-08T17:48:53.080060: step 16616, loss 1.61278e-05, acc 1
2017-08-08T17:48:53.318346: step 16617, loss 0.000281454, acc 1
2017-08-08T17:48:53.593438: step 16618, loss 3.43467e-05, acc 1
2017-08-08T17:48:53.930351: step 16619, loss 1.6577e-06, acc 1
2017-08-08T17:48:54.171574: step 16620, loss 1.1591e-05, acc 1
2017-08-08T17:48:54.401291: step 16621, loss 2.79397e-08, acc 1
2017-08-08T17:48:54.865934: step 16622, loss 8.56814e-08, acc 1
2017-08-08T17:48:55.109878: step 16623, loss 3.18498e-05, acc 1
2017-08-08T17:48:55.346581: step 16624, loss 1.17528e-06, acc 1
2017-08-08T17:48:55.714263: step 16625, loss 1.60683e-05, acc 1
2017-08-08T17:48:55.971044: step 16626, loss 6.21668e-06, acc 1
2017-08-08T17:48:56.237728: step 16627, loss 3.91155e-08, acc 1
2017-08-08T17:48:56.441411: step 16628, loss 3.31864e-05, acc 1
2017-08-08T17:48:56.681800: step 16629, loss 1.17529e-06, acc 1
2017-08-08T17:48:56.985602: step 16630, loss 5.43298e-05, acc 1
2017-08-08T17:48:57.186663: step 16631, loss 1.89989e-07, acc 1
2017-08-08T17:48:57.422128: step 16632, loss 0.000353493, acc 1
2017-08-08T17:48:57.729345: step 16633, loss 1.65775e-07, acc 1
2017-08-08T17:48:58.094329: step 16634, loss 0.000276018, acc 1
2017-08-08T17:48:58.363849: step 16635, loss 0.104917, acc 0.984375
2017-08-08T17:48:58.550792: step 16636, loss 6.2591e-06, acc 1
2017-08-08T17:48:58.735913: step 16637, loss 9.31322e-09, acc 1
2017-08-08T17:48:59.051789: step 16638, loss 3.64667e-06, acc 1
2017-08-08T17:48:59.252444: step 16639, loss 3.95616e-05, acc 1
2017-08-08T17:48:59.467227: step 16640, loss 1.4298e-05, acc 1
2017-08-08T17:48:59.717468: step 16641, loss 0.000412269, acc 1
2017-08-08T17:49:00.064926: step 16642, loss 5.7742e-08, acc 1
2017-08-08T17:49:00.397571: step 16643, loss 7.96365e-06, acc 1
2017-08-08T17:49:00.629882: step 16644, loss 2.68219e-07, acc 1
2017-08-08T17:49:00.832005: step 16645, loss 1.49012e-08, acc 1
2017-08-08T17:49:01.009158: step 16646, loss 3.1159e-05, acc 1
2017-08-08T17:49:01.407608: step 16647, loss 5.51644e-06, acc 1
2017-08-08T17:49:01.764915: step 16648, loss 8.19552e-07, acc 1
2017-08-08T17:49:02.024206: step 16649, loss 0.000103732, acc 1
2017-08-08T17:49:02.353564: step 16650, loss 1.49208e-06, acc 1
2017-08-08T17:49:02.727932: step 16651, loss 5.49408e-05, acc 1
2017-08-08T17:49:03.071195: step 16652, loss 3.05821e-06, acc 1
2017-08-08T17:49:03.384219: step 16653, loss 4.11032e-06, acc 1
2017-08-08T17:49:03.659183: step 16654, loss 5.40167e-08, acc 1
2017-08-08T17:49:04.121676: step 16655, loss 0.000460632, acc 1
2017-08-08T17:49:04.405364: step 16656, loss 2.65042e-06, acc 1
2017-08-08T17:49:04.658479: step 16657, loss 5.84865e-07, acc 1
2017-08-08T17:49:04.894614: step 16658, loss 5.81835e-06, acc 1
2017-08-08T17:49:05.205431: step 16659, loss 3.86084e-06, acc 1
2017-08-08T17:49:05.658748: step 16660, loss 0.000808081, acc 1
2017-08-08T17:49:06.065962: step 16661, loss 2.382e-05, acc 1
2017-08-08T17:49:06.411962: step 16662, loss 2.95038e-05, acc 1
2017-08-08T17:49:06.628076: step 16663, loss 0.00022719, acc 1
2017-08-08T17:49:06.931525: step 16664, loss 1.0818e-05, acc 1
2017-08-08T17:49:07.124117: step 16665, loss 2.01165e-07, acc 1
2017-08-08T17:49:07.352148: step 16666, loss 5.50823e-05, acc 1
2017-08-08T17:49:07.609334: step 16667, loss 2.12977e-05, acc 1
2017-08-08T17:49:07.885310: step 16668, loss 3.66456e-05, acc 1
2017-08-08T17:49:08.181974: step 16669, loss 0.000415422, acc 1
2017-08-08T17:49:08.540051: step 16670, loss 2.21268e-06, acc 1
2017-08-08T17:49:08.766630: step 16671, loss 9.87199e-08, acc 1
2017-08-08T17:49:09.106750: step 16672, loss 1.33127e-05, acc 1
2017-08-08T17:49:09.503457: step 16673, loss 1.34851e-06, acc 1
2017-08-08T17:49:09.776661: step 16674, loss 5.93753e-06, acc 1
2017-08-08T17:49:10.059266: step 16675, loss 0.000261645, acc 1
2017-08-08T17:49:10.478099: step 16676, loss 1.19766e-06, acc 1
2017-08-08T17:49:10.890978: step 16677, loss 6.33299e-08, acc 1
2017-08-08T17:49:11.254067: step 16678, loss 8.12833e-06, acc 1
2017-08-08T17:49:11.454519: step 16679, loss 0.0175543, acc 0.984375
2017-08-08T17:49:11.714365: step 16680, loss 0.000100364, acc 1
2017-08-08T17:49:12.003833: step 16681, loss 2.47156e-06, acc 1
2017-08-08T17:49:12.192647: step 16682, loss 2.25596e-05, acc 1
2017-08-08T17:49:12.404315: step 16683, loss 0.00198744, acc 1
2017-08-08T17:49:12.637319: step 16684, loss 9.45253e-05, acc 1
2017-08-08T17:49:12.973373: step 16685, loss 0.000133864, acc 1
2017-08-08T17:49:13.217322: step 16686, loss 0.000224173, acc 1
2017-08-08T17:49:13.479538: step 16687, loss 0.00266487, acc 1
2017-08-08T17:49:13.677321: step 16688, loss 7.52906e-06, acc 1
2017-08-08T17:49:13.940253: step 16689, loss 5.75551e-07, acc 1
2017-08-08T17:49:14.116238: step 16690, loss 0.00928905, acc 1
2017-08-08T17:49:14.339744: step 16691, loss 7.29145e-06, acc 1
2017-08-08T17:49:14.661155: step 16692, loss 7.45058e-09, acc 1
2017-08-08T17:49:14.908503: step 16693, loss 1.06916e-05, acc 1
2017-08-08T17:49:15.241355: step 16694, loss 1.33876e-05, acc 1
2017-08-08T17:49:15.481289: step 16695, loss 0.000180861, acc 1
2017-08-08T17:49:15.752245: step 16696, loss 2.22858e-05, acc 1
2017-08-08T17:49:16.123888: step 16697, loss 1.08311e-05, acc 1
2017-08-08T17:49:16.355916: step 16698, loss 6.93357e-06, acc 1
2017-08-08T17:49:16.596486: step 16699, loss 0.000133613, acc 1
2017-08-08T17:49:16.819898: step 16700, loss 7.45057e-08, acc 1

Evaluation:
2017-08-08T17:49:17.585333: step 16700, loss 5.56151, acc 0.71576

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-16700

2017-08-08T17:49:17.987984: step 16701, loss 0.000386187, acc 1
2017-08-08T17:49:18.217863: step 16702, loss 0, acc 1
2017-08-08T17:49:18.582925: step 16703, loss 9.31299e-07, acc 1
2017-08-08T17:49:18.823075: step 16704, loss 3.53902e-08, acc 1
2017-08-08T17:49:18.998985: step 16705, loss 9.92463e-05, acc 1
2017-08-08T17:49:19.205590: step 16706, loss 8.1907e-05, acc 1
2017-08-08T17:49:19.413423: step 16707, loss 9.47537e-06, acc 1
2017-08-08T17:49:19.729426: step 16708, loss 6.80653e-06, acc 1
2017-08-08T17:49:19.941428: step 16709, loss 4.69744e-05, acc 1
2017-08-08T17:49:20.152471: step 16710, loss 7.82309e-08, acc 1
2017-08-08T17:49:20.341469: step 16711, loss 4.02329e-07, acc 1
2017-08-08T17:49:20.636238: step 16712, loss 1.26096e-06, acc 1
2017-08-08T17:49:20.924249: step 16713, loss 1.10438e-05, acc 1
2017-08-08T17:49:21.232395: step 16714, loss 4.99181e-07, acc 1
2017-08-08T17:49:21.514750: step 16715, loss 0.000712183, acc 1
2017-08-08T17:49:21.833363: step 16716, loss 1.24599e-05, acc 1
2017-08-08T17:49:22.138677: step 16717, loss 3.45672e-06, acc 1
2017-08-08T17:49:22.451860: step 16718, loss 0.000968218, acc 1
2017-08-08T17:49:22.632849: step 16719, loss 0.000353655, acc 1
2017-08-08T17:49:22.803517: step 16720, loss 1.25705e-05, acc 1
2017-08-08T17:49:23.109475: step 16721, loss 2.68398e-06, acc 1
2017-08-08T17:49:23.345134: step 16722, loss 1.45284e-06, acc 1
2017-08-08T17:49:23.568976: step 16723, loss 7.07805e-08, acc 1
2017-08-08T17:49:23.772946: step 16724, loss 2.79397e-08, acc 1
2017-08-08T17:49:24.024371: step 16725, loss 6.65268e-06, acc 1
2017-08-08T17:49:24.428615: step 16726, loss 4.91731e-07, acc 1
2017-08-08T17:49:24.826826: step 16727, loss 2.64495e-07, acc 1
2017-08-08T17:49:25.157962: step 16728, loss 7.4005e-06, acc 1
2017-08-08T17:49:25.470880: step 16729, loss 2.55452e-05, acc 1
2017-08-08T17:49:25.712792: step 16730, loss 1.83647e-06, acc 1
2017-08-08T17:49:26.049359: step 16731, loss 1.08775e-06, acc 1
2017-08-08T17:49:26.392259: step 16732, loss 1.09496e-05, acc 1
2017-08-08T17:49:26.665395: step 16733, loss 2.5794e-05, acc 1
2017-08-08T17:49:26.958884: step 16734, loss 1.04292e-05, acc 1
2017-08-08T17:49:27.294397: step 16735, loss 5.58794e-09, acc 1
2017-08-08T17:49:27.758002: step 16736, loss 1.46211e-05, acc 1
2017-08-08T17:49:28.113829: step 16737, loss 0.000363651, acc 1
2017-08-08T17:49:28.477207: step 16738, loss 3.54648e-05, acc 1
2017-08-08T17:49:28.728116: step 16739, loss 5.12225e-07, acc 1
2017-08-08T17:49:29.109173: step 16740, loss 1.69123e-06, acc 1
2017-08-08T17:49:29.442488: step 16741, loss 1.54692e-05, acc 1
2017-08-08T17:49:29.641093: step 16742, loss 0.0299555, acc 0.984375
2017-08-08T17:49:29.841360: step 16743, loss 7.67397e-07, acc 1
2017-08-08T17:49:30.041745: step 16744, loss 1.37835e-07, acc 1
2017-08-08T17:49:30.394533: step 16745, loss 6.45207e-05, acc 1
2017-08-08T17:49:30.670377: step 16746, loss 0.000262501, acc 1
2017-08-08T17:49:30.982917: step 16747, loss 4.41571e-06, acc 1
2017-08-08T17:49:31.219739: step 16748, loss 7.87889e-07, acc 1
2017-08-08T17:49:31.676251: step 16749, loss 4.58522e-06, acc 1
2017-08-08T17:49:31.916191: step 16750, loss 0.00107289, acc 1
2017-08-08T17:49:32.158739: step 16751, loss 2.69316e-06, acc 1
2017-08-08T17:49:32.453402: step 16752, loss 6.81433e-06, acc 1
2017-08-08T17:49:32.809029: step 16753, loss 0.0048062, acc 1
2017-08-08T17:49:33.065370: step 16754, loss 2.14911e-05, acc 1
2017-08-08T17:49:33.318312: step 16755, loss 7.82309e-08, acc 1
2017-08-08T17:49:33.627010: step 16756, loss 1.78623e-06, acc 1
2017-08-08T17:49:33.952290: step 16757, loss 8.95924e-07, acc 1
2017-08-08T17:49:34.175814: step 16758, loss 0.00304714, acc 1
2017-08-08T17:49:34.433494: step 16759, loss 0.000203562, acc 1
2017-08-08T17:49:34.715244: step 16760, loss 9.31322e-09, acc 1
2017-08-08T17:49:35.105328: step 16761, loss 3.08715e-05, acc 1
2017-08-08T17:49:35.390305: step 16762, loss 6.32985e-06, acc 1
2017-08-08T17:49:35.662896: step 16763, loss 9.23847e-06, acc 1
2017-08-08T17:49:35.865822: step 16764, loss 3.72529e-08, acc 1
2017-08-08T17:49:36.057351: step 16765, loss 1.43232e-06, acc 1
2017-08-08T17:49:36.404361: step 16766, loss 3.52933e-06, acc 1
2017-08-08T17:49:36.598717: step 16767, loss 1.24795e-06, acc 1
2017-08-08T17:49:36.801994: step 16768, loss 1.41294e-05, acc 1
2017-08-08T17:49:37.063413: step 16769, loss 3.65076e-07, acc 1
2017-08-08T17:49:37.459568: step 16770, loss 0.00478481, acc 1
2017-08-08T17:49:37.689015: step 16771, loss 0.00120261, acc 1
2017-08-08T17:49:37.989833: step 16772, loss 1.54585e-05, acc 1
2017-08-08T17:49:38.161729: step 16773, loss 9.48114e-05, acc 1
2017-08-08T17:49:38.374666: step 16774, loss 1.26469e-06, acc 1
2017-08-08T17:49:38.686222: step 16775, loss 9.55522e-07, acc 1
2017-08-08T17:49:38.880189: step 16776, loss 5.58793e-09, acc 1
2017-08-08T17:49:39.114470: step 16777, loss 4.14536e-05, acc 1
2017-08-08T17:49:39.384242: step 16778, loss 7.79791e-06, acc 1
2017-08-08T17:49:39.680088: step 16779, loss 2.69015e-05, acc 1
2017-08-08T17:49:39.974201: step 16780, loss 3.73071e-06, acc 1
2017-08-08T17:49:40.254250: step 16781, loss 5.02914e-08, acc 1
2017-08-08T17:49:40.430442: step 16782, loss 0.000163743, acc 1
2017-08-08T17:49:40.776817: step 16783, loss 0.000575954, acc 1
2017-08-08T17:49:41.118194: step 16784, loss 1.76293e-05, acc 1
2017-08-08T17:49:41.316403: step 16785, loss 2.97623e-06, acc 1
2017-08-08T17:49:41.547450: step 16786, loss 3.19225e-06, acc 1
2017-08-08T17:49:41.757281: step 16787, loss 3.57153e-05, acc 1
2017-08-08T17:49:42.028775: step 16788, loss 2.81257e-07, acc 1
2017-08-08T17:49:42.246722: step 16789, loss 1.49939e-06, acc 1
2017-08-08T17:49:42.448937: step 16790, loss 1.01513e-06, acc 1
2017-08-08T17:49:42.637116: step 16791, loss 7.3608e-05, acc 1
2017-08-08T17:49:42.913321: step 16792, loss 3.42128e-05, acc 1
2017-08-08T17:49:43.224068: step 16793, loss 2.7567e-07, acc 1
2017-08-08T17:49:43.446987: step 16794, loss 1.67638e-08, acc 1
2017-08-08T17:49:43.658297: step 16795, loss 0.00120911, acc 1
2017-08-08T17:49:44.042874: step 16796, loss 9.17496e-06, acc 1
2017-08-08T17:49:44.313355: step 16797, loss 7.15465e-06, acc 1
2017-08-08T17:49:44.565379: step 16798, loss 3.89804e-06, acc 1
2017-08-08T17:49:44.765947: step 16799, loss 5.90163e-05, acc 1
2017-08-08T17:49:45.060398: step 16800, loss 6.65573e-07, acc 1

Evaluation:
2017-08-08T17:49:45.527956: step 16800, loss 5.5881, acc 0.709193

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-16800

2017-08-08T17:49:45.961641: step 16801, loss 0.000114554, acc 1
2017-08-08T17:49:46.433489: step 16802, loss 0.000527265, acc 1
2017-08-08T17:49:46.673309: step 16803, loss 3.17725e-05, acc 1
2017-08-08T17:49:46.895808: step 16804, loss 6.89178e-08, acc 1
2017-08-08T17:49:47.088238: step 16805, loss 1.46214e-06, acc 1
2017-08-08T17:49:47.349307: step 16806, loss 4.2109e-05, acc 1
2017-08-08T17:49:47.636156: step 16807, loss 6.87589e-05, acc 1
2017-08-08T17:49:47.892895: step 16808, loss 6.30629e-06, acc 1
2017-08-08T17:49:48.104914: step 16809, loss 5.6205e-06, acc 1
2017-08-08T17:49:48.489310: step 16810, loss 8.77281e-07, acc 1
2017-08-08T17:49:48.762654: step 16811, loss 0.000408562, acc 1
2017-08-08T17:49:49.020128: step 16812, loss 1.67638e-08, acc 1
2017-08-08T17:49:49.230582: step 16813, loss 0.000739999, acc 1
2017-08-08T17:49:49.461185: step 16814, loss 2.0861e-06, acc 1
2017-08-08T17:49:49.714576: step 16815, loss 1.56462e-07, acc 1
2017-08-08T17:49:49.934884: step 16816, loss 9.35143e-06, acc 1
2017-08-08T17:49:50.230232: step 16817, loss 1.00583e-07, acc 1
2017-08-08T17:49:50.541388: step 16818, loss 8.41168e-06, acc 1
2017-08-08T17:49:50.932202: step 16819, loss 0.000120966, acc 1
2017-08-08T17:49:51.221409: step 16820, loss 4.2314e-06, acc 1
2017-08-08T17:49:51.464743: step 16821, loss 5.8981e-05, acc 1
2017-08-08T17:49:51.657560: step 16822, loss 9.85233e-06, acc 1
2017-08-08T17:49:51.954642: step 16823, loss 1.8137e-05, acc 1
2017-08-08T17:49:52.140761: step 16824, loss 1.67638e-08, acc 1
2017-08-08T17:49:52.359514: step 16825, loss 1.40251e-06, acc 1
2017-08-08T17:49:52.569986: step 16826, loss 1.67638e-08, acc 1
2017-08-08T17:49:52.889263: step 16827, loss 1.71363e-07, acc 1
2017-08-08T17:49:53.141539: step 16828, loss 2.71945e-07, acc 1
2017-08-08T17:49:53.379273: step 16829, loss 8.01591e-05, acc 1
2017-08-08T17:49:53.584081: step 16830, loss 4.15176e-05, acc 1
2017-08-08T17:49:53.809863: step 16831, loss 1.09149e-06, acc 1
2017-08-08T17:49:54.194278: step 16832, loss 0.000221665, acc 1
2017-08-08T17:49:54.474025: step 16833, loss 3.27618e-06, acc 1
2017-08-08T17:49:54.746448: step 16834, loss 0.00547836, acc 1
2017-08-08T17:49:55.121370: step 16835, loss 1.03561e-06, acc 1
2017-08-08T17:49:55.544295: step 16836, loss 0.000741749, acc 1
2017-08-08T17:49:55.968698: step 16837, loss 4.44476e-05, acc 1
2017-08-08T17:49:56.187647: step 16838, loss 5.80081e-05, acc 1
2017-08-08T17:49:56.450640: step 16839, loss 0.000251615, acc 1
2017-08-08T17:49:56.798458: step 16840, loss 3.01664e-05, acc 1
2017-08-08T17:49:56.999484: step 16841, loss 4.98446e-05, acc 1
2017-08-08T17:49:57.223051: step 16842, loss 5.7235e-05, acc 1
2017-08-08T17:49:57.533024: step 16843, loss 1.86264e-08, acc 1
2017-08-08T17:49:57.882119: step 16844, loss 5.01046e-07, acc 1
2017-08-08T17:49:58.201395: step 16845, loss 2.03045e-05, acc 1
2017-08-08T17:49:58.471354: step 16846, loss 5.34576e-07, acc 1
2017-08-08T17:49:58.641691: step 16847, loss 1.49012e-08, acc 1
2017-08-08T17:49:58.841377: step 16848, loss 3.59824e-06, acc 1
2017-08-08T17:49:59.178225: step 16849, loss 2.70083e-07, acc 1
2017-08-08T17:49:59.359046: step 16850, loss 1.04118e-06, acc 1
2017-08-08T17:49:59.564905: step 16851, loss 2.17108e-05, acc 1
2017-08-08T17:49:59.838274: step 16852, loss 3.48266e-05, acc 1
2017-08-08T17:50:00.103251: step 16853, loss 9.75128e-05, acc 1
2017-08-08T17:50:00.321307: step 16854, loss 7.68159e-06, acc 1
2017-08-08T17:50:00.523780: step 16855, loss 3.85708e-06, acc 1
2017-08-08T17:50:00.704237: step 16856, loss 8.94067e-08, acc 1
2017-08-08T17:50:00.966823: step 16857, loss 5.30022e-06, acc 1
2017-08-08T17:50:01.186431: step 16858, loss 8.2071e-05, acc 1
2017-08-08T17:50:01.389374: step 16859, loss 1.95195e-06, acc 1
2017-08-08T17:50:01.650280: step 16860, loss 1.06171e-07, acc 1
2017-08-08T17:50:02.004664: step 16861, loss 2.11027e-06, acc 1
2017-08-08T17:50:02.502938: step 16862, loss 1.51616e-06, acc 1
2017-08-08T17:50:02.928147: step 16863, loss 3.183e-06, acc 1
2017-08-08T17:50:03.271315: step 16864, loss 0.000110909, acc 1
2017-08-08T17:50:03.583592: step 16865, loss 4.6938e-07, acc 1
2017-08-08T17:50:03.953694: step 16866, loss 0.000163, acc 1
2017-08-08T17:50:04.258889: step 16867, loss 7.70948e-06, acc 1
2017-08-08T17:50:04.489562: step 16868, loss 0.000222859, acc 1
2017-08-08T17:50:04.781237: step 16869, loss 3.16649e-08, acc 1
2017-08-08T17:50:05.117392: step 16870, loss 9.65391e-05, acc 1
2017-08-08T17:50:05.470482: step 16871, loss 0.000469252, acc 1
2017-08-08T17:50:05.802293: step 16872, loss 5.04772e-07, acc 1
2017-08-08T17:50:06.033222: step 16873, loss 4.02305e-06, acc 1
2017-08-08T17:50:06.413170: step 16874, loss 7.86024e-07, acc 1
2017-08-08T17:50:06.730634: step 16875, loss 8.24734e-05, acc 1
2017-08-08T17:50:07.003108: step 16876, loss 5.58794e-09, acc 1
2017-08-08T17:50:07.287748: step 16877, loss 7.6466e-05, acc 1
2017-08-08T17:50:07.544902: step 16878, loss 0.00324032, acc 1
2017-08-08T17:50:08.025363: step 16879, loss 8.0278e-07, acc 1
2017-08-08T17:50:08.413399: step 16880, loss 6.14673e-08, acc 1
2017-08-08T17:50:08.687828: step 16881, loss 0.00021573, acc 1
2017-08-08T17:50:08.895089: step 16882, loss 1.06825e-05, acc 1
2017-08-08T17:50:09.256884: step 16883, loss 1.36975e-05, acc 1
2017-08-08T17:50:09.503485: step 16884, loss 8.57078e-06, acc 1
2017-08-08T17:50:09.734803: step 16885, loss 2.25379e-07, acc 1
2017-08-08T17:50:09.975938: step 16886, loss 2.25184e-06, acc 1
2017-08-08T17:50:10.248732: step 16887, loss 1.06171e-07, acc 1
2017-08-08T17:50:10.585440: step 16888, loss 3.7157e-06, acc 1
2017-08-08T17:50:10.897449: step 16889, loss 0.000437812, acc 1
2017-08-08T17:50:11.178994: step 16890, loss 2.41196e-06, acc 1
2017-08-08T17:50:11.418321: step 16891, loss 3.15958e-05, acc 1
2017-08-08T17:50:11.606939: step 16892, loss 3.3729e-06, acc 1
2017-08-08T17:50:12.054286: step 16893, loss 4.28408e-08, acc 1
2017-08-08T17:50:12.362151: step 16894, loss 7.09663e-07, acc 1
2017-08-08T17:50:12.686003: step 16895, loss 7.46912e-07, acc 1
2017-08-08T17:50:12.895608: step 16896, loss 2.6258e-05, acc 1
2017-08-08T17:50:13.227177: step 16897, loss 5.49719e-05, acc 1
2017-08-08T17:50:13.598678: step 16898, loss 1.06171e-07, acc 1
2017-08-08T17:50:14.013572: step 16899, loss 8.2328e-07, acc 1
2017-08-08T17:50:14.349533: step 16900, loss 2.26483e-06, acc 1

Evaluation:
2017-08-08T17:50:15.315231: step 16900, loss 5.61434, acc 0.710131

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-16900

2017-08-08T17:50:15.770545: step 16901, loss 4.65661e-08, acc 1
2017-08-08T17:50:16.043258: step 16902, loss 1.91851e-07, acc 1
2017-08-08T17:50:16.532486: step 16903, loss 0.000420034, acc 1
2017-08-08T17:50:16.937051: step 16904, loss 9.1673e-05, acc 1
2017-08-08T17:50:17.289886: step 16905, loss 3.38999e-07, acc 1
2017-08-08T17:50:17.561881: step 16906, loss 1.5273e-06, acc 1
2017-08-08T17:50:17.785165: step 16907, loss 0.00880675, acc 1
2017-08-08T17:50:18.142252: step 16908, loss 0.000913845, acc 1
2017-08-08T17:50:18.372158: step 16909, loss 4.84287e-08, acc 1
2017-08-08T17:50:18.619906: step 16910, loss 2.37771e-05, acc 1
2017-08-08T17:50:18.914865: step 16911, loss 2.93903e-06, acc 1
2017-08-08T17:50:19.287493: step 16912, loss 3.82636e-05, acc 1
2017-08-08T17:50:19.763331: step 16913, loss 1.00583e-07, acc 1
2017-08-08T17:50:20.152687: step 16914, loss 7.72979e-07, acc 1
2017-08-08T17:50:20.465352: step 16915, loss 1.24137e-05, acc 1
2017-08-08T17:50:20.697881: step 16916, loss 5.92413e-05, acc 1
2017-08-08T17:50:21.027005: step 16917, loss 1.08697e-05, acc 1
2017-08-08T17:50:21.214802: step 16918, loss 4.48894e-07, acc 1
2017-08-08T17:50:21.422059: step 16919, loss 5.979e-07, acc 1
2017-08-08T17:50:21.679462: step 16920, loss 0.000580437, acc 1
2017-08-08T17:50:22.020443: step 16921, loss 4.19091e-07, acc 1
2017-08-08T17:50:22.361602: step 16922, loss 2.49594e-07, acc 1
2017-08-08T17:50:22.626862: step 16923, loss 4.77646e-05, acc 1
2017-08-08T17:50:22.832330: step 16924, loss 3.53902e-08, acc 1
2017-08-08T17:50:23.185349: step 16925, loss 2.66356e-07, acc 1
2017-08-08T17:50:23.373091: step 16926, loss 8.31381e-06, acc 1
2017-08-08T17:50:23.575774: step 16927, loss 0.000206774, acc 1
2017-08-08T17:50:23.773764: step 16928, loss 4.17229e-07, acc 1
2017-08-08T17:50:24.073479: step 16929, loss 0.00128816, acc 1
2017-08-08T17:50:24.368331: step 16930, loss 2.98021e-07, acc 1
2017-08-08T17:50:24.621334: step 16931, loss 4.87213e-06, acc 1
2017-08-08T17:50:24.830656: step 16932, loss 2.23319e-06, acc 1
2017-08-08T17:50:25.057664: step 16933, loss 0.000139841, acc 1
2017-08-08T17:50:25.484341: step 16934, loss 0.000206656, acc 1
2017-08-08T17:50:25.756814: step 16935, loss 8.63667e-06, acc 1
2017-08-08T17:50:26.050611: step 16936, loss 1.58471e-05, acc 1
2017-08-08T17:50:26.327057: step 16937, loss 1.86265e-09, acc 1
2017-08-08T17:50:26.711238: step 16938, loss 2.85657e-05, acc 1
2017-08-08T17:50:27.175373: step 16939, loss 0.000143868, acc 1
2017-08-08T17:50:27.550601: step 16940, loss 3.78167e-05, acc 1
2017-08-08T17:50:27.844786: step 16941, loss 0.00398862, acc 1
2017-08-08T17:50:28.093329: step 16942, loss 1.60928e-06, acc 1
2017-08-08T17:50:28.421363: step 16943, loss 3.21835e-06, acc 1
2017-08-08T17:50:28.668451: step 16944, loss 3.69706e-06, acc 1
2017-08-08T17:50:28.936807: step 16945, loss 0.000123466, acc 1
2017-08-08T17:50:29.141392: step 16946, loss 4.38979e-05, acc 1
2017-08-08T17:50:29.481288: step 16947, loss 2.54576e-05, acc 1
2017-08-08T17:50:29.821317: step 16948, loss 3.55544e-06, acc 1
2017-08-08T17:50:30.151558: step 16949, loss 1.91852e-07, acc 1
2017-08-08T17:50:30.355666: step 16950, loss 2.78155e-08, acc 1
2017-08-08T17:50:30.715856: step 16951, loss 7.07804e-08, acc 1
2017-08-08T17:50:30.970949: step 16952, loss 1.65892e-05, acc 1
2017-08-08T17:50:31.172062: step 16953, loss 9.98346e-07, acc 1
2017-08-08T17:50:31.393697: step 16954, loss 0.000197834, acc 1
2017-08-08T17:50:31.654889: step 16955, loss 1.28369e-05, acc 1
2017-08-08T17:50:31.968508: step 16956, loss 2.77532e-07, acc 1
2017-08-08T17:50:32.288758: step 16957, loss 2.6077e-08, acc 1
2017-08-08T17:50:32.587973: step 16958, loss 2.275e-05, acc 1
2017-08-08T17:50:32.775616: step 16959, loss 9.31322e-09, acc 1
2017-08-08T17:50:33.004809: step 16960, loss 7.45058e-09, acc 1
2017-08-08T17:50:33.345972: step 16961, loss 4.19093e-07, acc 1
2017-08-08T17:50:33.572197: step 16962, loss 5.2872e-05, acc 1
2017-08-08T17:50:33.791944: step 16963, loss 0.0328474, acc 0.984375
2017-08-08T17:50:34.078229: step 16964, loss 1.36716e-06, acc 1
2017-08-08T17:50:34.497260: step 16965, loss 0.00014741, acc 1
2017-08-08T17:50:34.857490: step 16966, loss 1.34054e-05, acc 1
2017-08-08T17:50:35.164728: step 16967, loss 6.14672e-08, acc 1
2017-08-08T17:50:35.452503: step 16968, loss 1.6205e-07, acc 1
2017-08-08T17:50:35.776704: step 16969, loss 2.6822e-07, acc 1
2017-08-08T17:50:36.182409: step 16970, loss 7.54115e-06, acc 1
2017-08-08T17:50:36.430358: step 16971, loss 2.79396e-07, acc 1
2017-08-08T17:50:36.672905: step 16972, loss 2.01155e-06, acc 1
2017-08-08T17:50:36.994223: step 16973, loss 9.31322e-09, acc 1
2017-08-08T17:50:37.466557: step 16974, loss 1.22781e-05, acc 1
2017-08-08T17:50:37.876405: step 16975, loss 1.40054e-05, acc 1
2017-08-08T17:50:38.191242: step 16976, loss 9.54131e-06, acc 1
2017-08-08T17:50:38.563366: step 16977, loss 1.99667e-06, acc 1
2017-08-08T17:50:38.980442: step 16978, loss 4.76056e-06, acc 1
2017-08-08T17:50:39.250947: step 16979, loss 1.53288e-06, acc 1
2017-08-08T17:50:39.448701: step 16980, loss 5.95191e-05, acc 1
2017-08-08T17:50:39.833869: step 16981, loss 0.000225417, acc 1
2017-08-08T17:50:40.172422: step 16982, loss 1.15484e-07, acc 1
2017-08-08T17:50:40.471562: step 16983, loss 0.000374364, acc 1
2017-08-08T17:50:40.764051: step 16984, loss 5.73996e-06, acc 1
2017-08-08T17:50:40.981121: step 16985, loss 1.30385e-08, acc 1
2017-08-08T17:50:41.339054: step 16986, loss 4.68972e-06, acc 1
2017-08-08T17:50:41.602855: step 16987, loss 3.42724e-07, acc 1
2017-08-08T17:50:41.849244: step 16988, loss 8.62369e-05, acc 1
2017-08-08T17:50:42.077186: step 16989, loss 1.67637e-07, acc 1
2017-08-08T17:50:42.417308: step 16990, loss 7.01381e-05, acc 1
2017-08-08T17:50:42.737003: step 16991, loss 0.000616828, acc 1
2017-08-08T17:50:43.001825: step 16992, loss 0.000367859, acc 1
2017-08-08T17:50:43.186763: step 16993, loss 0.00014932, acc 1
2017-08-08T17:50:43.392764: step 16994, loss 3.34331e-06, acc 1
2017-08-08T17:50:43.717021: step 16995, loss 0.000419665, acc 1
2017-08-08T17:50:43.928643: step 16996, loss 0.0011708, acc 1
2017-08-08T17:50:44.149620: step 16997, loss 2.33003e-06, acc 1
2017-08-08T17:50:44.376920: step 16998, loss 9.99511e-06, acc 1
2017-08-08T17:50:44.736436: step 16999, loss 0.00166204, acc 1
2017-08-08T17:50:45.051185: step 17000, loss 7.33874e-07, acc 1

Evaluation:
2017-08-08T17:50:45.547864: step 17000, loss 5.6297, acc 0.714822

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-17000

2017-08-08T17:50:46.046848: step 17001, loss 5.77419e-08, acc 1
2017-08-08T17:50:46.277728: step 17002, loss 8.22725e-06, acc 1
2017-08-08T17:50:46.489365: step 17003, loss 2.42144e-08, acc 1
2017-08-08T17:50:46.745398: step 17004, loss 0.0245776, acc 0.984375
2017-08-08T17:50:47.177414: step 17005, loss 1.26841e-06, acc 1
2017-08-08T17:50:47.604456: step 17006, loss 2.15368e-05, acc 1
2017-08-08T17:50:47.957896: step 17007, loss 0.00059742, acc 1
2017-08-08T17:50:48.205503: step 17008, loss 0.00165624, acc 1
2017-08-08T17:50:48.504814: step 17009, loss 0.000148786, acc 1
2017-08-08T17:50:48.887540: step 17010, loss 2.05647e-05, acc 1
2017-08-08T17:50:49.150478: step 17011, loss 0.000120133, acc 1
2017-08-08T17:50:49.359589: step 17012, loss 1.45286e-07, acc 1
2017-08-08T17:50:49.614883: step 17013, loss 6.06553e-05, acc 1
2017-08-08T17:50:50.064069: step 17014, loss 3.80144e-06, acc 1
2017-08-08T17:50:50.453469: step 17015, loss 2.19226e-06, acc 1
2017-08-08T17:50:50.797887: step 17016, loss 6.14932e-06, acc 1
2017-08-08T17:50:50.981760: step 17017, loss 1.21072e-07, acc 1
2017-08-08T17:50:51.226090: step 17018, loss 0.00601561, acc 1
2017-08-08T17:50:51.514612: step 17019, loss 3.72512e-06, acc 1
2017-08-08T17:50:51.724029: step 17020, loss 0.00105929, acc 1
2017-08-08T17:50:51.938427: step 17021, loss 3.12922e-07, acc 1
2017-08-08T17:50:52.167336: step 17022, loss 0.00172205, acc 1
2017-08-08T17:50:52.493938: step 17023, loss 6.92456e-05, acc 1
2017-08-08T17:50:52.761846: step 17024, loss 1.14995e-05, acc 1
2017-08-08T17:50:53.151101: step 17025, loss 1.77063e-05, acc 1
2017-08-08T17:50:53.366785: step 17026, loss 4.94439e-05, acc 1
2017-08-08T17:50:53.656296: step 17027, loss 0.000237375, acc 1
2017-08-08T17:50:54.086517: step 17028, loss 0.000151695, acc 1
2017-08-08T17:50:54.385056: step 17029, loss 0.000823733, acc 1
2017-08-08T17:50:54.646030: step 17030, loss 2.3563e-05, acc 1
2017-08-08T17:50:54.917784: step 17031, loss 4.92504e-05, acc 1
2017-08-08T17:50:55.264478: step 17032, loss 1.48819e-06, acc 1
2017-08-08T17:50:55.656669: step 17033, loss 4.26791e-05, acc 1
2017-08-08T17:50:55.964332: step 17034, loss 0.0325841, acc 0.984375
2017-08-08T17:50:56.175978: step 17035, loss 4.95462e-07, acc 1
2017-08-08T17:50:56.521418: step 17036, loss 3.68947e-06, acc 1
2017-08-08T17:50:56.792352: step 17037, loss 1.48819e-06, acc 1
2017-08-08T17:50:56.987283: step 17038, loss 5.26801e-05, acc 1
2017-08-08T17:50:57.190661: step 17039, loss 1.35226e-06, acc 1
2017-08-08T17:50:57.563263: step 17040, loss 5.00789e-06, acc 1
2017-08-08T17:50:57.992934: step 17041, loss 1.81227e-06, acc 1
2017-08-08T17:50:58.291704: step 17042, loss 1.33547e-05, acc 1
2017-08-08T17:50:58.529768: step 17043, loss 2.29104e-07, acc 1
2017-08-08T17:50:58.865872: step 17044, loss 9.05232e-07, acc 1
2017-08-08T17:50:59.100298: step 17045, loss 1.40294e-05, acc 1
2017-08-08T17:50:59.341033: step 17046, loss 6.36843e-05, acc 1
2017-08-08T17:50:59.596175: step 17047, loss 1.64466e-05, acc 1
2017-08-08T17:50:59.931988: step 17048, loss 7.11515e-07, acc 1
2017-08-08T17:51:00.205636: step 17049, loss 4.30234e-06, acc 1
2017-08-08T17:51:00.565357: step 17050, loss 3.07334e-07, acc 1
2017-08-08T17:51:00.810417: step 17051, loss 1.19392e-06, acc 1
2017-08-08T17:51:01.073913: step 17052, loss 1.86192e-05, acc 1
2017-08-08T17:51:01.501032: step 17053, loss 9.78967e-05, acc 1
2017-08-08T17:51:01.767468: step 17054, loss 0.0529319, acc 0.984375
2017-08-08T17:51:02.081377: step 17055, loss 3.74185e-06, acc 1
2017-08-08T17:51:02.374448: step 17056, loss 9.79725e-07, acc 1
2017-08-08T17:51:02.690466: step 17057, loss 6.57822e-05, acc 1
2017-08-08T17:51:03.146861: step 17058, loss 0.000953739, acc 1
2017-08-08T17:51:03.476430: step 17059, loss 0.0137153, acc 0.984375
2017-08-08T17:51:03.801017: step 17060, loss 1.40823e-05, acc 1
2017-08-08T17:51:04.101065: step 17061, loss 0.000156047, acc 1
2017-08-08T17:51:04.461414: step 17062, loss 7.88485e-06, acc 1
2017-08-08T17:51:04.774663: step 17063, loss 2.51997e-06, acc 1
2017-08-08T17:51:05.092437: step 17064, loss 4.28292e-05, acc 1
2017-08-08T17:51:05.490860: step 17065, loss 3.81359e-05, acc 1
2017-08-08T17:51:05.958150: step 17066, loss 3.93165e-06, acc 1
2017-08-08T17:51:06.364495: step 17067, loss 2.04891e-08, acc 1
2017-08-08T17:51:06.672810: step 17068, loss 6.85027e-06, acc 1
2017-08-08T17:51:06.937264: step 17069, loss 1.86264e-08, acc 1
2017-08-08T17:51:07.275443: step 17070, loss 4.29306e-05, acc 1
2017-08-08T17:51:07.595801: step 17071, loss 4.27344e-05, acc 1
2017-08-08T17:51:07.902700: step 17072, loss 1.92684e-05, acc 1
2017-08-08T17:51:08.274331: step 17073, loss 1.86265e-09, acc 1
2017-08-08T17:51:08.552771: step 17074, loss 1.84584e-06, acc 1
2017-08-08T17:51:08.833099: step 17075, loss 4.28408e-08, acc 1
2017-08-08T17:51:09.060837: step 17076, loss 6.25838e-05, acc 1
2017-08-08T17:51:09.271000: step 17077, loss 1.56462e-07, acc 1
2017-08-08T17:51:09.549365: step 17078, loss 0.000286025, acc 1
2017-08-08T17:51:09.844147: step 17079, loss 1.7215e-05, acc 1
2017-08-08T17:51:10.070486: step 17080, loss 0.00899127, acc 1
2017-08-08T17:51:10.377266: step 17081, loss 0.000132954, acc 1
2017-08-08T17:51:10.695347: step 17082, loss 2.96014e-05, acc 1
2017-08-08T17:51:10.949344: step 17083, loss 0.000447238, acc 1
2017-08-08T17:51:11.221584: step 17084, loss 5.85477e-05, acc 1
2017-08-08T17:51:11.435929: step 17085, loss 4.78627e-06, acc 1
2017-08-08T17:51:11.639083: step 17086, loss 1.19209e-07, acc 1
2017-08-08T17:51:12.014357: step 17087, loss 1.75088e-07, acc 1
2017-08-08T17:51:12.293541: step 17088, loss 0.000961122, acc 1
2017-08-08T17:51:12.569118: step 17089, loss 2.04891e-08, acc 1
2017-08-08T17:51:12.880457: step 17090, loss 7.05601e-06, acc 1
2017-08-08T17:51:13.195770: step 17091, loss 1.43423e-07, acc 1
2017-08-08T17:51:13.481386: step 17092, loss 4.82539e-06, acc 1
2017-08-08T17:51:13.765439: step 17093, loss 5.99615e-05, acc 1
2017-08-08T17:51:14.049414: step 17094, loss 8.12092e-07, acc 1
2017-08-08T17:51:14.267436: step 17095, loss 1.62231e-06, acc 1
2017-08-08T17:51:14.509393: step 17096, loss 4.95456e-07, acc 1
2017-08-08T17:51:14.829417: step 17097, loss 3.12229e-05, acc 1
2017-08-08T17:51:15.047814: step 17098, loss 0.0307423, acc 0.984375
2017-08-08T17:51:15.281357: step 17099, loss 1.33549e-06, acc 1
2017-08-08T17:51:15.537924: step 17100, loss 2.01938e-05, acc 1

Evaluation:
2017-08-08T17:51:16.160000: step 17100, loss 5.67608, acc 0.703565

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-17100

2017-08-08T17:51:16.498737: step 17101, loss 0.000304016, acc 1
2017-08-08T17:51:16.785269: step 17102, loss 7.04065e-07, acc 1
2017-08-08T17:51:17.044946: step 17103, loss 1.30849e-05, acc 1
2017-08-08T17:51:17.354053: step 17104, loss 1.86265e-09, acc 1
2017-08-08T17:51:17.631846: step 17105, loss 2.25379e-07, acc 1
2017-08-08T17:51:18.049383: step 17106, loss 6.09073e-07, acc 1
2017-08-08T17:51:18.484062: step 17107, loss 3.35276e-08, acc 1
2017-08-08T17:51:18.826415: step 17108, loss 3.12923e-07, acc 1
2017-08-08T17:51:19.052761: step 17109, loss 2.23517e-08, acc 1
2017-08-08T17:51:19.315909: step 17110, loss 0.000701841, acc 1
2017-08-08T17:51:19.648861: step 17111, loss 3.64294e-05, acc 1
2017-08-08T17:51:19.866632: step 17112, loss 1.76756e-06, acc 1
2017-08-08T17:51:20.068387: step 17113, loss 3.7995e-06, acc 1
2017-08-08T17:51:20.272506: step 17114, loss 0.00537849, acc 1
2017-08-08T17:51:20.545320: step 17115, loss 0.000333109, acc 1
2017-08-08T17:51:20.860943: step 17116, loss 5.53875e-06, acc 1
2017-08-08T17:51:21.182034: step 17117, loss 9.80725e-05, acc 1
2017-08-08T17:51:21.417371: step 17118, loss 1.49649e-05, acc 1
2017-08-08T17:51:21.616625: step 17119, loss 3.08742e-05, acc 1
2017-08-08T17:51:21.857469: step 17120, loss 3.64343e-05, acc 1
2017-08-08T17:51:22.084483: step 17121, loss 1.9744e-07, acc 1
2017-08-08T17:51:22.273037: step 17122, loss 6.133e-05, acc 1
2017-08-08T17:51:22.516987: step 17123, loss 3.21648e-06, acc 1
2017-08-08T17:51:22.841038: step 17124, loss 9.31322e-09, acc 1
2017-08-08T17:51:23.197325: step 17125, loss 2.04878e-06, acc 1
2017-08-08T17:51:23.437346: step 17126, loss 7.8231e-08, acc 1
2017-08-08T17:51:23.694926: step 17127, loss 5.40244e-05, acc 1
2017-08-08T17:51:23.953610: step 17128, loss 8.0781e-06, acc 1
2017-08-08T17:51:24.410751: step 17129, loss 4.28408e-08, acc 1
2017-08-08T17:51:24.697762: step 17130, loss 0.000532274, acc 1
2017-08-08T17:51:24.980435: step 17131, loss 7.10546e-05, acc 1
2017-08-08T17:51:25.211842: step 17132, loss 1.27616e-05, acc 1
2017-08-08T17:51:25.597981: step 17133, loss 9.26678e-06, acc 1
2017-08-08T17:51:25.989385: step 17134, loss 0.000323811, acc 1
2017-08-08T17:51:26.320355: step 17135, loss 0.000722097, acc 1
2017-08-08T17:51:26.520461: step 17136, loss 0.0216234, acc 0.984375
2017-08-08T17:51:26.723379: step 17137, loss 1.86264e-08, acc 1
2017-08-08T17:51:27.031646: step 17138, loss 3.93016e-07, acc 1
2017-08-08T17:51:27.238768: step 17139, loss 3.72529e-08, acc 1
2017-08-08T17:51:27.490748: step 17140, loss 1.28333e-06, acc 1
2017-08-08T17:51:27.750425: step 17141, loss 1.22557e-06, acc 1
2017-08-08T17:51:28.196697: step 17142, loss 1.8942e-06, acc 1
2017-08-08T17:51:28.687748: step 17143, loss 5.96046e-08, acc 1
2017-08-08T17:51:29.032246: step 17144, loss 7.82297e-07, acc 1
2017-08-08T17:51:29.359343: step 17145, loss 4.01021e-05, acc 1
2017-08-08T17:51:29.757257: step 17146, loss 0.000128372, acc 1
2017-08-08T17:51:30.064602: step 17147, loss 1.4379e-06, acc 1
2017-08-08T17:51:30.343363: step 17148, loss 8.14387e-05, acc 1
2017-08-08T17:51:30.690507: step 17149, loss 1.22143e-05, acc 1
2017-08-08T17:51:31.105738: step 17150, loss 2.59636e-06, acc 1
2017-08-08T17:51:31.492702: step 17151, loss 0.000248057, acc 1
2017-08-08T17:51:31.838643: step 17152, loss 4.33994e-07, acc 1
2017-08-08T17:51:32.088003: step 17153, loss 5.36321e-05, acc 1
2017-08-08T17:51:32.285346: step 17154, loss 7.45058e-09, acc 1
2017-08-08T17:51:32.564589: step 17155, loss 1.24797e-07, acc 1
2017-08-08T17:51:32.785756: step 17156, loss 1.04308e-07, acc 1
2017-08-08T17:51:33.010521: step 17157, loss 3.61151e-06, acc 1
2017-08-08T17:51:33.256700: step 17158, loss 9.31322e-09, acc 1
2017-08-08T17:51:33.613166: step 17159, loss 3.58591e-05, acc 1
2017-08-08T17:51:34.043776: step 17160, loss 1.32246e-06, acc 1
2017-08-08T17:51:34.420376: step 17161, loss 4.15316e-06, acc 1
2017-08-08T17:51:34.647915: step 17162, loss 0.000121502, acc 1
2017-08-08T17:51:34.931894: step 17163, loss 0, acc 1
2017-08-08T17:51:35.276215: step 17164, loss 2.18559e-05, acc 1
2017-08-08T17:51:35.459850: step 17165, loss 1.74707e-06, acc 1
2017-08-08T17:51:35.676177: step 17166, loss 0.000259223, acc 1
2017-08-08T17:51:35.905053: step 17167, loss 1.6115e-05, acc 1
2017-08-08T17:51:36.233366: step 17168, loss 1.86265e-09, acc 1
2017-08-08T17:51:36.535129: step 17169, loss 6.86234e-06, acc 1
2017-08-08T17:51:36.855953: step 17170, loss 0.000250745, acc 1
2017-08-08T17:51:37.045676: step 17171, loss 7.23212e-06, acc 1
2017-08-08T17:51:37.274303: step 17172, loss 0.000107659, acc 1
2017-08-08T17:51:37.573042: step 17173, loss 3.42262e-05, acc 1
2017-08-08T17:51:37.739307: step 17174, loss 2.90004e-06, acc 1
2017-08-08T17:51:37.920583: step 17175, loss 7.47511e-06, acc 1
2017-08-08T17:51:38.224983: step 17176, loss 1.26842e-06, acc 1
2017-08-08T17:51:38.638165: step 17177, loss 5.96046e-08, acc 1
2017-08-08T17:51:39.058867: step 17178, loss 1.67292e-05, acc 1
2017-08-08T17:51:39.341475: step 17179, loss 8.19562e-08, acc 1
2017-08-08T17:51:39.565293: step 17180, loss 6.0785e-05, acc 1
2017-08-08T17:51:39.736410: step 17181, loss 3.72529e-09, acc 1
2017-08-08T17:51:39.933311: step 17182, loss 9.02266e-05, acc 1
2017-08-08T17:51:40.244901: step 17183, loss 0.0989251, acc 0.984375
2017-08-08T17:51:40.454741: step 17184, loss 2.12514e-06, acc 1
2017-08-08T17:51:40.650260: step 17185, loss 1.3597e-06, acc 1
2017-08-08T17:51:40.866669: step 17186, loss 1.67638e-08, acc 1
2017-08-08T17:51:41.218718: step 17187, loss 5.42681e-06, acc 1
2017-08-08T17:51:41.550579: step 17188, loss 4.32128e-07, acc 1
2017-08-08T17:51:41.820988: step 17189, loss 4.92411e-05, acc 1
2017-08-08T17:51:42.026587: step 17190, loss 1.31671e-05, acc 1
2017-08-08T17:51:42.346401: step 17191, loss 1.43956e-05, acc 1
2017-08-08T17:51:42.555550: step 17192, loss 0.000249743, acc 1
2017-08-08T17:51:42.731189: step 17193, loss 0.00115391, acc 1
2017-08-08T17:51:42.926962: step 17194, loss 5.49387e-06, acc 1
2017-08-08T17:51:43.228675: step 17195, loss 0.0025184, acc 1
2017-08-08T17:51:43.555912: step 17196, loss 0.147821, acc 0.96875
2017-08-08T17:51:43.905604: step 17197, loss 0.00074513, acc 1
2017-08-08T17:51:44.161486: step 17198, loss 7.63683e-08, acc 1
2017-08-08T17:51:44.393158: step 17199, loss 1.00583e-07, acc 1
2017-08-08T17:51:44.628495: step 17200, loss 2.10101e-05, acc 1

Evaluation:
2017-08-08T17:51:45.235626: step 17200, loss 5.68784, acc 0.711069

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-17200

2017-08-08T17:51:45.724448: step 17201, loss 1.96136e-05, acc 1
2017-08-08T17:51:46.005304: step 17202, loss 2.77329e-05, acc 1
2017-08-08T17:51:46.207453: step 17203, loss 8.05297e-06, acc 1
2017-08-08T17:51:46.460581: step 17204, loss 4.55752e-06, acc 1
2017-08-08T17:51:46.677574: step 17205, loss 8.19562e-08, acc 1
2017-08-08T17:51:46.849411: step 17206, loss 0.000346597, acc 1
2017-08-08T17:51:47.106537: step 17207, loss 0.00344626, acc 1
2017-08-08T17:51:47.318578: step 17208, loss 1.86342e-05, acc 1
2017-08-08T17:51:47.593359: step 17209, loss 0.000198012, acc 1
2017-08-08T17:51:47.837883: step 17210, loss 2.30967e-07, acc 1
2017-08-08T17:51:48.065843: step 17211, loss 9.74147e-07, acc 1
2017-08-08T17:51:48.508144: step 17212, loss 0.00151628, acc 1
2017-08-08T17:51:48.817275: step 17213, loss 9.04658e-05, acc 1
2017-08-08T17:51:49.209549: step 17214, loss 2.00635e-05, acc 1
2017-08-08T17:51:49.552924: step 17215, loss 1.93714e-07, acc 1
2017-08-08T17:51:49.862110: step 17216, loss 6.89178e-08, acc 1
2017-08-08T17:51:50.275953: step 17217, loss 2.86846e-07, acc 1
2017-08-08T17:51:50.547775: step 17218, loss 0.000107368, acc 1
2017-08-08T17:51:50.895854: step 17219, loss 3.37855e-06, acc 1
2017-08-08T17:51:51.130087: step 17220, loss 0.105554, acc 0.96875
2017-08-08T17:51:51.497161: step 17221, loss 1.0154e-05, acc 1
2017-08-08T17:51:51.703963: step 17222, loss 0.0026569, acc 1
2017-08-08T17:51:52.056268: step 17223, loss 0.000488214, acc 1
2017-08-08T17:51:52.270663: step 17224, loss 1.66144e-06, acc 1
2017-08-08T17:51:52.565874: step 17225, loss 2.96159e-07, acc 1
2017-08-08T17:51:52.829907: step 17226, loss 1.86264e-08, acc 1
2017-08-08T17:51:53.184200: step 17227, loss 1.86265e-09, acc 1
2017-08-08T17:51:53.436349: step 17228, loss 6.74033e-05, acc 1
2017-08-08T17:51:53.732110: step 17229, loss 0.000270939, acc 1
2017-08-08T17:51:54.006505: step 17230, loss 2.90174e-06, acc 1
2017-08-08T17:51:54.209392: step 17231, loss 4.51467e-06, acc 1
2017-08-08T17:51:54.497360: step 17232, loss 3.26943e-05, acc 1
2017-08-08T17:51:54.788284: step 17233, loss 0.00665725, acc 1
2017-08-08T17:51:55.008386: step 17234, loss 0.00033609, acc 1
2017-08-08T17:51:55.341029: step 17235, loss 1.67638e-08, acc 1
2017-08-08T17:51:55.682552: step 17236, loss 0.000950866, acc 1
2017-08-08T17:51:56.127356: step 17237, loss 0.000103303, acc 1
2017-08-08T17:51:56.466278: step 17238, loss 0.0019754, acc 1
2017-08-08T17:51:56.735970: step 17239, loss 2.81422e-05, acc 1
2017-08-08T17:51:56.955620: step 17240, loss 9.31322e-09, acc 1
2017-08-08T17:51:57.129380: step 17241, loss 8.76598e-06, acc 1
2017-08-08T17:51:57.425679: step 17242, loss 1.11759e-08, acc 1
2017-08-08T17:51:57.598249: step 17243, loss 9.74151e-07, acc 1
2017-08-08T17:51:57.772857: step 17244, loss 1.05075e-05, acc 1
2017-08-08T17:51:57.946872: step 17245, loss 2.03827e-05, acc 1
2017-08-08T17:51:58.274367: step 17246, loss 0.000356352, acc 1
2017-08-08T17:51:58.627607: step 17247, loss 0.000580205, acc 1
2017-08-08T17:51:59.028358: step 17248, loss 0.000426556, acc 1
2017-08-08T17:51:59.344180: step 17249, loss 0.00142251, acc 1
2017-08-08T17:51:59.672361: step 17250, loss 6.56944e-06, acc 1
2017-08-08T17:51:59.959115: step 17251, loss 1.30385e-08, acc 1
2017-08-08T17:52:00.171244: step 17252, loss 2.00602e-06, acc 1
2017-08-08T17:52:00.396397: step 17253, loss 3.48669e-06, acc 1
2017-08-08T17:52:00.723387: step 17254, loss 3.09488e-05, acc 1
2017-08-08T17:52:01.213376: step 17255, loss 1.21139e-05, acc 1
2017-08-08T17:52:01.565388: step 17256, loss 3.8328e-05, acc 1
2017-08-08T17:52:01.848608: step 17257, loss 2.47761e-05, acc 1
2017-08-08T17:52:02.111310: step 17258, loss 0.000440787, acc 1
2017-08-08T17:52:02.541298: step 17259, loss 4.51401e-05, acc 1
2017-08-08T17:52:02.806283: step 17260, loss 0.000978865, acc 1
2017-08-08T17:52:03.062973: step 17261, loss 0.0531237, acc 0.984375
2017-08-08T17:52:03.307819: step 17262, loss 1.1938e-05, acc 1
2017-08-08T17:52:03.582320: step 17263, loss 8.75426e-07, acc 1
2017-08-08T17:52:03.972277: step 17264, loss 2.98023e-08, acc 1
2017-08-08T17:52:04.271059: step 17265, loss 2.43988e-06, acc 1
2017-08-08T17:52:04.542594: step 17266, loss 0.00053541, acc 1
2017-08-08T17:52:04.797593: step 17267, loss 7.45057e-08, acc 1
2017-08-08T17:52:05.069378: step 17268, loss 1.0263e-06, acc 1
2017-08-08T17:52:05.515569: step 17269, loss 4.2468e-07, acc 1
2017-08-08T17:52:05.730938: step 17270, loss 5.96896e-06, acc 1
2017-08-08T17:52:05.925321: step 17271, loss 2.81622e-05, acc 1
2017-08-08T17:52:06.138741: step 17272, loss 7.63143e-06, acc 1
2017-08-08T17:52:06.517903: step 17273, loss 3.14028e-06, acc 1
2017-08-08T17:52:06.724069: step 17274, loss 3.91155e-08, acc 1
2017-08-08T17:52:07.006076: step 17275, loss 3.35276e-08, acc 1
2017-08-08T17:52:07.182098: step 17276, loss 9.2377e-05, acc 1
2017-08-08T17:52:07.367539: step 17277, loss 3.09456e-05, acc 1
2017-08-08T17:52:07.680433: step 17278, loss 0.000173924, acc 1
2017-08-08T17:52:07.866170: step 17279, loss 0.00153578, acc 1
2017-08-08T17:52:08.042736: step 17280, loss 5.45788e-05, acc 1
2017-08-08T17:52:08.280124: step 17281, loss 5.88587e-07, acc 1
2017-08-08T17:52:08.643777: step 17282, loss 0.000169476, acc 1
2017-08-08T17:52:08.946879: step 17283, loss 4.0621e-06, acc 1
2017-08-08T17:52:09.225378: step 17284, loss 1.89989e-07, acc 1
2017-08-08T17:52:09.424162: step 17285, loss 1.86265e-09, acc 1
2017-08-08T17:52:09.701672: step 17286, loss 0.00013342, acc 1
2017-08-08T17:52:09.960952: step 17287, loss 4.07914e-07, acc 1
2017-08-08T17:52:10.160926: step 17288, loss 4.47034e-08, acc 1
2017-08-08T17:52:10.358038: step 17289, loss 0.00139839, acc 1
2017-08-08T17:52:10.588771: step 17290, loss 3.31143e-06, acc 1
2017-08-08T17:52:10.939528: step 17291, loss 1.73225e-07, acc 1
2017-08-08T17:52:11.177375: step 17292, loss 1.97484e-05, acc 1
2017-08-08T17:52:11.458095: step 17293, loss 1.50498e-06, acc 1
2017-08-08T17:52:11.652292: step 17294, loss 1.36713e-06, acc 1
2017-08-08T17:52:11.827766: step 17295, loss 0.00953905, acc 1
2017-08-08T17:52:12.152347: step 17296, loss 2.93877e-05, acc 1
2017-08-08T17:52:12.524446: step 17297, loss 2.39518e-06, acc 1
2017-08-08T17:52:12.801128: step 17298, loss 0.000305747, acc 1
2017-08-08T17:52:13.032179: step 17299, loss 3.16649e-08, acc 1
2017-08-08T17:52:13.277389: step 17300, loss 0.0266531, acc 0.984375

Evaluation:
2017-08-08T17:52:13.951961: step 17300, loss 5.8097, acc 0.712946

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-17300

2017-08-08T17:52:14.406360: step 17301, loss 1.41559e-06, acc 1
2017-08-08T17:52:14.580186: step 17302, loss 1.73364e-05, acc 1
2017-08-08T17:52:14.947884: step 17303, loss 2.51631e-06, acc 1
2017-08-08T17:52:15.147316: step 17304, loss 3.72529e-09, acc 1
2017-08-08T17:52:15.358068: step 17305, loss 1.99255e-05, acc 1
2017-08-08T17:52:15.548915: step 17306, loss 0.000403198, acc 1
2017-08-08T17:52:15.783211: step 17307, loss 9.12695e-08, acc 1
2017-08-08T17:52:16.114176: step 17308, loss 5.45705e-06, acc 1
2017-08-08T17:52:16.452063: step 17309, loss 1.97243e-06, acc 1
2017-08-08T17:52:16.699927: step 17310, loss 0, acc 1
2017-08-08T17:52:16.957683: step 17311, loss 1.695e-07, acc 1
2017-08-08T17:52:17.271244: step 17312, loss 1.613e-06, acc 1
2017-08-08T17:52:17.486031: step 17313, loss 1.3411e-07, acc 1
2017-08-08T17:52:17.727253: step 17314, loss 3.01348e-06, acc 1
2017-08-08T17:52:17.952234: step 17315, loss 2.79395e-07, acc 1
2017-08-08T17:52:18.297972: step 17316, loss 5.34584e-05, acc 1
2017-08-08T17:52:18.565837: step 17317, loss 0.000148858, acc 1
2017-08-08T17:52:18.831852: step 17318, loss 8.19563e-08, acc 1
2017-08-08T17:52:19.015945: step 17319, loss 1.34293e-06, acc 1
2017-08-08T17:52:19.227307: step 17320, loss 2.67015e-05, acc 1
2017-08-08T17:52:19.521707: step 17321, loss 3.20372e-07, acc 1
2017-08-08T17:52:19.789339: step 17322, loss 0.00197621, acc 1
2017-08-08T17:52:20.039796: step 17323, loss 0.000313211, acc 1
2017-08-08T17:52:20.294579: step 17324, loss 1.7601e-06, acc 1
2017-08-08T17:52:20.626692: step 17325, loss 0.000380106, acc 1
2017-08-08T17:52:20.933815: step 17326, loss 2.39445e-05, acc 1
2017-08-08T17:52:21.209503: step 17327, loss 0.000605155, acc 1
2017-08-08T17:52:21.406347: step 17328, loss 6.2166e-05, acc 1
2017-08-08T17:52:21.565822: step 17329, loss 2.08788e-05, acc 1
2017-08-08T17:52:21.925818: step 17330, loss 0, acc 1
2017-08-08T17:52:22.151367: step 17331, loss 3.42725e-07, acc 1
2017-08-08T17:52:22.380517: step 17332, loss 0.000580707, acc 1
2017-08-08T17:52:22.672702: step 17333, loss 1.11758e-07, acc 1
2017-08-08T17:52:23.065588: step 17334, loss 3.8702e-06, acc 1
2017-08-08T17:52:23.333357: step 17335, loss 2.47154e-06, acc 1
2017-08-08T17:52:23.630818: step 17336, loss 5.96679e-06, acc 1
2017-08-08T17:52:23.856560: step 17337, loss 0.0139884, acc 0.984375
2017-08-08T17:52:24.097338: step 17338, loss 1.39943e-05, acc 1
2017-08-08T17:52:24.400036: step 17339, loss 1.97804e-06, acc 1
2017-08-08T17:52:24.643832: step 17340, loss 3.67744e-05, acc 1
2017-08-08T17:52:24.850914: step 17341, loss 9.29446e-07, acc 1
2017-08-08T17:52:25.045562: step 17342, loss 0.00011523, acc 1
2017-08-08T17:52:25.293331: step 17343, loss 2.30967e-07, acc 1
2017-08-08T17:52:25.541310: step 17344, loss 2.04891e-08, acc 1
2017-08-08T17:52:25.802376: step 17345, loss 0.000324363, acc 1
2017-08-08T17:52:25.986590: step 17346, loss 0.000591689, acc 1
2017-08-08T17:52:26.159283: step 17347, loss 1.86265e-09, acc 1
2017-08-08T17:52:26.481416: step 17348, loss 0, acc 1
2017-08-08T17:52:26.701181: step 17349, loss 3.01746e-07, acc 1
2017-08-08T17:52:26.911643: step 17350, loss 2.51224e-05, acc 1
2017-08-08T17:52:27.136116: step 17351, loss 4.61515e-05, acc 1
2017-08-08T17:52:27.493390: step 17352, loss 0.000114012, acc 1
2017-08-08T17:52:27.853795: step 17353, loss 0.000134404, acc 1
2017-08-08T17:52:28.167885: step 17354, loss 6.14672e-08, acc 1
2017-08-08T17:52:28.360101: step 17355, loss 3.19598e-06, acc 1
2017-08-08T17:52:28.661353: step 17356, loss 3.56284e-06, acc 1
2017-08-08T17:52:29.046386: step 17357, loss 7.56216e-07, acc 1
2017-08-08T17:52:29.293851: step 17358, loss 0.00121143, acc 1
2017-08-08T17:52:29.513822: step 17359, loss 2.79397e-08, acc 1
2017-08-08T17:52:29.777754: step 17360, loss 8.99026e-06, acc 1
2017-08-08T17:52:30.184538: step 17361, loss 0.000332337, acc 1
2017-08-08T17:52:30.518476: step 17362, loss 3.89292e-07, acc 1
2017-08-08T17:52:30.808393: step 17363, loss 1.05611e-06, acc 1
2017-08-08T17:52:31.056517: step 17364, loss 8.87556e-05, acc 1
2017-08-08T17:52:31.549027: step 17365, loss 4.65661e-08, acc 1
2017-08-08T17:52:31.849381: step 17366, loss 1.13571e-05, acc 1
2017-08-08T17:52:32.108421: step 17367, loss 3.63212e-07, acc 1
2017-08-08T17:52:32.530554: step 17368, loss 2.12341e-07, acc 1
2017-08-08T17:52:32.822510: step 17369, loss 5.02906e-07, acc 1
2017-08-08T17:52:33.048870: step 17370, loss 0.000233231, acc 1
2017-08-08T17:52:33.245565: step 17371, loss 0.000148349, acc 1
2017-08-08T17:52:33.560525: step 17372, loss 0.00162119, acc 1
2017-08-08T17:52:33.797569: step 17373, loss 2.1084e-06, acc 1
2017-08-08T17:52:34.070802: step 17374, loss 1.13621e-07, acc 1
2017-08-08T17:52:34.335018: step 17375, loss 0.000620004, acc 1
2017-08-08T17:52:34.755569: step 17376, loss 0.00254912, acc 1
2017-08-08T17:52:35.112942: step 17377, loss 0.028869, acc 0.984375
2017-08-08T17:52:35.365481: step 17378, loss 0.00265129, acc 1
2017-08-08T17:52:35.622618: step 17379, loss 1.66305e-05, acc 1
2017-08-08T17:52:35.841425: step 17380, loss 5.87624e-06, acc 1
2017-08-08T17:52:36.278957: step 17381, loss 9.3132e-08, acc 1
2017-08-08T17:52:36.519640: step 17382, loss 1.42599e-05, acc 1
2017-08-08T17:52:36.719370: step 17383, loss 1.02445e-07, acc 1
2017-08-08T17:52:36.922903: step 17384, loss 4.06559e-05, acc 1
2017-08-08T17:52:37.224419: step 17385, loss 1.23707e-05, acc 1
2017-08-08T17:52:37.491472: step 17386, loss 1.17346e-07, acc 1
2017-08-08T17:52:37.842986: step 17387, loss 6.21951e-05, acc 1
2017-08-08T17:52:38.065497: step 17388, loss 2.99883e-07, acc 1
2017-08-08T17:52:38.313944: step 17389, loss 2.08605e-06, acc 1
2017-08-08T17:52:38.633368: step 17390, loss 4.77714e-06, acc 1
2017-08-08T17:52:38.857160: step 17391, loss 6.64754e-05, acc 1
2017-08-08T17:52:39.061644: step 17392, loss 0.000253286, acc 1
2017-08-08T17:52:39.270204: step 17393, loss 4.47034e-08, acc 1
2017-08-08T17:52:39.557357: step 17394, loss 1.59063e-06, acc 1
2017-08-08T17:52:39.940320: step 17395, loss 0.0315876, acc 0.984375
2017-08-08T17:52:40.207546: step 17396, loss 0.000908053, acc 1
2017-08-08T17:52:40.414314: step 17397, loss 1.10796e-05, acc 1
2017-08-08T17:52:40.597395: step 17398, loss 1.24797e-07, acc 1
2017-08-08T17:52:40.989150: step 17399, loss 4.47034e-08, acc 1
2017-08-08T17:52:41.224879: step 17400, loss 4.2095e-05, acc 1

Evaluation:
2017-08-08T17:52:41.881197: step 17400, loss 5.76891, acc 0.710131

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-17400

2017-08-08T17:52:42.486193: step 17401, loss 2.98023e-08, acc 1
2017-08-08T17:52:42.719073: step 17402, loss 1.57573e-06, acc 1
2017-08-08T17:52:42.895022: step 17403, loss 7.36588e-05, acc 1
2017-08-08T17:52:43.097248: step 17404, loss 1.67638e-08, acc 1
2017-08-08T17:52:43.392950: step 17405, loss 6.51923e-07, acc 1
2017-08-08T17:52:43.632384: step 17406, loss 8.85411e-05, acc 1
2017-08-08T17:52:43.888952: step 17407, loss 9.3316e-07, acc 1
2017-08-08T17:52:44.102674: step 17408, loss 2.35055e-06, acc 1
2017-08-08T17:52:44.476594: step 17409, loss 1.53614e-05, acc 1
2017-08-08T17:52:44.734386: step 17410, loss 6.94821e-06, acc 1
2017-08-08T17:52:44.993194: step 17411, loss 3.72529e-09, acc 1
2017-08-08T17:52:45.207862: step 17412, loss 9.30986e-05, acc 1
2017-08-08T17:52:45.547394: step 17413, loss 0.000217927, acc 1
2017-08-08T17:52:45.843671: step 17414, loss 4.44947e-06, acc 1
2017-08-08T17:52:46.076685: step 17415, loss 8.77292e-07, acc 1
2017-08-08T17:52:46.305578: step 17416, loss 1.86265e-09, acc 1
2017-08-08T17:52:46.638270: step 17417, loss 4.23095e-05, acc 1
2017-08-08T17:52:47.072508: step 17418, loss 1.95941e-06, acc 1
2017-08-08T17:52:47.373388: step 17419, loss 0.0839087, acc 0.984375
2017-08-08T17:52:47.636317: step 17420, loss 2.6822e-07, acc 1
2017-08-08T17:52:47.829969: step 17421, loss 5.16434e-05, acc 1
2017-08-08T17:52:48.124115: step 17422, loss 6.0041e-06, acc 1
2017-08-08T17:52:48.433265: step 17423, loss 4.79559e-06, acc 1
2017-08-08T17:52:48.684031: step 17424, loss 8.96895e-05, acc 1
2017-08-08T17:52:48.979960: step 17425, loss 7.44163e-06, acc 1
2017-08-08T17:52:49.336782: step 17426, loss 0.000245896, acc 1
2017-08-08T17:52:49.641373: step 17427, loss 5.148e-06, acc 1
2017-08-08T17:52:49.936217: step 17428, loss 1.78804e-06, acc 1
2017-08-08T17:52:50.194064: step 17429, loss 9.85912e-06, acc 1
2017-08-08T17:52:50.450616: step 17430, loss 0.000568064, acc 1
2017-08-08T17:52:50.887765: step 17431, loss 4.08838e-05, acc 1
2017-08-08T17:52:51.119366: step 17432, loss 3.79898e-05, acc 1
2017-08-08T17:52:51.345869: step 17433, loss 6.63089e-07, acc 1
2017-08-08T17:52:51.575789: step 17434, loss 3.86271e-06, acc 1
2017-08-08T17:52:51.870701: step 17435, loss 1.43211e-05, acc 1
2017-08-08T17:52:52.159173: step 17436, loss 1.39322e-06, acc 1
2017-08-08T17:52:52.454232: step 17437, loss 2.16991e-06, acc 1
2017-08-08T17:52:52.763375: step 17438, loss 1.11941e-06, acc 1
2017-08-08T17:52:52.972855: step 17439, loss 1.86265e-09, acc 1
2017-08-08T17:52:53.212921: step 17440, loss 1.67638e-08, acc 1
2017-08-08T17:52:53.497636: step 17441, loss 0.000559391, acc 1
2017-08-08T17:52:53.798700: step 17442, loss 4.74344e-06, acc 1
2017-08-08T17:52:54.006609: step 17443, loss 6.20249e-07, acc 1
2017-08-08T17:52:54.305409: step 17444, loss 0.000147099, acc 1
2017-08-08T17:52:54.625421: step 17445, loss 3.53122e-06, acc 1
2017-08-08T17:52:55.009826: step 17446, loss 0.000265056, acc 1
2017-08-08T17:52:55.255693: step 17447, loss 4.66558e-06, acc 1
2017-08-08T17:52:55.453727: step 17448, loss 1.38764e-06, acc 1
2017-08-08T17:52:55.791619: step 17449, loss 4.04191e-07, acc 1
2017-08-08T17:52:56.002742: step 17450, loss 0.000187357, acc 1
2017-08-08T17:52:56.213568: step 17451, loss 0.010445, acc 1
2017-08-08T17:52:56.439589: step 17452, loss 0, acc 1
2017-08-08T17:52:56.752574: step 17453, loss 0.000104917, acc 1
2017-08-08T17:52:57.055145: step 17454, loss 5.58793e-09, acc 1
2017-08-08T17:52:57.335349: step 17455, loss 2.44554e-06, acc 1
2017-08-08T17:52:57.533405: step 17456, loss 0.000127963, acc 1
2017-08-08T17:52:57.728515: step 17457, loss 1.28705e-06, acc 1
2017-08-08T17:52:58.153953: step 17458, loss 7.45058e-08, acc 1
2017-08-08T17:52:58.366231: step 17459, loss 2.2947e-06, acc 1
2017-08-08T17:52:58.592364: step 17460, loss 2.79394e-07, acc 1
2017-08-08T17:52:58.830468: step 17461, loss 0.00329269, acc 1
2017-08-08T17:52:59.139315: step 17462, loss 7.23194e-05, acc 1
2017-08-08T17:52:59.487351: step 17463, loss 3.31547e-07, acc 1
2017-08-08T17:52:59.848387: step 17464, loss 2.82061e-05, acc 1
2017-08-08T17:53:00.092444: step 17465, loss 0.000242606, acc 1
2017-08-08T17:53:00.359720: step 17466, loss 6.33299e-08, acc 1
2017-08-08T17:53:00.758112: step 17467, loss 0.000124671, acc 1
2017-08-08T17:53:01.016942: step 17468, loss 0.00173416, acc 1
2017-08-08T17:53:01.290487: step 17469, loss 9.12468e-06, acc 1
2017-08-08T17:53:01.604742: step 17470, loss 0.0674604, acc 0.984375
2017-08-08T17:53:02.100575: step 17471, loss 0.00246541, acc 1
2017-08-08T17:53:02.494774: step 17472, loss 0.00102566, acc 1
2017-08-08T17:53:02.825149: step 17473, loss 4.33992e-07, acc 1
2017-08-08T17:53:03.105935: step 17474, loss 9.31322e-09, acc 1
2017-08-08T17:53:03.595112: step 17475, loss 1.423e-06, acc 1
2017-08-08T17:53:03.896733: step 17476, loss 5.40167e-08, acc 1
2017-08-08T17:53:04.168933: step 17477, loss 3.32261e-06, acc 1
2017-08-08T17:53:04.474973: step 17478, loss 0.000105053, acc 1
2017-08-08T17:53:04.847473: step 17479, loss 6.41478e-05, acc 1
2017-08-08T17:53:05.188062: step 17480, loss 0.00054789, acc 1
2017-08-08T17:53:05.547132: step 17481, loss 6.17385e-06, acc 1
2017-08-08T17:53:05.865187: step 17482, loss 8.66117e-07, acc 1
2017-08-08T17:53:06.062157: step 17483, loss 4.37663e-06, acc 1
2017-08-08T17:53:06.337854: step 17484, loss 5.96046e-08, acc 1
2017-08-08T17:53:06.681562: step 17485, loss 7.58051e-05, acc 1
2017-08-08T17:53:06.943840: step 17486, loss 2.37591e-05, acc 1
2017-08-08T17:53:07.200505: step 17487, loss 0.00025044, acc 1
2017-08-08T17:53:07.593894: step 17488, loss 7.48771e-07, acc 1
2017-08-08T17:53:07.921773: step 17489, loss 3.40624e-05, acc 1
2017-08-08T17:53:08.283135: step 17490, loss 0.00048004, acc 1
2017-08-08T17:53:08.511239: step 17491, loss 0.000266778, acc 1
2017-08-08T17:53:08.728253: step 17492, loss 8.18405e-05, acc 1
2017-08-08T17:53:09.090620: step 17493, loss 0.000362926, acc 1
2017-08-08T17:53:09.321497: step 17494, loss 8.01138e-05, acc 1
2017-08-08T17:53:09.529734: step 17495, loss 0.0282109, acc 0.984375
2017-08-08T17:53:09.768270: step 17496, loss 0.00218282, acc 1
2017-08-08T17:53:10.073473: step 17497, loss 4.47034e-08, acc 1
2017-08-08T17:53:10.418247: step 17498, loss 2.30581e-06, acc 1
2017-08-08T17:53:10.675496: step 17499, loss 7.00943e-06, acc 1
2017-08-08T17:53:10.872154: step 17500, loss 0.0313751, acc 0.984375

Evaluation:
2017-08-08T17:53:11.499465: step 17500, loss 5.8719, acc 0.710131

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-17500

2017-08-08T17:53:11.835441: step 17501, loss 0.000132159, acc 1
2017-08-08T17:53:12.165154: step 17502, loss 0.065488, acc 0.984375
2017-08-08T17:53:12.556309: step 17503, loss 1.91287e-06, acc 1
2017-08-08T17:53:12.854794: step 17504, loss 4.15878e-06, acc 1
2017-08-08T17:53:13.054649: step 17505, loss 9.87201e-08, acc 1
2017-08-08T17:53:13.263378: step 17506, loss 1.30385e-07, acc 1
2017-08-08T17:53:13.690359: step 17507, loss 9.40726e-05, acc 1
2017-08-08T17:53:13.959415: step 17508, loss 2.28875e-05, acc 1
2017-08-08T17:53:14.266309: step 17509, loss 1.79028e-05, acc 1
2017-08-08T17:53:14.637740: step 17510, loss 9.42486e-07, acc 1
2017-08-08T17:53:15.004275: step 17511, loss 0.000171196, acc 1
2017-08-08T17:53:15.331960: step 17512, loss 0.0626217, acc 0.984375
2017-08-08T17:53:15.547134: step 17513, loss 1.92773e-06, acc 1
2017-08-08T17:53:15.825321: step 17514, loss 9.57319e-05, acc 1
2017-08-08T17:53:16.160378: step 17515, loss 2.42144e-08, acc 1
2017-08-08T17:53:16.365923: step 17516, loss 3.429e-06, acc 1
2017-08-08T17:53:16.593400: step 17517, loss 1.46959e-06, acc 1
2017-08-08T17:53:16.933359: step 17518, loss 2.73807e-07, acc 1
2017-08-08T17:53:17.307196: step 17519, loss 0.000402194, acc 1
2017-08-08T17:53:17.621726: step 17520, loss 3.60081e-05, acc 1
2017-08-08T17:53:17.897329: step 17521, loss 3.12924e-07, acc 1
2017-08-08T17:53:18.145374: step 17522, loss 2.75467e-06, acc 1
2017-08-08T17:53:18.512223: step 17523, loss 2.05436e-06, acc 1
2017-08-08T17:53:18.766907: step 17524, loss 0.000194102, acc 1
2017-08-08T17:53:19.043849: step 17525, loss 1.25168e-06, acc 1
2017-08-08T17:53:19.321423: step 17526, loss 0, acc 1
2017-08-08T17:53:19.703564: step 17527, loss 0.00148622, acc 1
2017-08-08T17:53:20.117399: step 17528, loss 1.07471e-06, acc 1
2017-08-08T17:53:20.461363: step 17529, loss 0.00266753, acc 1
2017-08-08T17:53:20.705339: step 17530, loss 7.65926e-06, acc 1
2017-08-08T17:53:20.907470: step 17531, loss 0.00014396, acc 1
2017-08-08T17:53:21.333550: step 17532, loss 1.19817e-05, acc 1
2017-08-08T17:53:21.613882: step 17533, loss 0.00126051, acc 1
2017-08-08T17:53:21.918415: step 17534, loss 1.83348e-05, acc 1
2017-08-08T17:53:22.253506: step 17535, loss 1.35973e-07, acc 1
2017-08-08T17:53:22.626562: step 17536, loss 3.72529e-09, acc 1
2017-08-08T17:53:22.994356: step 17537, loss 1.17346e-07, acc 1
2017-08-08T17:53:23.302106: step 17538, loss 1.13247e-06, acc 1
2017-08-08T17:53:23.621422: step 17539, loss 0.000108068, acc 1
2017-08-08T17:53:23.958773: step 17540, loss 1.49753e-06, acc 1
2017-08-08T17:53:24.252160: step 17541, loss 0.00123612, acc 1
2017-08-08T17:53:24.574633: step 17542, loss 3.35276e-08, acc 1
2017-08-08T17:53:24.973411: step 17543, loss 0.00110729, acc 1
2017-08-08T17:53:25.341471: step 17544, loss 3.21111e-06, acc 1
2017-08-08T17:53:25.600097: step 17545, loss 4.96156e-06, acc 1
2017-08-08T17:53:25.825685: step 17546, loss 6.03491e-07, acc 1
2017-08-08T17:53:26.080294: step 17547, loss 0.00125365, acc 1
2017-08-08T17:53:26.468093: step 17548, loss 1.49012e-08, acc 1
2017-08-08T17:53:26.788782: step 17549, loss 7.45057e-08, acc 1
2017-08-08T17:53:27.068865: step 17550, loss 9.34427e-06, acc 1
2017-08-08T17:53:27.359660: step 17551, loss 1.28522e-07, acc 1
2017-08-08T17:53:27.636653: step 17552, loss 1.2942e-05, acc 1
2017-08-08T17:53:28.077423: step 17553, loss 1.67637e-07, acc 1
2017-08-08T17:53:28.489395: step 17554, loss 5.86439e-06, acc 1
2017-08-08T17:53:28.819228: step 17555, loss 8.04803e-05, acc 1
2017-08-08T17:53:29.049121: step 17556, loss 0.000788958, acc 1
2017-08-08T17:53:29.352696: step 17557, loss 2.6077e-08, acc 1
2017-08-08T17:53:29.717103: step 17558, loss 8.32298e-06, acc 1
2017-08-08T17:53:30.017459: step 17559, loss 2.55181e-07, acc 1
2017-08-08T17:53:30.218721: step 17560, loss 4.84282e-07, acc 1
2017-08-08T17:53:30.590835: step 17561, loss 2.79538e-05, acc 1
2017-08-08T17:53:30.945549: step 17562, loss 3.42725e-07, acc 1
2017-08-08T17:53:31.293558: step 17563, loss 0.000190456, acc 1
2017-08-08T17:53:31.614868: step 17564, loss 8.34445e-06, acc 1
2017-08-08T17:53:31.866334: step 17565, loss 2.41687e-05, acc 1
2017-08-08T17:53:32.068220: step 17566, loss 0, acc 1
2017-08-08T17:53:32.311869: step 17567, loss 7.83233e-06, acc 1
2017-08-08T17:53:32.645358: step 17568, loss 0, acc 1
2017-08-08T17:53:32.955352: step 17569, loss 4.77156e-06, acc 1
2017-08-08T17:53:33.281598: step 17570, loss 1.14064e-05, acc 1
2017-08-08T17:53:33.520318: step 17571, loss 2.53318e-07, acc 1
2017-08-08T17:53:33.760332: step 17572, loss 8.28866e-07, acc 1
2017-08-08T17:53:34.139325: step 17573, loss 1.73226e-07, acc 1
2017-08-08T17:53:34.432824: step 17574, loss 2.45782e-05, acc 1
2017-08-08T17:53:34.655998: step 17575, loss 0.000162402, acc 1
2017-08-08T17:53:34.977261: step 17576, loss 4.47564e-06, acc 1
2017-08-08T17:53:35.305276: step 17577, loss 4.88006e-07, acc 1
2017-08-08T17:53:35.561128: step 17578, loss 0.0653365, acc 0.984375
2017-08-08T17:53:35.760464: step 17579, loss 2.26688e-05, acc 1
2017-08-08T17:53:36.008518: step 17580, loss 1.11759e-08, acc 1
2017-08-08T17:53:36.368908: step 17581, loss 1.2442e-06, acc 1
2017-08-08T17:53:36.575709: step 17582, loss 1.49012e-08, acc 1
2017-08-08T17:53:36.768221: step 17583, loss 9.68575e-08, acc 1
2017-08-08T17:53:37.036782: step 17584, loss 6.51925e-08, acc 1
2017-08-08T17:53:37.371023: step 17585, loss 2.61396e-05, acc 1
2017-08-08T17:53:37.594862: step 17586, loss 0.00400942, acc 1
2017-08-08T17:53:37.817445: step 17587, loss 6.94377e-05, acc 1
2017-08-08T17:53:37.991886: step 17588, loss 2.52986e-05, acc 1
2017-08-08T17:53:38.243696: step 17589, loss 6.01939e-05, acc 1
2017-08-08T17:53:38.541321: step 17590, loss 7.45058e-09, acc 1
2017-08-08T17:53:38.790058: step 17591, loss 0.000304499, acc 1
2017-08-08T17:53:38.979747: step 17592, loss 1.83646e-06, acc 1
2017-08-08T17:53:39.217463: step 17593, loss 4.04142e-06, acc 1
2017-08-08T17:53:39.636561: step 17594, loss 7.07804e-08, acc 1
2017-08-08T17:53:39.975746: step 17595, loss 3.0436e-05, acc 1
2017-08-08T17:53:40.319323: step 17596, loss 7.45057e-08, acc 1
2017-08-08T17:53:40.566130: step 17597, loss 0.000273832, acc 1
2017-08-08T17:53:40.905765: step 17598, loss 2.04891e-08, acc 1
2017-08-08T17:53:41.201823: step 17599, loss 1.27959e-06, acc 1
2017-08-08T17:53:41.391686: step 17600, loss 6.51912e-07, acc 1

Evaluation:
2017-08-08T17:53:41.843168: step 17600, loss 5.89579, acc 0.712946

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-17600

2017-08-08T17:53:42.483415: step 17601, loss 2.28849e-05, acc 1
2017-08-08T17:53:42.671920: step 17602, loss 3.46451e-07, acc 1
2017-08-08T17:53:42.859828: step 17603, loss 2.80109e-05, acc 1
2017-08-08T17:53:43.037450: step 17604, loss 3.32081e-06, acc 1
2017-08-08T17:53:43.341145: step 17605, loss 1.80857e-06, acc 1
2017-08-08T17:53:43.546652: step 17606, loss 1.10206e-05, acc 1
2017-08-08T17:53:43.775787: step 17607, loss 4.28408e-08, acc 1
2017-08-08T17:53:43.970737: step 17608, loss 0.00353717, acc 1
2017-08-08T17:53:44.308294: step 17609, loss 0.00204465, acc 1
2017-08-08T17:53:44.601750: step 17610, loss 5.89615e-06, acc 1
2017-08-08T17:53:44.811869: step 17611, loss 3.49212e-06, acc 1
2017-08-08T17:53:45.102290: step 17612, loss 7.20163e-05, acc 1
2017-08-08T17:53:45.289067: step 17613, loss 7.56218e-07, acc 1
2017-08-08T17:53:45.629322: step 17614, loss 5.13111e-05, acc 1
2017-08-08T17:53:45.818419: step 17615, loss 3.25194e-06, acc 1
2017-08-08T17:53:45.986651: step 17616, loss 1.09707e-06, acc 1
2017-08-08T17:53:46.222554: step 17617, loss 3.08114e-05, acc 1
2017-08-08T17:53:46.601348: step 17618, loss 3.39548e-05, acc 1
2017-08-08T17:53:46.977321: step 17619, loss 0.0165789, acc 0.984375
2017-08-08T17:53:47.245982: step 17620, loss 5.8048e-06, acc 1
2017-08-08T17:53:47.458395: step 17621, loss 0.000246316, acc 1
2017-08-08T17:53:47.804221: step 17622, loss 7.17516e-06, acc 1
2017-08-08T17:53:48.000703: step 17623, loss 0.00016907, acc 1
2017-08-08T17:53:48.207797: step 17624, loss 3.94879e-07, acc 1
2017-08-08T17:53:48.443750: step 17625, loss 0.0621381, acc 0.984375
2017-08-08T17:53:48.801445: step 17626, loss 6.17767e-06, acc 1
2017-08-08T17:53:49.076295: step 17627, loss 9.21085e-05, acc 1
2017-08-08T17:53:49.324404: step 17628, loss 0.00290571, acc 1
2017-08-08T17:53:49.551206: step 17629, loss 0.000114824, acc 1
2017-08-08T17:53:49.885398: step 17630, loss 1.3038e-06, acc 1
2017-08-08T17:53:50.255075: step 17631, loss 8.64037e-06, acc 1
2017-08-08T17:53:50.503490: step 17632, loss 1.30385e-08, acc 1
2017-08-08T17:53:50.699592: step 17633, loss 2.79397e-08, acc 1
2017-08-08T17:53:50.973049: step 17634, loss 4.27236e-06, acc 1
2017-08-08T17:53:51.353290: step 17635, loss 8.36225e-05, acc 1
2017-08-08T17:53:51.713378: step 17636, loss 1.4826e-06, acc 1
2017-08-08T17:53:52.073539: step 17637, loss 2.01154e-06, acc 1
2017-08-08T17:53:52.283970: step 17638, loss 4.17005e-06, acc 1
2017-08-08T17:53:52.559553: step 17639, loss 0.000165441, acc 1
2017-08-08T17:53:52.805163: step 17640, loss 1.8716e-05, acc 1
2017-08-08T17:53:53.089364: step 17641, loss 2.70762e-05, acc 1
2017-08-08T17:53:53.355207: step 17642, loss 1.4917e-05, acc 1
2017-08-08T17:53:53.563887: step 17643, loss 0.000112307, acc 1
2017-08-08T17:53:53.897460: step 17644, loss 0.00326616, acc 1
2017-08-08T17:53:54.120125: step 17645, loss 5.40167e-08, acc 1
2017-08-08T17:53:54.387224: step 17646, loss 3.72527e-07, acc 1
2017-08-08T17:53:54.633145: step 17647, loss 1.47074e-05, acc 1
2017-08-08T17:53:54.972376: step 17648, loss 0.000849081, acc 1
2017-08-08T17:53:55.270166: step 17649, loss 0.0446182, acc 0.984375
2017-08-08T17:53:55.582167: step 17650, loss 1.11759e-08, acc 1
2017-08-08T17:53:55.854433: step 17651, loss 1.90723e-06, acc 1
2017-08-08T17:53:56.156716: step 17652, loss 1.11759e-08, acc 1
2017-08-08T17:53:56.483807: step 17653, loss 1.36153e-06, acc 1
2017-08-08T17:53:56.768401: step 17654, loss 7.80429e-07, acc 1
2017-08-08T17:53:57.084535: step 17655, loss 0.0217993, acc 0.984375
2017-08-08T17:53:57.483796: step 17656, loss 3.39903e-06, acc 1
2017-08-08T17:53:57.762027: step 17657, loss 6.79764e-06, acc 1
2017-08-08T17:53:57.976817: step 17658, loss 1.99302e-07, acc 1
2017-08-08T17:53:58.159959: step 17659, loss 2.23516e-07, acc 1
2017-08-08T17:53:58.342200: step 17660, loss 1.30755e-06, acc 1
2017-08-08T17:53:58.642784: step 17661, loss 2.21653e-07, acc 1
2017-08-08T17:53:58.801651: step 17662, loss 3.5144e-05, acc 1
2017-08-08T17:53:59.031416: step 17663, loss 3.18607e-05, acc 1
2017-08-08T17:53:59.263817: step 17664, loss 6.33298e-08, acc 1
2017-08-08T17:53:59.601354: step 17665, loss 0.000188745, acc 1
2017-08-08T17:53:59.890055: step 17666, loss 4.65971e-05, acc 1
2017-08-08T17:54:00.169313: step 17667, loss 1.87981e-05, acc 1
2017-08-08T17:54:00.420413: step 17668, loss 1.56083e-06, acc 1
2017-08-08T17:54:00.673363: step 17669, loss 1.31686e-06, acc 1
2017-08-08T17:54:01.020882: step 17670, loss 0.0501216, acc 0.984375
2017-08-08T17:54:01.303420: step 17671, loss 0.000104733, acc 1
2017-08-08T17:54:01.626976: step 17672, loss 0.0037971, acc 1
2017-08-08T17:54:01.898711: step 17673, loss 0.000154373, acc 1
2017-08-08T17:54:02.444729: step 17674, loss 0.0747503, acc 0.984375
2017-08-08T17:54:02.829369: step 17675, loss 1.24161e-05, acc 1
2017-08-08T17:54:03.272415: step 17676, loss 0.0980589, acc 0.984375
2017-08-08T17:54:03.540882: step 17677, loss 3.29687e-07, acc 1
2017-08-08T17:54:03.872012: step 17678, loss 8.03535e-06, acc 1
2017-08-08T17:54:04.304162: step 17679, loss 1.5534e-06, acc 1
2017-08-08T17:54:04.549954: step 17680, loss 3.8929e-07, acc 1
2017-08-08T17:54:04.800357: step 17681, loss 0.00812721, acc 1
2017-08-08T17:54:05.033600: step 17682, loss 2.89988e-06, acc 1
2017-08-08T17:54:05.380371: step 17683, loss 3.28912e-06, acc 1
2017-08-08T17:54:05.669126: step 17684, loss 0.000354346, acc 1
2017-08-08T17:54:06.056202: step 17685, loss 2.98749e-06, acc 1
2017-08-08T17:54:06.330194: step 17686, loss 7.6373e-05, acc 1
2017-08-08T17:54:06.597250: step 17687, loss 0.000130499, acc 1
2017-08-08T17:54:06.949316: step 17688, loss 0.00599269, acc 1
2017-08-08T17:54:07.230769: step 17689, loss 5.77419e-08, acc 1
2017-08-08T17:54:07.488672: step 17690, loss 1.60759e-05, acc 1
2017-08-08T17:54:07.709476: step 17691, loss 0.000251298, acc 1
2017-08-08T17:54:08.049037: step 17692, loss 7.68901e-05, acc 1
2017-08-08T17:54:08.381363: step 17693, loss 0.000344353, acc 1
2017-08-08T17:54:08.743464: step 17694, loss 3.64713e-05, acc 1
2017-08-08T17:54:09.003098: step 17695, loss 1.11291e-05, acc 1
2017-08-08T17:54:09.252138: step 17696, loss 3.91155e-08, acc 1
2017-08-08T17:54:09.655874: step 17697, loss 2.92413e-06, acc 1
2017-08-08T17:54:09.834381: step 17698, loss 3.66886e-05, acc 1
2017-08-08T17:54:10.048335: step 17699, loss 8.17691e-07, acc 1
2017-08-08T17:54:10.278289: step 17700, loss 5.52327e-07, acc 1

Evaluation:
2017-08-08T17:54:11.316911: step 17700, loss 6.02404, acc 0.712946

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-17700

2017-08-08T17:54:11.873054: step 17701, loss 1.08033e-07, acc 1
2017-08-08T17:54:12.103036: step 17702, loss 1.29451e-06, acc 1
2017-08-08T17:54:12.373321: step 17703, loss 1.33303e-05, acc 1
2017-08-08T17:54:12.656093: step 17704, loss 1.93153e-05, acc 1
2017-08-08T17:54:12.884605: step 17705, loss 0.0360454, acc 0.984375
2017-08-08T17:54:13.086439: step 17706, loss 1.60366e-06, acc 1
2017-08-08T17:54:13.283769: step 17707, loss 0.000178887, acc 1
2017-08-08T17:54:13.530551: step 17708, loss 0.000426211, acc 1
2017-08-08T17:54:13.801318: step 17709, loss 6.24395e-05, acc 1
2017-08-08T17:54:14.013471: step 17710, loss 8.32722e-05, acc 1
2017-08-08T17:54:14.200270: step 17711, loss 4.82421e-07, acc 1
2017-08-08T17:54:14.494855: step 17712, loss 2.08616e-07, acc 1
2017-08-08T17:54:14.766658: step 17713, loss 3.22236e-07, acc 1
2017-08-08T17:54:15.026228: step 17714, loss 5.02166e-05, acc 1
2017-08-08T17:54:15.272384: step 17715, loss 1.30385e-08, acc 1
2017-08-08T17:54:15.465771: step 17716, loss 0.000169979, acc 1
2017-08-08T17:54:15.745347: step 17717, loss 1.88578e-05, acc 1
2017-08-08T17:54:16.043371: step 17718, loss 0.0019253, acc 1
2017-08-08T17:54:16.286967: step 17719, loss 0.000454817, acc 1
2017-08-08T17:54:16.502575: step 17720, loss 0.000879612, acc 1
2017-08-08T17:54:16.685505: step 17721, loss 6.10371e-05, acc 1
2017-08-08T17:54:16.972568: step 17722, loss 2.23516e-07, acc 1
2017-08-08T17:54:17.171913: step 17723, loss 0.000416888, acc 1
2017-08-08T17:54:17.390844: step 17724, loss 0.000107858, acc 1
2017-08-08T17:54:17.569981: step 17725, loss 1.34106e-06, acc 1
2017-08-08T17:54:17.901334: step 17726, loss 8.08367e-07, acc 1
2017-08-08T17:54:18.185917: step 17727, loss 0.00374352, acc 1
2017-08-08T17:54:18.405991: step 17728, loss 4.57342e-05, acc 1
2017-08-08T17:54:18.573177: step 17729, loss 1.52916e-06, acc 1
2017-08-08T17:54:18.836110: step 17730, loss 2.71945e-07, acc 1
2017-08-08T17:54:19.120879: step 17731, loss 0.000113334, acc 1
2017-08-08T17:54:19.373842: step 17732, loss 4.12327e-05, acc 1
2017-08-08T17:54:19.588667: step 17733, loss 0.00011586, acc 1
2017-08-08T17:54:19.895428: step 17734, loss 0.0432161, acc 0.984375
2017-08-08T17:54:20.138557: step 17735, loss 1.91851e-07, acc 1
2017-08-08T17:54:20.372155: step 17736, loss 7.45058e-09, acc 1
2017-08-08T17:54:20.550606: step 17737, loss 0.00303445, acc 1
2017-08-08T17:54:20.801412: step 17738, loss 1.11759e-08, acc 1
2017-08-08T17:54:21.101190: step 17739, loss 2.17929e-07, acc 1
2017-08-08T17:54:21.298966: step 17740, loss 3.00085e-05, acc 1
2017-08-08T17:54:21.562695: step 17741, loss 1.66699e-06, acc 1
2017-08-08T17:54:21.821332: step 17742, loss 1.63712e-05, acc 1
2017-08-08T17:54:22.129316: step 17743, loss 6.33296e-07, acc 1
2017-08-08T17:54:22.346315: step 17744, loss 8.43291e-05, acc 1
2017-08-08T17:54:22.570199: step 17745, loss 5.02914e-08, acc 1
2017-08-08T17:54:23.040500: step 17746, loss 5.82999e-07, acc 1
2017-08-08T17:54:23.262640: step 17747, loss 1.58324e-07, acc 1
2017-08-08T17:54:23.457752: step 17748, loss 3.4586e-06, acc 1
2017-08-08T17:54:23.686952: step 17749, loss 0.00989244, acc 1
2017-08-08T17:54:23.986222: step 17750, loss 2.06e-06, acc 1
2017-08-08T17:54:24.377375: step 17751, loss 8.36318e-07, acc 1
2017-08-08T17:54:24.667818: step 17752, loss 5.60373e-06, acc 1
2017-08-08T17:54:24.923610: step 17753, loss 7.95894e-06, acc 1
2017-08-08T17:54:25.134806: step 17754, loss 8.38188e-08, acc 1
2017-08-08T17:54:25.490784: step 17755, loss 0.000700226, acc 1
2017-08-08T17:54:25.790920: step 17756, loss 3.7885e-05, acc 1
2017-08-08T17:54:26.015294: step 17757, loss 7.00386e-05, acc 1
2017-08-08T17:54:26.243931: step 17758, loss 3.09862e-05, acc 1
2017-08-08T17:54:26.606445: step 17759, loss 9.27062e-06, acc 1
2017-08-08T17:54:26.958219: step 17760, loss 2.96707e-06, acc 1
2017-08-08T17:54:27.264683: step 17761, loss 1.02629e-06, acc 1
2017-08-08T17:54:27.499668: step 17762, loss 2.38401e-06, acc 1
2017-08-08T17:54:27.675706: step 17763, loss 3.72529e-09, acc 1
2017-08-08T17:54:27.869557: step 17764, loss 5.05293e-06, acc 1
2017-08-08T17:54:28.229341: step 17765, loss 1.90174e-05, acc 1
2017-08-08T17:54:28.448939: step 17766, loss 0.000350878, acc 1
2017-08-08T17:54:28.734807: step 17767, loss 0, acc 1
2017-08-08T17:54:28.928332: step 17768, loss 6.7058e-05, acc 1
2017-08-08T17:54:29.349553: step 17769, loss 1.30385e-07, acc 1
2017-08-08T17:54:29.726225: step 17770, loss 2.62631e-07, acc 1
2017-08-08T17:54:29.939970: step 17771, loss 5.63954e-06, acc 1
2017-08-08T17:54:30.171206: step 17772, loss 7.11793e-05, acc 1
2017-08-08T17:54:30.499596: step 17773, loss 4.40356e-05, acc 1
2017-08-08T17:54:30.737455: step 17774, loss 0.000133929, acc 1
2017-08-08T17:54:30.971982: step 17775, loss 7.93467e-07, acc 1
2017-08-08T17:54:31.296854: step 17776, loss 4.43305e-07, acc 1
2017-08-08T17:54:31.719278: step 17777, loss 5.79929e-06, acc 1
2017-08-08T17:54:32.055199: step 17778, loss 0.000184335, acc 1
2017-08-08T17:54:32.387884: step 17779, loss 3.52037e-07, acc 1
2017-08-08T17:54:32.646470: step 17780, loss 5.58793e-09, acc 1
2017-08-08T17:54:33.033373: step 17781, loss 1.59063e-06, acc 1
2017-08-08T17:54:33.402504: step 17782, loss 3.0768e-06, acc 1
2017-08-08T17:54:33.668759: step 17783, loss 0.0184872, acc 0.984375
2017-08-08T17:54:33.942119: step 17784, loss 2.6077e-08, acc 1
2017-08-08T17:54:34.373818: step 17785, loss 0.00583044, acc 1
2017-08-08T17:54:34.868288: step 17786, loss 1.7508e-06, acc 1
2017-08-08T17:54:35.217343: step 17787, loss 2.34006e-05, acc 1
2017-08-08T17:54:35.486309: step 17788, loss 3.24817e-06, acc 1
2017-08-08T17:54:35.943587: step 17789, loss 3.66938e-07, acc 1
2017-08-08T17:54:36.181524: step 17790, loss 1.91851e-07, acc 1
2017-08-08T17:54:36.381692: step 17791, loss 3.73435e-06, acc 1
2017-08-08T17:54:36.579730: step 17792, loss 2.42144e-08, acc 1
2017-08-08T17:54:36.879944: step 17793, loss 1.17157e-06, acc 1
2017-08-08T17:54:37.181322: step 17794, loss 1.84376e-05, acc 1
2017-08-08T17:54:37.515264: step 17795, loss 1.1708e-05, acc 1
2017-08-08T17:54:37.699725: step 17796, loss 1.71127e-05, acc 1
2017-08-08T17:54:38.008988: step 17797, loss 1.49103e-05, acc 1
2017-08-08T17:54:38.362055: step 17798, loss 1.49012e-08, acc 1
2017-08-08T17:54:38.567320: step 17799, loss 0.00251836, acc 1
2017-08-08T17:54:38.810020: step 17800, loss 7.69263e-07, acc 1

Evaluation:
2017-08-08T17:54:39.298736: step 17800, loss 5.99832, acc 0.711069

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-17800

2017-08-08T17:54:39.736059: step 17801, loss 1.73225e-07, acc 1
2017-08-08T17:54:39.991314: step 17802, loss 1.86264e-08, acc 1
2017-08-08T17:54:40.251816: step 17803, loss 2.6704e-05, acc 1
2017-08-08T17:54:40.584537: step 17804, loss 1.38948e-06, acc 1
2017-08-08T17:54:40.862118: step 17805, loss 1.73226e-07, acc 1
2017-08-08T17:54:41.057160: step 17806, loss 9.27342e-06, acc 1
2017-08-08T17:54:41.300085: step 17807, loss 1.95197e-05, acc 1
2017-08-08T17:54:41.541200: step 17808, loss 0.000223779, acc 1
2017-08-08T17:54:41.973800: step 17809, loss 0.0649696, acc 0.984375
2017-08-08T17:54:42.418980: step 17810, loss 2.54977e-06, acc 1
2017-08-08T17:54:42.712676: step 17811, loss 4.16257e-06, acc 1
2017-08-08T17:54:42.900842: step 17812, loss 0.000113139, acc 1
2017-08-08T17:54:43.221384: step 17813, loss 1.11759e-08, acc 1
2017-08-08T17:54:43.510075: step 17814, loss 0.000146381, acc 1
2017-08-08T17:54:43.810008: step 17815, loss 0.00122773, acc 1
2017-08-08T17:54:44.176401: step 17816, loss 3.45112e-06, acc 1
2017-08-08T17:54:44.553428: step 17817, loss 1.20653e-05, acc 1
2017-08-08T17:54:44.888494: step 17818, loss 2.92311e-05, acc 1
2017-08-08T17:54:45.183242: step 17819, loss 0.000162648, acc 1
2017-08-08T17:54:45.411200: step 17820, loss 0.000305879, acc 1
2017-08-08T17:54:45.749022: step 17821, loss 0.000619193, acc 1
2017-08-08T17:54:46.051309: step 17822, loss 0.000102851, acc 1
2017-08-08T17:54:46.291350: step 17823, loss 5.59003e-05, acc 1
2017-08-08T17:54:46.528434: step 17824, loss 8.75442e-08, acc 1
2017-08-08T17:54:46.788162: step 17825, loss 9.07262e-06, acc 1
2017-08-08T17:54:47.061419: step 17826, loss 7.45058e-09, acc 1
2017-08-08T17:54:47.342377: step 17827, loss 0.000164365, acc 1
2017-08-08T17:54:47.599498: step 17828, loss 2.44356e-05, acc 1
2017-08-08T17:54:47.782298: step 17829, loss 4.91736e-07, acc 1
2017-08-08T17:54:48.063868: step 17830, loss 2.76765e-06, acc 1
2017-08-08T17:54:48.334572: step 17831, loss 0.000180979, acc 1
2017-08-08T17:54:48.548147: step 17832, loss 0.000362248, acc 1
2017-08-08T17:54:48.769002: step 17833, loss 1.30385e-08, acc 1
2017-08-08T17:54:48.982878: step 17834, loss 7.45057e-08, acc 1
2017-08-08T17:54:49.318791: step 17835, loss 0.00105126, acc 1
2017-08-08T17:54:49.599898: step 17836, loss 2.5572e-06, acc 1
2017-08-08T17:54:49.939779: step 17837, loss 4.19762e-05, acc 1
2017-08-08T17:54:50.129905: step 17838, loss 1.71362e-07, acc 1
2017-08-08T17:54:50.285288: step 17839, loss 2.31086e-05, acc 1
2017-08-08T17:54:50.516082: step 17840, loss 0.000119992, acc 1
2017-08-08T17:54:50.697868: step 17841, loss 3.59488e-07, acc 1
2017-08-08T17:54:50.873766: step 17842, loss 1.3411e-07, acc 1
2017-08-08T17:54:51.055506: step 17843, loss 4.09781e-08, acc 1
2017-08-08T17:54:51.380943: step 17844, loss 2.90002e-06, acc 1
2017-08-08T17:54:51.601362: step 17845, loss 0.000143806, acc 1
2017-08-08T17:54:51.839703: step 17846, loss 1.64277e-06, acc 1
2017-08-08T17:54:52.009013: step 17847, loss 3.20373e-07, acc 1
2017-08-08T17:54:52.264530: step 17848, loss 0.000281173, acc 1
2017-08-08T17:54:52.461143: step 17849, loss 8.60521e-07, acc 1
2017-08-08T17:54:52.663036: step 17850, loss 0.000296148, acc 1
2017-08-08T17:54:52.874270: step 17851, loss 5.22984e-06, acc 1
2017-08-08T17:54:53.260334: step 17852, loss 2.23517e-08, acc 1
2017-08-08T17:54:53.617520: step 17853, loss 0, acc 1
2017-08-08T17:54:53.799834: step 17854, loss 0.000822021, acc 1
2017-08-08T17:54:53.986847: step 17855, loss 1.2666e-07, acc 1
2017-08-08T17:54:54.288990: step 17856, loss 1.39698e-07, acc 1
2017-08-08T17:54:54.469036: step 17857, loss 6.0678e-06, acc 1
2017-08-08T17:54:54.662043: step 17858, loss 3.72529e-08, acc 1
2017-08-08T17:54:54.986606: step 17859, loss 7.82309e-08, acc 1
2017-08-08T17:54:55.286315: step 17860, loss 6.70551e-08, acc 1
2017-08-08T17:54:55.570224: step 17861, loss 1.95883e-05, acc 1
2017-08-08T17:54:55.866828: step 17862, loss 9.05177e-06, acc 1
2017-08-08T17:54:56.068112: step 17863, loss 2.29124e-05, acc 1
2017-08-08T17:54:56.385871: step 17864, loss 1.37284e-05, acc 1
2017-08-08T17:54:56.663938: step 17865, loss 0.00022239, acc 1
2017-08-08T17:54:56.939047: step 17866, loss 8.75442e-08, acc 1
2017-08-08T17:54:57.165825: step 17867, loss 0.00113809, acc 1
2017-08-08T17:54:57.371322: step 17868, loss 1.40586e-05, acc 1
2017-08-08T17:54:57.687752: step 17869, loss 7.11516e-07, acc 1
2017-08-08T17:54:57.997376: step 17870, loss 0.000134742, acc 1
2017-08-08T17:54:58.272029: step 17871, loss 3.16649e-08, acc 1
2017-08-08T17:54:58.503846: step 17872, loss 4.12149e-06, acc 1
2017-08-08T17:54:58.728388: step 17873, loss 4.51763e-05, acc 1
2017-08-08T17:54:59.034920: step 17874, loss 1.67638e-08, acc 1
2017-08-08T17:54:59.250007: step 17875, loss 1.98919e-06, acc 1
2017-08-08T17:54:59.455265: step 17876, loss 1.84401e-07, acc 1
2017-08-08T17:54:59.729362: step 17877, loss 0.000249399, acc 1
2017-08-08T17:55:00.018911: step 17878, loss 1.86265e-09, acc 1
2017-08-08T17:55:00.317510: step 17879, loss 1.775e-06, acc 1
2017-08-08T17:55:00.518904: step 17880, loss 0.000170013, acc 1
2017-08-08T17:55:00.733419: step 17881, loss 0.0565152, acc 0.96875
2017-08-08T17:55:01.116282: step 17882, loss 0.11569, acc 0.984375
2017-08-08T17:55:01.358669: step 17883, loss 5.13978e-05, acc 1
2017-08-08T17:55:01.677968: step 17884, loss 2.14631e-05, acc 1
2017-08-08T17:55:02.063041: step 17885, loss 4.59825e-06, acc 1
2017-08-08T17:55:02.785444: step 17886, loss 1.86265e-09, acc 1
2017-08-08T17:55:03.173608: step 17887, loss 1.54974e-05, acc 1
2017-08-08T17:55:03.535990: step 17888, loss 1.2566e-05, acc 1
2017-08-08T17:55:03.825903: step 17889, loss 5.58793e-09, acc 1
2017-08-08T17:55:04.125631: step 17890, loss 6.1798e-06, acc 1
2017-08-08T17:55:04.617973: step 17891, loss 2.21276e-06, acc 1
2017-08-08T17:55:04.862299: step 17892, loss 8.39673e-05, acc 1
2017-08-08T17:55:05.129400: step 17893, loss 0.000196712, acc 1
2017-08-08T17:55:05.495842: step 17894, loss 0.0493859, acc 0.984375
2017-08-08T17:55:05.901380: step 17895, loss 0.00145147, acc 1
2017-08-08T17:55:06.352803: step 17896, loss 0.000177405, acc 1
2017-08-08T17:55:06.650357: step 17897, loss 0.000411111, acc 1
2017-08-08T17:55:06.884741: step 17898, loss 2.26115e-06, acc 1
2017-08-08T17:55:07.309413: step 17899, loss 2.57214e-06, acc 1
2017-08-08T17:55:07.628540: step 17900, loss 2.03077e-05, acc 1

Evaluation:
2017-08-08T17:55:08.322923: step 17900, loss 6.25472, acc 0.706379

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-17900

2017-08-08T17:55:09.117207: step 17901, loss 1.40439e-06, acc 1
2017-08-08T17:55:09.471104: step 17902, loss 1.94955e-05, acc 1
2017-08-08T17:55:09.709912: step 17903, loss 2.04891e-08, acc 1
2017-08-08T17:55:09.965508: step 17904, loss 0.000442561, acc 1
2017-08-08T17:55:10.305336: step 17905, loss 2.98023e-08, acc 1
2017-08-08T17:55:10.540537: step 17906, loss 6.6123e-07, acc 1
2017-08-08T17:55:10.808100: step 17907, loss 1.67638e-08, acc 1
2017-08-08T17:55:11.081483: step 17908, loss 0.0224439, acc 0.984375
2017-08-08T17:55:11.397392: step 17909, loss 3.53045e-05, acc 1
2017-08-08T17:55:11.776962: step 17910, loss 0.00135921, acc 1
2017-08-08T17:55:12.123869: step 17911, loss 1.39698e-07, acc 1
2017-08-08T17:55:12.411748: step 17912, loss 1.13433e-06, acc 1
2017-08-08T17:55:12.835873: step 17913, loss 3.72529e-09, acc 1
2017-08-08T17:55:13.208007: step 17914, loss 0.00676314, acc 1
2017-08-08T17:55:13.472116: step 17915, loss 2.89428e-06, acc 1
2017-08-08T17:55:13.749430: step 17916, loss 8.70547e-06, acc 1
2017-08-08T17:55:14.025384: step 17917, loss 2.23516e-07, acc 1
2017-08-08T17:55:14.434737: step 17918, loss 7.66775e-05, acc 1
2017-08-08T17:55:14.799022: step 17919, loss 8.83772e-05, acc 1
2017-08-08T17:55:15.095536: step 17920, loss 2.03027e-07, acc 1
2017-08-08T17:55:15.326397: step 17921, loss 0.000143838, acc 1
2017-08-08T17:55:15.659261: step 17922, loss 1.08388e-05, acc 1
2017-08-08T17:55:16.069903: step 17923, loss 2.19782e-06, acc 1
2017-08-08T17:55:16.319349: step 17924, loss 7.63683e-08, acc 1
2017-08-08T17:55:16.614825: step 17925, loss 6.33299e-08, acc 1
2017-08-08T17:55:17.059065: step 17926, loss 8.03155e-06, acc 1
2017-08-08T17:55:17.469987: step 17927, loss 6.12876e-06, acc 1
2017-08-08T17:55:17.868452: step 17928, loss 1.29401e-05, acc 1
2017-08-08T17:55:18.138050: step 17929, loss 3.22236e-07, acc 1
2017-08-08T17:55:18.384520: step 17930, loss 1.93867e-05, acc 1
2017-08-08T17:55:18.769559: step 17931, loss 1.98925e-06, acc 1
2017-08-08T17:55:19.087060: step 17932, loss 0.000227417, acc 1
2017-08-08T17:55:19.389816: step 17933, loss 1.82237e-05, acc 1
2017-08-08T17:55:19.670021: step 17934, loss 3.03769e-06, acc 1
2017-08-08T17:55:20.110058: step 17935, loss 9.61664e-06, acc 1
2017-08-08T17:55:20.556054: step 17936, loss 1.86067e-06, acc 1
2017-08-08T17:55:20.975272: step 17937, loss 2.02672e-05, acc 1
2017-08-08T17:55:21.238556: step 17938, loss 4.16824e-05, acc 1
2017-08-08T17:55:21.439238: step 17939, loss 0.104615, acc 0.984375
2017-08-08T17:55:21.760622: step 17940, loss 5.75547e-07, acc 1
2017-08-08T17:55:21.952475: step 17941, loss 1.67451e-05, acc 1
2017-08-08T17:55:22.156926: step 17942, loss 1.06484e-05, acc 1
2017-08-08T17:55:22.387298: step 17943, loss 0.00103644, acc 1
2017-08-08T17:55:22.674560: step 17944, loss 1.52907e-05, acc 1
2017-08-08T17:55:23.077390: step 17945, loss 0.000940424, acc 1
2017-08-08T17:55:23.503276: step 17946, loss 0.000375254, acc 1
2017-08-08T17:55:23.707400: step 17947, loss 0.00596385, acc 1
2017-08-08T17:55:24.102576: step 17948, loss 2.72068e-05, acc 1
2017-08-08T17:55:24.383623: step 17949, loss 1.19672e-05, acc 1
2017-08-08T17:55:24.606946: step 17950, loss 0.00326173, acc 1
2017-08-08T17:55:24.861780: step 17951, loss 0.0245661, acc 0.984375
2017-08-08T17:55:25.290825: step 17952, loss 2.44733e-06, acc 1
2017-08-08T17:55:25.663097: step 17953, loss 0.00202709, acc 1
2017-08-08T17:55:25.934916: step 17954, loss 0.00973832, acc 1
2017-08-08T17:55:26.169952: step 17955, loss 1.41186e-06, acc 1
2017-08-08T17:55:26.380465: step 17956, loss 3.66939e-07, acc 1
2017-08-08T17:55:26.663930: step 17957, loss 2.10478e-07, acc 1
2017-08-08T17:55:26.905302: step 17958, loss 1.1121e-05, acc 1
2017-08-08T17:55:27.133370: step 17959, loss 0.00030857, acc 1
2017-08-08T17:55:27.317329: step 17960, loss 1.03678e-05, acc 1
2017-08-08T17:55:27.657761: step 17961, loss 3.05473e-07, acc 1
2017-08-08T17:55:27.978611: step 17962, loss 9.98209e-05, acc 1
2017-08-08T17:55:28.337906: step 17963, loss 3.72529e-09, acc 1
2017-08-08T17:55:28.558865: step 17964, loss 4.67489e-06, acc 1
2017-08-08T17:55:28.801980: step 17965, loss 0.000202135, acc 1
2017-08-08T17:55:29.105516: step 17966, loss 8.95305e-06, acc 1
2017-08-08T17:55:29.365882: step 17967, loss 3.74389e-07, acc 1
2017-08-08T17:55:29.605153: step 17968, loss 1.67638e-08, acc 1
2017-08-08T17:55:29.876895: step 17969, loss 1.39322e-06, acc 1
2017-08-08T17:55:30.207587: step 17970, loss 2.86846e-07, acc 1
2017-08-08T17:55:30.673366: step 17971, loss 3.48492e-05, acc 1
2017-08-08T17:55:31.072188: step 17972, loss 1.32247e-07, acc 1
2017-08-08T17:55:31.405201: step 17973, loss 2.29105e-07, acc 1
2017-08-08T17:55:31.644065: step 17974, loss 2.84982e-07, acc 1
2017-08-08T17:55:32.057360: step 17975, loss 0.00313076, acc 1
2017-08-08T17:55:32.436018: step 17976, loss 3.08708e-05, acc 1
2017-08-08T17:55:32.693239: step 17977, loss 0.00677484, acc 1
2017-08-08T17:55:32.970868: step 17978, loss 2.4773e-07, acc 1
2017-08-08T17:55:33.286635: step 17979, loss 0.11888, acc 0.984375
2017-08-08T17:55:33.647000: step 17980, loss 1.86264e-07, acc 1
2017-08-08T17:55:33.940902: step 17981, loss 0.00110715, acc 1
2017-08-08T17:55:34.169921: step 17982, loss 0.00470814, acc 1
2017-08-08T17:55:34.394869: step 17983, loss 4.24587e-05, acc 1
2017-08-08T17:55:34.656636: step 17984, loss 3.5349e-06, acc 1
2017-08-08T17:55:34.902069: step 17985, loss 0.000492858, acc 1
2017-08-08T17:55:35.120480: step 17986, loss 5.06744e-06, acc 1
2017-08-08T17:55:35.321375: step 17987, loss 0.000150907, acc 1
2017-08-08T17:55:35.553604: step 17988, loss 0.00849011, acc 1
2017-08-08T17:55:35.927991: step 17989, loss 5.85878e-06, acc 1
2017-08-08T17:55:36.324396: step 17990, loss 0.00339391, acc 1
2017-08-08T17:55:36.663428: step 17991, loss 3.83291e-06, acc 1
2017-08-08T17:55:36.946038: step 17992, loss 3.72529e-09, acc 1
2017-08-08T17:55:37.139311: step 17993, loss 2.22757e-06, acc 1
2017-08-08T17:55:37.561424: step 17994, loss 1.32247e-07, acc 1
2017-08-08T17:55:37.825629: step 17995, loss 6.87309e-07, acc 1
2017-08-08T17:55:38.111872: step 17996, loss 0.00407409, acc 1
2017-08-08T17:55:38.396189: step 17997, loss 9.31322e-09, acc 1
2017-08-08T17:55:38.853144: step 17998, loss 0.0395109, acc 0.984375
2017-08-08T17:55:39.313193: step 17999, loss 0.000294901, acc 1
2017-08-08T17:55:39.652527: step 18000, loss 1.6192e-06, acc 1

Evaluation:
2017-08-08T17:55:40.327537: step 18000, loss 6.04531, acc 0.717636

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-18000

2017-08-08T17:55:40.834980: step 18001, loss 5.40166e-08, acc 1
2017-08-08T17:55:41.042363: step 18002, loss 0.00592145, acc 1
2017-08-08T17:55:41.259863: step 18003, loss 2.06752e-07, acc 1
2017-08-08T17:55:41.697454: step 18004, loss 1.12687e-06, acc 1
2017-08-08T17:55:42.016385: step 18005, loss 0.000186743, acc 1
2017-08-08T17:55:42.285988: step 18006, loss 0.0721222, acc 0.984375
2017-08-08T17:55:42.527779: step 18007, loss 8.9777e-07, acc 1
2017-08-08T17:55:42.759669: step 18008, loss 0, acc 1
2017-08-08T17:55:42.982609: step 18009, loss 0.00389403, acc 1
2017-08-08T17:55:43.200305: step 18010, loss 1.67638e-08, acc 1
2017-08-08T17:55:43.401771: step 18011, loss 1.63848e-05, acc 1
2017-08-08T17:55:43.669389: step 18012, loss 1.56462e-07, acc 1
2017-08-08T17:55:43.985355: step 18013, loss 0.0150036, acc 0.984375
2017-08-08T17:55:44.348115: step 18014, loss 0.0159052, acc 0.984375
2017-08-08T17:55:44.703595: step 18015, loss 9.26166e-05, acc 1
2017-08-08T17:55:44.994728: step 18016, loss 0.000103224, acc 1
2017-08-08T17:55:45.244867: step 18017, loss 2.42039e-05, acc 1
2017-08-08T17:55:45.620504: step 18018, loss 1.30385e-08, acc 1
2017-08-08T17:55:45.893687: step 18019, loss 1.30791e-05, acc 1
2017-08-08T17:55:46.196419: step 18020, loss 0, acc 1
2017-08-08T17:55:46.569387: step 18021, loss 0.000145504, acc 1
2017-08-08T17:55:46.982695: step 18022, loss 9.87177e-07, acc 1
2017-08-08T17:55:47.260927: step 18023, loss 1.86265e-09, acc 1
2017-08-08T17:55:47.521454: step 18024, loss 0.000256871, acc 1
2017-08-08T17:55:47.756044: step 18025, loss 3.33412e-07, acc 1
2017-08-08T17:55:48.089668: step 18026, loss 0.000155665, acc 1
2017-08-08T17:55:48.340930: step 18027, loss 1.05423e-06, acc 1
2017-08-08T17:55:48.607273: step 18028, loss 4.84287e-08, acc 1
2017-08-08T17:55:48.904376: step 18029, loss 0.0267609, acc 0.984375
2017-08-08T17:55:49.346169: step 18030, loss 0.0004299, acc 1
2017-08-08T17:55:49.708709: step 18031, loss 2.87939e-06, acc 1
2017-08-08T17:55:50.085304: step 18032, loss 8.53075e-07, acc 1
2017-08-08T17:55:50.362315: step 18033, loss 2.68494e-05, acc 1
2017-08-08T17:55:50.605602: step 18034, loss 3.40838e-06, acc 1
2017-08-08T17:55:51.067777: step 18035, loss 0.0198154, acc 0.984375
2017-08-08T17:55:51.301812: step 18036, loss 3.38999e-07, acc 1
2017-08-08T17:55:51.518209: step 18037, loss 9.2199e-07, acc 1
2017-08-08T17:55:51.740249: step 18038, loss 2.47345e-06, acc 1
2017-08-08T17:55:52.057450: step 18039, loss 1.09169e-05, acc 1
2017-08-08T17:55:52.341381: step 18040, loss 5.17809e-07, acc 1
2017-08-08T17:55:52.689515: step 18041, loss 1.13564e-05, acc 1
2017-08-08T17:55:52.945931: step 18042, loss 7.39456e-07, acc 1
2017-08-08T17:55:53.166829: step 18043, loss 0.000126161, acc 1
2017-08-08T17:55:53.569054: step 18044, loss 3.1665e-08, acc 1
2017-08-08T17:55:53.837379: step 18045, loss 3.72857e-06, acc 1
2017-08-08T17:55:54.121712: step 18046, loss 1.27027e-06, acc 1
2017-08-08T17:55:54.433927: step 18047, loss 3.34436e-05, acc 1
2017-08-08T17:55:54.761667: step 18048, loss 0.000548611, acc 1
2017-08-08T17:55:55.048938: step 18049, loss 7.06992e-06, acc 1
2017-08-08T17:55:55.266961: step 18050, loss 0.00166585, acc 1
2017-08-08T17:55:55.490550: step 18051, loss 0.00707297, acc 1
2017-08-08T17:55:55.829365: step 18052, loss 7.45041e-07, acc 1
2017-08-08T17:55:56.144153: step 18053, loss 3.76395e-06, acc 1
2017-08-08T17:55:56.433504: step 18054, loss 0.000568681, acc 1
2017-08-08T17:55:56.678709: step 18055, loss 0.000243287, acc 1
2017-08-08T17:55:57.032848: step 18056, loss 1.27492e-05, acc 1
2017-08-08T17:55:57.444828: step 18057, loss 4.915e-06, acc 1
2017-08-08T17:55:57.905739: step 18058, loss 8.30727e-07, acc 1
2017-08-08T17:55:58.252687: step 18059, loss 6.65942e-06, acc 1
2017-08-08T17:55:58.511738: step 18060, loss 1.15853e-06, acc 1
2017-08-08T17:55:58.912694: step 18061, loss 1.30385e-07, acc 1
2017-08-08T17:55:59.103420: step 18062, loss 8.56937e-05, acc 1
2017-08-08T17:55:59.329143: step 18063, loss 1.33734e-06, acc 1
2017-08-08T17:55:59.538671: step 18064, loss 1.45655e-06, acc 1
2017-08-08T17:55:59.915774: step 18065, loss 0.000134638, acc 1
2017-08-08T17:56:00.297338: step 18066, loss 1.86265e-09, acc 1
2017-08-08T17:56:00.555334: step 18067, loss 0.000209637, acc 1
2017-08-08T17:56:00.737713: step 18068, loss 0.000148395, acc 1
2017-08-08T17:56:00.969492: step 18069, loss 7.00841e-06, acc 1
2017-08-08T17:56:01.298340: step 18070, loss 3.70086e-06, acc 1
2017-08-08T17:56:01.507193: step 18071, loss 0.000110166, acc 1
2017-08-08T17:56:01.780779: step 18072, loss 1.26659e-07, acc 1
2017-08-08T17:56:02.079394: step 18073, loss 2.12773e-05, acc 1
2017-08-08T17:56:02.489553: step 18074, loss 2.03578e-06, acc 1
2017-08-08T17:56:02.922550: step 18075, loss 1.06171e-07, acc 1
2017-08-08T17:56:03.258061: step 18076, loss 3.1665e-08, acc 1
2017-08-08T17:56:03.612406: step 18077, loss 2.67833e-06, acc 1
2017-08-08T17:56:03.864921: step 18078, loss 1.80855e-06, acc 1
2017-08-08T17:56:04.172838: step 18079, loss 2.14849e-05, acc 1
2017-08-08T17:56:04.579943: step 18080, loss 2.49578e-06, acc 1
2017-08-08T17:56:04.846248: step 18081, loss 1.3959e-05, acc 1
2017-08-08T17:56:05.227164: step 18082, loss 1.84798e-05, acc 1
2017-08-08T17:56:05.483653: step 18083, loss 1.85886e-06, acc 1
2017-08-08T17:56:05.752129: step 18084, loss 5.29514e-05, acc 1
2017-08-08T17:56:06.122367: step 18085, loss 0.000149147, acc 1
2017-08-08T17:56:06.303303: step 18086, loss 1.98674e-05, acc 1
2017-08-08T17:56:06.613376: step 18087, loss 3.01746e-07, acc 1
2017-08-08T17:56:06.848045: step 18088, loss 3.20144e-05, acc 1
2017-08-08T17:56:07.230902: step 18089, loss 6.63096e-07, acc 1
2017-08-08T17:56:07.486782: step 18090, loss 1.95577e-07, acc 1
2017-08-08T17:56:07.854036: step 18091, loss 0.000393131, acc 1
2017-08-08T17:56:08.216725: step 18092, loss 0.00185871, acc 1
2017-08-08T17:56:08.489646: step 18093, loss 1.00264e-05, acc 1
2017-08-08T17:56:08.923188: step 18094, loss 2.32815e-06, acc 1
2017-08-08T17:56:09.157493: step 18095, loss 2.65416e-06, acc 1
2017-08-08T17:56:09.480895: step 18096, loss 7.45058e-09, acc 1
2017-08-08T17:56:09.668441: step 18097, loss 5.58793e-08, acc 1
2017-08-08T17:56:09.982359: step 18098, loss 1.2293e-06, acc 1
2017-08-08T17:56:10.203585: step 18099, loss 0.000108615, acc 1
2017-08-08T17:56:10.416624: step 18100, loss 5.44356e-06, acc 1

Evaluation:
2017-08-08T17:56:10.959716: step 18100, loss 6.17068, acc 0.714822

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-18100

2017-08-08T17:56:11.612153: step 18101, loss 0.000105082, acc 1
2017-08-08T17:56:11.901673: step 18102, loss 0, acc 1
2017-08-08T17:56:12.254400: step 18103, loss 5.66904e-06, acc 1
2017-08-08T17:56:12.597962: step 18104, loss 0.0142767, acc 0.984375
2017-08-08T17:56:12.878670: step 18105, loss 2.01164e-07, acc 1
2017-08-08T17:56:13.311980: step 18106, loss 0.0256394, acc 0.984375
2017-08-08T17:56:13.612426: step 18107, loss 4.79559e-06, acc 1
2017-08-08T17:56:13.969548: step 18108, loss 0.000237821, acc 1
2017-08-08T17:56:14.296973: step 18109, loss 8.1268e-06, acc 1
2017-08-08T17:56:14.501842: step 18110, loss 0.000146663, acc 1
2017-08-08T17:56:14.896329: step 18111, loss 1.24797e-07, acc 1
2017-08-08T17:56:15.165377: step 18112, loss 5.67586e-05, acc 1
2017-08-08T17:56:15.445965: step 18113, loss 0.0985383, acc 0.984375
2017-08-08T17:56:15.763349: step 18114, loss 2.52935e-06, acc 1
2017-08-08T17:56:15.998353: step 18115, loss 1.04998e-05, acc 1
2017-08-08T17:56:16.256151: step 18116, loss 0.000249579, acc 1
2017-08-08T17:56:16.602715: step 18117, loss 3.72529e-09, acc 1
2017-08-08T17:56:16.919298: step 18118, loss 0.000273286, acc 1
2017-08-08T17:56:17.178606: step 18119, loss 3.43438e-06, acc 1
2017-08-08T17:56:17.444497: step 18120, loss 3.53902e-08, acc 1
2017-08-08T17:56:17.743432: step 18121, loss 0.00234943, acc 1
2017-08-08T17:56:18.030376: step 18122, loss 2.30966e-07, acc 1
2017-08-08T17:56:18.210931: step 18123, loss 3.23704e-05, acc 1
2017-08-08T17:56:18.426831: step 18124, loss 4.13502e-07, acc 1
2017-08-08T17:56:18.642579: step 18125, loss 7.65463e-06, acc 1
2017-08-08T17:56:18.919433: step 18126, loss 4.84287e-08, acc 1
2017-08-08T17:56:19.255543: step 18127, loss 2.04891e-08, acc 1
2017-08-08T17:56:19.569606: step 18128, loss 4.77889e-06, acc 1
2017-08-08T17:56:19.775320: step 18129, loss 1.73405e-06, acc 1
2017-08-08T17:56:19.968594: step 18130, loss 0.000626909, acc 1
2017-08-08T17:56:20.368363: step 18131, loss 3.09925e-06, acc 1
2017-08-08T17:56:20.609174: step 18132, loss 1.11759e-08, acc 1
2017-08-08T17:56:20.889579: step 18133, loss 5.2154e-08, acc 1
2017-08-08T17:56:21.141407: step 18134, loss 3.72529e-09, acc 1
2017-08-08T17:56:21.576346: step 18135, loss 5.77419e-08, acc 1
2017-08-08T17:56:22.000581: step 18136, loss 0.00128638, acc 1
2017-08-08T17:56:22.256169: step 18137, loss 1.86265e-09, acc 1
2017-08-08T17:56:22.465016: step 18138, loss 2.79396e-07, acc 1
2017-08-08T17:56:22.817780: step 18139, loss 0.000183568, acc 1
2017-08-08T17:56:23.146935: step 18140, loss 8.90599e-06, acc 1
2017-08-08T17:56:23.417018: step 18141, loss 5.56459e-06, acc 1
2017-08-08T17:56:23.688894: step 18142, loss 1.49966e-05, acc 1
2017-08-08T17:56:24.118321: step 18143, loss 7.80714e-06, acc 1
2017-08-08T17:56:24.453063: step 18144, loss 4.27232e-06, acc 1
2017-08-08T17:56:24.765112: step 18145, loss 1.86265e-09, acc 1
2017-08-08T17:56:24.961384: step 18146, loss 7.8231e-08, acc 1
2017-08-08T17:56:25.314788: step 18147, loss 1.50874e-07, acc 1
2017-08-08T17:56:25.692913: step 18148, loss 1.21701e-05, acc 1
2017-08-08T17:56:25.954440: step 18149, loss 5.82997e-07, acc 1
2017-08-08T17:56:26.208252: step 18150, loss 0.000530396, acc 1
2017-08-08T17:56:26.502818: step 18151, loss 1.47517e-06, acc 1
2017-08-08T17:56:26.975167: step 18152, loss 1.14027e-05, acc 1
2017-08-08T17:56:27.305362: step 18153, loss 0.000295294, acc 1
2017-08-08T17:56:27.624798: step 18154, loss 1.86265e-09, acc 1
2017-08-08T17:56:27.876858: step 18155, loss 2.36922e-06, acc 1
2017-08-08T17:56:28.110758: step 18156, loss 1.76951e-07, acc 1
2017-08-08T17:56:28.461773: step 18157, loss 5.78295e-06, acc 1
2017-08-08T17:56:28.819070: step 18158, loss 5.43662e-06, acc 1
2017-08-08T17:56:29.103622: step 18159, loss 3.1961e-05, acc 1
2017-08-08T17:56:29.418057: step 18160, loss 7.45058e-09, acc 1
2017-08-08T17:56:29.755900: step 18161, loss 0.000127243, acc 1
2017-08-08T17:56:30.238465: step 18162, loss 0.000193194, acc 1
2017-08-08T17:56:30.644731: step 18163, loss 0.000193214, acc 1
2017-08-08T17:56:30.916904: step 18164, loss 2.42144e-08, acc 1
2017-08-08T17:56:31.238138: step 18165, loss 4.84287e-08, acc 1
2017-08-08T17:56:31.623266: step 18166, loss 1.02889e-05, acc 1
2017-08-08T17:56:31.894473: step 18167, loss 5.40159e-07, acc 1
2017-08-08T17:56:32.176040: step 18168, loss 9.76978e-06, acc 1
2017-08-08T17:56:32.470634: step 18169, loss 0.000480814, acc 1
2017-08-08T17:56:32.941364: step 18170, loss 3.91155e-08, acc 1
2017-08-08T17:56:33.341391: step 18171, loss 6.10942e-07, acc 1
2017-08-08T17:56:33.734020: step 18172, loss 0.00376952, acc 1
2017-08-08T17:56:33.933146: step 18173, loss 0.000134552, acc 1
2017-08-08T17:56:34.211802: step 18174, loss 6.10261e-05, acc 1
2017-08-08T17:56:34.611008: step 18175, loss 1.10451e-06, acc 1
2017-08-08T17:56:34.884563: step 18176, loss 0.000561223, acc 1
2017-08-08T17:56:35.209949: step 18177, loss 3.14211e-06, acc 1
2017-08-08T17:56:35.468937: step 18178, loss 0.00037111, acc 1
2017-08-08T17:56:35.837400: step 18179, loss 2.09721e-06, acc 1
2017-08-08T17:56:36.198322: step 18180, loss 0.0012305, acc 1
2017-08-08T17:56:36.547735: step 18181, loss 0.000430062, acc 1
2017-08-08T17:56:36.794976: step 18182, loss 3.72529e-09, acc 1
2017-08-08T17:56:37.037324: step 18183, loss 1.71363e-07, acc 1
2017-08-08T17:56:37.412687: step 18184, loss 1.11759e-08, acc 1
2017-08-08T17:56:37.650433: step 18185, loss 1.03934e-06, acc 1
2017-08-08T17:56:37.932221: step 18186, loss 3.1106e-07, acc 1
2017-08-08T17:56:38.190742: step 18187, loss 1.24606e-06, acc 1
2017-08-08T17:56:38.648508: step 18188, loss 7.02209e-07, acc 1
2017-08-08T17:56:39.017688: step 18189, loss 1.39879e-06, acc 1
2017-08-08T17:56:39.279973: step 18190, loss 4.0228e-06, acc 1
2017-08-08T17:56:39.488873: step 18191, loss 0.15108, acc 0.984375
2017-08-08T17:56:39.764734: step 18192, loss 1.11755e-06, acc 1
2017-08-08T17:56:40.094029: step 18193, loss 3.78071e-06, acc 1
2017-08-08T17:56:40.315769: step 18194, loss 0.000217361, acc 1
2017-08-08T17:56:40.540131: step 18195, loss 0.00370289, acc 1
2017-08-08T17:56:40.825377: step 18196, loss 9.31321e-08, acc 1
2017-08-08T17:56:41.125373: step 18197, loss 0.000305939, acc 1
2017-08-08T17:56:41.417362: step 18198, loss 1.11759e-08, acc 1
2017-08-08T17:56:41.685722: step 18199, loss 6.55638e-07, acc 1
2017-08-08T17:56:41.908722: step 18200, loss 4.35387e-05, acc 1

Evaluation:
2017-08-08T17:56:42.605374: step 18200, loss 6.18169, acc 0.712946

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-18200

2017-08-08T17:56:42.981367: step 18201, loss 1.56087e-06, acc 1
2017-08-08T17:56:43.253436: step 18202, loss 2.91291e-06, acc 1
2017-08-08T17:56:43.607021: step 18203, loss 8.99634e-07, acc 1
2017-08-08T17:56:43.983603: step 18204, loss 9.68573e-08, acc 1
2017-08-08T17:56:44.285485: step 18205, loss 3.72529e-09, acc 1
2017-08-08T17:56:44.509156: step 18206, loss 8.84995e-06, acc 1
2017-08-08T17:56:44.915673: step 18207, loss 0.000461084, acc 1
2017-08-08T17:56:45.194633: step 18208, loss 1.40997e-06, acc 1
2017-08-08T17:56:45.445217: step 18209, loss 0.00055966, acc 1
2017-08-08T17:56:45.722364: step 18210, loss 0.0264766, acc 0.984375
2017-08-08T17:56:46.052191: step 18211, loss 1.15484e-07, acc 1
2017-08-08T17:56:46.506054: step 18212, loss 1.68653e-05, acc 1
2017-08-08T17:56:46.858766: step 18213, loss 3.35274e-07, acc 1
2017-08-08T17:56:47.231762: step 18214, loss 6.16714e-05, acc 1
2017-08-08T17:56:47.490196: step 18215, loss 3.82736e-06, acc 1
2017-08-08T17:56:47.790504: step 18216, loss 3.07333e-07, acc 1
2017-08-08T17:56:48.121693: step 18217, loss 0.00872593, acc 1
2017-08-08T17:56:48.410336: step 18218, loss 1.05795e-06, acc 1
2017-08-08T17:56:48.687437: step 18219, loss 0.000321383, acc 1
2017-08-08T17:56:48.959373: step 18220, loss 2.96159e-07, acc 1
2017-08-08T17:56:49.310076: step 18221, loss 9.52826e-06, acc 1
2017-08-08T17:56:49.664111: step 18222, loss 4.2502e-06, acc 1
2017-08-08T17:56:49.947729: step 18223, loss 3.44013e-06, acc 1
2017-08-08T17:56:50.238188: step 18224, loss 1.67638e-08, acc 1
2017-08-08T17:56:50.491127: step 18225, loss 0.000884479, acc 1
2017-08-08T17:56:50.928980: step 18226, loss 3.72529e-08, acc 1
2017-08-08T17:56:51.240669: step 18227, loss 8.15817e-07, acc 1
2017-08-08T17:56:51.555883: step 18228, loss 3.91155e-08, acc 1
2017-08-08T17:56:51.773158: step 18229, loss 2.96159e-07, acc 1
2017-08-08T17:56:52.069365: step 18230, loss 0.000122475, acc 1
2017-08-08T17:56:52.441204: step 18231, loss 4.13504e-07, acc 1
2017-08-08T17:56:52.813551: step 18232, loss 1.464e-06, acc 1
2017-08-08T17:56:53.037868: step 18233, loss 3.92597e-06, acc 1
2017-08-08T17:56:53.211300: step 18234, loss 1.58243e-05, acc 1
2017-08-08T17:56:53.455269: step 18235, loss 1.76884e-05, acc 1
2017-08-08T17:56:53.791598: step 18236, loss 0.000285103, acc 1
2017-08-08T17:56:54.035618: step 18237, loss 1.20136e-06, acc 1
2017-08-08T17:56:54.286620: step 18238, loss 7.18261e-06, acc 1
2017-08-08T17:56:54.606230: step 18239, loss 1.65572e-05, acc 1
2017-08-08T17:56:54.982492: step 18240, loss 0.000456224, acc 1
2017-08-08T17:56:55.422280: step 18241, loss 6.17035e-06, acc 1
2017-08-08T17:56:55.759946: step 18242, loss 0.00805484, acc 1
2017-08-08T17:56:56.009091: step 18243, loss 2.04891e-08, acc 1
2017-08-08T17:56:56.277094: step 18244, loss 4.1164e-07, acc 1
2017-08-08T17:56:56.642425: step 18245, loss 4.28408e-08, acc 1
2017-08-08T17:56:56.859582: step 18246, loss 4.84287e-08, acc 1
2017-08-08T17:56:57.130313: step 18247, loss 0.00073201, acc 1
2017-08-08T17:56:57.398863: step 18248, loss 2.71178e-06, acc 1
2017-08-08T17:56:57.813353: step 18249, loss 5.94174e-07, acc 1
2017-08-08T17:56:58.237530: step 18250, loss 5.90449e-07, acc 1
2017-08-08T17:56:58.597228: step 18251, loss 3.88923e-05, acc 1
2017-08-08T17:56:58.857792: step 18252, loss 0.00583762, acc 1
2017-08-08T17:56:59.092518: step 18253, loss 0.0219106, acc 0.984375
2017-08-08T17:56:59.459421: step 18254, loss 0.000118029, acc 1
2017-08-08T17:56:59.700926: step 18255, loss 2.66356e-07, acc 1
2017-08-08T17:56:59.901832: step 18256, loss 3.63e-06, acc 1
2017-08-08T17:57:00.149275: step 18257, loss 5.58794e-09, acc 1
2017-08-08T17:57:00.456766: step 18258, loss 3.72529e-09, acc 1
2017-08-08T17:57:00.766688: step 18259, loss 0.044851, acc 0.984375
2017-08-08T17:57:01.083633: step 18260, loss 5.51073e-06, acc 1
2017-08-08T17:57:01.402527: step 18261, loss 9.23064e-05, acc 1
2017-08-08T17:57:01.617467: step 18262, loss 5.84862e-07, acc 1
2017-08-08T17:57:01.968867: step 18263, loss 2.93433e-05, acc 1
2017-08-08T17:57:02.380729: step 18264, loss 8.6052e-07, acc 1
2017-08-08T17:57:02.706638: step 18265, loss 1.14176e-06, acc 1
2017-08-08T17:57:02.946430: step 18266, loss 4.75133e-05, acc 1
2017-08-08T17:57:03.223134: step 18267, loss 1.89989e-07, acc 1
2017-08-08T17:57:03.669686: step 18268, loss 9.12694e-08, acc 1
2017-08-08T17:57:04.029571: step 18269, loss 0, acc 1
2017-08-08T17:57:04.297473: step 18270, loss 2.42918e-05, acc 1
2017-08-08T17:57:04.572287: step 18271, loss 2.59775e-05, acc 1
2017-08-08T17:57:05.023020: step 18272, loss 0.0450236, acc 0.96875
2017-08-08T17:57:05.268285: step 18273, loss 1.41561e-07, acc 1
2017-08-08T17:57:05.527082: step 18274, loss 6.29075e-06, acc 1
2017-08-08T17:57:05.805009: step 18275, loss 0.000152886, acc 1
2017-08-08T17:57:06.259296: step 18276, loss 1.41443e-05, acc 1
2017-08-08T17:57:06.629157: step 18277, loss 3.82008e-06, acc 1
2017-08-08T17:57:06.928145: step 18278, loss 4.60937e-06, acc 1
2017-08-08T17:57:07.132206: step 18279, loss 0.000751512, acc 1
2017-08-08T17:57:07.384434: step 18280, loss 0, acc 1
2017-08-08T17:57:07.768152: step 18281, loss 0.000277287, acc 1
2017-08-08T17:57:08.008091: step 18282, loss 1.2666e-07, acc 1
2017-08-08T17:57:08.267692: step 18283, loss 1.91851e-07, acc 1
2017-08-08T17:57:08.482750: step 18284, loss 0.000111388, acc 1
2017-08-08T17:57:08.967337: step 18285, loss 1.11759e-08, acc 1
2017-08-08T17:57:09.333315: step 18286, loss 2.44005e-07, acc 1
2017-08-08T17:57:09.571576: step 18287, loss 9.0834e-06, acc 1
2017-08-08T17:57:09.771033: step 18288, loss 1.75088e-07, acc 1
2017-08-08T17:57:10.095017: step 18289, loss 8.00919e-07, acc 1
2017-08-08T17:57:10.390823: step 18290, loss 4.35856e-07, acc 1
2017-08-08T17:57:10.610875: step 18291, loss 1.25511e-05, acc 1
2017-08-08T17:57:10.867153: step 18292, loss 0.0463412, acc 0.984375
2017-08-08T17:57:11.165442: step 18293, loss 5.90454e-07, acc 1
2017-08-08T17:57:11.517311: step 18294, loss 1.18792e-05, acc 1
2017-08-08T17:57:11.844573: step 18295, loss 3.53902e-08, acc 1
2017-08-08T17:57:12.011874: step 18296, loss 4.4703e-07, acc 1
2017-08-08T17:57:12.195970: step 18297, loss 1.87748e-06, acc 1
2017-08-08T17:57:12.523745: step 18298, loss 5.58793e-09, acc 1
2017-08-08T17:57:12.721642: step 18299, loss 5.77419e-08, acc 1
2017-08-08T17:57:12.937278: step 18300, loss 0.000153258, acc 1

Evaluation:
2017-08-08T17:57:13.577357: step 18300, loss 6.20186, acc 0.710131

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-18300

2017-08-08T17:57:14.134362: step 18301, loss 1.86264e-08, acc 1
2017-08-08T17:57:14.440268: step 18302, loss 2.78826e-06, acc 1
2017-08-08T17:57:14.696284: step 18303, loss 5.58793e-09, acc 1
2017-08-08T17:57:15.041444: step 18304, loss 0.000274242, acc 1
2017-08-08T17:57:15.245864: step 18305, loss 0.0017065, acc 1
2017-08-08T17:57:15.488692: step 18306, loss 1.26501e-05, acc 1
2017-08-08T17:57:15.752344: step 18307, loss 9.31322e-09, acc 1
2017-08-08T17:57:16.080039: step 18308, loss 2.39342e-06, acc 1
2017-08-08T17:57:16.508319: step 18309, loss 4.09782e-08, acc 1
2017-08-08T17:57:16.912736: step 18310, loss 2.80535e-05, acc 1
2017-08-08T17:57:17.234364: step 18311, loss 1.0474e-05, acc 1
2017-08-08T17:57:17.444856: step 18312, loss 0, acc 1
2017-08-08T17:57:17.686057: step 18313, loss 0.00675857, acc 1
2017-08-08T17:57:18.019666: step 18314, loss 1.11758e-07, acc 1
2017-08-08T17:57:18.331258: step 18315, loss 3.27795e-06, acc 1
2017-08-08T17:57:18.639151: step 18316, loss 1.37835e-07, acc 1
2017-08-08T17:57:18.909616: step 18317, loss 1.45742e-05, acc 1
2017-08-08T17:57:19.428499: step 18318, loss 3.74387e-07, acc 1
2017-08-08T17:57:19.764539: step 18319, loss 0.000175867, acc 1
2017-08-08T17:57:20.009887: step 18320, loss 2.61695e-05, acc 1
2017-08-08T17:57:20.212934: step 18321, loss 1.28332e-06, acc 1
2017-08-08T17:57:20.453519: step 18322, loss 3.48311e-07, acc 1
2017-08-08T17:57:20.729775: step 18323, loss 4.23351e-06, acc 1
2017-08-08T17:57:20.990489: step 18324, loss 1.17346e-07, acc 1
2017-08-08T17:57:21.283019: step 18325, loss 5.96046e-08, acc 1
2017-08-08T17:57:21.575588: step 18326, loss 4.17227e-07, acc 1
2017-08-08T17:57:21.968456: step 18327, loss 2.45609e-05, acc 1
2017-08-08T17:57:22.361842: step 18328, loss 5.22206e-06, acc 1
2017-08-08T17:57:22.698688: step 18329, loss 4.81133e-05, acc 1
2017-08-08T17:57:22.942693: step 18330, loss 3.95224e-05, acc 1
2017-08-08T17:57:23.288043: step 18331, loss 0.000363369, acc 1
2017-08-08T17:57:23.672272: step 18332, loss 3.94876e-07, acc 1
2017-08-08T17:57:23.887987: step 18333, loss 8.56815e-08, acc 1
2017-08-08T17:57:24.113388: step 18334, loss 6.5005e-07, acc 1
2017-08-08T17:57:24.335692: step 18335, loss 2.41945e-06, acc 1
2017-08-08T17:57:24.733241: step 18336, loss 2.43057e-06, acc 1
2017-08-08T17:57:25.114924: step 18337, loss 2.89987e-06, acc 1
2017-08-08T17:57:25.502717: step 18338, loss 8.49352e-07, acc 1
2017-08-08T17:57:25.792138: step 18339, loss 0.000846932, acc 1
2017-08-08T17:57:26.117600: step 18340, loss 4.3772e-07, acc 1
2017-08-08T17:57:26.482118: step 18341, loss 2.80878e-06, acc 1
2017-08-08T17:57:26.813027: step 18342, loss 6.20256e-07, acc 1
2017-08-08T17:57:27.058590: step 18343, loss 4.49475e-05, acc 1
2017-08-08T17:57:27.359522: step 18344, loss 2.2889e-05, acc 1
2017-08-08T17:57:27.691816: step 18345, loss 1.95384e-06, acc 1
2017-08-08T17:57:28.087438: step 18346, loss 0.000123833, acc 1
2017-08-08T17:57:28.401887: step 18347, loss 2.52192e-06, acc 1
2017-08-08T17:57:28.640791: step 18348, loss 1.80117e-05, acc 1
2017-08-08T17:57:28.928969: step 18349, loss 5.08808e-06, acc 1
2017-08-08T17:57:29.284883: step 18350, loss 1.38736e-05, acc 1
2017-08-08T17:57:29.512936: step 18351, loss 0.0439877, acc 0.984375
2017-08-08T17:57:29.781482: step 18352, loss 7.39463e-07, acc 1
2017-08-08T17:57:30.145355: step 18353, loss 1.80676e-07, acc 1
2017-08-08T17:57:30.509366: step 18354, loss 5.96041e-07, acc 1
2017-08-08T17:57:30.812672: step 18355, loss 0.00045636, acc 1
2017-08-08T17:57:31.054465: step 18356, loss 6.62854e-06, acc 1
2017-08-08T17:57:31.358736: step 18357, loss 1.55896e-06, acc 1
2017-08-08T17:57:31.780592: step 18358, loss 0.00127188, acc 1
2017-08-08T17:57:32.057701: step 18359, loss 8.40039e-07, acc 1
2017-08-08T17:57:32.371089: step 18360, loss 4.68057e-06, acc 1
2017-08-08T17:57:32.649458: step 18361, loss 8.85214e-05, acc 1
2017-08-08T17:57:32.956280: step 18362, loss 9.87199e-08, acc 1
2017-08-08T17:57:33.278211: step 18363, loss 2.6077e-08, acc 1
2017-08-08T17:57:33.656090: step 18364, loss 1.00464e-05, acc 1
2017-08-08T17:57:33.930825: step 18365, loss 0, acc 1
2017-08-08T17:57:34.166161: step 18366, loss 3.26508e-06, acc 1
2017-08-08T17:57:34.596265: step 18367, loss 3.91665e-06, acc 1
2017-08-08T17:57:34.897831: step 18368, loss 4.24629e-06, acc 1
2017-08-08T17:57:35.175959: step 18369, loss 0.000726191, acc 1
2017-08-08T17:57:35.494210: step 18370, loss 0.0108075, acc 1
2017-08-08T17:57:35.876505: step 18371, loss 5.96942e-05, acc 1
2017-08-08T17:57:36.256692: step 18372, loss 4.81663e-05, acc 1
2017-08-08T17:57:36.587466: step 18373, loss 0.000133922, acc 1
2017-08-08T17:57:36.818535: step 18374, loss 7.9403e-06, acc 1
2017-08-08T17:57:37.090793: step 18375, loss 1.52327e-05, acc 1
2017-08-08T17:57:37.488143: step 18376, loss 3.16649e-08, acc 1
2017-08-08T17:57:37.721378: step 18377, loss 4.19996e-06, acc 1
2017-08-08T17:57:37.992227: step 18378, loss 4.54866e-05, acc 1
2017-08-08T17:57:38.277367: step 18379, loss 5.30019e-06, acc 1
2017-08-08T17:57:38.695269: step 18380, loss 1.82538e-07, acc 1
2017-08-08T17:57:39.027472: step 18381, loss 8.56816e-08, acc 1
2017-08-08T17:57:39.384232: step 18382, loss 1.11756e-06, acc 1
2017-08-08T17:57:39.595106: step 18383, loss 0.00699364, acc 1
2017-08-08T17:57:39.874835: step 18384, loss 1.47148e-07, acc 1
2017-08-08T17:57:40.198556: step 18385, loss 9.25249e-05, acc 1
2017-08-08T17:57:40.458881: step 18386, loss 6.85862e-06, acc 1
2017-08-08T17:57:40.769913: step 18387, loss 8.58662e-07, acc 1
2017-08-08T17:57:41.123477: step 18388, loss 3.51805e-05, acc 1
2017-08-08T17:57:41.535797: step 18389, loss 0.000258978, acc 1
2017-08-08T17:57:41.845344: step 18390, loss 1.8495e-06, acc 1
2017-08-08T17:57:42.166498: step 18391, loss 3.8012e-06, acc 1
2017-08-08T17:57:42.469819: step 18392, loss 2.40913e-05, acc 1
2017-08-08T17:57:42.934200: step 18393, loss 7.45058e-09, acc 1
2017-08-08T17:57:43.216400: step 18394, loss 1.86265e-09, acc 1
2017-08-08T17:57:43.526618: step 18395, loss 1.42447e-05, acc 1
2017-08-08T17:57:43.859287: step 18396, loss 1.19277e-05, acc 1
2017-08-08T17:57:44.281367: step 18397, loss 0.0010563, acc 1
2017-08-08T17:57:44.628931: step 18398, loss 3.21087e-06, acc 1
2017-08-08T17:57:45.038281: step 18399, loss 6.89178e-08, acc 1
2017-08-08T17:57:45.324461: step 18400, loss 1.32802e-06, acc 1

Evaluation:
2017-08-08T17:57:45.917892: step 18400, loss 6.23247, acc 0.712008

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-18400

2017-08-08T17:57:46.387948: step 18401, loss 3.52037e-07, acc 1
2017-08-08T17:57:46.813927: step 18402, loss 6.40737e-07, acc 1
2017-08-08T17:57:47.229430: step 18403, loss 0.00618267, acc 1
2017-08-08T17:57:47.505989: step 18404, loss 1.57817e-05, acc 1
2017-08-08T17:57:47.681662: step 18405, loss 1.46397e-06, acc 1
2017-08-08T17:57:48.014313: step 18406, loss 0.000883925, acc 1
2017-08-08T17:57:48.358265: step 18407, loss 3.14026e-06, acc 1
2017-08-08T17:57:48.610564: step 18408, loss 1.40251e-06, acc 1
2017-08-08T17:57:48.854363: step 18409, loss 0.000398373, acc 1
2017-08-08T17:57:49.118683: step 18410, loss 2.62632e-07, acc 1
2017-08-08T17:57:49.521832: step 18411, loss 0.000165321, acc 1
2017-08-08T17:57:49.811452: step 18412, loss 0, acc 1
2017-08-08T17:57:50.045892: step 18413, loss 8.52405e-05, acc 1
2017-08-08T17:57:50.274770: step 18414, loss 2.71945e-07, acc 1
2017-08-08T17:57:50.534470: step 18415, loss 7.2643e-08, acc 1
2017-08-08T17:57:50.730696: step 18416, loss 0.000238187, acc 1
2017-08-08T17:57:50.910806: step 18417, loss 8.18171e-05, acc 1
2017-08-08T17:57:51.181216: step 18418, loss 0.000226641, acc 1
2017-08-08T17:57:51.486755: step 18419, loss 2.97996e-06, acc 1
2017-08-08T17:57:51.790752: step 18420, loss 5.49155e-05, acc 1
2017-08-08T17:57:52.047109: step 18421, loss 0.000353831, acc 1
2017-08-08T17:57:52.227198: step 18422, loss 1.06169e-06, acc 1
2017-08-08T17:57:52.379983: step 18423, loss 3.01746e-07, acc 1
2017-08-08T17:57:52.810055: step 18424, loss 3.72529e-09, acc 1
2017-08-08T17:57:53.081994: step 18425, loss 1.15484e-07, acc 1
2017-08-08T17:57:53.353307: step 18426, loss 5.27637e-06, acc 1
2017-08-08T17:57:53.636699: step 18427, loss 5.9154e-05, acc 1
2017-08-08T17:57:54.065515: step 18428, loss 3.14784e-07, acc 1
2017-08-08T17:57:54.469169: step 18429, loss 1.11998e-05, acc 1
2017-08-08T17:57:54.787434: step 18430, loss 0.00066864, acc 1
2017-08-08T17:57:55.111902: step 18431, loss 4.19803e-06, acc 1
2017-08-08T17:57:55.421762: step 18432, loss 4.83784e-05, acc 1
2017-08-08T17:57:55.696680: step 18433, loss 0.0014583, acc 1
2017-08-08T17:57:55.942865: step 18434, loss 3.65077e-07, acc 1
2017-08-08T17:57:56.233388: step 18435, loss 3.34497e-06, acc 1
2017-08-08T17:57:56.644524: step 18436, loss 3.70062e-05, acc 1
2017-08-08T17:57:57.050985: step 18437, loss 6.74267e-07, acc 1
2017-08-08T17:57:57.328479: step 18438, loss 0.00037761, acc 1
2017-08-08T17:57:57.516552: step 18439, loss 4.99712e-05, acc 1
2017-08-08T17:57:57.777638: step 18440, loss 0.000684957, acc 1
2017-08-08T17:57:58.006888: step 18441, loss 2.79396e-07, acc 1
2017-08-08T17:57:58.184203: step 18442, loss 9.05986e-05, acc 1
2017-08-08T17:57:58.396936: step 18443, loss 2.72112e-06, acc 1
2017-08-08T17:57:58.645616: step 18444, loss 1.02443e-06, acc 1
2017-08-08T17:57:59.015687: step 18445, loss 1.23854e-05, acc 1
2017-08-08T17:57:59.349400: step 18446, loss 1.62049e-07, acc 1
2017-08-08T17:57:59.577579: step 18447, loss 9.14664e-06, acc 1
2017-08-08T17:57:59.839151: step 18448, loss 3.32129e-05, acc 1
2017-08-08T17:58:00.161526: step 18449, loss 4.6336e-05, acc 1
2017-08-08T17:58:00.420380: step 18450, loss 0.000445754, acc 1
2017-08-08T17:58:00.683419: step 18451, loss 1.00453e-05, acc 1
2017-08-08T17:58:00.989384: step 18452, loss 3.28358e-06, acc 1
2017-08-08T17:58:01.310498: step 18453, loss 0.000265764, acc 1
2017-08-08T17:58:01.650273: step 18454, loss 8.35699e-05, acc 1
2017-08-08T17:58:02.008885: step 18455, loss 3.29685e-07, acc 1
2017-08-08T17:58:02.244014: step 18456, loss 6.253e-05, acc 1
2017-08-08T17:58:02.597364: step 18457, loss 1.67638e-08, acc 1
2017-08-08T17:58:02.915750: step 18458, loss 5.84575e-06, acc 1
2017-08-08T17:58:03.177178: step 18459, loss 0, acc 1
2017-08-08T17:58:03.486350: step 18460, loss 0.000200003, acc 1
2017-08-08T17:58:03.769444: step 18461, loss 0, acc 1
2017-08-08T17:58:04.268287: step 18462, loss 9.99522e-05, acc 1
2017-08-08T17:58:04.635525: step 18463, loss 9.31321e-08, acc 1
2017-08-08T17:58:04.912250: step 18464, loss 9.17642e-06, acc 1
2017-08-08T17:58:05.120665: step 18465, loss 6.33298e-08, acc 1
2017-08-08T17:58:05.497357: step 18466, loss 8.56815e-08, acc 1
2017-08-08T17:58:05.801289: step 18467, loss 1.82563e-05, acc 1
2017-08-08T17:58:06.083425: step 18468, loss 7.4504e-07, acc 1
2017-08-08T17:58:06.378386: step 18469, loss 3.03726e-05, acc 1
2017-08-08T17:58:06.724888: step 18470, loss 8.25142e-05, acc 1
2017-08-08T17:58:07.065404: step 18471, loss 2.41484e-05, acc 1
2017-08-08T17:58:07.349367: step 18472, loss 3.399e-06, acc 1
2017-08-08T17:58:07.558002: step 18473, loss 7.07804e-08, acc 1
2017-08-08T17:58:07.738997: step 18474, loss 3.72529e-09, acc 1
2017-08-08T17:58:08.037820: step 18475, loss 9.68573e-08, acc 1
2017-08-08T17:58:08.363755: step 18476, loss 1.08217e-06, acc 1
2017-08-08T17:58:08.576518: step 18477, loss 3.42725e-07, acc 1
2017-08-08T17:58:08.888527: step 18478, loss 3.72529e-09, acc 1
2017-08-08T17:58:09.170067: step 18479, loss 1.86264e-08, acc 1
2017-08-08T17:58:09.560426: step 18480, loss 3.94274e-05, acc 1
2017-08-08T17:58:09.813639: step 18481, loss 5.58794e-09, acc 1
2017-08-08T17:58:10.052637: step 18482, loss 1.2177e-05, acc 1
2017-08-08T17:58:10.275110: step 18483, loss 1.14111e-05, acc 1
2017-08-08T17:58:10.458930: step 18484, loss 0, acc 1
2017-08-08T17:58:10.824417: step 18485, loss 2.20525e-06, acc 1
2017-08-08T17:58:11.118321: step 18486, loss 2.48779e-05, acc 1
2017-08-08T17:58:11.364487: step 18487, loss 0.000126266, acc 1
2017-08-08T17:58:11.621331: step 18488, loss 9.30353e-06, acc 1
2017-08-08T17:58:11.960657: step 18489, loss 0.000566842, acc 1
2017-08-08T17:58:12.334181: step 18490, loss 7.1524e-07, acc 1
2017-08-08T17:58:12.551940: step 18491, loss 2.34127e-05, acc 1
2017-08-08T17:58:12.776140: step 18492, loss 5.7322e-06, acc 1
2017-08-08T17:58:13.155843: step 18493, loss 0.00177901, acc 1
2017-08-08T17:58:13.415356: step 18494, loss 3.1086e-06, acc 1
2017-08-08T17:58:13.685913: step 18495, loss 0.00107513, acc 1
2017-08-08T17:58:13.982851: step 18496, loss 0.000152289, acc 1
2017-08-08T17:58:14.391416: step 18497, loss 2.23137e-06, acc 1
2017-08-08T17:58:14.814271: step 18498, loss 9.86501e-06, acc 1
2017-08-08T17:58:15.184517: step 18499, loss 4.08455e-05, acc 1
2017-08-08T17:58:15.431578: step 18500, loss 2.23517e-08, acc 1

Evaluation:
2017-08-08T17:58:16.125371: step 18500, loss 6.20546, acc 0.717636

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-18500

2017-08-08T17:58:16.469840: step 18501, loss 1.05e-05, acc 1
2017-08-08T17:58:16.683569: step 18502, loss 1.67638e-08, acc 1
2017-08-08T17:58:16.989274: step 18503, loss 1.75088e-07, acc 1
2017-08-08T17:58:17.257838: step 18504, loss 0.000252864, acc 1
2017-08-08T17:58:17.663588: step 18505, loss 9.44333e-07, acc 1
2017-08-08T17:58:17.964535: step 18506, loss 7.71128e-07, acc 1
2017-08-08T17:58:18.201983: step 18507, loss 5.05455e-06, acc 1
2017-08-08T17:58:18.469215: step 18508, loss 4.21472e-06, acc 1
2017-08-08T17:58:18.908337: step 18509, loss 4.80662e-05, acc 1
2017-08-08T17:58:19.178408: step 18510, loss 1.6407e-05, acc 1
2017-08-08T17:58:19.467152: step 18511, loss 7.63683e-08, acc 1
2017-08-08T17:58:19.747767: step 18512, loss 1.95577e-07, acc 1
2017-08-08T17:58:20.161369: step 18513, loss 0.000181697, acc 1
2017-08-08T17:58:20.593383: step 18514, loss 3.53902e-08, acc 1
2017-08-08T17:58:20.899608: step 18515, loss 7.45057e-08, acc 1
2017-08-08T17:58:21.094792: step 18516, loss 1.40672e-05, acc 1
2017-08-08T17:58:21.357384: step 18517, loss 8.74475e-06, acc 1
2017-08-08T17:58:21.685797: step 18518, loss 3.40733e-05, acc 1
2017-08-08T17:58:21.902473: step 18519, loss 1.00022e-06, acc 1
2017-08-08T17:58:22.144885: step 18520, loss 5.9604e-07, acc 1
2017-08-08T17:58:22.410729: step 18521, loss 2.17929e-07, acc 1
2017-08-08T17:58:22.861360: step 18522, loss 5.77872e-06, acc 1
2017-08-08T17:58:23.257369: step 18523, loss 5.22819e-05, acc 1
2017-08-08T17:58:23.547291: step 18524, loss 0.000224838, acc 1
2017-08-08T17:58:23.766623: step 18525, loss 3.53902e-08, acc 1
2017-08-08T17:58:24.091839: step 18526, loss 8.75442e-08, acc 1
2017-08-08T17:58:24.453597: step 18527, loss 3.10669e-06, acc 1
2017-08-08T17:58:24.656240: step 18528, loss 0.000524729, acc 1
2017-08-08T17:58:24.857046: step 18529, loss 8.94069e-08, acc 1
2017-08-08T17:58:25.142120: step 18530, loss 0.00128898, acc 1
2017-08-08T17:58:25.403034: step 18531, loss 6.88367e-06, acc 1
2017-08-08T17:58:25.659100: step 18532, loss 1.14551e-06, acc 1
2017-08-08T17:58:25.832852: step 18533, loss 2.96693e-06, acc 1
2017-08-08T17:58:26.152796: step 18534, loss 1.30385e-08, acc 1
2017-08-08T17:58:26.546154: step 18535, loss 1.35523e-05, acc 1
2017-08-08T17:58:26.840590: step 18536, loss 1.43423e-07, acc 1
2017-08-08T17:58:27.120620: step 18537, loss 2.98022e-07, acc 1
2017-08-08T17:58:27.504635: step 18538, loss 3.72529e-08, acc 1
2017-08-08T17:58:27.889407: step 18539, loss 4.54267e-06, acc 1
2017-08-08T17:58:28.204601: step 18540, loss 9.72275e-07, acc 1
2017-08-08T17:58:28.398238: step 18541, loss 9.92112e-05, acc 1
2017-08-08T17:58:28.607754: step 18542, loss 1.67638e-08, acc 1
2017-08-08T17:58:28.937506: step 18543, loss 1.22934e-07, acc 1
2017-08-08T17:58:29.269454: step 18544, loss 9.31322e-09, acc 1
2017-08-08T17:58:29.543976: step 18545, loss 5.00974e-06, acc 1
2017-08-08T17:58:29.722861: step 18546, loss 1.0334e-05, acc 1
2017-08-08T17:58:29.929391: step 18547, loss 1.04308e-07, acc 1
2017-08-08T17:58:30.257391: step 18548, loss 2.43277e-05, acc 1
2017-08-08T17:58:30.501264: step 18549, loss 3.41224e-06, acc 1
2017-08-08T17:58:30.701819: step 18550, loss 6.23888e-06, acc 1
2017-08-08T17:58:30.899179: step 18551, loss 0.0161949, acc 0.984375
2017-08-08T17:58:31.188765: step 18552, loss 1.15855e-06, acc 1
2017-08-08T17:58:31.433918: step 18553, loss 9.45819e-05, acc 1
2017-08-08T17:58:31.634696: step 18554, loss 9.61108e-07, acc 1
2017-08-08T17:58:31.869750: step 18555, loss 0, acc 1
2017-08-08T17:58:32.209146: step 18556, loss 3.0469e-05, acc 1
2017-08-08T17:58:32.583940: step 18557, loss 1.35597e-06, acc 1
2017-08-08T17:58:32.928470: step 18558, loss 0.000361938, acc 1
2017-08-08T17:58:33.195590: step 18559, loss 2.18846e-06, acc 1
2017-08-08T17:58:33.413530: step 18560, loss 0.0114205, acc 0.984375
2017-08-08T17:58:33.804354: step 18561, loss 0.00426009, acc 1
2017-08-08T17:58:34.110061: step 18562, loss 0.000215745, acc 1
2017-08-08T17:58:34.345383: step 18563, loss 4.91735e-07, acc 1
2017-08-08T17:58:34.656319: step 18564, loss 5.7832e-05, acc 1
2017-08-08T17:58:35.067366: step 18565, loss 1.01202e-05, acc 1
2017-08-08T17:58:35.454018: step 18566, loss 0.000102066, acc 1
2017-08-08T17:58:35.793347: step 18567, loss 3.85123e-05, acc 1
2017-08-08T17:58:36.033125: step 18568, loss 3.07334e-07, acc 1
2017-08-08T17:58:36.233452: step 18569, loss 1.68565e-06, acc 1
2017-08-08T17:58:36.489360: step 18570, loss 8.90322e-07, acc 1
2017-08-08T17:58:36.775414: step 18571, loss 1.42114e-06, acc 1
2017-08-08T17:58:36.977409: step 18572, loss 3.15511e-06, acc 1
2017-08-08T17:58:37.205034: step 18573, loss 2.34692e-07, acc 1
2017-08-08T17:58:37.530744: step 18574, loss 3.65794e-06, acc 1
2017-08-08T17:58:37.881497: step 18575, loss 3.91151e-07, acc 1
2017-08-08T17:58:38.228212: step 18576, loss 0.154552, acc 0.984375
2017-08-08T17:58:38.477574: step 18577, loss 6.46212e-06, acc 1
2017-08-08T17:58:38.673032: step 18578, loss 6.05804e-06, acc 1
2017-08-08T17:58:38.960518: step 18579, loss 0.000145859, acc 1
2017-08-08T17:58:39.184089: step 18580, loss 1.82534e-06, acc 1
2017-08-08T17:58:39.398250: step 18581, loss 3.08439e-06, acc 1
2017-08-08T17:58:39.663344: step 18582, loss 4.92374e-05, acc 1
2017-08-08T17:58:40.032503: step 18583, loss 1.58043e-05, acc 1
2017-08-08T17:58:40.429353: step 18584, loss 2.0842e-06, acc 1
2017-08-08T17:58:40.757946: step 18585, loss 1.93565e-05, acc 1
2017-08-08T17:58:41.037217: step 18586, loss 2.38418e-07, acc 1
2017-08-08T17:58:41.278544: step 18587, loss 1.78809e-06, acc 1
2017-08-08T17:58:41.685746: step 18588, loss 2.6077e-08, acc 1
2017-08-08T17:58:41.942130: step 18589, loss 2.9828e-05, acc 1
2017-08-08T17:58:42.171834: step 18590, loss 1.75884e-05, acc 1
2017-08-08T17:58:42.423012: step 18591, loss 5.74893e-06, acc 1
2017-08-08T17:58:42.777651: step 18592, loss 1.11571e-06, acc 1
2017-08-08T17:58:43.197337: step 18593, loss 0, acc 1
2017-08-08T17:58:43.530899: step 18594, loss 4.95457e-07, acc 1
2017-08-08T17:58:43.808796: step 18595, loss 2.43564e-05, acc 1
2017-08-08T17:58:44.170258: step 18596, loss 9.10357e-06, acc 1
2017-08-08T17:58:44.493457: step 18597, loss 0.00124932, acc 1
2017-08-08T17:58:44.766256: step 18598, loss 2.9425e-05, acc 1
2017-08-08T17:58:45.094109: step 18599, loss 4.99185e-07, acc 1
2017-08-08T17:58:45.513407: step 18600, loss 6.09618e-05, acc 1

Evaluation:
2017-08-08T17:58:46.420390: step 18600, loss 6.20865, acc 0.713884

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-18600

2017-08-08T17:58:46.889563: step 18601, loss 0.00033317, acc 1
2017-08-08T17:58:47.345385: step 18602, loss 1.30987e-05, acc 1
2017-08-08T17:58:47.635050: step 18603, loss 1.86264e-08, acc 1
2017-08-08T17:58:47.901267: step 18604, loss 1.31435e-05, acc 1
2017-08-08T17:58:48.199342: step 18605, loss 4.22815e-07, acc 1
2017-08-08T17:58:48.617621: step 18606, loss 1.04308e-07, acc 1
2017-08-08T17:58:48.935339: step 18607, loss 5.28356e-06, acc 1
2017-08-08T17:58:49.170285: step 18608, loss 7.66028e-05, acc 1
2017-08-08T17:58:49.347378: step 18609, loss 0.00516301, acc 1
2017-08-08T17:58:49.586122: step 18610, loss 1.84391e-06, acc 1
2017-08-08T17:58:49.954773: step 18611, loss 1.79902e-05, acc 1
2017-08-08T17:58:50.212115: step 18612, loss 3.7958e-06, acc 1
2017-08-08T17:58:50.521915: step 18613, loss 4.8142e-06, acc 1
2017-08-08T17:58:50.785433: step 18614, loss 4.07916e-07, acc 1
2017-08-08T17:58:51.113398: step 18615, loss 5.02913e-08, acc 1
2017-08-08T17:58:51.473647: step 18616, loss 8.16442e-05, acc 1
2017-08-08T17:58:51.772856: step 18617, loss 3.97824e-06, acc 1
2017-08-08T17:58:51.955325: step 18618, loss 9.34527e-06, acc 1
2017-08-08T17:58:52.161394: step 18619, loss 0.00263364, acc 1
2017-08-08T17:58:52.543834: step 18620, loss 0.000167422, acc 1
2017-08-08T17:58:52.801898: step 18621, loss 0.000886741, acc 1
2017-08-08T17:58:53.036633: step 18622, loss 7.63683e-08, acc 1
2017-08-08T17:58:53.429661: step 18623, loss 0, acc 1
2017-08-08T17:58:53.850037: step 18624, loss 1.90352e-06, acc 1
2017-08-08T17:58:54.141013: step 18625, loss 0.000601313, acc 1
2017-08-08T17:58:54.409510: step 18626, loss 5.30555e-05, acc 1
2017-08-08T17:58:54.647876: step 18627, loss 0, acc 1
2017-08-08T17:58:55.107332: step 18628, loss 1.01883e-06, acc 1
2017-08-08T17:58:55.417450: step 18629, loss 1.87224e-05, acc 1
2017-08-08T17:58:55.695778: step 18630, loss 3.79977e-07, acc 1
2017-08-08T17:58:55.949199: step 18631, loss 0.000109308, acc 1
2017-08-08T17:58:56.293365: step 18632, loss 1.67638e-08, acc 1
2017-08-08T17:58:56.667822: step 18633, loss 7.26425e-07, acc 1
2017-08-08T17:58:57.010461: step 18634, loss 1.87471e-05, acc 1
2017-08-08T17:58:57.297388: step 18635, loss 1.00757e-05, acc 1
2017-08-08T17:58:57.575270: step 18636, loss 2.90745e-06, acc 1
2017-08-08T17:58:58.056874: step 18637, loss 1.30007e-06, acc 1
2017-08-08T17:58:58.347210: step 18638, loss 0.00159298, acc 1
2017-08-08T17:58:58.640018: step 18639, loss 3.53902e-08, acc 1
2017-08-08T17:58:58.909414: step 18640, loss 9.31322e-09, acc 1
2017-08-08T17:58:59.297630: step 18641, loss 5.2425e-06, acc 1
2017-08-08T17:58:59.645972: step 18642, loss 2.72306e-06, acc 1
2017-08-08T17:58:59.963934: step 18643, loss 9.80448e-05, acc 1
2017-08-08T17:59:00.224994: step 18644, loss 1.67075e-06, acc 1
2017-08-08T17:59:00.470375: step 18645, loss 0.0493401, acc 0.984375
2017-08-08T17:59:00.929230: step 18646, loss 0.000302862, acc 1
2017-08-08T17:59:01.191036: step 18647, loss 2.51251e-06, acc 1
2017-08-08T17:59:01.462243: step 18648, loss 0.000188826, acc 1
2017-08-08T17:59:01.825325: step 18649, loss 4.37716e-07, acc 1
2017-08-08T17:59:02.233800: step 18650, loss 1.39698e-07, acc 1
2017-08-08T17:59:02.717830: step 18651, loss 5.87371e-06, acc 1
2017-08-08T17:59:03.207103: step 18652, loss 0, acc 1
2017-08-08T17:59:03.504232: step 18653, loss 0.0179278, acc 0.984375
2017-08-08T17:59:03.772540: step 18654, loss 4.74968e-07, acc 1
2017-08-08T17:59:04.171425: step 18655, loss 9.44262e-06, acc 1
2017-08-08T17:59:04.483369: step 18656, loss 0.00322702, acc 1
2017-08-08T17:59:04.776689: step 18657, loss 1.51243e-06, acc 1
2017-08-08T17:59:05.027189: step 18658, loss 1.58503e-06, acc 1
2017-08-08T17:59:05.401008: step 18659, loss 3.22589e-06, acc 1
2017-08-08T17:59:05.788207: step 18660, loss 6.46068e-06, acc 1
2017-08-08T17:59:06.202097: step 18661, loss 1.27229e-05, acc 1
2017-08-08T17:59:06.472276: step 18662, loss 0.000222948, acc 1
2017-08-08T17:59:06.666310: step 18663, loss 4.2654e-07, acc 1
2017-08-08T17:59:07.019075: step 18664, loss 5.33644e-05, acc 1
2017-08-08T17:59:07.292212: step 18665, loss 4.59602e-05, acc 1
2017-08-08T17:59:07.509984: step 18666, loss 0, acc 1
2017-08-08T17:59:07.747486: step 18667, loss 0.000128016, acc 1
2017-08-08T17:59:08.129310: step 18668, loss 4.96329e-06, acc 1
2017-08-08T17:59:08.407758: step 18669, loss 4.3768e-06, acc 1
2017-08-08T17:59:08.653310: step 18670, loss 5.19593e-06, acc 1
2017-08-08T17:59:08.926182: step 18671, loss 0.0690899, acc 0.984375
2017-08-08T17:59:09.173486: step 18672, loss 8.2136e-05, acc 1
2017-08-08T17:59:09.618651: step 18673, loss 3.31908e-05, acc 1
2017-08-08T17:59:09.909872: step 18674, loss 0.000331145, acc 1
2017-08-08T17:59:10.135209: step 18675, loss 0.00189524, acc 1
2017-08-08T17:59:10.471985: step 18676, loss 4.96976e-05, acc 1
2017-08-08T17:59:10.849893: step 18677, loss 4.65661e-08, acc 1
2017-08-08T17:59:11.166626: step 18678, loss 2.19218e-06, acc 1
2017-08-08T17:59:11.416619: step 18679, loss 7.07804e-08, acc 1
2017-08-08T17:59:11.619755: step 18680, loss 5.56549e-05, acc 1
2017-08-08T17:59:11.909457: step 18681, loss 1.80108e-06, acc 1
2017-08-08T17:59:12.187984: step 18682, loss 3.45605e-05, acc 1
2017-08-08T17:59:12.415393: step 18683, loss 9.31322e-09, acc 1
2017-08-08T17:59:12.705990: step 18684, loss 2.27241e-07, acc 1
2017-08-08T17:59:13.126325: step 18685, loss 0.00341386, acc 1
2017-08-08T17:59:13.504637: step 18686, loss 0.000421207, acc 1
2017-08-08T17:59:13.804255: step 18687, loss 3.57896e-05, acc 1
2017-08-08T17:59:14.018377: step 18688, loss 5.01048e-07, acc 1
2017-08-08T17:59:14.329356: step 18689, loss 0.00585414, acc 1
2017-08-08T17:59:14.755540: step 18690, loss 5.41734e-05, acc 1
2017-08-08T17:59:15.042945: step 18691, loss 1.30385e-08, acc 1
2017-08-08T17:59:15.309748: step 18692, loss 1.52916e-06, acc 1
2017-08-08T17:59:15.514998: step 18693, loss 1.36649e-05, acc 1
2017-08-08T17:59:15.773476: step 18694, loss 8.68138e-05, acc 1
2017-08-08T17:59:16.089328: step 18695, loss 0.000372602, acc 1
2017-08-08T17:59:16.364621: step 18696, loss 5.47033e-05, acc 1
2017-08-08T17:59:16.554444: step 18697, loss 4.65661e-08, acc 1
2017-08-08T17:59:16.813332: step 18698, loss 3.52266e-05, acc 1
2017-08-08T17:59:17.130450: step 18699, loss 2.23517e-08, acc 1
2017-08-08T17:59:17.336125: step 18700, loss 0.00960098, acc 1

Evaluation:
2017-08-08T17:59:18.054139: step 18700, loss 6.21443, acc 0.712008

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-18700

2017-08-08T17:59:18.584796: step 18701, loss 9.31322e-09, acc 1
2017-08-08T17:59:18.819347: step 18702, loss 0.000515369, acc 1
2017-08-08T17:59:19.048436: step 18703, loss 4.9911e-06, acc 1
2017-08-08T17:59:19.295711: step 18704, loss 5.58793e-09, acc 1
2017-08-08T17:59:19.461473: step 18705, loss 0.00535435, acc 1
2017-08-08T17:59:19.696628: step 18706, loss 0.000606111, acc 1
2017-08-08T17:59:19.948577: step 18707, loss 3.72529e-09, acc 1
2017-08-08T17:59:20.313468: step 18708, loss 3.72529e-09, acc 1
2017-08-08T17:59:20.569630: step 18709, loss 0.0343161, acc 0.984375
2017-08-08T17:59:20.834557: step 18710, loss 1.00583e-07, acc 1
2017-08-08T17:59:21.077357: step 18711, loss 0.000103195, acc 1
2017-08-08T17:59:21.405043: step 18712, loss 1.43976e-06, acc 1
2017-08-08T17:59:21.629419: step 18713, loss 0.00030421, acc 1
2017-08-08T17:59:21.871237: step 18714, loss 3.70664e-07, acc 1
2017-08-08T17:59:22.093126: step 18715, loss 2.34307e-06, acc 1
2017-08-08T17:59:22.403196: step 18716, loss 5.18313e-06, acc 1
2017-08-08T17:59:22.666703: step 18717, loss 6.1275e-05, acc 1
2017-08-08T17:59:22.887737: step 18718, loss 1.06826e-05, acc 1
2017-08-08T17:59:23.067057: step 18719, loss 6.67496e-06, acc 1
2017-08-08T17:59:23.389959: step 18720, loss 2.84412e-06, acc 1
2017-08-08T17:59:23.582817: step 18721, loss 7.69736e-05, acc 1
2017-08-08T17:59:23.836371: step 18722, loss 1.466e-05, acc 1
2017-08-08T17:59:24.089660: step 18723, loss 1.04308e-07, acc 1
2017-08-08T17:59:24.434619: step 18724, loss 3.72527e-07, acc 1
2017-08-08T17:59:24.789118: step 18725, loss 1.60187e-07, acc 1
2017-08-08T17:59:25.089365: step 18726, loss 1.82346e-06, acc 1
2017-08-08T17:59:25.455782: step 18727, loss 2.09184e-05, acc 1
2017-08-08T17:59:25.708672: step 18728, loss 1.68564e-06, acc 1
2017-08-08T17:59:26.025521: step 18729, loss 6.38852e-05, acc 1
2017-08-08T17:59:26.420550: step 18730, loss 6.51925e-08, acc 1
2017-08-08T17:59:26.660560: step 18731, loss 4.32517e-05, acc 1
2017-08-08T17:59:26.858578: step 18732, loss 2.55162e-06, acc 1
2017-08-08T17:59:27.135736: step 18733, loss 0.00253709, acc 1
2017-08-08T17:59:27.466291: step 18734, loss 0, acc 1
2017-08-08T17:59:27.765476: step 18735, loss 3.86081e-06, acc 1
2017-08-08T17:59:27.963765: step 18736, loss 9.31322e-09, acc 1
2017-08-08T17:59:28.187996: step 18737, loss 2.34596e-05, acc 1
2017-08-08T17:59:28.529355: step 18738, loss 2.42144e-08, acc 1
2017-08-08T17:59:28.814149: step 18739, loss 4.02329e-07, acc 1
2017-08-08T17:59:29.083787: step 18740, loss 0.000530723, acc 1
2017-08-08T17:59:29.371184: step 18741, loss 3.73989e-06, acc 1
2017-08-08T17:59:29.633489: step 18742, loss 6.79945e-06, acc 1
2017-08-08T17:59:29.944535: step 18743, loss 0.00090415, acc 1
2017-08-08T17:59:30.297412: step 18744, loss 2.04891e-08, acc 1
2017-08-08T17:59:30.597734: step 18745, loss 1.86265e-09, acc 1
2017-08-08T17:59:30.809528: step 18746, loss 1.50874e-07, acc 1
2017-08-08T17:59:31.020878: step 18747, loss 8.58656e-07, acc 1
2017-08-08T17:59:31.371430: step 18748, loss 2.42144e-08, acc 1
2017-08-08T17:59:31.669755: step 18749, loss 7.21686e-05, acc 1
2017-08-08T17:59:31.900592: step 18750, loss 0.00220497, acc 1
2017-08-08T17:59:32.154440: step 18751, loss 4.1164e-07, acc 1
2017-08-08T17:59:32.489353: step 18752, loss 1.86264e-08, acc 1
2017-08-08T17:59:32.781346: step 18753, loss 1.99455e-05, acc 1
2017-08-08T17:59:33.121619: step 18754, loss 9.872e-08, acc 1
2017-08-08T17:59:33.306264: step 18755, loss 9.872e-08, acc 1
2017-08-08T17:59:33.493325: step 18756, loss 9.67215e-05, acc 1
2017-08-08T17:59:33.769506: step 18757, loss 3.63993e-05, acc 1
2017-08-08T17:59:34.010856: step 18758, loss 3.36026e-05, acc 1
2017-08-08T17:59:34.268503: step 18759, loss 7.21843e-06, acc 1
2017-08-08T17:59:34.481387: step 18760, loss 5.99567e-05, acc 1
2017-08-08T17:59:34.677954: step 18761, loss 1.07844e-06, acc 1
2017-08-08T17:59:34.939793: step 18762, loss 4.39366e-05, acc 1
2017-08-08T17:59:35.221292: step 18763, loss 3.33205e-06, acc 1
2017-08-08T17:59:35.499968: step 18764, loss 1.67638e-08, acc 1
2017-08-08T17:59:35.700469: step 18765, loss 2.42144e-08, acc 1
2017-08-08T17:59:35.919833: step 18766, loss 3.89143e-05, acc 1
2017-08-08T17:59:36.297525: step 18767, loss 3.47451e-05, acc 1
2017-08-08T17:59:36.530444: step 18768, loss 1.65775e-07, acc 1
2017-08-08T17:59:36.780311: step 18769, loss 9.74147e-07, acc 1
2017-08-08T17:59:37.041108: step 18770, loss 4.47034e-08, acc 1
2017-08-08T17:59:37.362618: step 18771, loss 0.000870529, acc 1
2017-08-08T17:59:37.698915: step 18772, loss 8.94046e-07, acc 1
2017-08-08T17:59:37.937355: step 18773, loss 5.58794e-09, acc 1
2017-08-08T17:59:38.143146: step 18774, loss 1.16971e-06, acc 1
2017-08-08T17:59:38.401473: step 18775, loss 0.00136935, acc 1
2017-08-08T17:59:38.759906: step 18776, loss 1.56517e-05, acc 1
2017-08-08T17:59:38.956915: step 18777, loss 1.2666e-07, acc 1
2017-08-08T17:59:39.192948: step 18778, loss 0, acc 1
2017-08-08T17:59:39.414611: step 18779, loss 3.24098e-07, acc 1
2017-08-08T17:59:39.762973: step 18780, loss 4.15317e-06, acc 1
2017-08-08T17:59:40.045623: step 18781, loss 3.13642e-06, acc 1
2017-08-08T17:59:40.231137: step 18782, loss 3.72529e-09, acc 1
2017-08-08T17:59:40.427859: step 18783, loss 4.39154e-06, acc 1
2017-08-08T17:59:40.761334: step 18784, loss 0.0857093, acc 0.984375
2017-08-08T17:59:41.044276: step 18785, loss 1.3411e-07, acc 1
2017-08-08T17:59:41.315436: step 18786, loss 2.42144e-08, acc 1
2017-08-08T17:59:41.547592: step 18787, loss 8.43762e-07, acc 1
2017-08-08T17:59:41.867513: step 18788, loss 0.000175552, acc 1
2017-08-08T17:59:42.271477: step 18789, loss 0.0031333, acc 1
2017-08-08T17:59:42.628976: step 18790, loss 5.17197e-06, acc 1
2017-08-08T17:59:42.811917: step 18791, loss 3.23518e-06, acc 1
2017-08-08T17:59:42.991756: step 18792, loss 7.47206e-06, acc 1
2017-08-08T17:59:43.363273: step 18793, loss 3.04141e-06, acc 1
2017-08-08T17:59:43.602001: step 18794, loss 3.46125e-05, acc 1
2017-08-08T17:59:43.868131: step 18795, loss 5.27124e-07, acc 1
2017-08-08T17:59:44.146279: step 18796, loss 0.144781, acc 0.984375
2017-08-08T17:59:44.458709: step 18797, loss 9.7786e-07, acc 1
2017-08-08T17:59:44.853637: step 18798, loss 0.000174834, acc 1
2017-08-08T17:59:45.205804: step 18799, loss 0.0001117, acc 1
2017-08-08T17:59:45.511569: step 18800, loss 4.54824e-06, acc 1

Evaluation:
2017-08-08T17:59:46.214928: step 18800, loss 6.22849, acc 0.710131

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-18800

2017-08-08T17:59:46.723030: step 18801, loss 0.000783782, acc 1
2017-08-08T17:59:46.978577: step 18802, loss 1.39695e-06, acc 1
2017-08-08T17:59:47.209375: step 18803, loss 3.7174e-06, acc 1
2017-08-08T17:59:47.559850: step 18804, loss 6.09342e-06, acc 1
2017-08-08T17:59:47.883737: step 18805, loss 0.000444278, acc 1
2017-08-08T17:59:48.263926: step 18806, loss 1.50874e-07, acc 1
2017-08-08T17:59:48.494094: step 18807, loss 3.75524e-05, acc 1
2017-08-08T17:59:48.696995: step 18808, loss 1.54594e-06, acc 1
2017-08-08T17:59:49.133766: step 18809, loss 1.11759e-08, acc 1
2017-08-08T17:59:49.498741: step 18810, loss 2.92239e-06, acc 1
2017-08-08T17:59:49.827005: step 18811, loss 3.28158e-05, acc 1
2017-08-08T17:59:50.218814: step 18812, loss 2.53318e-07, acc 1
2017-08-08T17:59:50.638223: step 18813, loss 6.98483e-07, acc 1
2017-08-08T17:59:50.981466: step 18814, loss 1.35973e-07, acc 1
2017-08-08T17:59:51.289436: step 18815, loss 2.42144e-08, acc 1
2017-08-08T17:59:51.553485: step 18816, loss 8.61516e-05, acc 1
2017-08-08T17:59:51.800992: step 18817, loss 0.00109185, acc 1
2017-08-08T17:59:52.161752: step 18818, loss 1.52361e-06, acc 1
2017-08-08T17:59:52.435295: step 18819, loss 2.45869e-07, acc 1
2017-08-08T17:59:52.741168: step 18820, loss 1.28893e-06, acc 1
2017-08-08T17:59:53.089325: step 18821, loss 1.88127e-07, acc 1
2017-08-08T17:59:53.415067: step 18822, loss 8.10237e-07, acc 1
2017-08-08T17:59:53.807781: step 18823, loss 0.000225175, acc 1
2017-08-08T17:59:54.146683: step 18824, loss 4.02329e-07, acc 1
2017-08-08T17:59:54.397985: step 18825, loss 3.7538e-05, acc 1
2017-08-08T17:59:54.691486: step 18826, loss 4.6193e-05, acc 1
2017-08-08T17:59:55.026145: step 18827, loss 3.30773e-06, acc 1
2017-08-08T17:59:55.348759: step 18828, loss 8.94068e-08, acc 1
2017-08-08T17:59:55.697210: step 18829, loss 3.35276e-08, acc 1
2017-08-08T17:59:56.100288: step 18830, loss 9.71854e-06, acc 1
2017-08-08T17:59:56.452236: step 18831, loss 1.46958e-06, acc 1
2017-08-08T17:59:56.777146: step 18832, loss 0.000135365, acc 1
2017-08-08T17:59:57.211804: step 18833, loss 2.15493e-06, acc 1
2017-08-08T17:59:57.466882: step 18834, loss 2.40268e-06, acc 1
2017-08-08T17:59:57.733295: step 18835, loss 0.000698813, acc 1
2017-08-08T17:59:58.155551: step 18836, loss 1.45286e-07, acc 1
2017-08-08T17:59:58.565848: step 18837, loss 0.000830375, acc 1
2017-08-08T17:59:58.930097: step 18838, loss 0.000132847, acc 1
2017-08-08T17:59:59.301470: step 18839, loss 9.872e-08, acc 1
2017-08-08T17:59:59.723285: step 18840, loss 1.50874e-07, acc 1
2017-08-08T18:00:00.148784: step 18841, loss 1.48701e-05, acc 1
2017-08-08T18:00:00.470622: step 18842, loss 6.68779e-06, acc 1
2017-08-08T18:00:00.697847: step 18843, loss 7.6926e-07, acc 1
2017-08-08T18:00:01.111563: step 18844, loss 3.72529e-09, acc 1
2017-08-08T18:00:01.445042: step 18845, loss 1.45898e-05, acc 1
2017-08-08T18:00:01.805645: step 18846, loss 1.09896e-07, acc 1
2017-08-08T18:00:02.396017: step 18847, loss 5.69934e-06, acc 1
2017-08-08T18:00:02.858036: step 18848, loss 0.0175214, acc 0.984375
2017-08-08T18:00:03.355551: step 18849, loss 1.74403e-05, acc 1
2017-08-08T18:00:03.892642: step 18850, loss 3.72529e-09, acc 1
2017-08-08T18:00:04.272477: step 18851, loss 0.0630396, acc 0.984375
2017-08-08T18:00:04.621589: step 18852, loss 0, acc 1
2017-08-08T18:00:05.064348: step 18853, loss 5.40167e-08, acc 1
2017-08-08T18:00:05.486062: step 18854, loss 0, acc 1
2017-08-08T18:00:05.905436: step 18855, loss 6.40349e-05, acc 1
2017-08-08T18:00:06.178213: step 18856, loss 1.08033e-07, acc 1
2017-08-08T18:00:06.463001: step 18857, loss 1.30385e-08, acc 1
2017-08-08T18:00:06.789348: step 18858, loss 5.96046e-08, acc 1
2017-08-08T18:00:07.138902: step 18859, loss 1.2666e-07, acc 1
2017-08-08T18:00:07.504905: step 18860, loss 3.65342e-05, acc 1
2017-08-08T18:00:07.810712: step 18861, loss 2.08615e-07, acc 1
2017-08-08T18:00:08.230562: step 18862, loss 9.14788e-05, acc 1
2017-08-08T18:00:08.646631: step 18863, loss 4.16915e-05, acc 1
2017-08-08T18:00:09.029482: step 18864, loss 1.86264e-07, acc 1
2017-08-08T18:00:09.267472: step 18865, loss 0.000111031, acc 1
2017-08-08T18:00:09.463595: step 18866, loss 1.92476e-05, acc 1
2017-08-08T18:00:09.857220: step 18867, loss 9.31322e-09, acc 1
2017-08-08T18:00:10.086007: step 18868, loss 8.18986e-06, acc 1
2017-08-08T18:00:10.325713: step 18869, loss 1.13245e-06, acc 1
2017-08-08T18:00:10.578593: step 18870, loss 4.8391e-05, acc 1
2017-08-08T18:00:10.872269: step 18871, loss 1.49012e-08, acc 1
2017-08-08T18:00:11.161371: step 18872, loss 4.77323e-06, acc 1
2017-08-08T18:00:11.416772: step 18873, loss 7.75717e-05, acc 1
2017-08-08T18:00:11.645267: step 18874, loss 7.45056e-08, acc 1
2017-08-08T18:00:11.994488: step 18875, loss 1.09334e-06, acc 1
2017-08-08T18:00:12.261972: step 18876, loss 5.58794e-09, acc 1
2017-08-08T18:00:12.536150: step 18877, loss 3.17363e-06, acc 1
2017-08-08T18:00:12.845151: step 18878, loss 5.81412e-06, acc 1
2017-08-08T18:00:13.308017: step 18879, loss 0.000249865, acc 1
2017-08-08T18:00:13.755703: step 18880, loss 7.45057e-08, acc 1
2017-08-08T18:00:14.083472: step 18881, loss 6.96625e-07, acc 1
2017-08-08T18:00:14.363929: step 18882, loss 3.72529e-09, acc 1
2017-08-08T18:00:14.632961: step 18883, loss 7.17359e-05, acc 1
2017-08-08T18:00:15.078942: step 18884, loss 2.20973e-05, acc 1
2017-08-08T18:00:15.341454: step 18885, loss 1.68563e-06, acc 1
2017-08-08T18:00:15.632684: step 18886, loss 9.12695e-08, acc 1
2017-08-08T18:00:15.933197: step 18887, loss 3.70662e-07, acc 1
2017-08-08T18:00:16.349722: step 18888, loss 1.06945e-05, acc 1
2017-08-08T18:00:16.781473: step 18889, loss 0.233695, acc 0.984375
2017-08-08T18:00:17.107658: step 18890, loss 7.78916e-05, acc 1
2017-08-08T18:00:17.382817: step 18891, loss 0.00025279, acc 1
2017-08-08T18:00:17.766611: step 18892, loss 3.78446e-06, acc 1
2017-08-08T18:00:18.010949: step 18893, loss 3.14198e-06, acc 1
2017-08-08T18:00:18.248429: step 18894, loss 1.45286e-07, acc 1
2017-08-08T18:00:18.521907: step 18895, loss 1.06228e-05, acc 1
2017-08-08T18:00:18.873829: step 18896, loss 1.04308e-07, acc 1
2017-08-08T18:00:19.141921: step 18897, loss 4.29744e-05, acc 1
2017-08-08T18:00:19.461488: step 18898, loss 3.17384e-06, acc 1
2017-08-08T18:00:19.731337: step 18899, loss 2.60768e-07, acc 1
2017-08-08T18:00:19.985387: step 18900, loss 4.82793e-07, acc 1

Evaluation:
2017-08-08T18:00:20.748451: step 18900, loss 6.28709, acc 0.707317

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-18900

2017-08-08T18:00:21.237539: step 18901, loss 4.75854e-06, acc 1
2017-08-08T18:00:21.630462: step 18902, loss 6.31351e-05, acc 1
2017-08-08T18:00:22.081778: step 18903, loss 0.00095048, acc 1
2017-08-08T18:00:22.412943: step 18904, loss 3.44586e-07, acc 1
2017-08-08T18:00:22.616406: step 18905, loss 3.87196e-06, acc 1
2017-08-08T18:00:22.878968: step 18906, loss 6.64959e-07, acc 1
2017-08-08T18:00:23.145090: step 18907, loss 0.0530272, acc 0.984375
2017-08-08T18:00:23.378330: step 18908, loss 5.83e-07, acc 1
2017-08-08T18:00:23.623967: step 18909, loss 5.48646e-06, acc 1
2017-08-08T18:00:23.981377: step 18910, loss 1.45908e-05, acc 1
2017-08-08T18:00:24.353878: step 18911, loss 4.6373e-06, acc 1
2017-08-08T18:00:24.721470: step 18912, loss 6.34623e-05, acc 1
2017-08-08T18:00:24.979338: step 18913, loss 5.72296e-06, acc 1
2017-08-08T18:00:25.225305: step 18914, loss 0.000125546, acc 1
2017-08-08T18:00:25.594737: step 18915, loss 2.29104e-07, acc 1
2017-08-08T18:00:25.902638: step 18916, loss 0.000186002, acc 1
2017-08-08T18:00:26.170264: step 18917, loss 4.28408e-08, acc 1
2017-08-08T18:00:26.453178: step 18918, loss 0.00083842, acc 1
2017-08-08T18:00:26.733384: step 18919, loss 0.000124061, acc 1
2017-08-08T18:00:27.184412: step 18920, loss 0, acc 1
2017-08-08T18:00:27.557420: step 18921, loss 2.1136e-05, acc 1
2017-08-08T18:00:27.842569: step 18922, loss 1.0414e-05, acc 1
2017-08-08T18:00:28.064662: step 18923, loss 2.17928e-07, acc 1
2017-08-08T18:00:28.388233: step 18924, loss 6.80836e-06, acc 1
2017-08-08T18:00:28.721895: step 18925, loss 4.42327e-06, acc 1
2017-08-08T18:00:28.988961: step 18926, loss 5.58794e-09, acc 1
2017-08-08T18:00:29.283747: step 18927, loss 6.68966e-06, acc 1
2017-08-08T18:00:29.785832: step 18928, loss 1.86264e-08, acc 1
2017-08-08T18:00:30.175692: step 18929, loss 7.45057e-08, acc 1
2017-08-08T18:00:30.436719: step 18930, loss 3.54649e-05, acc 1
2017-08-08T18:00:30.677454: step 18931, loss 1.86265e-09, acc 1
2017-08-08T18:00:30.897904: step 18932, loss 4.144e-06, acc 1
2017-08-08T18:00:31.266917: step 18933, loss 1.22557e-06, acc 1
2017-08-08T18:00:31.459018: step 18934, loss 3.72529e-09, acc 1
2017-08-08T18:00:31.657769: step 18935, loss 1.54592e-06, acc 1
2017-08-08T18:00:31.879019: step 18936, loss 9.55433e-06, acc 1
2017-08-08T18:00:32.173324: step 18937, loss 3.18238e-05, acc 1
2017-08-08T18:00:32.561313: step 18938, loss 8.06505e-07, acc 1
2017-08-08T18:00:32.866168: step 18939, loss 1.75281e-05, acc 1
2017-08-08T18:00:33.029239: step 18940, loss 9.12693e-08, acc 1
2017-08-08T18:00:33.205967: step 18941, loss 8.1381e-05, acc 1
2017-08-08T18:00:33.440737: step 18942, loss 2.01526e-06, acc 1
2017-08-08T18:00:33.611105: step 18943, loss 0.000134281, acc 1
2017-08-08T18:00:33.781330: step 18944, loss 0.00039515, acc 1
2017-08-08T18:00:33.981333: step 18945, loss 3.10863e-06, acc 1
2017-08-08T18:00:34.268752: step 18946, loss 2.38591e-05, acc 1
2017-08-08T18:00:34.562379: step 18947, loss 1.93715e-07, acc 1
2017-08-08T18:00:34.794617: step 18948, loss 4.26542e-07, acc 1
2017-08-08T18:00:34.975628: step 18949, loss 0.00610833, acc 1
2017-08-08T18:00:35.234434: step 18950, loss 1.572e-06, acc 1
2017-08-08T18:00:35.439609: step 18951, loss 4.44612e-05, acc 1
2017-08-08T18:00:35.625042: step 18952, loss 0.00014869, acc 1
2017-08-08T18:00:35.793471: step 18953, loss 3.96259e-05, acc 1
2017-08-08T18:00:35.986056: step 18954, loss 0.000265252, acc 1
2017-08-08T18:00:36.261313: step 18955, loss 1.695e-07, acc 1
2017-08-08T18:00:36.487813: step 18956, loss 6.41048e-05, acc 1
2017-08-08T18:00:36.691354: step 18957, loss 4.07494e-06, acc 1
2017-08-08T18:00:36.856427: step 18958, loss 4.35855e-07, acc 1
2017-08-08T18:00:37.167934: step 18959, loss 2.39343e-06, acc 1
2017-08-08T18:00:37.333572: step 18960, loss 0.00336814, acc 1
2017-08-08T18:00:37.509597: step 18961, loss 0.00138171, acc 1
2017-08-08T18:00:37.693325: step 18962, loss 2.52631e-05, acc 1
2017-08-08T18:00:37.969141: step 18963, loss 5.62509e-07, acc 1
2017-08-08T18:00:38.207826: step 18964, loss 1.52825e-05, acc 1
2017-08-08T18:00:38.386795: step 18965, loss 2.54136e-05, acc 1
2017-08-08T18:00:38.563709: step 18966, loss 0.00059816, acc 1
2017-08-08T18:00:38.784183: step 18967, loss 7.32002e-07, acc 1
2017-08-08T18:00:39.034536: step 18968, loss 4.34817e-05, acc 1
2017-08-08T18:00:39.258422: step 18969, loss 8.92902e-06, acc 1
2017-08-08T18:00:39.503559: step 18970, loss 3.1851e-07, acc 1
2017-08-08T18:00:39.717464: step 18971, loss 0.000251886, acc 1
2017-08-08T18:00:40.052459: step 18972, loss 0.000562408, acc 1
2017-08-08T18:00:40.363776: step 18973, loss 5.96046e-08, acc 1
2017-08-08T18:00:40.637358: step 18974, loss 5.63162e-06, acc 1
2017-08-08T18:00:41.046711: step 18975, loss 2.28083e-05, acc 1
2017-08-08T18:00:41.426613: step 18976, loss 7.63683e-08, acc 1
2017-08-08T18:00:41.787029: step 18977, loss 5.15642e-05, acc 1
2017-08-08T18:00:41.967953: step 18978, loss 1.22644e-05, acc 1
2017-08-08T18:00:42.204906: step 18979, loss 1.7695e-07, acc 1
2017-08-08T18:00:42.504522: step 18980, loss 2.84332e-05, acc 1
2017-08-08T18:00:42.732462: step 18981, loss 4.06206e-06, acc 1
2017-08-08T18:00:43.085385: step 18982, loss 9.2757e-07, acc 1
2017-08-08T18:00:43.402659: step 18983, loss 1.21025e-05, acc 1
2017-08-08T18:00:43.667021: step 18984, loss 2.94297e-07, acc 1
2017-08-08T18:00:44.116684: step 18985, loss 1.0412e-06, acc 1
2017-08-08T18:00:44.372012: step 18986, loss 2.98023e-08, acc 1
2017-08-08T18:00:44.767852: step 18987, loss 2.08796e-06, acc 1
2017-08-08T18:00:45.132174: step 18988, loss 2.3283e-07, acc 1
2017-08-08T18:00:45.481996: step 18989, loss 2.64292e-06, acc 1
2017-08-08T18:00:45.873415: step 18990, loss 4.28405e-07, acc 1
2017-08-08T18:00:46.250872: step 18991, loss 1.30385e-08, acc 1
2017-08-08T18:00:46.638636: step 18992, loss 8.73569e-07, acc 1
2017-08-08T18:00:47.000583: step 18993, loss 0.000267169, acc 1
2017-08-08T18:00:47.398432: step 18994, loss 3.70064e-06, acc 1
2017-08-08T18:00:47.642398: step 18995, loss 2.57211e-06, acc 1
2017-08-08T18:00:48.053406: step 18996, loss 2.74721e-06, acc 1
2017-08-08T18:00:48.278845: step 18997, loss 1.42115e-06, acc 1
2017-08-08T18:00:48.526501: step 18998, loss 1.88034e-05, acc 1
2017-08-08T18:00:48.907058: step 18999, loss 8.00489e-05, acc 1
2017-08-08T18:00:49.169512: step 19000, loss 0.00525956, acc 1

Evaluation:
2017-08-08T18:00:50.261479: step 19000, loss 6.25693, acc 0.702627

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-19000

2017-08-08T18:00:50.675498: step 19001, loss 7.18284e-06, acc 1
2017-08-08T18:00:50.887757: step 19002, loss 9.31322e-09, acc 1
2017-08-08T18:00:51.087601: step 19003, loss 2.42144e-08, acc 1
2017-08-08T18:00:51.320249: step 19004, loss 9.39806e-06, acc 1
2017-08-08T18:00:51.513959: step 19005, loss 5.72848e-06, acc 1
2017-08-08T18:00:51.703707: step 19006, loss 6.38088e-05, acc 1
2017-08-08T18:00:52.175184: step 19007, loss 0.00357745, acc 1
2017-08-08T18:00:52.479494: step 19008, loss 0.000872605, acc 1
2017-08-08T18:00:52.702156: step 19009, loss 6.06577e-06, acc 1
2017-08-08T18:00:52.933631: step 19010, loss 3.40862e-07, acc 1
2017-08-08T18:00:53.232960: step 19011, loss 0.00889498, acc 1
2017-08-08T18:00:53.553392: step 19012, loss 0.000118423, acc 1
2017-08-08T18:00:53.882856: step 19013, loss 7.37238e-05, acc 1
2017-08-08T18:00:54.161875: step 19014, loss 0.000103836, acc 1
2017-08-08T18:00:54.595215: step 19015, loss 1.93038e-05, acc 1
2017-08-08T18:00:54.885639: step 19016, loss 0.000460159, acc 1
2017-08-08T18:00:55.120659: step 19017, loss 3.17746e-06, acc 1
2017-08-08T18:00:55.336139: step 19018, loss 3.76251e-07, acc 1
2017-08-08T18:00:55.674593: step 19019, loss 0.000678264, acc 1
2017-08-08T18:00:55.933307: step 19020, loss 1.17994e-05, acc 1
2017-08-08T18:00:56.133316: step 19021, loss 1.39698e-07, acc 1
2017-08-08T18:00:56.335176: step 19022, loss 0.00301901, acc 1
2017-08-08T18:00:56.547988: step 19023, loss 5.95402e-06, acc 1
2017-08-08T18:00:56.863447: step 19024, loss 0.000133739, acc 1
2017-08-08T18:00:57.101480: step 19025, loss 2.42262e-05, acc 1
2017-08-08T18:00:57.340485: step 19026, loss 1.2036e-05, acc 1
2017-08-08T18:00:57.565385: step 19027, loss 2.98542e-05, acc 1
2017-08-08T18:00:57.976614: step 19028, loss 5.58794e-09, acc 1
2017-08-08T18:00:58.246838: step 19029, loss 2.96159e-07, acc 1
2017-08-08T18:00:58.508627: step 19030, loss 0.000559255, acc 1
2017-08-08T18:00:58.735535: step 19031, loss 7.26431e-08, acc 1
2017-08-08T18:00:59.109332: step 19032, loss 2.53318e-07, acc 1
2017-08-08T18:00:59.342963: step 19033, loss 1.89991e-05, acc 1
2017-08-08T18:00:59.561626: step 19034, loss 1.45286e-07, acc 1
2017-08-08T18:00:59.845329: step 19035, loss 2.0041e-06, acc 1
2017-08-08T18:01:00.156427: step 19036, loss 3.50175e-07, acc 1
2017-08-08T18:01:00.529376: step 19037, loss 2.79397e-08, acc 1
2017-08-08T18:01:00.842084: step 19038, loss 6.85198e-06, acc 1
2017-08-08T18:01:01.161120: step 19039, loss 0.00502575, acc 1
2017-08-08T18:01:01.535150: step 19040, loss 6.75643e-06, acc 1
2017-08-08T18:01:01.830292: step 19041, loss 4.17229e-07, acc 1
2017-08-08T18:01:02.075372: step 19042, loss 0.0002913, acc 1
2017-08-08T18:01:02.369383: step 19043, loss 6.79032e-06, acc 1
2017-08-08T18:01:02.809976: step 19044, loss 9.83733e-06, acc 1
2017-08-08T18:01:03.339393: step 19045, loss 1.07659e-06, acc 1
2017-08-08T18:01:03.840690: step 19046, loss 2.07354e-05, acc 1
2017-08-08T18:01:04.136820: step 19047, loss 6.54478e-06, acc 1
2017-08-08T18:01:04.430076: step 19048, loss 1.59808e-06, acc 1
2017-08-08T18:01:04.868010: step 19049, loss 5.75548e-07, acc 1
2017-08-08T18:01:05.134711: step 19050, loss 1.65499e-06, acc 1
2017-08-08T18:01:05.363817: step 19051, loss 3.68799e-07, acc 1
2017-08-08T18:01:05.642928: step 19052, loss 0, acc 1
2017-08-08T18:01:06.041371: step 19053, loss 1.86265e-09, acc 1
2017-08-08T18:01:06.435103: step 19054, loss 0.000133638, acc 1
2017-08-08T18:01:06.822815: step 19055, loss 2.7567e-07, acc 1
2017-08-08T18:01:07.092148: step 19056, loss 0.00108796, acc 1
2017-08-08T18:01:07.342492: step 19057, loss 5.58793e-08, acc 1
2017-08-08T18:01:07.724940: step 19058, loss 7.2643e-08, acc 1
2017-08-08T18:01:07.966926: step 19059, loss 2.74902e-06, acc 1
2017-08-08T18:01:08.174877: step 19060, loss 2.44006e-07, acc 1
2017-08-08T18:01:08.411060: step 19061, loss 0.000888873, acc 1
2017-08-08T18:01:08.681289: step 19062, loss 1.89989e-07, acc 1
2017-08-08T18:01:09.025711: step 19063, loss 1.39698e-07, acc 1
2017-08-08T18:01:09.374261: step 19064, loss 0.000325107, acc 1
2017-08-08T18:01:09.784021: step 19065, loss 1.03001e-06, acc 1
2017-08-08T18:01:09.969264: step 19066, loss 6.22111e-07, acc 1
2017-08-08T18:01:10.290411: step 19067, loss 0.00065832, acc 1
2017-08-08T18:01:10.571761: step 19068, loss 1.3783e-06, acc 1
2017-08-08T18:01:10.813914: step 19069, loss 3.78115e-07, acc 1
2017-08-08T18:01:11.013475: step 19070, loss 0.000590093, acc 1
2017-08-08T18:01:11.335978: step 19071, loss 3.48343e-05, acc 1
2017-08-08T18:01:11.691670: step 19072, loss 5.36392e-06, acc 1
2017-08-08T18:01:11.939572: step 19073, loss 8.10243e-07, acc 1
2017-08-08T18:01:12.213217: step 19074, loss 2.75833e-06, acc 1
2017-08-08T18:01:12.446820: step 19075, loss 0.00286023, acc 1
2017-08-08T18:01:12.739638: step 19076, loss 0.0126545, acc 0.984375
2017-08-08T18:01:13.020415: step 19077, loss 8.99991e-05, acc 1
2017-08-08T18:01:13.233892: step 19078, loss 2.73808e-07, acc 1
2017-08-08T18:01:13.425625: step 19079, loss 2.80497e-06, acc 1
2017-08-08T18:01:13.640690: step 19080, loss 3.0881e-06, acc 1
2017-08-08T18:01:14.013353: step 19081, loss 1.01139e-06, acc 1
2017-08-08T18:01:14.508230: step 19082, loss 1.67638e-07, acc 1
2017-08-08T18:01:14.813823: step 19083, loss 1.19108e-05, acc 1
2017-08-08T18:01:15.133379: step 19084, loss 0.00406173, acc 1
2017-08-08T18:01:15.451108: step 19085, loss 5.90453e-07, acc 1
2017-08-08T18:01:15.681432: step 19086, loss 2.80204e-05, acc 1
2017-08-08T18:01:15.970505: step 19087, loss 5.58918e-05, acc 1
2017-08-08T18:01:16.208969: step 19088, loss 3.80296e-05, acc 1
2017-08-08T18:01:16.701568: step 19089, loss 0.00305443, acc 1
2017-08-08T18:01:17.053773: step 19090, loss 0.000199809, acc 1
2017-08-08T18:01:17.340392: step 19091, loss 6.01711e-06, acc 1
2017-08-08T18:01:17.569928: step 19092, loss 1.6205e-07, acc 1
2017-08-08T18:01:17.999519: step 19093, loss 5.52564e-06, acc 1
2017-08-08T18:01:18.257258: step 19094, loss 4.58337e-05, acc 1
2017-08-08T18:01:18.432783: step 19095, loss 4.4258e-05, acc 1
2017-08-08T18:01:18.620683: step 19096, loss 2.32118e-05, acc 1
2017-08-08T18:01:18.903070: step 19097, loss 7.09658e-05, acc 1
2017-08-08T18:01:19.335021: step 19098, loss 0.000166574, acc 1
2017-08-08T18:01:19.737425: step 19099, loss 5.36434e-07, acc 1
2017-08-08T18:01:20.006238: step 19100, loss 9.05358e-05, acc 1

Evaluation:
2017-08-08T18:01:20.736733: step 19100, loss 6.27936, acc 0.714822

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-19100

2017-08-08T18:01:21.243777: step 19101, loss 2.23517e-08, acc 1
2017-08-08T18:01:21.521499: step 19102, loss 2.832e-05, acc 1
2017-08-08T18:01:21.970741: step 19103, loss 2.69318e-06, acc 1
2017-08-08T18:01:22.480075: step 19104, loss 2.46272e-05, acc 1
2017-08-08T18:01:22.824017: step 19105, loss 0.00263665, acc 1
2017-08-08T18:01:23.108896: step 19106, loss 5.56921e-07, acc 1
2017-08-08T18:01:23.490392: step 19107, loss 1.49012e-08, acc 1
2017-08-08T18:01:23.828550: step 19108, loss 6.59367e-07, acc 1
2017-08-08T18:01:24.046958: step 19109, loss 3.72529e-08, acc 1
2017-08-08T18:01:24.263603: step 19110, loss 0.000617048, acc 1
2017-08-08T18:01:24.546414: step 19111, loss 0.000243591, acc 1
2017-08-08T18:01:24.851739: step 19112, loss 1.43607e-06, acc 1
2017-08-08T18:01:25.151255: step 19113, loss 1.12724e-05, acc 1
2017-08-08T18:01:25.382009: step 19114, loss 0.0150749, acc 0.984375
2017-08-08T18:01:25.582456: step 19115, loss 5.58793e-08, acc 1
2017-08-08T18:01:25.751975: step 19116, loss 0.000155593, acc 1
2017-08-08T18:01:26.019832: step 19117, loss 8.33245e-05, acc 1
2017-08-08T18:01:26.252097: step 19118, loss 1.38575e-06, acc 1
2017-08-08T18:01:26.476493: step 19119, loss 0.00012121, acc 1
2017-08-08T18:01:26.703310: step 19120, loss 5.72335e-06, acc 1
2017-08-08T18:01:27.090912: step 19121, loss 7.33908e-06, acc 1
2017-08-08T18:01:27.310346: step 19122, loss 0.000353085, acc 1
2017-08-08T18:01:27.580526: step 19123, loss 1.88245e-05, acc 1
2017-08-08T18:01:27.805857: step 19124, loss 7.13839e-06, acc 1
2017-08-08T18:01:28.041321: step 19125, loss 0.000118148, acc 1
2017-08-08T18:01:28.341993: step 19126, loss 5.40167e-08, acc 1
2017-08-08T18:01:28.565529: step 19127, loss 3.09196e-07, acc 1
2017-08-08T18:01:28.833763: step 19128, loss 8.69833e-07, acc 1
2017-08-08T18:01:29.176210: step 19129, loss 1.40674e-05, acc 1
2017-08-08T18:01:29.451952: step 19130, loss 0.000160865, acc 1
2017-08-08T18:01:29.660293: step 19131, loss 1.24302e-05, acc 1
2017-08-08T18:01:29.853033: step 19132, loss 1.49012e-08, acc 1
2017-08-08T18:01:30.031971: step 19133, loss 1.76951e-07, acc 1
2017-08-08T18:01:30.318827: step 19134, loss 9.00892e-06, acc 1
2017-08-08T18:01:30.609591: step 19135, loss 2.57044e-07, acc 1
2017-08-08T18:01:30.841444: step 19136, loss 2.2084e-05, acc 1
2017-08-08T18:01:31.110219: step 19137, loss 7.45058e-09, acc 1
2017-08-08T18:01:31.453373: step 19138, loss 6.04877e-05, acc 1
2017-08-08T18:01:31.880877: step 19139, loss 0.00165564, acc 1
2017-08-08T18:01:32.235490: step 19140, loss 8.17681e-07, acc 1
2017-08-08T18:01:32.628227: step 19141, loss 2.03925e-05, acc 1
2017-08-08T18:01:32.894683: step 19142, loss 3.2672e-05, acc 1
2017-08-08T18:01:33.178140: step 19143, loss 0.000288999, acc 1
2017-08-08T18:01:33.522603: step 19144, loss 3.25763e-05, acc 1
2017-08-08T18:01:33.745006: step 19145, loss 0.00146757, acc 1
2017-08-08T18:01:34.007509: step 19146, loss 0.000165023, acc 1
2017-08-08T18:01:34.364220: step 19147, loss 3.53902e-08, acc 1
2017-08-08T18:01:34.747678: step 19148, loss 4.65661e-08, acc 1
2017-08-08T18:01:35.101411: step 19149, loss 1.59437e-06, acc 1
2017-08-08T18:01:35.403569: step 19150, loss 1.61113e-05, acc 1
2017-08-08T18:01:35.594085: step 19151, loss 1.35821e-05, acc 1
2017-08-08T18:01:35.852865: step 19152, loss 6.33298e-08, acc 1
2017-08-08T18:01:36.098552: step 19153, loss 1.0207e-06, acc 1
2017-08-08T18:01:36.315055: step 19154, loss 2.28536e-06, acc 1
2017-08-08T18:01:36.557387: step 19155, loss 7.17555e-05, acc 1
2017-08-08T18:01:36.965410: step 19156, loss 0.0054732, acc 1
2017-08-08T18:01:37.381368: step 19157, loss 8.10232e-07, acc 1
2017-08-08T18:01:37.677198: step 19158, loss 0.000648933, acc 1
2017-08-08T18:01:37.886547: step 19159, loss 6.30378e-06, acc 1
2017-08-08T18:01:38.191199: step 19160, loss 7.30146e-07, acc 1
2017-08-08T18:01:38.479407: step 19161, loss 8.49351e-07, acc 1
2017-08-08T18:01:38.782821: step 19162, loss 0.000729062, acc 1
2017-08-08T18:01:39.055847: step 19163, loss 1.98377e-05, acc 1
2017-08-08T18:01:39.407902: step 19164, loss 0.00473815, acc 1
2017-08-08T18:01:39.778840: step 19165, loss 1.86265e-09, acc 1
2017-08-08T18:01:40.158808: step 19166, loss 2.41662e-05, acc 1
2017-08-08T18:01:40.464947: step 19167, loss 0, acc 1
2017-08-08T18:01:40.694120: step 19168, loss 0.00102566, acc 1
2017-08-08T18:01:41.047187: step 19169, loss 2.6077e-08, acc 1
2017-08-08T18:01:41.286037: step 19170, loss 2.15496e-06, acc 1
2017-08-08T18:01:41.497258: step 19171, loss 0, acc 1
2017-08-08T18:01:41.780683: step 19172, loss 3.35249e-06, acc 1
2017-08-08T18:01:42.130958: step 19173, loss 0.000118028, acc 1
2017-08-08T18:01:42.433733: step 19174, loss 1.32618e-06, acc 1
2017-08-08T18:01:42.715542: step 19175, loss 1.20176e-05, acc 1
2017-08-08T18:01:42.950947: step 19176, loss 8.89324e-06, acc 1
2017-08-08T18:01:43.264321: step 19177, loss 0.000135959, acc 1
2017-08-08T18:01:43.536537: step 19178, loss 1.4156e-07, acc 1
2017-08-08T18:01:43.817393: step 19179, loss 4.38172e-05, acc 1
2017-08-08T18:01:44.043982: step 19180, loss 0.000437634, acc 1
2017-08-08T18:01:44.294131: step 19181, loss 2.04891e-08, acc 1
2017-08-08T18:01:44.545566: step 19182, loss 0.000283802, acc 1
2017-08-08T18:01:44.818894: step 19183, loss 6.86808e-06, acc 1
2017-08-08T18:01:45.029582: step 19184, loss 4.36404e-05, acc 1
2017-08-08T18:01:45.237316: step 19185, loss 2.73807e-07, acc 1
2017-08-08T18:01:45.671338: step 19186, loss 7.26034e-05, acc 1
2017-08-08T18:01:45.874945: step 19187, loss 4.59297e-06, acc 1
2017-08-08T18:01:46.076089: step 19188, loss 1.40254e-06, acc 1
2017-08-08T18:01:46.284917: step 19189, loss 1.46586e-06, acc 1
2017-08-08T18:01:46.548316: step 19190, loss 0.000184494, acc 1
2017-08-08T18:01:46.892534: step 19191, loss 2.92968e-06, acc 1
2017-08-08T18:01:47.195139: step 19192, loss 9.49947e-08, acc 1
2017-08-08T18:01:47.401062: step 19193, loss 4.84287e-08, acc 1
2017-08-08T18:01:47.692476: step 19194, loss 1.3411e-07, acc 1
2017-08-08T18:01:48.136226: step 19195, loss 7.38004e-06, acc 1
2017-08-08T18:01:48.374133: step 19196, loss 1.57623e-05, acc 1
2017-08-08T18:01:48.656614: step 19197, loss 5.21535e-07, acc 1
2017-08-08T18:01:48.881364: step 19198, loss 1.30026e-05, acc 1
2017-08-08T18:01:49.196510: step 19199, loss 3.72529e-09, acc 1
2017-08-08T18:01:49.478226: step 19200, loss 0.000336655, acc 1

Evaluation:
2017-08-08T18:01:49.907105: step 19200, loss 6.42168, acc 0.713884

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-19200

2017-08-08T18:01:50.385326: step 19201, loss 2.20114e-05, acc 1
2017-08-08T18:01:50.591784: step 19202, loss 3.48424e-05, acc 1
2017-08-08T18:01:50.811270: step 19203, loss 1.13059e-06, acc 1
2017-08-08T18:01:51.017285: step 19204, loss 0.000130484, acc 1
2017-08-08T18:01:51.400682: step 19205, loss 8.72965e-06, acc 1
2017-08-08T18:01:51.737131: step 19206, loss 5.60342e-05, acc 1
2017-08-08T18:01:51.993907: step 19207, loss 2.13217e-05, acc 1
2017-08-08T18:01:52.287709: step 19208, loss 1.86067e-06, acc 1
2017-08-08T18:01:52.544954: step 19209, loss 5.02913e-08, acc 1
2017-08-08T18:01:52.709311: step 19210, loss 0.000586786, acc 1
2017-08-08T18:01:52.914092: step 19211, loss 0.00938778, acc 1
2017-08-08T18:01:53.144685: step 19212, loss 1.32803e-06, acc 1
2017-08-08T18:01:53.470809: step 19213, loss 7.20874e-06, acc 1
2017-08-08T18:01:53.800245: step 19214, loss 0.000199648, acc 1
2017-08-08T18:01:53.995725: step 19215, loss 3.27985e-05, acc 1
2017-08-08T18:01:54.193361: step 19216, loss 3.59489e-07, acc 1
2017-08-08T18:01:54.559874: step 19217, loss 1.0617e-07, acc 1
2017-08-08T18:01:54.754821: step 19218, loss 0.000573754, acc 1
2017-08-08T18:01:55.013677: step 19219, loss 3.21298e-05, acc 1
2017-08-08T18:01:55.297336: step 19220, loss 2.05439e-06, acc 1
2017-08-08T18:01:55.740612: step 19221, loss 5.58794e-09, acc 1
2017-08-08T18:01:56.125836: step 19222, loss 1.65775e-07, acc 1
2017-08-08T18:01:56.361342: step 19223, loss 2.32351e-05, acc 1
2017-08-08T18:01:56.640614: step 19224, loss 7.39453e-07, acc 1
2017-08-08T18:01:56.853495: step 19225, loss 5.58793e-09, acc 1
2017-08-08T18:01:57.240888: step 19226, loss 1.9631e-06, acc 1
2017-08-08T18:01:57.481833: step 19227, loss 2.18964e-05, acc 1
2017-08-08T18:01:57.760028: step 19228, loss 4.83309e-06, acc 1
2017-08-08T18:01:58.058729: step 19229, loss 9.61526e-06, acc 1
2017-08-08T18:01:58.389336: step 19230, loss 0.00878501, acc 1
2017-08-08T18:01:58.653331: step 19231, loss 0.000257152, acc 1
2017-08-08T18:01:58.866392: step 19232, loss 1.69123e-06, acc 1
2017-08-08T18:01:59.058562: step 19233, loss 2.84931e-05, acc 1
2017-08-08T18:01:59.312137: step 19234, loss 2.79395e-07, acc 1
2017-08-08T18:01:59.592407: step 19235, loss 3.49792e-06, acc 1
2017-08-08T18:01:59.819303: step 19236, loss 5.79274e-07, acc 1
2017-08-08T18:02:00.069375: step 19237, loss 2.14196e-06, acc 1
2017-08-08T18:02:00.432648: step 19238, loss 7.51512e-05, acc 1
2017-08-08T18:02:00.932770: step 19239, loss 0.000296854, acc 1
2017-08-08T18:02:01.289801: step 19240, loss 6.62007e-05, acc 1
2017-08-08T18:02:01.542311: step 19241, loss 7.45058e-09, acc 1
2017-08-08T18:02:01.847747: step 19242, loss 0.000107592, acc 1
2017-08-08T18:02:02.207332: step 19243, loss 0.000349845, acc 1
2017-08-08T18:02:02.486485: step 19244, loss 2.45209e-05, acc 1
2017-08-08T18:02:02.801286: step 19245, loss 1.38947e-06, acc 1
2017-08-08T18:02:03.217559: step 19246, loss 7.89742e-07, acc 1
2017-08-08T18:02:03.701761: step 19247, loss 2.70082e-07, acc 1
2017-08-08T18:02:04.031741: step 19248, loss 1.30385e-08, acc 1
2017-08-08T18:02:04.301697: step 19249, loss 8.22142e-06, acc 1
2017-08-08T18:02:04.518458: step 19250, loss 1.41201e-05, acc 1
2017-08-08T18:02:04.849046: step 19251, loss 9.31322e-09, acc 1
2017-08-08T18:02:05.263084: step 19252, loss 2.70081e-07, acc 1
2017-08-08T18:02:05.513870: step 19253, loss 6.5005e-07, acc 1
2017-08-08T18:02:05.776916: step 19254, loss 6.3515e-07, acc 1
2017-08-08T18:02:06.009392: step 19255, loss 5.77409e-07, acc 1
2017-08-08T18:02:06.387882: step 19256, loss 4.39658e-05, acc 1
2017-08-08T18:02:06.773496: step 19257, loss 2.38417e-07, acc 1
2017-08-08T18:02:07.168151: step 19258, loss 0.00876292, acc 1
2017-08-08T18:02:07.368874: step 19259, loss 5.40166e-08, acc 1
2017-08-08T18:02:07.556805: step 19260, loss 0.000135855, acc 1
2017-08-08T18:02:07.918086: step 19261, loss 5.08495e-07, acc 1
2017-08-08T18:02:08.225565: step 19262, loss 5.19674e-07, acc 1
2017-08-08T18:02:08.491821: step 19263, loss 1.77315e-06, acc 1
2017-08-08T18:02:08.689641: step 19264, loss 1.35968e-06, acc 1
2017-08-08T18:02:08.998442: step 19265, loss 9.49948e-08, acc 1
2017-08-08T18:02:09.376627: step 19266, loss 1.83834e-06, acc 1
2017-08-08T18:02:09.710600: step 19267, loss 3.72529e-09, acc 1
2017-08-08T18:02:09.948435: step 19268, loss 2.04878e-06, acc 1
2017-08-08T18:02:10.272328: step 19269, loss 0, acc 1
2017-08-08T18:02:10.453143: step 19270, loss 0.00024366, acc 1
2017-08-08T18:02:10.626976: step 19271, loss 0, acc 1
2017-08-08T18:02:10.801049: step 19272, loss 1.91851e-07, acc 1
2017-08-08T18:02:11.061300: step 19273, loss 1.74219e-05, acc 1
2017-08-08T18:02:11.313341: step 19274, loss 5.56719e-05, acc 1
2017-08-08T18:02:11.523448: step 19275, loss 2.99885e-07, acc 1
2017-08-08T18:02:11.728317: step 19276, loss 2.16238e-06, acc 1
2017-08-08T18:02:11.989319: step 19277, loss 2.22757e-06, acc 1
2017-08-08T18:02:12.230047: step 19278, loss 0.0001389, acc 1
2017-08-08T18:02:12.425532: step 19279, loss 3.87106e-05, acc 1
2017-08-08T18:02:12.661573: step 19280, loss 2.60769e-07, acc 1
2017-08-08T18:02:12.859316: step 19281, loss 1.67638e-08, acc 1
2017-08-08T18:02:13.171401: step 19282, loss 2.86845e-07, acc 1
2017-08-08T18:02:13.527573: step 19283, loss 5.58793e-08, acc 1
2017-08-08T18:02:13.855712: step 19284, loss 1.18274e-06, acc 1
2017-08-08T18:02:14.057403: step 19285, loss 4.28404e-07, acc 1
2017-08-08T18:02:14.326968: step 19286, loss 2.16238e-06, acc 1
2017-08-08T18:02:14.617865: step 19287, loss 1.67638e-08, acc 1
2017-08-08T18:02:14.836236: step 19288, loss 1.37835e-07, acc 1
2017-08-08T18:02:15.067593: step 19289, loss 3.37863e-06, acc 1
2017-08-08T18:02:15.389409: step 19290, loss 2.68219e-07, acc 1
2017-08-08T18:02:15.689180: step 19291, loss 2.34692e-07, acc 1
2017-08-08T18:02:15.946644: step 19292, loss 3.5295e-06, acc 1
2017-08-08T18:02:16.118456: step 19293, loss 4.00464e-07, acc 1
2017-08-08T18:02:16.329566: step 19294, loss 3.35439e-06, acc 1
2017-08-08T18:02:16.622534: step 19295, loss 2.80862e-06, acc 1
2017-08-08T18:02:16.852491: step 19296, loss 3.72529e-09, acc 1
2017-08-08T18:02:17.051553: step 19297, loss 9.66701e-07, acc 1
2017-08-08T18:02:17.260874: step 19298, loss 2.859e-06, acc 1
2017-08-08T18:02:17.450999: step 19299, loss 1.9762e-06, acc 1
2017-08-08T18:02:17.758594: step 19300, loss 6.13302e-06, acc 1

Evaluation:
2017-08-08T18:02:18.400284: step 19300, loss 6.37981, acc 0.712946

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-19300

2017-08-08T18:02:18.709880: step 19301, loss 3.37119e-06, acc 1
2017-08-08T18:02:18.954051: step 19302, loss 1.77314e-06, acc 1
2017-08-08T18:02:19.226468: step 19303, loss 0, acc 1
2017-08-08T18:02:19.421249: step 19304, loss 6.1752e-05, acc 1
2017-08-08T18:02:19.613341: step 19305, loss 1.39698e-07, acc 1
2017-08-08T18:02:19.799413: step 19306, loss 2.42144e-08, acc 1
2017-08-08T18:02:20.047086: step 19307, loss 1.53296e-05, acc 1
2017-08-08T18:02:20.333911: step 19308, loss 0.00402021, acc 1
2017-08-08T18:02:20.708539: step 19309, loss 0.000100753, acc 1
2017-08-08T18:02:20.942418: step 19310, loss 1.01314e-05, acc 1
2017-08-08T18:02:21.276183: step 19311, loss 4.39284e-05, acc 1
2017-08-08T18:02:21.508294: step 19312, loss 1.49012e-08, acc 1
2017-08-08T18:02:21.705192: step 19313, loss 2.70954e-05, acc 1
2017-08-08T18:02:21.881195: step 19314, loss 4.24818e-06, acc 1
2017-08-08T18:02:22.189322: step 19315, loss 5.2712e-07, acc 1
2017-08-08T18:02:22.420512: step 19316, loss 0.000263869, acc 1
2017-08-08T18:02:22.694633: step 19317, loss 9.63455e-06, acc 1
2017-08-08T18:02:22.884989: step 19318, loss 0.000183897, acc 1
2017-08-08T18:02:23.073447: step 19319, loss 4.63792e-07, acc 1
2017-08-08T18:02:23.429828: step 19320, loss 1.24979e-06, acc 1
2017-08-08T18:02:23.638939: step 19321, loss 6.98035e-05, acc 1
2017-08-08T18:02:23.868339: step 19322, loss 3.483e-06, acc 1
2017-08-08T18:02:24.083447: step 19323, loss 5.58793e-08, acc 1
2017-08-08T18:02:24.416069: step 19324, loss 6.51924e-08, acc 1
2017-08-08T18:02:24.746152: step 19325, loss 8.43765e-07, acc 1
2017-08-08T18:02:24.991063: step 19326, loss 3.66392e-05, acc 1
2017-08-08T18:02:25.211950: step 19327, loss 0.000536255, acc 1
2017-08-08T18:02:25.553377: step 19328, loss 0.000398771, acc 1
2017-08-08T18:02:25.799009: step 19329, loss 4.23159e-06, acc 1
2017-08-08T18:02:26.037474: step 19330, loss 2.88708e-07, acc 1
2017-08-08T18:02:26.290245: step 19331, loss 1.83275e-06, acc 1
2017-08-08T18:02:26.537445: step 19332, loss 4.84802e-06, acc 1
2017-08-08T18:02:26.874956: step 19333, loss 1.37832e-06, acc 1
2017-08-08T18:02:27.142276: step 19334, loss 9.84932e-05, acc 1
2017-08-08T18:02:27.328267: step 19335, loss 1.95577e-07, acc 1
2017-08-08T18:02:27.637827: step 19336, loss 0.000307037, acc 1
2017-08-08T18:02:27.919180: step 19337, loss 0.0451408, acc 0.984375
2017-08-08T18:02:28.131791: step 19338, loss 6.32829e-06, acc 1
2017-08-08T18:02:28.377979: step 19339, loss 0.000685738, acc 1
2017-08-08T18:02:28.629480: step 19340, loss 0.000976513, acc 1
2017-08-08T18:02:28.951360: step 19341, loss 0, acc 1
2017-08-08T18:02:29.207582: step 19342, loss 1.86265e-09, acc 1
2017-08-08T18:02:29.430328: step 19343, loss 9.39436e-06, acc 1
2017-08-08T18:02:29.697475: step 19344, loss 3.78638e-06, acc 1
2017-08-08T18:02:30.172137: step 19345, loss 3.01918e-06, acc 1
2017-08-08T18:02:30.381858: step 19346, loss 3.50898e-06, acc 1
2017-08-08T18:02:30.596496: step 19347, loss 3.35276e-08, acc 1
2017-08-08T18:02:30.860588: step 19348, loss 7.98412e-06, acc 1
2017-08-08T18:02:31.179178: step 19349, loss 2.16401e-05, acc 1
2017-08-08T18:02:31.405123: step 19350, loss 1.11262e-07, acc 1
2017-08-08T18:02:31.627602: step 19351, loss 5.58793e-08, acc 1
2017-08-08T18:02:31.829492: step 19352, loss 1.97167e-05, acc 1
2017-08-08T18:02:32.157393: step 19353, loss 2.98023e-08, acc 1
2017-08-08T18:02:32.389952: step 19354, loss 0.000218264, acc 1
2017-08-08T18:02:32.654432: step 19355, loss 5.58793e-08, acc 1
2017-08-08T18:02:32.933637: step 19356, loss 6.14672e-08, acc 1
2017-08-08T18:02:33.253330: step 19357, loss 0.000363066, acc 1
2017-08-08T18:02:33.570570: step 19358, loss 5.58789e-07, acc 1
2017-08-08T18:02:33.801461: step 19359, loss 5.40167e-08, acc 1
2017-08-08T18:02:34.012534: step 19360, loss 0.000564381, acc 1
2017-08-08T18:02:34.218693: step 19361, loss 5.96046e-08, acc 1
2017-08-08T18:02:34.657633: step 19362, loss 6.76e-06, acc 1
2017-08-08T18:02:34.894557: step 19363, loss 1.13621e-07, acc 1
2017-08-08T18:02:35.146237: step 19364, loss 1.56461e-07, acc 1
2017-08-08T18:02:35.347907: step 19365, loss 7.75473e-06, acc 1
2017-08-08T18:02:35.716424: step 19366, loss 3.2947e-06, acc 1
2017-08-08T18:02:36.135877: step 19367, loss 0, acc 1
2017-08-08T18:02:36.461362: step 19368, loss 1.53128e-05, acc 1
2017-08-08T18:02:36.773574: step 19369, loss 7.00573e-06, acc 1
2017-08-08T18:02:37.005023: step 19370, loss 1.2501e-05, acc 1
2017-08-08T18:02:37.404579: step 19371, loss 1.09159e-05, acc 1
2017-08-08T18:02:37.648328: step 19372, loss 0.000274297, acc 1
2017-08-08T18:02:37.891808: step 19373, loss 4.65661e-08, acc 1
2017-08-08T18:02:38.181715: step 19374, loss 9.31322e-09, acc 1
2017-08-08T18:02:38.603237: step 19375, loss 0, acc 1
2017-08-08T18:02:38.959220: step 19376, loss 1.86265e-09, acc 1
2017-08-08T18:02:39.271437: step 19377, loss 5.01044e-07, acc 1
2017-08-08T18:02:39.483998: step 19378, loss 2.07859e-06, acc 1
2017-08-08T18:02:39.703419: step 19379, loss 3.98954e-05, acc 1
2017-08-08T18:02:40.132446: step 19380, loss 0, acc 1
2017-08-08T18:02:40.313474: step 19381, loss 1.51988e-06, acc 1
2017-08-08T18:02:40.548802: step 19382, loss 9.72271e-07, acc 1
2017-08-08T18:02:40.783665: step 19383, loss 0, acc 1
2017-08-08T18:02:41.161775: step 19384, loss 0.000268819, acc 1
2017-08-08T18:02:41.426365: step 19385, loss 9.18974e-06, acc 1
2017-08-08T18:02:41.677369: step 19386, loss 7.65763e-05, acc 1
2017-08-08T18:02:41.906456: step 19387, loss 1.49011e-07, acc 1
2017-08-08T18:02:42.189351: step 19388, loss 2.52927e-06, acc 1
2017-08-08T18:02:42.461695: step 19389, loss 0.000137706, acc 1
2017-08-08T18:02:42.672508: step 19390, loss 5.46986e-06, acc 1
2017-08-08T18:02:42.880857: step 19391, loss 2.32265e-06, acc 1
2017-08-08T18:02:43.053838: step 19392, loss 3.50174e-07, acc 1
2017-08-08T18:02:43.428365: step 19393, loss 1.53456e-05, acc 1
2017-08-08T18:02:43.809369: step 19394, loss 5.40665e-06, acc 1
2017-08-08T18:02:44.089913: step 19395, loss 5.58793e-09, acc 1
2017-08-08T18:02:44.350781: step 19396, loss 8.36308e-07, acc 1
2017-08-08T18:02:44.584423: step 19397, loss 1.62788e-06, acc 1
2017-08-08T18:02:45.023703: step 19398, loss 5.75547e-07, acc 1
2017-08-08T18:02:45.351074: step 19399, loss 1.4547e-06, acc 1
2017-08-08T18:02:45.573178: step 19400, loss 3.72529e-09, acc 1

Evaluation:
2017-08-08T18:02:46.182964: step 19400, loss 6.37147, acc 0.709193

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-19400

2017-08-08T18:02:46.681313: step 19401, loss 3.72529e-09, acc 1
2017-08-08T18:02:46.947988: step 19402, loss 2.67826e-06, acc 1
2017-08-08T18:02:47.128305: step 19403, loss 1.0617e-07, acc 1
2017-08-08T18:02:47.339838: step 19404, loss 1.39698e-07, acc 1
2017-08-08T18:02:47.701156: step 19405, loss 8.94068e-08, acc 1
2017-08-08T18:02:47.914130: step 19406, loss 2.91272e-05, acc 1
2017-08-08T18:02:48.155906: step 19407, loss 3.72529e-08, acc 1
2017-08-08T18:02:48.416785: step 19408, loss 7.45058e-09, acc 1
2017-08-08T18:02:48.698391: step 19409, loss 1.695e-07, acc 1
2017-08-08T18:02:49.012891: step 19410, loss 0.00430352, acc 1
2017-08-08T18:02:49.198305: step 19411, loss 0.000320666, acc 1
2017-08-08T18:02:49.488964: step 19412, loss 0.000215413, acc 1
2017-08-08T18:02:49.771620: step 19413, loss 2.81257e-07, acc 1
2017-08-08T18:02:50.008806: step 19414, loss 4.64476e-06, acc 1
2017-08-08T18:02:50.231055: step 19415, loss 2.10509e-05, acc 1
2017-08-08T18:02:50.464611: step 19416, loss 1.01274e-05, acc 1
2017-08-08T18:02:50.818895: step 19417, loss 0.00223881, acc 1
2017-08-08T18:02:51.150127: step 19418, loss 5.43655e-06, acc 1
2017-08-08T18:02:51.422134: step 19419, loss 5.02913e-08, acc 1
2017-08-08T18:02:51.730359: step 19420, loss 2.8012e-05, acc 1
2017-08-08T18:02:51.978602: step 19421, loss 3.72529e-09, acc 1
2017-08-08T18:02:52.361387: step 19422, loss 1.42303e-06, acc 1
2017-08-08T18:02:52.647094: step 19423, loss 3.72529e-08, acc 1
2017-08-08T18:02:52.961813: step 19424, loss 3.68761e-06, acc 1
2017-08-08T18:02:53.251752: step 19425, loss 6.35152e-07, acc 1
2017-08-08T18:02:53.617395: step 19426, loss 0.000126982, acc 1
2017-08-08T18:02:54.047101: step 19427, loss 2.79168e-05, acc 1
2017-08-08T18:02:54.482462: step 19428, loss 9.08057e-06, acc 1
2017-08-08T18:02:54.764225: step 19429, loss 3.96507e-06, acc 1
2017-08-08T18:02:55.074435: step 19430, loss 2.71945e-07, acc 1
2017-08-08T18:02:55.489244: step 19431, loss 2.73806e-07, acc 1
2017-08-08T18:02:55.741022: step 19432, loss 1.09896e-07, acc 1
2017-08-08T18:02:56.037051: step 19433, loss 2.47238e-05, acc 1
2017-08-08T18:02:56.332344: step 19434, loss 3.17383e-06, acc 1
2017-08-08T18:02:56.693408: step 19435, loss 1.70861e-05, acc 1
2017-08-08T18:02:56.973344: step 19436, loss 2.4028e-07, acc 1
2017-08-08T18:02:57.267020: step 19437, loss 1.49051e-05, acc 1
2017-08-08T18:02:57.467067: step 19438, loss 1.86265e-09, acc 1
2017-08-08T18:02:57.768752: step 19439, loss 3.93326e-05, acc 1
2017-08-08T18:02:58.039924: step 19440, loss 9.31322e-09, acc 1
2017-08-08T18:02:58.230117: step 19441, loss 7.2643e-08, acc 1
2017-08-08T18:02:58.441508: step 19442, loss 6.45091e-06, acc 1
2017-08-08T18:02:58.825454: step 19443, loss 0, acc 1
2017-08-08T18:02:59.081388: step 19444, loss 1.00583e-07, acc 1
2017-08-08T18:02:59.297038: step 19445, loss 2.42384e-05, acc 1
2017-08-08T18:02:59.476916: step 19446, loss 1.57822e-05, acc 1
2017-08-08T18:02:59.681731: step 19447, loss 4.44374e-06, acc 1
2017-08-08T18:03:00.017434: step 19448, loss 2.02119e-05, acc 1
2017-08-08T18:03:00.202184: step 19449, loss 2.49592e-07, acc 1
2017-08-08T18:03:00.430382: step 19450, loss 1.89726e-05, acc 1
2017-08-08T18:03:00.635630: step 19451, loss 0.000483122, acc 1
2017-08-08T18:03:00.953374: step 19452, loss 2.51455e-07, acc 1
2017-08-08T18:03:01.258270: step 19453, loss 3.60945e-06, acc 1
2017-08-08T18:03:01.550199: step 19454, loss 2.91113e-06, acc 1
2017-08-08T18:03:01.757301: step 19455, loss 1.38577e-06, acc 1
2017-08-08T18:03:02.015845: step 19456, loss 2.65032e-06, acc 1
2017-08-08T18:03:02.390676: step 19457, loss 8.48126e-06, acc 1
2017-08-08T18:03:02.675271: step 19458, loss 2.25794e-05, acc 1
2017-08-08T18:03:02.990659: step 19459, loss 5.2154e-08, acc 1
2017-08-08T18:03:03.267826: step 19460, loss 0.00104636, acc 1
2017-08-08T18:03:03.646302: step 19461, loss 1.27588e-06, acc 1
2017-08-08T18:03:04.079725: step 19462, loss 3.72529e-09, acc 1
2017-08-08T18:03:04.368077: step 19463, loss 3.72529e-09, acc 1
2017-08-08T18:03:04.609381: step 19464, loss 0, acc 1
2017-08-08T18:03:04.842290: step 19465, loss 3.20213e-05, acc 1
2017-08-08T18:03:05.283748: step 19466, loss 0.0025756, acc 1
2017-08-08T18:03:05.565488: step 19467, loss 0.000957601, acc 1
2017-08-08T18:03:05.813186: step 19468, loss 1.57343e-05, acc 1
2017-08-08T18:03:06.072997: step 19469, loss 7.0034e-07, acc 1
2017-08-08T18:03:06.371255: step 19470, loss 1.06396e-05, acc 1
2017-08-08T18:03:06.651604: step 19471, loss 2.49593e-07, acc 1
2017-08-08T18:03:06.887780: step 19472, loss 7.8788e-07, acc 1
2017-08-08T18:03:07.146928: step 19473, loss 1.67638e-08, acc 1
2017-08-08T18:03:07.432289: step 19474, loss 1.06726e-06, acc 1
2017-08-08T18:03:07.753330: step 19475, loss 0.000479282, acc 1
2017-08-08T18:03:08.148323: step 19476, loss 0.000108809, acc 1
2017-08-08T18:03:08.343012: step 19477, loss 8.15629e-06, acc 1
2017-08-08T18:03:08.557564: step 19478, loss 2.29275e-06, acc 1
2017-08-08T18:03:08.751148: step 19479, loss 3.1106e-07, acc 1
2017-08-08T18:03:09.131245: step 19480, loss 1.59175e-05, acc 1
2017-08-08T18:03:09.461323: step 19481, loss 4.5164e-06, acc 1
2017-08-08T18:03:09.717757: step 19482, loss 1.86265e-09, acc 1
2017-08-08T18:03:09.895673: step 19483, loss 0.0875362, acc 0.984375
2017-08-08T18:03:10.145501: step 19484, loss 0.000354604, acc 1
2017-08-08T18:03:10.451980: step 19485, loss 3.3369e-05, acc 1
2017-08-08T18:03:10.651833: step 19486, loss 1.0058e-06, acc 1
2017-08-08T18:03:10.858560: step 19487, loss 0, acc 1
2017-08-08T18:03:11.172825: step 19488, loss 9.31322e-09, acc 1
2017-08-08T18:03:11.423636: step 19489, loss 1.93138e-05, acc 1
2017-08-08T18:03:11.660228: step 19490, loss 3.72529e-09, acc 1
2017-08-08T18:03:11.838522: step 19491, loss 5.01902e-06, acc 1
2017-08-08T18:03:12.033372: step 19492, loss 3.24098e-07, acc 1
2017-08-08T18:03:12.337036: step 19493, loss 0.00164118, acc 1
2017-08-08T18:03:12.550385: step 19494, loss 1.2666e-07, acc 1
2017-08-08T18:03:12.746127: step 19495, loss 1.11759e-08, acc 1
2017-08-08T18:03:12.958800: step 19496, loss 5.65785e-06, acc 1
2017-08-08T18:03:13.324957: step 19497, loss 4.41441e-07, acc 1
2017-08-08T18:03:13.604913: step 19498, loss 7.5419e-06, acc 1
2017-08-08T18:03:13.869638: step 19499, loss 5.08314e-05, acc 1
2017-08-08T18:03:14.125490: step 19500, loss 2.03836e-06, acc 1

Evaluation:
2017-08-08T18:03:14.984219: step 19500, loss 6.71652, acc 0.701689

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-19500

2017-08-08T18:03:15.350405: step 19501, loss 1.44134e-05, acc 1
2017-08-08T18:03:15.657193: step 19502, loss 0.000246068, acc 1
2017-08-08T18:03:15.991254: step 19503, loss 5.02914e-08, acc 1
2017-08-08T18:03:16.313369: step 19504, loss 2.04891e-08, acc 1
2017-08-08T18:03:16.511333: step 19505, loss 2.08975e-06, acc 1
2017-08-08T18:03:16.685341: step 19506, loss 0.000103408, acc 1
2017-08-08T18:03:16.960552: step 19507, loss 0.0382976, acc 0.984375
2017-08-08T18:03:17.131636: step 19508, loss 7.93467e-07, acc 1
2017-08-08T18:03:17.337333: step 19509, loss 2.78794e-05, acc 1
2017-08-08T18:03:17.558383: step 19510, loss 5.90447e-07, acc 1
2017-08-08T18:03:17.881560: step 19511, loss 4.09338e-05, acc 1
2017-08-08T18:03:18.254693: step 19512, loss 0.000162823, acc 1
2017-08-08T18:03:18.538183: step 19513, loss 5.70648e-06, acc 1
2017-08-08T18:03:18.789581: step 19514, loss 4.71739e-06, acc 1
2017-08-08T18:03:19.160580: step 19515, loss 5.86724e-07, acc 1
2017-08-08T18:03:19.412488: step 19516, loss 1.64835e-06, acc 1
2017-08-08T18:03:19.620900: step 19517, loss 5.58794e-09, acc 1
2017-08-08T18:03:19.785361: step 19518, loss 7.57727e-06, acc 1
2017-08-08T18:03:19.993415: step 19519, loss 6.91161e-06, acc 1
2017-08-08T18:03:20.244577: step 19520, loss 8.83128e-06, acc 1
2017-08-08T18:03:20.505318: step 19521, loss 1.15484e-07, acc 1
2017-08-08T18:03:20.705102: step 19522, loss 1.50874e-07, acc 1
2017-08-08T18:03:20.912678: step 19523, loss 0.000540419, acc 1
2017-08-08T18:03:21.190830: step 19524, loss 1.86265e-09, acc 1
2017-08-08T18:03:21.487449: step 19525, loss 3.12922e-07, acc 1
2017-08-08T18:03:21.688262: step 19526, loss 3.44587e-07, acc 1
2017-08-08T18:03:21.885769: step 19527, loss 1.86265e-09, acc 1
2017-08-08T18:03:22.128160: step 19528, loss 3.32638e-06, acc 1
2017-08-08T18:03:22.485453: step 19529, loss 0.000961271, acc 1
2017-08-08T18:03:22.919173: step 19530, loss 3.72527e-07, acc 1
2017-08-08T18:03:23.233814: step 19531, loss 2.82478e-05, acc 1
2017-08-08T18:03:23.486652: step 19532, loss 1.45982e-05, acc 1
2017-08-08T18:03:23.837714: step 19533, loss 3.1085e-06, acc 1
2017-08-08T18:03:24.211381: step 19534, loss 1.02445e-07, acc 1
2017-08-08T18:03:24.493604: step 19535, loss 9.31322e-09, acc 1
2017-08-08T18:03:24.796741: step 19536, loss 0.000122659, acc 1
2017-08-08T18:03:25.185374: step 19537, loss 0.000232088, acc 1
2017-08-08T18:03:25.624468: step 19538, loss 1.49012e-08, acc 1
2017-08-08T18:03:26.032654: step 19539, loss 4.62815e-06, acc 1
2017-08-08T18:03:26.267655: step 19540, loss 5.75547e-07, acc 1
2017-08-08T18:03:26.501340: step 19541, loss 3.48462e-06, acc 1
2017-08-08T18:03:26.850152: step 19542, loss 4.62806e-05, acc 1
2017-08-08T18:03:27.061930: step 19543, loss 1.49012e-08, acc 1
2017-08-08T18:03:27.265157: step 19544, loss 1.24797e-07, acc 1
2017-08-08T18:03:27.497785: step 19545, loss 4.05653e-05, acc 1
2017-08-08T18:03:27.866631: step 19546, loss 2.49594e-07, acc 1
2017-08-08T18:03:28.243117: step 19547, loss 1.11759e-08, acc 1
2017-08-08T18:03:28.459475: step 19548, loss 0.0190502, acc 0.984375
2017-08-08T18:03:28.652190: step 19549, loss 1.69453e-05, acc 1
2017-08-08T18:03:29.089992: step 19550, loss 2.98023e-08, acc 1
2017-08-08T18:03:29.373545: step 19551, loss 0.000692969, acc 1
2017-08-08T18:03:29.674669: step 19552, loss 1.93518e-06, acc 1
2017-08-08T18:03:29.975841: step 19553, loss 1.19797e-05, acc 1
2017-08-08T18:03:30.409153: step 19554, loss 1.24979e-06, acc 1
2017-08-08T18:03:30.852842: step 19555, loss 0.00146144, acc 1
2017-08-08T18:03:31.208372: step 19556, loss 1.13621e-07, acc 1
2017-08-08T18:03:31.469746: step 19557, loss 3.89289e-07, acc 1
2017-08-08T18:03:31.667901: step 19558, loss 0.00254187, acc 1
2017-08-08T18:03:31.965311: step 19559, loss 3.72529e-09, acc 1
2017-08-08T18:03:32.253990: step 19560, loss 5.08494e-07, acc 1
2017-08-08T18:03:32.496944: step 19561, loss 0, acc 1
2017-08-08T18:03:32.748558: step 19562, loss 1.75088e-07, acc 1
2017-08-08T18:03:33.009111: step 19563, loss 1.7287e-05, acc 1
2017-08-08T18:03:33.432259: step 19564, loss 8.19026e-06, acc 1
2017-08-08T18:03:33.821901: step 19565, loss 4.23143e-06, acc 1
2017-08-08T18:03:34.110544: step 19566, loss 1.5273e-06, acc 1
2017-08-08T18:03:34.360464: step 19567, loss 4.06052e-07, acc 1
2017-08-08T18:03:34.681518: step 19568, loss 4.26312e-06, acc 1
2017-08-08T18:03:34.976909: step 19569, loss 2.39339e-06, acc 1
2017-08-08T18:03:35.238994: step 19570, loss 5.81135e-07, acc 1
2017-08-08T18:03:35.505943: step 19571, loss 1.75088e-07, acc 1
2017-08-08T18:03:35.901378: step 19572, loss 4.1644e-06, acc 1
2017-08-08T18:03:36.289388: step 19573, loss 0.0017337, acc 1
2017-08-08T18:03:36.656203: step 19574, loss 2.92434e-07, acc 1
2017-08-08T18:03:36.876433: step 19575, loss 2.23517e-08, acc 1
2017-08-08T18:03:37.079353: step 19576, loss 1.34239e-05, acc 1
2017-08-08T18:03:37.381313: step 19577, loss 1.49011e-07, acc 1
2017-08-08T18:03:37.599390: step 19578, loss 3.44586e-07, acc 1
2017-08-08T18:03:37.851980: step 19579, loss 0.000133569, acc 1
2017-08-08T18:03:38.098713: step 19580, loss 0.00184927, acc 1
2017-08-08T18:03:38.395659: step 19581, loss 3.3734e-05, acc 1
2017-08-08T18:03:38.763660: step 19582, loss 1.08031e-06, acc 1
2017-08-08T18:03:39.016066: step 19583, loss 7.49545e-06, acc 1
2017-08-08T18:03:39.232740: step 19584, loss 2.21154e-05, acc 1
2017-08-08T18:03:39.597587: step 19585, loss 4.74517e-05, acc 1
2017-08-08T18:03:39.880700: step 19586, loss 9.22611e-05, acc 1
2017-08-08T18:03:40.093388: step 19587, loss 2.30201e-05, acc 1
2017-08-08T18:03:40.347483: step 19588, loss 2.25379e-07, acc 1
2017-08-08T18:03:40.705422: step 19589, loss 5.32712e-07, acc 1
2017-08-08T18:03:41.039447: step 19590, loss 8.21405e-07, acc 1
2017-08-08T18:03:41.341343: step 19591, loss 2.36555e-07, acc 1
2017-08-08T18:03:41.636292: step 19592, loss 1.84401e-07, acc 1
2017-08-08T18:03:41.827035: step 19593, loss 4.47035e-08, acc 1
2017-08-08T18:03:42.089822: step 19594, loss 9.97324e-06, acc 1
2017-08-08T18:03:42.450633: step 19595, loss 8.66614e-06, acc 1
2017-08-08T18:03:42.731242: step 19596, loss 2.8863e-05, acc 1
2017-08-08T18:03:42.988917: step 19597, loss 5.11455e-05, acc 1
2017-08-08T18:03:43.281387: step 19598, loss 0.00721246, acc 1
2017-08-08T18:03:43.731571: step 19599, loss 1.42113e-06, acc 1
2017-08-08T18:03:44.149829: step 19600, loss 0.000110659, acc 1

Evaluation:
2017-08-08T18:03:44.959169: step 19600, loss 6.41756, acc 0.697936

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-19600

2017-08-08T18:03:45.487642: step 19601, loss 4.97319e-07, acc 1
2017-08-08T18:03:45.826744: step 19602, loss 1.86264e-08, acc 1
2017-08-08T18:03:46.119754: step 19603, loss 1.39696e-06, acc 1
2017-08-08T18:03:46.406868: step 19604, loss 5.8124e-06, acc 1
2017-08-08T18:03:46.775228: step 19605, loss 2.98023e-08, acc 1
2017-08-08T18:03:47.188243: step 19606, loss 0.000220015, acc 1
2017-08-08T18:03:47.619308: step 19607, loss 1.17903e-06, acc 1
2017-08-08T18:03:47.866690: step 19608, loss 7.45058e-09, acc 1
2017-08-08T18:03:48.108307: step 19609, loss 1.8586e-05, acc 1
2017-08-08T18:03:48.605398: step 19610, loss 0, acc 1
2017-08-08T18:03:48.918824: step 19611, loss 0, acc 1
2017-08-08T18:03:49.220428: step 19612, loss 3.91155e-08, acc 1
2017-08-08T18:03:49.524215: step 19613, loss 2.4362e-06, acc 1
2017-08-08T18:03:49.885365: step 19614, loss 1.11759e-08, acc 1
2017-08-08T18:03:50.205782: step 19615, loss 0.0114737, acc 0.984375
2017-08-08T18:03:50.522695: step 19616, loss 1.93152e-06, acc 1
2017-08-08T18:03:50.798037: step 19617, loss 9.16376e-06, acc 1
2017-08-08T18:03:51.006499: step 19618, loss 5.58794e-09, acc 1
2017-08-08T18:03:51.321923: step 19619, loss 8.19562e-08, acc 1
2017-08-08T18:03:51.600832: step 19620, loss 0.000175575, acc 1
2017-08-08T18:03:51.862682: step 19621, loss 0.0114854, acc 0.984375
2017-08-08T18:03:52.161764: step 19622, loss 1.49034e-05, acc 1
2017-08-08T18:03:52.657399: step 19623, loss 1.11759e-08, acc 1
2017-08-08T18:03:53.026729: step 19624, loss 0.000372166, acc 1
2017-08-08T18:03:53.297224: step 19625, loss 1.11759e-08, acc 1
2017-08-08T18:03:53.501332: step 19626, loss 7.96266e-06, acc 1
2017-08-08T18:03:53.693164: step 19627, loss 1.86265e-09, acc 1
2017-08-08T18:03:54.082385: step 19628, loss 0.000108322, acc 1
2017-08-08T18:03:54.294952: step 19629, loss 1.82105e-05, acc 1
2017-08-08T18:03:54.520838: step 19630, loss 2.86079e-06, acc 1
2017-08-08T18:03:54.741012: step 19631, loss 0.00345671, acc 1
2017-08-08T18:03:55.009624: step 19632, loss 5.1222e-07, acc 1
2017-08-08T18:03:55.308709: step 19633, loss 0.000111338, acc 1
2017-08-08T18:03:55.596161: step 19634, loss 4.93931e-06, acc 1
2017-08-08T18:03:55.842425: step 19635, loss 6.70551e-08, acc 1
2017-08-08T18:03:56.150431: step 19636, loss 0.000159212, acc 1
2017-08-08T18:03:56.540469: step 19637, loss 1.49012e-08, acc 1
2017-08-08T18:03:56.823330: step 19638, loss 9.13291e-05, acc 1
2017-08-08T18:03:57.118141: step 19639, loss 3.72529e-08, acc 1
2017-08-08T18:03:57.379418: step 19640, loss 5.62509e-07, acc 1
2017-08-08T18:03:57.803917: step 19641, loss 7.00341e-07, acc 1
2017-08-08T18:03:58.166912: step 19642, loss 2.68829e-05, acc 1
2017-08-08T18:03:58.561708: step 19643, loss 1.17346e-07, acc 1
2017-08-08T18:03:58.867614: step 19644, loss 2.49026e-06, acc 1
2017-08-08T18:03:59.106229: step 19645, loss 5.56924e-07, acc 1
2017-08-08T18:03:59.564991: step 19646, loss 8.13878e-05, acc 1
2017-08-08T18:03:59.843301: step 19647, loss 2.6077e-08, acc 1
2017-08-08T18:04:00.060755: step 19648, loss 3.01747e-07, acc 1
2017-08-08T18:04:00.329712: step 19649, loss 8.27175e-05, acc 1
2017-08-08T18:04:00.721202: step 19650, loss 0, acc 1
2017-08-08T18:04:01.121325: step 19651, loss 2.86845e-07, acc 1
2017-08-08T18:04:01.509029: step 19652, loss 1.13621e-07, acc 1
2017-08-08T18:04:01.796787: step 19653, loss 7.73924e-06, acc 1
2017-08-08T18:04:02.260325: step 19654, loss 5.58794e-09, acc 1
2017-08-08T18:04:02.515246: step 19655, loss 2.54379e-05, acc 1
2017-08-08T18:04:02.813393: step 19656, loss 0, acc 1
2017-08-08T18:04:03.083472: step 19657, loss 3.58648e-05, acc 1
2017-08-08T18:04:03.454150: step 19658, loss 3.03398e-06, acc 1
2017-08-08T18:04:03.765401: step 19659, loss 8.63629e-05, acc 1
2017-08-08T18:04:04.181838: step 19660, loss 4.14208e-06, acc 1
2017-08-08T18:04:04.527732: step 19661, loss 0.0669697, acc 0.984375
2017-08-08T18:04:04.790513: step 19662, loss 5.2154e-08, acc 1
2017-08-08T18:04:05.213409: step 19663, loss 2.1479e-05, acc 1
2017-08-08T18:04:05.509319: step 19664, loss 7.71323e-06, acc 1
2017-08-08T18:04:05.762725: step 19665, loss 4.37321e-06, acc 1
2017-08-08T18:04:06.026759: step 19666, loss 3.59845e-06, acc 1
2017-08-08T18:04:06.360494: step 19667, loss 2.88709e-07, acc 1
2017-08-08T18:04:06.657962: step 19668, loss 2.55801e-05, acc 1
2017-08-08T18:04:06.926372: step 19669, loss 1.11759e-08, acc 1
2017-08-08T18:04:07.141874: step 19670, loss 7.63684e-08, acc 1
2017-08-08T18:04:07.361361: step 19671, loss 7.45057e-08, acc 1
2017-08-08T18:04:07.701788: step 19672, loss 1.65746e-05, acc 1
2017-08-08T18:04:07.891348: step 19673, loss 7.63683e-08, acc 1
2017-08-08T18:04:08.123733: step 19674, loss 1.06171e-07, acc 1
2017-08-08T18:04:08.414997: step 19675, loss 3.18854e-06, acc 1
2017-08-08T18:04:08.662155: step 19676, loss 6.76126e-07, acc 1
2017-08-08T18:04:09.041352: step 19677, loss 0.000128944, acc 1
2017-08-08T18:04:09.309156: step 19678, loss 9.31322e-09, acc 1
2017-08-08T18:04:09.549930: step 19679, loss 4.62252e-06, acc 1
2017-08-08T18:04:09.782059: step 19680, loss 9.77873e-07, acc 1
2017-08-08T18:04:10.169750: step 19681, loss 4.60412e-06, acc 1
2017-08-08T18:04:10.479834: step 19682, loss 0.000121677, acc 1
2017-08-08T18:04:10.755257: step 19683, loss 1.43231e-06, acc 1
2017-08-08T18:04:10.967352: step 19684, loss 3.62994e-06, acc 1
2017-08-08T18:04:11.336455: step 19685, loss 1.17531e-06, acc 1
2017-08-08T18:04:11.708340: step 19686, loss 5.58786e-07, acc 1
2017-08-08T18:04:12.120092: step 19687, loss 0.000146651, acc 1
2017-08-08T18:04:12.339136: step 19688, loss 3.38999e-07, acc 1
2017-08-08T18:04:12.574269: step 19689, loss 9.679e-05, acc 1
2017-08-08T18:04:12.939679: step 19690, loss 7.72988e-07, acc 1
2017-08-08T18:04:13.141011: step 19691, loss 6.46326e-07, acc 1
2017-08-08T18:04:13.361459: step 19692, loss 1.57572e-06, acc 1
2017-08-08T18:04:13.667724: step 19693, loss 1.47701e-06, acc 1
2017-08-08T18:04:14.030569: step 19694, loss 3.00456e-05, acc 1
2017-08-08T18:04:14.337354: step 19695, loss 1.12947e-05, acc 1
2017-08-08T18:04:14.792936: step 19696, loss 1.03438e-05, acc 1
2017-08-08T18:04:15.016840: step 19697, loss 0.00371297, acc 1
2017-08-08T18:04:15.288832: step 19698, loss 3.90176e-06, acc 1
2017-08-08T18:04:15.616067: step 19699, loss 5.42205e-05, acc 1
2017-08-08T18:04:15.879430: step 19700, loss 6.95055e-06, acc 1

Evaluation:
2017-08-08T18:04:16.423390: step 19700, loss 6.67119, acc 0.701689

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-19700

2017-08-08T18:04:16.977331: step 19701, loss 0.000342565, acc 1
2017-08-08T18:04:17.267722: step 19702, loss 1.43417e-06, acc 1
2017-08-08T18:04:17.635730: step 19703, loss 9.95389e-05, acc 1
2017-08-08T18:04:17.923673: step 19704, loss 1.04491e-06, acc 1
2017-08-08T18:04:18.277468: step 19705, loss 7.63683e-08, acc 1
2017-08-08T18:04:18.538194: step 19706, loss 1.43423e-07, acc 1
2017-08-08T18:04:18.787312: step 19707, loss 5.5497e-06, acc 1
2017-08-08T18:04:19.176100: step 19708, loss 7.07804e-08, acc 1
2017-08-08T18:04:19.597254: step 19709, loss 7.57924e-06, acc 1
2017-08-08T18:04:19.978702: step 19710, loss 9.4806e-07, acc 1
2017-08-08T18:04:20.292928: step 19711, loss 4.04193e-07, acc 1
2017-08-08T18:04:20.522294: step 19712, loss 0.00624081, acc 1
2017-08-08T18:04:20.710417: step 19713, loss 2.87255e-05, acc 1
2017-08-08T18:04:20.965394: step 19714, loss 0.000762598, acc 1
2017-08-08T18:04:21.280464: step 19715, loss 1.27586e-06, acc 1
2017-08-08T18:04:21.487073: step 19716, loss 5.51335e-07, acc 1
2017-08-08T18:04:21.683867: step 19717, loss 4.35053e-06, acc 1
2017-08-08T18:04:21.865358: step 19718, loss 1.32248e-07, acc 1
2017-08-08T18:04:22.193317: step 19719, loss 0.000136806, acc 1
2017-08-08T18:04:22.640785: step 19720, loss 2.69886e-06, acc 1
2017-08-08T18:04:22.949479: step 19721, loss 1.86265e-09, acc 1
2017-08-08T18:04:23.145706: step 19722, loss 1.695e-07, acc 1
2017-08-08T18:04:23.394862: step 19723, loss 5.46475e-05, acc 1
2017-08-08T18:04:23.771815: step 19724, loss 3.22391e-06, acc 1
2017-08-08T18:04:24.076162: step 19725, loss 2.98023e-08, acc 1
2017-08-08T18:04:24.415427: step 19726, loss 5.58793e-09, acc 1
2017-08-08T18:04:24.955634: step 19727, loss 5.58793e-08, acc 1
2017-08-08T18:04:25.348552: step 19728, loss 0.00590321, acc 1
2017-08-08T18:04:25.587860: step 19729, loss 1.44721e-06, acc 1
2017-08-08T18:04:25.821941: step 19730, loss 6.97875e-06, acc 1
2017-08-08T18:04:26.104676: step 19731, loss 1.86265e-09, acc 1
2017-08-08T18:04:26.343440: step 19732, loss 7.63684e-08, acc 1
2017-08-08T18:04:26.628254: step 19733, loss 0, acc 1
2017-08-08T18:04:26.863069: step 19734, loss 1.45286e-07, acc 1
2017-08-08T18:04:27.188550: step 19735, loss 3.91155e-08, acc 1
2017-08-08T18:04:27.459549: step 19736, loss 1.11759e-08, acc 1
2017-08-08T18:04:27.729845: step 19737, loss 1.3411e-07, acc 1
2017-08-08T18:04:27.942221: step 19738, loss 6.87301e-07, acc 1
2017-08-08T18:04:28.134652: step 19739, loss 2.42144e-08, acc 1
2017-08-08T18:04:28.507286: step 19740, loss 7.22167e-06, acc 1
2017-08-08T18:04:28.742826: step 19741, loss 3.72529e-09, acc 1
2017-08-08T18:04:29.037110: step 19742, loss 4.50757e-07, acc 1
2017-08-08T18:04:29.319412: step 19743, loss 2.23517e-08, acc 1
2017-08-08T18:04:29.730943: step 19744, loss 3.72529e-09, acc 1
2017-08-08T18:04:30.157333: step 19745, loss 1.66512e-06, acc 1
2017-08-08T18:04:30.476142: step 19746, loss 2.10465e-06, acc 1
2017-08-08T18:04:30.669406: step 19747, loss 7.35736e-07, acc 1
2017-08-08T18:04:30.874765: step 19748, loss 1.30385e-08, acc 1
2017-08-08T18:04:31.172525: step 19749, loss 4.15349e-06, acc 1
2017-08-08T18:04:31.387674: step 19750, loss 0.000239657, acc 1
2017-08-08T18:04:31.614526: step 19751, loss 9.31321e-08, acc 1
2017-08-08T18:04:31.830185: step 19752, loss 0.00114622, acc 1
2017-08-08T18:04:32.133348: step 19753, loss 3.63213e-07, acc 1
2017-08-08T18:04:32.491771: step 19754, loss 1.73403e-06, acc 1
2017-08-08T18:04:32.765313: step 19755, loss 9.03782e-05, acc 1
2017-08-08T18:04:32.958926: step 19756, loss 2.98023e-08, acc 1
2017-08-08T18:04:33.217407: step 19757, loss 3.91155e-08, acc 1
2017-08-08T18:04:33.635256: step 19758, loss 0.0002464, acc 1
2017-08-08T18:04:33.903930: step 19759, loss 8.94068e-08, acc 1
2017-08-08T18:04:34.168273: step 19760, loss 2.3896e-06, acc 1
2017-08-08T18:04:34.468282: step 19761, loss 2.84983e-07, acc 1
2017-08-08T18:04:34.910215: step 19762, loss 4.28408e-08, acc 1
2017-08-08T18:04:35.382431: step 19763, loss 4.47029e-07, acc 1
2017-08-08T18:04:35.754182: step 19764, loss 0.000970746, acc 1
2017-08-08T18:04:35.977383: step 19765, loss 5.96045e-08, acc 1
2017-08-08T18:04:36.242178: step 19766, loss 5.38592e-06, acc 1
2017-08-08T18:04:36.510978: step 19767, loss 1.03375e-06, acc 1
2017-08-08T18:04:36.699330: step 19768, loss 0.000182034, acc 1
2017-08-08T18:04:36.909190: step 19769, loss 9.87199e-08, acc 1
2017-08-08T18:04:37.286086: step 19770, loss 5.81782e-06, acc 1
2017-08-08T18:04:37.565125: step 19771, loss 4.45143e-06, acc 1
2017-08-08T18:04:37.797269: step 19772, loss 0.00022816, acc 1
2017-08-08T18:04:38.070370: step 19773, loss 5.35432e-05, acc 1
2017-08-08T18:04:38.342461: step 19774, loss 0.000699685, acc 1
2017-08-08T18:04:38.530203: step 19775, loss 0.000207791, acc 1
2017-08-08T18:04:38.763326: step 19776, loss 1.16991e-05, acc 1
2017-08-08T18:04:39.027569: step 19777, loss 0.000443211, acc 1
2017-08-08T18:04:39.359488: step 19778, loss 6.88471e-06, acc 1
2017-08-08T18:04:39.613236: step 19779, loss 0.00456565, acc 1
2017-08-08T18:04:39.803897: step 19780, loss 7.20833e-07, acc 1
2017-08-08T18:04:40.123950: step 19781, loss 3.34318e-06, acc 1
2017-08-08T18:04:40.458700: step 19782, loss 0, acc 1
2017-08-08T18:04:40.768941: step 19783, loss 3.60586e-05, acc 1
2017-08-08T18:04:41.174327: step 19784, loss 1.1963e-05, acc 1
2017-08-08T18:04:41.585204: step 19785, loss 1.41465e-05, acc 1
2017-08-08T18:04:41.865380: step 19786, loss 7.69722e-05, acc 1
2017-08-08T18:04:42.080127: step 19787, loss 4.28543e-06, acc 1
2017-08-08T18:04:42.411736: step 19788, loss 1.71012e-05, acc 1
2017-08-08T18:04:42.701308: step 19789, loss 4.15315e-06, acc 1
2017-08-08T18:04:43.000173: step 19790, loss 1.30385e-08, acc 1
2017-08-08T18:04:43.267824: step 19791, loss 5.01044e-07, acc 1
2017-08-08T18:04:43.558707: step 19792, loss 0.000362095, acc 1
2017-08-08T18:04:44.062835: step 19793, loss 3.67303e-05, acc 1
2017-08-08T18:04:44.411623: step 19794, loss 0, acc 1
2017-08-08T18:04:44.749934: step 19795, loss 1.52861e-05, acc 1
2017-08-08T18:04:45.007752: step 19796, loss 8.40042e-07, acc 1
2017-08-08T18:04:45.401405: step 19797, loss 2.65219e-06, acc 1
2017-08-08T18:04:45.744747: step 19798, loss 9.72289e-07, acc 1
2017-08-08T18:04:46.026588: step 19799, loss 4.09781e-08, acc 1
2017-08-08T18:04:46.301285: step 19800, loss 8.18449e-06, acc 1

Evaluation:
2017-08-08T18:04:47.144040: step 19800, loss 6.73381, acc 0.710131

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-19800

2017-08-08T18:04:47.833178: step 19801, loss 0.000306334, acc 1
2017-08-08T18:04:48.098153: step 19802, loss 0.000184166, acc 1
2017-08-08T18:04:48.367939: step 19803, loss 3.29685e-07, acc 1
2017-08-08T18:04:48.733649: step 19804, loss 7.33759e-05, acc 1
2017-08-08T18:04:49.011121: step 19805, loss 2.14547e-05, acc 1
2017-08-08T18:04:49.318416: step 19806, loss 4.69382e-07, acc 1
2017-08-08T18:04:49.577411: step 19807, loss 0.000374737, acc 1
2017-08-08T18:04:49.909338: step 19808, loss 3.35276e-08, acc 1
2017-08-08T18:04:50.297574: step 19809, loss 1.95577e-07, acc 1
2017-08-08T18:04:50.597664: step 19810, loss 2.01898e-06, acc 1
2017-08-08T18:04:50.788665: step 19811, loss 1.11179e-05, acc 1
2017-08-08T18:04:50.993331: step 19812, loss 1.71491e-05, acc 1
2017-08-08T18:04:51.322315: step 19813, loss 5.13262e-06, acc 1
2017-08-08T18:04:51.505608: step 19814, loss 0.000149254, acc 1
2017-08-08T18:04:51.687316: step 19815, loss 1.82347e-06, acc 1
2017-08-08T18:04:51.928720: step 19816, loss 3.72529e-09, acc 1
2017-08-08T18:04:52.257351: step 19817, loss 1.47149e-07, acc 1
2017-08-08T18:04:52.514191: step 19818, loss 0, acc 1
2017-08-08T18:04:52.721406: step 19819, loss 7.36069e-05, acc 1
2017-08-08T18:04:53.006254: step 19820, loss 2.22006e-05, acc 1
2017-08-08T18:04:53.532593: step 19821, loss 1.1846e-06, acc 1
2017-08-08T18:04:53.889518: step 19822, loss 1.10659e-05, acc 1
2017-08-08T18:04:54.217224: step 19823, loss 1.19209e-07, acc 1
2017-08-08T18:04:54.593308: step 19824, loss 1.62049e-07, acc 1
2017-08-08T18:04:54.916462: step 19825, loss 4.55537e-06, acc 1
2017-08-08T18:04:55.253930: step 19826, loss 9.10755e-06, acc 1
2017-08-08T18:04:55.587490: step 19827, loss 2.73599e-06, acc 1
2017-08-08T18:04:55.908897: step 19828, loss 7.63683e-08, acc 1
2017-08-08T18:04:56.293608: step 19829, loss 0.000665911, acc 1
2017-08-08T18:04:56.606891: step 19830, loss 1.86265e-09, acc 1
2017-08-08T18:04:56.878631: step 19831, loss 0, acc 1
2017-08-08T18:04:57.161263: step 19832, loss 6.36235e-05, acc 1
2017-08-08T18:04:57.428146: step 19833, loss 8.25866e-06, acc 1
2017-08-08T18:04:57.677454: step 19834, loss 1.26409e-05, acc 1
2017-08-08T18:04:57.905573: step 19835, loss 7.10162e-05, acc 1
2017-08-08T18:04:58.081229: step 19836, loss 5.70477e-06, acc 1
2017-08-08T18:04:58.276988: step 19837, loss 0.00160144, acc 1
2017-08-08T18:04:58.597504: step 19838, loss 7.8231e-08, acc 1
2017-08-08T18:04:58.833959: step 19839, loss 0.000268283, acc 1
2017-08-08T18:04:59.137135: step 19840, loss 4.84287e-08, acc 1
2017-08-08T18:04:59.393783: step 19841, loss 1.32615e-06, acc 1
2017-08-08T18:04:59.735186: step 19842, loss 0, acc 1
2017-08-08T18:05:00.106701: step 19843, loss 3.16649e-08, acc 1
2017-08-08T18:05:00.410883: step 19844, loss 0.000170585, acc 1
2017-08-08T18:05:00.627748: step 19845, loss 1.56462e-07, acc 1
2017-08-08T18:05:00.879947: step 19846, loss 1.82538e-07, acc 1
2017-08-08T18:05:01.160258: step 19847, loss 6.15336e-06, acc 1
2017-08-08T18:05:01.403114: step 19848, loss 3.32074e-06, acc 1
2017-08-08T18:05:01.650008: step 19849, loss 0.000898177, acc 1
2017-08-08T18:05:01.907829: step 19850, loss 3.77605e-05, acc 1
2017-08-08T18:05:02.292932: step 19851, loss 4.7555e-05, acc 1
2017-08-08T18:05:02.721899: step 19852, loss 3.72529e-09, acc 1
2017-08-08T18:05:03.163457: step 19853, loss 0.000500024, acc 1
2017-08-08T18:05:03.551969: step 19854, loss 7.37994e-06, acc 1
2017-08-08T18:05:03.892791: step 19855, loss 5.58794e-09, acc 1
2017-08-08T18:05:04.183376: step 19856, loss 5.03609e-06, acc 1
2017-08-08T18:05:04.540228: step 19857, loss 5.58794e-09, acc 1
2017-08-08T18:05:04.892518: step 19858, loss 0, acc 1
2017-08-08T18:05:05.191370: step 19859, loss 7.6553e-07, acc 1
2017-08-08T18:05:05.538035: step 19860, loss 0.00230619, acc 1
2017-08-08T18:05:05.945686: step 19861, loss 0, acc 1
2017-08-08T18:05:06.220812: step 19862, loss 1.71363e-07, acc 1
2017-08-08T18:05:06.671754: step 19863, loss 4.09781e-08, acc 1
2017-08-08T18:05:06.969537: step 19864, loss 4.64044e-05, acc 1
2017-08-08T18:05:07.500372: step 19865, loss 0.000104589, acc 1
2017-08-08T18:05:07.782089: step 19866, loss 3.35276e-08, acc 1
2017-08-08T18:05:08.145400: step 19867, loss 2.54515e-05, acc 1
2017-08-08T18:05:08.538657: step 19868, loss 8.56815e-08, acc 1
2017-08-08T18:05:08.869793: step 19869, loss 5.58793e-09, acc 1
2017-08-08T18:05:09.281568: step 19870, loss 1.19622e-05, acc 1
2017-08-08T18:05:09.545163: step 19871, loss 2.42144e-08, acc 1
2017-08-08T18:05:10.034297: step 19872, loss 4.29097e-06, acc 1
2017-08-08T18:05:10.253381: step 19873, loss 7.64552e-06, acc 1
2017-08-08T18:05:10.553391: step 19874, loss 5.75978e-05, acc 1
2017-08-08T18:05:10.879006: step 19875, loss 5.96181e-05, acc 1
2017-08-08T18:05:11.129371: step 19876, loss 1.15481e-06, acc 1
2017-08-08T18:05:11.573363: step 19877, loss 8.00936e-08, acc 1
2017-08-08T18:05:11.872729: step 19878, loss 3.07334e-07, acc 1
2017-08-08T18:05:12.189552: step 19879, loss 3.0023e-06, acc 1
2017-08-08T18:05:12.628693: step 19880, loss 0.0113927, acc 0.984375
2017-08-08T18:05:12.889797: step 19881, loss 1.7695e-07, acc 1
2017-08-08T18:05:13.319556: step 19882, loss 3.35276e-08, acc 1
2017-08-08T18:05:13.580341: step 19883, loss 9.02117e-05, acc 1
2017-08-08T18:05:13.913409: step 19884, loss 1.86265e-09, acc 1
2017-08-08T18:05:14.253520: step 19885, loss 9.76005e-07, acc 1
2017-08-08T18:05:14.509335: step 19886, loss 1.53128e-05, acc 1
2017-08-08T18:05:14.889833: step 19887, loss 1.86265e-09, acc 1
2017-08-08T18:05:15.228861: step 19888, loss 1.03187e-06, acc 1
2017-08-08T18:05:15.641407: step 19889, loss 2.42144e-08, acc 1
2017-08-08T18:05:15.906009: step 19890, loss 4.87162e-05, acc 1
2017-08-08T18:05:16.268144: step 19891, loss 1.61038e-05, acc 1
2017-08-08T18:05:16.587071: step 19892, loss 3.94853e-05, acc 1
2017-08-08T18:05:16.849328: step 19893, loss 2.79397e-08, acc 1
2017-08-08T18:05:17.209382: step 19894, loss 1.86265e-09, acc 1
2017-08-08T18:05:17.596902: step 19895, loss 7.89046e-05, acc 1
2017-08-08T18:05:17.870611: step 19896, loss 1.6223e-06, acc 1
2017-08-08T18:05:18.132479: step 19897, loss 3.72529e-09, acc 1
2017-08-08T18:05:18.383588: step 19898, loss 1.00583e-07, acc 1
2017-08-08T18:05:18.741855: step 19899, loss 6.33967e-05, acc 1
2017-08-08T18:05:19.072225: step 19900, loss 1.21827e-05, acc 1

Evaluation:
2017-08-08T18:05:19.878940: step 19900, loss 6.65634, acc 0.704503

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-19900

2017-08-08T18:05:20.455093: step 19901, loss 1.86264e-08, acc 1
2017-08-08T18:05:20.755220: step 19902, loss 2.52563e-06, acc 1
2017-08-08T18:05:21.038152: step 19903, loss 4.13081e-06, acc 1
2017-08-08T18:05:21.368293: step 19904, loss 7.45058e-09, acc 1
2017-08-08T18:05:21.845417: step 19905, loss 1.30385e-08, acc 1
2017-08-08T18:05:22.251257: step 19906, loss 0.00019258, acc 1
2017-08-08T18:05:22.555774: step 19907, loss 5.58793e-09, acc 1
2017-08-08T18:05:22.836441: step 19908, loss 1.14924e-06, acc 1
2017-08-08T18:05:23.259626: step 19909, loss 0.00108853, acc 1
2017-08-08T18:05:23.558652: step 19910, loss 0.000270415, acc 1
2017-08-08T18:05:23.822639: step 19911, loss 2.25178e-06, acc 1
2017-08-08T18:05:24.154484: step 19912, loss 9.98359e-07, acc 1
2017-08-08T18:05:24.551245: step 19913, loss 2.44548e-06, acc 1
2017-08-08T18:05:24.986806: step 19914, loss 1.17346e-07, acc 1
2017-08-08T18:05:25.322058: step 19915, loss 1.11759e-08, acc 1
2017-08-08T18:05:25.576407: step 19916, loss 4.97319e-07, acc 1
2017-08-08T18:05:26.037752: step 19917, loss 1.27502e-05, acc 1
2017-08-08T18:05:26.332089: step 19918, loss 7.61827e-06, acc 1
2017-08-08T18:05:26.552705: step 19919, loss 5.31137e-06, acc 1
2017-08-08T18:05:26.737346: step 19920, loss 8.61181e-06, acc 1
2017-08-08T18:05:27.100406: step 19921, loss 1.39698e-07, acc 1
2017-08-08T18:05:27.334419: step 19922, loss 1.42702e-05, acc 1
2017-08-08T18:05:27.572432: step 19923, loss 4.1164e-07, acc 1
2017-08-08T18:05:27.780910: step 19924, loss 9.31322e-09, acc 1
2017-08-08T18:05:28.065552: step 19925, loss 0.000846114, acc 1
2017-08-08T18:05:28.301376: step 19926, loss 2.04891e-08, acc 1
2017-08-08T18:05:28.538454: step 19927, loss 4.24677e-07, acc 1
2017-08-08T18:05:28.767043: step 19928, loss 2.6077e-08, acc 1
2017-08-08T18:05:29.096720: step 19929, loss 2.49757e-05, acc 1
2017-08-08T18:05:29.397312: step 19930, loss 7.82294e-07, acc 1
2017-08-08T18:05:29.645337: step 19931, loss 1.09896e-07, acc 1
2017-08-08T18:05:29.828681: step 19932, loss 1.00952e-06, acc 1
2017-08-08T18:05:30.050472: step 19933, loss 1.88489e-06, acc 1
2017-08-08T18:05:30.338559: step 19934, loss 0.000128682, acc 1
2017-08-08T18:05:30.535925: step 19935, loss 2.79394e-07, acc 1
2017-08-08T18:05:30.747329: step 19936, loss 5.41905e-05, acc 1
2017-08-08T18:05:30.959694: step 19937, loss 6.51925e-08, acc 1
2017-08-08T18:05:31.167302: step 19938, loss 6.94751e-07, acc 1
2017-08-08T18:05:31.453970: step 19939, loss 2.47898e-06, acc 1
2017-08-08T18:05:31.789878: step 19940, loss 0.00227557, acc 1
2017-08-08T18:05:32.090649: step 19941, loss 1.50874e-07, acc 1
2017-08-08T18:05:32.328048: step 19942, loss 3.53902e-08, acc 1
2017-08-08T18:05:32.640144: step 19943, loss 5.14083e-07, acc 1
2017-08-08T18:05:33.083359: step 19944, loss 6.40992e-06, acc 1
2017-08-08T18:05:33.392797: step 19945, loss 3.63561e-05, acc 1
2017-08-08T18:05:33.678660: step 19946, loss 5.02914e-08, acc 1
2017-08-08T18:05:34.025048: step 19947, loss 4.65779e-06, acc 1
2017-08-08T18:05:34.456263: step 19948, loss 5.94671e-05, acc 1
2017-08-08T18:05:34.794760: step 19949, loss 8.16395e-06, acc 1
2017-08-08T18:05:35.130258: step 19950, loss 1.33005e-05, acc 1
2017-08-08T18:05:35.405147: step 19951, loss 9.59252e-07, acc 1
2017-08-08T18:05:35.858712: step 19952, loss 1.49012e-08, acc 1
2017-08-08T18:05:36.176333: step 19953, loss 6.44469e-07, acc 1
2017-08-08T18:05:36.415894: step 19954, loss 1.33363e-06, acc 1
2017-08-08T18:05:36.671696: step 19955, loss 4.39975e-05, acc 1
2017-08-08T18:05:37.060650: step 19956, loss 2.65973e-06, acc 1
2017-08-08T18:05:37.459856: step 19957, loss 3.08216e-05, acc 1
2017-08-08T18:05:37.781341: step 19958, loss 6.46186e-05, acc 1
2017-08-08T18:05:38.016795: step 19959, loss 4.35054e-06, acc 1
2017-08-08T18:05:38.291676: step 19960, loss 0.000186627, acc 1
2017-08-08T18:05:38.642881: step 19961, loss 0, acc 1
2017-08-08T18:05:38.899076: step 19962, loss 2.03027e-07, acc 1
2017-08-08T18:05:39.112697: step 19963, loss 1.11759e-08, acc 1
2017-08-08T18:05:39.333848: step 19964, loss 9.01761e-06, acc 1
2017-08-08T18:05:39.748868: step 19965, loss 6.33298e-08, acc 1
2017-08-08T18:05:40.105364: step 19966, loss 1.49012e-08, acc 1
2017-08-08T18:05:40.463474: step 19967, loss 5.65025e-06, acc 1
2017-08-08T18:05:40.721278: step 19968, loss 2.42144e-08, acc 1
2017-08-08T18:05:41.047876: step 19969, loss 4.65661e-08, acc 1
2017-08-08T18:05:41.434285: step 19970, loss 3.63379e-06, acc 1
2017-08-08T18:05:41.717667: step 19971, loss 2.19963e-06, acc 1
2017-08-08T18:05:41.997123: step 19972, loss 2.19971e-06, acc 1
2017-08-08T18:05:42.411628: step 19973, loss 0, acc 1
2017-08-08T18:05:42.842577: step 19974, loss 0, acc 1
2017-08-08T18:05:43.263848: step 19975, loss 5.0477e-07, acc 1
2017-08-08T18:05:43.468518: step 19976, loss 1.39637e-05, acc 1
2017-08-08T18:05:43.677416: step 19977, loss 0, acc 1
2017-08-08T18:05:44.149474: step 19978, loss 1.03932e-06, acc 1
2017-08-08T18:05:44.403565: step 19979, loss 0.00111715, acc 1
2017-08-08T18:05:44.653058: step 19980, loss 3.56098e-06, acc 1
2017-08-08T18:05:44.887659: step 19981, loss 2.9596e-06, acc 1
2017-08-08T18:05:45.316985: step 19982, loss 5.77419e-08, acc 1
2017-08-08T18:05:45.717150: step 19983, loss 2.36555e-07, acc 1
2017-08-08T18:05:46.078581: step 19984, loss 6.88843e-06, acc 1
2017-08-08T18:05:46.355170: step 19985, loss 3.1665e-08, acc 1
2017-08-08T18:05:46.613377: step 19986, loss 3.11059e-07, acc 1
2017-08-08T18:05:46.984761: step 19987, loss 0, acc 1
2017-08-08T18:05:47.269624: step 19988, loss 4.05345e-05, acc 1
2017-08-08T18:05:47.504407: step 19989, loss 1.10807e-05, acc 1
2017-08-08T18:05:47.787546: step 19990, loss 1.79921e-06, acc 1
2017-08-08T18:05:48.165348: step 19991, loss 5.2154e-08, acc 1
2017-08-08T18:05:48.565284: step 19992, loss 8.74408e-05, acc 1
2017-08-08T18:05:48.996678: step 19993, loss 2.79397e-08, acc 1
2017-08-08T18:05:49.344095: step 19994, loss 3.72529e-08, acc 1
2017-08-08T18:05:49.588410: step 19995, loss 5.82156e-06, acc 1
2017-08-08T18:05:49.989379: step 19996, loss 1.88127e-07, acc 1
2017-08-08T18:05:50.345282: step 19997, loss 3.72529e-09, acc 1
2017-08-08T18:05:50.647192: step 19998, loss 1.11759e-08, acc 1
2017-08-08T18:05:50.958653: step 19999, loss 8.43756e-07, acc 1
2017-08-08T18:05:51.393133: step 20000, loss 1.38765e-06, acc 1

Evaluation:
2017-08-08T18:05:52.209396: step 20000, loss 6.70072, acc 0.705441

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-20000

2017-08-08T18:05:52.560596: step 20001, loss 1.67638e-08, acc 1
2017-08-08T18:05:52.951702: step 20002, loss 1.53276e-05, acc 1
2017-08-08T18:05:53.223902: step 20003, loss 8.77522e-06, acc 1
2017-08-08T18:05:53.518866: step 20004, loss 0.000102686, acc 1
2017-08-08T18:05:53.787754: step 20005, loss 3.95204e-06, acc 1
2017-08-08T18:05:54.154397: step 20006, loss 5.4016e-07, acc 1
2017-08-08T18:05:54.567904: step 20007, loss 0.000118736, acc 1
2017-08-08T18:05:54.965548: step 20008, loss 2.34872e-06, acc 1
2017-08-08T18:05:55.265446: step 20009, loss 2.83121e-07, acc 1
2017-08-08T18:05:55.485261: step 20010, loss 1.36713e-06, acc 1
2017-08-08T18:05:55.929394: step 20011, loss 0.000225919, acc 1
2017-08-08T18:05:56.261492: step 20012, loss 7.1454e-06, acc 1
2017-08-08T18:05:56.489393: step 20013, loss 2.65034e-06, acc 1
2017-08-08T18:05:56.764121: step 20014, loss 9.02046e-05, acc 1
2017-08-08T18:05:57.106776: step 20015, loss 0.0824733, acc 0.984375
2017-08-08T18:05:57.430115: step 20016, loss 0, acc 1
2017-08-08T18:05:57.827728: step 20017, loss 7.3844e-05, acc 1
2017-08-08T18:05:58.226738: step 20018, loss 5.77419e-08, acc 1
2017-08-08T18:05:58.476541: step 20019, loss 2.0437e-05, acc 1
2017-08-08T18:05:58.762351: step 20020, loss 0.000311447, acc 1
2017-08-08T18:05:59.250347: step 20021, loss 4.28404e-07, acc 1
2017-08-08T18:05:59.590052: step 20022, loss 2.03027e-07, acc 1
2017-08-08T18:05:59.906782: step 20023, loss 5.53909e-05, acc 1
2017-08-08T18:06:00.220056: step 20024, loss 1.3485e-06, acc 1
2017-08-08T18:06:00.671409: step 20025, loss 2.14203e-07, acc 1
2017-08-08T18:06:01.055562: step 20026, loss 4.47034e-08, acc 1
2017-08-08T18:06:01.399009: step 20027, loss 0, acc 1
2017-08-08T18:06:01.644269: step 20028, loss 0.000257297, acc 1
2017-08-08T18:06:01.925462: step 20029, loss 0.0486166, acc 0.984375
2017-08-08T18:06:02.338044: step 20030, loss 3.72529e-09, acc 1
2017-08-08T18:06:02.649149: step 20031, loss 0.0630675, acc 0.984375
2017-08-08T18:06:02.918515: step 20032, loss 6.66536e-06, acc 1
2017-08-08T18:06:03.189143: step 20033, loss 2.86451e-06, acc 1
2017-08-08T18:06:03.603729: step 20034, loss 1.84401e-07, acc 1
2017-08-08T18:06:04.053321: step 20035, loss 6.61785e-05, acc 1
2017-08-08T18:06:04.423378: step 20036, loss 1.67638e-08, acc 1
2017-08-08T18:06:04.678107: step 20037, loss 8.10638e-06, acc 1
2017-08-08T18:06:04.941112: step 20038, loss 1.13621e-07, acc 1
2017-08-08T18:06:05.379938: step 20039, loss 5.45751e-07, acc 1
2017-08-08T18:06:05.685399: step 20040, loss 2.06753e-07, acc 1
2017-08-08T18:06:06.031294: step 20041, loss 0.000338017, acc 1
2017-08-08T18:06:06.334407: step 20042, loss 2.98023e-08, acc 1
2017-08-08T18:06:06.803771: step 20043, loss 1.80852e-06, acc 1
2017-08-08T18:06:07.204877: step 20044, loss 5.25364e-06, acc 1
2017-08-08T18:06:07.563822: step 20045, loss 2.98023e-08, acc 1
2017-08-08T18:06:07.925736: step 20046, loss 1.77319e-06, acc 1
2017-08-08T18:06:08.189359: step 20047, loss 4.14954e-06, acc 1
2017-08-08T18:06:08.440345: step 20048, loss 4.7683e-07, acc 1
2017-08-08T18:06:08.886134: step 20049, loss 4.35577e-05, acc 1
2017-08-08T18:06:09.202061: step 20050, loss 1.88782e-05, acc 1
2017-08-08T18:06:09.442427: step 20051, loss 9.41022e-06, acc 1
2017-08-08T18:06:09.696742: step 20052, loss 3.6617e-06, acc 1
2017-08-08T18:06:10.041554: step 20053, loss 0.000308867, acc 1
2017-08-08T18:06:10.385360: step 20054, loss 2.98023e-08, acc 1
2017-08-08T18:06:10.665215: step 20055, loss 3.23802e-05, acc 1
2017-08-08T18:06:10.861537: step 20056, loss 0.0111049, acc 0.984375
2017-08-08T18:06:11.081388: step 20057, loss 1.54534e-05, acc 1
2017-08-08T18:06:11.452074: step 20058, loss 5.16657e-06, acc 1
2017-08-08T18:06:11.702841: step 20059, loss 0.000313799, acc 1
2017-08-08T18:06:12.004656: step 20060, loss 0.00013444, acc 1
2017-08-08T18:06:12.324175: step 20061, loss 5.77419e-08, acc 1
2017-08-08T18:06:12.653114: step 20062, loss 6.46327e-07, acc 1
2017-08-08T18:06:12.962127: step 20063, loss 0, acc 1
2017-08-08T18:06:13.224096: step 20064, loss 1.37457e-06, acc 1
2017-08-08T18:06:13.451533: step 20065, loss 1.30385e-07, acc 1
2017-08-08T18:06:13.805751: step 20066, loss 6.76126e-07, acc 1
2017-08-08T18:06:14.168865: step 20067, loss 1.695e-07, acc 1
2017-08-08T18:06:14.486010: step 20068, loss 0.00029844, acc 1
2017-08-08T18:06:14.765543: step 20069, loss 2.81258e-07, acc 1
2017-08-08T18:06:15.146007: step 20070, loss 5.49779e-05, acc 1
2017-08-08T18:06:15.557226: step 20071, loss 3.96322e-06, acc 1
2017-08-08T18:06:15.953645: step 20072, loss 0.000246051, acc 1
2017-08-08T18:06:16.246072: step 20073, loss 6.24107e-06, acc 1
2017-08-08T18:06:16.603431: step 20074, loss 0.00799169, acc 1
2017-08-08T18:06:16.993670: step 20075, loss 6.62098e-06, acc 1
2017-08-08T18:06:17.265855: step 20076, loss 2.34692e-07, acc 1
2017-08-08T18:06:17.548718: step 20077, loss 1.43606e-06, acc 1
2017-08-08T18:06:17.835078: step 20078, loss 1.96125e-06, acc 1
2017-08-08T18:06:18.267167: step 20079, loss 3.48311e-07, acc 1
2017-08-08T18:06:18.672885: step 20080, loss 0.000651641, acc 1
2017-08-08T18:06:19.068214: step 20081, loss 2.42409e-05, acc 1
2017-08-08T18:06:19.380310: step 20082, loss 0.000222527, acc 1
2017-08-08T18:06:19.628541: step 20083, loss 0.000148153, acc 1
2017-08-08T18:06:20.003416: step 20084, loss 3.49021e-06, acc 1
2017-08-08T18:06:20.308277: step 20085, loss 0.000421018, acc 1
2017-08-08T18:06:20.697365: step 20086, loss 7.55313e-06, acc 1
2017-08-08T18:06:21.089378: step 20087, loss 0.00821923, acc 1
2017-08-08T18:06:21.337937: step 20088, loss 3.72529e-08, acc 1
2017-08-08T18:06:21.724933: step 20089, loss 2.6077e-08, acc 1
2017-08-08T18:06:21.981276: step 20090, loss 7.28281e-07, acc 1
2017-08-08T18:06:22.241227: step 20091, loss 4.11755e-05, acc 1
2017-08-08T18:06:22.475332: step 20092, loss 2.04891e-08, acc 1
2017-08-08T18:06:22.834764: step 20093, loss 3.72529e-09, acc 1
2017-08-08T18:06:23.201542: step 20094, loss 0.000170475, acc 1
2017-08-08T18:06:23.584976: step 20095, loss 0.000369061, acc 1
2017-08-08T18:06:23.874211: step 20096, loss 5.58794e-09, acc 1
2017-08-08T18:06:24.101052: step 20097, loss 3.35273e-07, acc 1
2017-08-08T18:06:24.594399: step 20098, loss 5.45488e-06, acc 1
2017-08-08T18:06:24.896211: step 20099, loss 2.3896e-06, acc 1
2017-08-08T18:06:25.168079: step 20100, loss 4.17232e-08, acc 1

Evaluation:
2017-08-08T18:06:25.903844: step 20100, loss 6.7983, acc 0.704503

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-20100

2017-08-08T18:06:26.656962: step 20101, loss 6.57766e-05, acc 1
2017-08-08T18:06:26.906568: step 20102, loss 7.8231e-08, acc 1
2017-08-08T18:06:27.135977: step 20103, loss 1.17344e-06, acc 1
2017-08-08T18:06:27.500079: step 20104, loss 6.89178e-08, acc 1
2017-08-08T18:06:27.784781: step 20105, loss 7.45058e-09, acc 1
2017-08-08T18:06:28.039885: step 20106, loss 9.12694e-08, acc 1
2017-08-08T18:06:28.303838: step 20107, loss 4.96574e-05, acc 1
2017-08-08T18:06:28.615905: step 20108, loss 0.0110497, acc 0.984375
2017-08-08T18:06:29.069915: step 20109, loss 0.000333234, acc 1
2017-08-08T18:06:29.414954: step 20110, loss 3.63214e-07, acc 1
2017-08-08T18:06:29.701381: step 20111, loss 2.42144e-08, acc 1
2017-08-08T18:06:29.970057: step 20112, loss 2.4436e-06, acc 1
2017-08-08T18:06:30.331739: step 20113, loss 3.68799e-07, acc 1
2017-08-08T18:06:30.649331: step 20114, loss 1.6279e-06, acc 1
2017-08-08T18:06:30.931762: step 20115, loss 1.49011e-07, acc 1
2017-08-08T18:06:31.187019: step 20116, loss 4.47034e-08, acc 1
2017-08-08T18:06:31.612122: step 20117, loss 2.88504e-05, acc 1
2017-08-08T18:06:32.000406: step 20118, loss 1.04308e-07, acc 1
2017-08-08T18:06:32.305244: step 20119, loss 1.11759e-08, acc 1
2017-08-08T18:06:32.584089: step 20120, loss 3.53902e-08, acc 1
2017-08-08T18:06:32.821352: step 20121, loss 3.30081e-05, acc 1
2017-08-08T18:06:33.205357: step 20122, loss 7.2643e-08, acc 1
2017-08-08T18:06:33.431527: step 20123, loss 2.24434e-06, acc 1
2017-08-08T18:06:33.690140: step 20124, loss 3.44588e-07, acc 1
2017-08-08T18:06:33.925451: step 20125, loss 0.000156871, acc 1
2017-08-08T18:06:34.341389: step 20126, loss 6.52764e-06, acc 1
2017-08-08T18:06:34.746750: step 20127, loss 1.92724e-05, acc 1
2017-08-08T18:06:35.085390: step 20128, loss 3.29666e-06, acc 1
2017-08-08T18:06:35.442695: step 20129, loss 0.000500395, acc 1
2017-08-08T18:06:35.687791: step 20130, loss 8.27261e-05, acc 1
2017-08-08T18:06:36.156085: step 20131, loss 0.000297816, acc 1
2017-08-08T18:06:36.414663: step 20132, loss 4.7304e-06, acc 1
2017-08-08T18:06:36.700723: step 20133, loss 9.94622e-07, acc 1
2017-08-08T18:06:36.945248: step 20134, loss 9.20133e-07, acc 1
2017-08-08T18:06:37.330161: step 20135, loss 0.000118843, acc 1
2017-08-08T18:06:37.656052: step 20136, loss 6.70551e-08, acc 1
2017-08-08T18:06:37.895641: step 20137, loss 2.57591e-06, acc 1
2017-08-08T18:06:38.172991: step 20138, loss 3.11043e-06, acc 1
2017-08-08T18:06:38.394605: step 20139, loss 0.00169096, acc 1
2017-08-08T18:06:38.661061: step 20140, loss 0.0020352, acc 1
2017-08-08T18:06:39.068698: step 20141, loss 1.54412e-05, acc 1
2017-08-08T18:06:39.283937: step 20142, loss 1.14312e-05, acc 1
2017-08-08T18:06:39.535719: step 20143, loss 8.53977e-06, acc 1
2017-08-08T18:06:39.781737: step 20144, loss 7.18368e-05, acc 1
2017-08-08T18:06:40.178549: step 20145, loss 1.47515e-06, acc 1
2017-08-08T18:06:40.561367: step 20146, loss 1.21072e-07, acc 1
2017-08-08T18:06:40.905367: step 20147, loss 6.14672e-08, acc 1
2017-08-08T18:06:41.210776: step 20148, loss 5.789e-05, acc 1
2017-08-08T18:06:41.529195: step 20149, loss 2.75647e-06, acc 1
2017-08-08T18:06:41.985272: step 20150, loss 2.02651e-06, acc 1
2017-08-08T18:06:42.246824: step 20151, loss 1.09893e-06, acc 1
2017-08-08T18:06:42.488749: step 20152, loss 5.40167e-08, acc 1
2017-08-08T18:06:42.705342: step 20153, loss 5.58794e-09, acc 1
2017-08-08T18:06:43.061667: step 20154, loss 0, acc 1
2017-08-08T18:06:43.352275: step 20155, loss 0.000262537, acc 1
2017-08-08T18:06:43.763208: step 20156, loss 0.00010227, acc 1
2017-08-08T18:06:44.143473: step 20157, loss 4.49222e-06, acc 1
2017-08-08T18:06:44.434475: step 20158, loss 7.24694e-06, acc 1
2017-08-08T18:06:44.711005: step 20159, loss 7.79695e-06, acc 1
2017-08-08T18:06:45.039134: step 20160, loss 1.16041e-05, acc 1
2017-08-08T18:06:45.292740: step 20161, loss 8.77282e-07, acc 1
2017-08-08T18:06:45.545930: step 20162, loss 1.42117e-06, acc 1
2017-08-08T18:06:45.882767: step 20163, loss 4.98758e-05, acc 1
2017-08-08T18:06:46.185649: step 20164, loss 9.12695e-08, acc 1
2017-08-08T18:06:46.613642: step 20165, loss 1.49012e-08, acc 1
2017-08-08T18:06:46.998264: step 20166, loss 4.54479e-07, acc 1
2017-08-08T18:06:47.284575: step 20167, loss 2.1008e-05, acc 1
2017-08-08T18:06:47.677971: step 20168, loss 4.40713e-05, acc 1
2017-08-08T18:06:48.020828: step 20169, loss 1.86265e-09, acc 1
2017-08-08T18:06:48.320933: step 20170, loss 3.50174e-07, acc 1
2017-08-08T18:06:48.619569: step 20171, loss 1.39698e-07, acc 1
2017-08-08T18:06:48.989358: step 20172, loss 1.86265e-09, acc 1
2017-08-08T18:06:49.373370: step 20173, loss 1.02445e-07, acc 1
2017-08-08T18:06:49.804815: step 20174, loss 4.47034e-08, acc 1
2017-08-08T18:06:50.071944: step 20175, loss 2.9057e-07, acc 1
2017-08-08T18:06:50.312244: step 20176, loss 9.90896e-07, acc 1
2017-08-08T18:06:50.681796: step 20177, loss 1.86265e-09, acc 1
2017-08-08T18:06:50.988545: step 20178, loss 1.82285e-05, acc 1
2017-08-08T18:06:51.286756: step 20179, loss 3.6003e-06, acc 1
2017-08-08T18:06:51.557738: step 20180, loss 9.5365e-07, acc 1
2017-08-08T18:06:51.947386: step 20181, loss 0, acc 1
2017-08-08T18:06:52.368473: step 20182, loss 1.01587e-05, acc 1
2017-08-08T18:06:52.613665: step 20183, loss 3.16648e-07, acc 1
2017-08-08T18:06:52.847190: step 20184, loss 3.68044e-06, acc 1
2017-08-08T18:06:53.214665: step 20185, loss 2.11081e-05, acc 1
2017-08-08T18:06:53.569960: step 20186, loss 0.000162867, acc 1
2017-08-08T18:06:53.821839: step 20187, loss 3.14397e-06, acc 1
2017-08-08T18:06:54.086388: step 20188, loss 1.71363e-07, acc 1
2017-08-08T18:06:54.411701: step 20189, loss 2.14203e-07, acc 1
2017-08-08T18:06:54.747614: step 20190, loss 6.56877e-06, acc 1
2017-08-08T18:06:55.082598: step 20191, loss 6.70551e-08, acc 1
2017-08-08T18:06:55.424286: step 20192, loss 4.24677e-07, acc 1
2017-08-08T18:06:55.670765: step 20193, loss 0, acc 1
2017-08-08T18:06:56.021745: step 20194, loss 8.38189e-08, acc 1
2017-08-08T18:06:56.478220: step 20195, loss 0.0113361, acc 0.984375
2017-08-08T18:06:56.734756: step 20196, loss 0.0380308, acc 0.984375
2017-08-08T18:06:56.985770: step 20197, loss 0.000195799, acc 1
2017-08-08T18:06:57.321325: step 20198, loss 3.39e-07, acc 1
2017-08-08T18:06:57.724589: step 20199, loss 1.74334e-06, acc 1
2017-08-08T18:06:58.042949: step 20200, loss 4.50756e-07, acc 1

Evaluation:
2017-08-08T18:06:58.503708: step 20200, loss 6.82956, acc 0.711069

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-20200

2017-08-08T18:06:58.998286: step 20201, loss 1.52919e-06, acc 1
2017-08-08T18:06:59.220099: step 20202, loss 7.13773e-05, acc 1
2017-08-08T18:06:59.454892: step 20203, loss 3.15693e-06, acc 1
2017-08-08T18:06:59.794963: step 20204, loss 2.04891e-08, acc 1
2017-08-08T18:07:00.060747: step 20205, loss 2.36554e-07, acc 1
2017-08-08T18:07:00.379447: step 20206, loss 1.06167e-06, acc 1
2017-08-08T18:07:00.664349: step 20207, loss 4.84287e-08, acc 1
2017-08-08T18:07:00.885363: step 20208, loss 1.42855e-05, acc 1
2017-08-08T18:07:01.345965: step 20209, loss 2.15875e-06, acc 1
2017-08-08T18:07:01.654125: step 20210, loss 0, acc 1
2017-08-08T18:07:01.929912: step 20211, loss 1.11759e-08, acc 1
2017-08-08T18:07:02.340788: step 20212, loss 4.77525e-05, acc 1
2017-08-08T18:07:02.919706: step 20213, loss 6.51925e-08, acc 1
2017-08-08T18:07:03.322477: step 20214, loss 6.08071e-05, acc 1
2017-08-08T18:07:03.571649: step 20215, loss 1.06171e-07, acc 1
2017-08-08T18:07:03.902048: step 20216, loss 3.29686e-07, acc 1
2017-08-08T18:07:04.353409: step 20217, loss 3.72529e-09, acc 1
2017-08-08T18:07:04.708067: step 20218, loss 4.84287e-08, acc 1
2017-08-08T18:07:05.046805: step 20219, loss 5.84941e-05, acc 1
2017-08-08T18:07:05.331615: step 20220, loss 1.67638e-08, acc 1
2017-08-08T18:07:05.860686: step 20221, loss 3.72529e-09, acc 1
2017-08-08T18:07:06.317353: step 20222, loss 2.91473e-05, acc 1
2017-08-08T18:07:06.653118: step 20223, loss 1.04308e-07, acc 1
2017-08-08T18:07:06.883084: step 20224, loss 6.46333e-07, acc 1
2017-08-08T18:07:07.233762: step 20225, loss 1.18832e-06, acc 1
2017-08-08T18:07:07.596281: step 20226, loss 1.3411e-07, acc 1
2017-08-08T18:07:07.886825: step 20227, loss 1.26097e-06, acc 1
2017-08-08T18:07:08.140678: step 20228, loss 0, acc 1
2017-08-08T18:07:08.471707: step 20229, loss 1.92057e-05, acc 1
2017-08-08T18:07:08.902970: step 20230, loss 9.91732e-06, acc 1
2017-08-08T18:07:09.230571: step 20231, loss 3.66162e-06, acc 1
2017-08-08T18:07:09.451316: step 20232, loss 3.92071e-06, acc 1
2017-08-08T18:07:09.692160: step 20233, loss 5.88585e-07, acc 1
2017-08-08T18:07:09.974421: step 20234, loss 1.41561e-07, acc 1
2017-08-08T18:07:10.219221: step 20235, loss 2.23517e-08, acc 1
2017-08-08T18:07:10.469373: step 20236, loss 1.52736e-07, acc 1
2017-08-08T18:07:10.878776: step 20237, loss 0, acc 1
2017-08-08T18:07:11.258599: step 20238, loss 6.51925e-08, acc 1
2017-08-08T18:07:11.684582: step 20239, loss 3.72529e-09, acc 1
2017-08-08T18:07:11.993694: step 20240, loss 2.46232e-06, acc 1
2017-08-08T18:07:12.265368: step 20241, loss 1.42301e-06, acc 1
2017-08-08T18:07:12.685339: step 20242, loss 5.79391e-06, acc 1
2017-08-08T18:07:12.952880: step 20243, loss 0.0706742, acc 0.984375
2017-08-08T18:07:13.265009: step 20244, loss 2.6077e-08, acc 1
2017-08-08T18:07:13.528105: step 20245, loss 3.72529e-08, acc 1
2017-08-08T18:07:13.959526: step 20246, loss 0.00034265, acc 1
2017-08-08T18:07:14.345360: step 20247, loss 5.6065e-07, acc 1
2017-08-08T18:07:14.661787: step 20248, loss 0.000942621, acc 1
2017-08-08T18:07:14.944077: step 20249, loss 0.100148, acc 0.96875
2017-08-08T18:07:15.233380: step 20250, loss 7.94728e-09, acc 1
2017-08-08T18:07:15.535590: step 20251, loss 0.000266924, acc 1
2017-08-08T18:07:15.771258: step 20252, loss 2.16065e-07, acc 1
2017-08-08T18:07:16.072137: step 20253, loss 1.82717e-06, acc 1
2017-08-08T18:07:16.479300: step 20254, loss 9.49948e-08, acc 1
2017-08-08T18:07:16.913342: step 20255, loss 9.9649e-07, acc 1
2017-08-08T18:07:17.224032: step 20256, loss 6.90159e-05, acc 1
2017-08-08T18:07:17.473474: step 20257, loss 0, acc 1
2017-08-08T18:07:17.766323: step 20258, loss 3.65074e-07, acc 1
2017-08-08T18:07:18.230396: step 20259, loss 2.60769e-07, acc 1
2017-08-08T18:07:18.492439: step 20260, loss 7.57588e-05, acc 1
2017-08-08T18:07:18.827130: step 20261, loss 1.11755e-06, acc 1
2017-08-08T18:07:19.223713: step 20262, loss 1.92216e-06, acc 1
2017-08-08T18:07:19.643603: step 20263, loss 0.000184863, acc 1
2017-08-08T18:07:19.948687: step 20264, loss 8.19546e-07, acc 1
2017-08-08T18:07:20.280646: step 20265, loss 9.31322e-09, acc 1
2017-08-08T18:07:20.505246: step 20266, loss 5.02461e-06, acc 1
2017-08-08T18:07:20.833914: step 20267, loss 1.71363e-07, acc 1
2017-08-08T18:07:21.162553: step 20268, loss 0.000105534, acc 1
2017-08-08T18:07:21.467529: step 20269, loss 5.51872e-05, acc 1
2017-08-08T18:07:21.759468: step 20270, loss 1.67638e-08, acc 1
2017-08-08T18:07:22.161364: step 20271, loss 3.433e-05, acc 1
2017-08-08T18:07:22.504607: step 20272, loss 0.00331469, acc 1
2017-08-08T18:07:22.814277: step 20273, loss 4.6193e-07, acc 1
2017-08-08T18:07:23.080278: step 20274, loss 2.01164e-07, acc 1
2017-08-08T18:07:23.315691: step 20275, loss 2.06753e-07, acc 1
2017-08-08T18:07:23.832182: step 20276, loss 0.000172634, acc 1
2017-08-08T18:07:24.108672: step 20277, loss 1.7285e-06, acc 1
2017-08-08T18:07:24.450029: step 20278, loss 1.13991e-06, acc 1
2017-08-08T18:07:24.791365: step 20279, loss 2.08951e-05, acc 1
2017-08-08T18:07:25.184779: step 20280, loss 0.000972306, acc 1
2017-08-08T18:07:25.527215: step 20281, loss 1.5683e-06, acc 1
2017-08-08T18:07:25.864145: step 20282, loss 1.3411e-07, acc 1
2017-08-08T18:07:26.129097: step 20283, loss 0.000176377, acc 1
2017-08-08T18:07:26.418500: step 20284, loss 5.58793e-09, acc 1
2017-08-08T18:07:26.752496: step 20285, loss 4.99117e-05, acc 1
2017-08-08T18:07:27.013055: step 20286, loss 0.0153734, acc 0.984375
2017-08-08T18:07:27.369015: step 20287, loss 0.000135704, acc 1
2017-08-08T18:07:27.592470: step 20288, loss 8.97769e-07, acc 1
2017-08-08T18:07:28.085595: step 20289, loss 1.67638e-08, acc 1
2017-08-08T18:07:28.480919: step 20290, loss 3.72529e-08, acc 1
2017-08-08T18:07:28.688834: step 20291, loss 5.34119e-06, acc 1
2017-08-08T18:07:28.947875: step 20292, loss 0.0187008, acc 0.984375
2017-08-08T18:07:29.334333: step 20293, loss 1.34778e-05, acc 1
2017-08-08T18:07:29.640399: step 20294, loss 2.34057e-05, acc 1
2017-08-08T18:07:29.886767: step 20295, loss 0.000437091, acc 1
2017-08-08T18:07:30.148093: step 20296, loss 0.00280784, acc 1
2017-08-08T18:07:30.537352: step 20297, loss 3.44195e-06, acc 1
2017-08-08T18:07:30.996154: step 20298, loss 7.93478e-07, acc 1
2017-08-08T18:07:31.413677: step 20299, loss 7.15248e-07, acc 1
2017-08-08T18:07:31.689657: step 20300, loss 3.16649e-08, acc 1

Evaluation:
2017-08-08T18:07:32.318264: step 20300, loss 7.11964, acc 0.707317

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-20300

2017-08-08T18:07:32.772351: step 20301, loss 1.32219e-05, acc 1
2017-08-08T18:07:33.056603: step 20302, loss 1.70428e-06, acc 1
2017-08-08T18:07:33.421436: step 20303, loss 1.25764e-05, acc 1
2017-08-08T18:07:33.797375: step 20304, loss 1.97057e-06, acc 1
2017-08-08T18:07:34.190626: step 20305, loss 6.09081e-07, acc 1
2017-08-08T18:07:34.523202: step 20306, loss 2.62933e-05, acc 1
2017-08-08T18:07:34.729215: step 20307, loss 2.50308e-05, acc 1
2017-08-08T18:07:35.114283: step 20308, loss 5.22013e-06, acc 1
2017-08-08T18:07:35.432338: step 20309, loss 5.47535e-05, acc 1
2017-08-08T18:07:35.692541: step 20310, loss 5.70046e-05, acc 1
2017-08-08T18:07:35.914915: step 20311, loss 0.00189554, acc 1
2017-08-08T18:07:36.270215: step 20312, loss 2.5777e-06, acc 1
2017-08-08T18:07:36.668200: step 20313, loss 0.00131977, acc 1
2017-08-08T18:07:37.045334: step 20314, loss 4.65661e-08, acc 1
2017-08-08T18:07:37.387131: step 20315, loss 2.60684e-05, acc 1
2017-08-08T18:07:37.691166: step 20316, loss 0.00731562, acc 1
2017-08-08T18:07:37.982574: step 20317, loss 3.35276e-08, acc 1
2017-08-08T18:07:38.204616: step 20318, loss 2.66356e-07, acc 1
2017-08-08T18:07:38.459321: step 20319, loss 0.00530092, acc 1
2017-08-08T18:07:38.745287: step 20320, loss 6.56442e-05, acc 1
2017-08-08T18:07:39.164196: step 20321, loss 0.0730809, acc 0.984375
2017-08-08T18:07:39.556314: step 20322, loss 0.000109879, acc 1
2017-08-08T18:07:39.901761: step 20323, loss 8.41902e-07, acc 1
2017-08-08T18:07:40.198608: step 20324, loss 1.25455e-05, acc 1
2017-08-08T18:07:40.444153: step 20325, loss 3.42516e-05, acc 1
2017-08-08T18:07:40.811231: step 20326, loss 1.67638e-08, acc 1
2017-08-08T18:07:41.111392: step 20327, loss 6.33298e-08, acc 1
2017-08-08T18:07:41.412167: step 20328, loss 0.000145, acc 1
2017-08-08T18:07:41.625402: step 20329, loss 2.87194e-06, acc 1
2017-08-08T18:07:41.998375: step 20330, loss 4.80556e-07, acc 1
2017-08-08T18:07:42.417345: step 20331, loss 9.96499e-07, acc 1
2017-08-08T18:07:42.771685: step 20332, loss 8.00919e-06, acc 1
2017-08-08T18:07:43.050181: step 20333, loss 0.000385104, acc 1
2017-08-08T18:07:43.407511: step 20334, loss 2.24063e-06, acc 1
2017-08-08T18:07:43.793395: step 20335, loss 4.09782e-08, acc 1
2017-08-08T18:07:44.082729: step 20336, loss 9.38739e-05, acc 1
2017-08-08T18:07:44.398711: step 20337, loss 1.86264e-08, acc 1
2017-08-08T18:07:44.770841: step 20338, loss 4.69317e-06, acc 1
2017-08-08T18:07:45.158086: step 20339, loss 1.30385e-08, acc 1
2017-08-08T18:07:45.612240: step 20340, loss 1.75638e-06, acc 1
2017-08-08T18:07:45.902141: step 20341, loss 5.06991e-05, acc 1
2017-08-08T18:07:46.148854: step 20342, loss 3.22205e-06, acc 1
2017-08-08T18:07:46.474687: step 20343, loss 8.82583e-06, acc 1
2017-08-08T18:07:46.755472: step 20344, loss 0, acc 1
2017-08-08T18:07:47.065325: step 20345, loss 1.41186e-06, acc 1
2017-08-08T18:07:47.388230: step 20346, loss 0.0369483, acc 0.984375
2017-08-08T18:07:47.762352: step 20347, loss 5.51334e-07, acc 1
2017-08-08T18:07:48.117372: step 20348, loss 9.25707e-07, acc 1
2017-08-08T18:07:48.415877: step 20349, loss 2.22541e-05, acc 1
2017-08-08T18:07:48.696883: step 20350, loss 5.43889e-07, acc 1
2017-08-08T18:07:48.936144: step 20351, loss 0.000191006, acc 1
2017-08-08T18:07:49.304415: step 20352, loss 9.31322e-09, acc 1
2017-08-08T18:07:49.656428: step 20353, loss 0, acc 1
2017-08-08T18:07:49.885908: step 20354, loss 0.00210276, acc 1
2017-08-08T18:07:50.180269: step 20355, loss 5.58793e-08, acc 1
2017-08-08T18:07:50.529307: step 20356, loss 1.3336e-06, acc 1
2017-08-08T18:07:50.893444: step 20357, loss 0, acc 1
2017-08-08T18:07:51.140720: step 20358, loss 1.18089e-06, acc 1
2017-08-08T18:07:51.405000: step 20359, loss 1.68747e-06, acc 1
2017-08-08T18:07:51.659705: step 20360, loss 9.15225e-06, acc 1
2017-08-08T18:07:52.071578: step 20361, loss 1.06169e-06, acc 1
2017-08-08T18:07:52.330188: step 20362, loss 7.45058e-09, acc 1
2017-08-08T18:07:52.635211: step 20363, loss 0, acc 1
2017-08-08T18:07:53.000225: step 20364, loss 1.41636e-05, acc 1
2017-08-08T18:07:53.421632: step 20365, loss 1.58886e-05, acc 1
2017-08-08T18:07:53.819795: step 20366, loss 2.79397e-08, acc 1
2017-08-08T18:07:54.018443: step 20367, loss 2.25275e-05, acc 1
2017-08-08T18:07:54.256922: step 20368, loss 8.86174e-05, acc 1
2017-08-08T18:07:54.702116: step 20369, loss 3.53902e-08, acc 1
2017-08-08T18:07:55.021421: step 20370, loss 2.766e-05, acc 1
2017-08-08T18:07:55.344421: step 20371, loss 0.0302814, acc 0.984375
2017-08-08T18:07:55.591076: step 20372, loss 3.44586e-07, acc 1
2017-08-08T18:07:56.040837: step 20373, loss 3.54959e-05, acc 1
2017-08-08T18:07:56.462649: step 20374, loss 0.0890439, acc 0.984375
2017-08-08T18:07:56.792732: step 20375, loss 8.19562e-08, acc 1
2017-08-08T18:07:57.075319: step 20376, loss 0.000280411, acc 1
2017-08-08T18:07:57.532585: step 20377, loss 2.6077e-08, acc 1
2017-08-08T18:07:57.837696: step 20378, loss 1.39698e-07, acc 1
2017-08-08T18:07:58.095545: step 20379, loss 0, acc 1
2017-08-08T18:07:58.384919: step 20380, loss 1.65055e-05, acc 1
2017-08-08T18:07:58.814148: step 20381, loss 3.53902e-08, acc 1
2017-08-08T18:07:59.249964: step 20382, loss 1.03373e-06, acc 1
2017-08-08T18:07:59.557281: step 20383, loss 1.23389e-05, acc 1
2017-08-08T18:07:59.760392: step 20384, loss 1.86265e-09, acc 1
2017-08-08T18:07:59.959571: step 20385, loss 0, acc 1
2017-08-08T18:08:00.303028: step 20386, loss 1.54313e-05, acc 1
2017-08-08T18:08:00.624203: step 20387, loss 7.45058e-09, acc 1
2017-08-08T18:08:00.965324: step 20388, loss 0.000594267, acc 1
2017-08-08T18:08:01.231154: step 20389, loss 3.70664e-07, acc 1
2017-08-08T18:08:01.673520: step 20390, loss 9.31321e-08, acc 1
2017-08-08T18:08:02.021375: step 20391, loss 2.4773e-07, acc 1
2017-08-08T18:08:02.397005: step 20392, loss 6.35042e-06, acc 1
2017-08-08T18:08:02.694240: step 20393, loss 4.47861e-05, acc 1
2017-08-08T18:08:03.033353: step 20394, loss 1.86264e-07, acc 1
2017-08-08T18:08:03.427195: step 20395, loss 1.4156e-07, acc 1
2017-08-08T18:08:03.711818: step 20396, loss 1.11759e-08, acc 1
2017-08-08T18:08:04.080481: step 20397, loss 6.05266e-06, acc 1
2017-08-08T18:08:04.385216: step 20398, loss 2.9818e-05, acc 1
2017-08-08T18:08:04.815413: step 20399, loss 3.98601e-07, acc 1
2017-08-08T18:08:05.204433: step 20400, loss 0.000304948, acc 1

Evaluation:
2017-08-08T18:08:05.942998: step 20400, loss 6.96665, acc 0.716698

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-20400

2017-08-08T18:08:06.451584: step 20401, loss 1.43423e-07, acc 1
2017-08-08T18:08:06.855083: step 20402, loss 8.27589e-05, acc 1
2017-08-08T18:08:07.161645: step 20403, loss 3.49398e-06, acc 1
2017-08-08T18:08:07.439248: step 20404, loss 2.97836e-05, acc 1
2017-08-08T18:08:07.870198: step 20405, loss 2.99485e-06, acc 1
2017-08-08T18:08:08.250055: step 20406, loss 1.21733e-05, acc 1
2017-08-08T18:08:08.579637: step 20407, loss 0.000613084, acc 1
2017-08-08T18:08:08.918950: step 20408, loss 5.05654e-06, acc 1
2017-08-08T18:08:09.299389: step 20409, loss 1.67638e-08, acc 1
2017-08-08T18:08:09.723962: step 20410, loss 7.63683e-08, acc 1
2017-08-08T18:08:09.986796: step 20411, loss 4.63792e-07, acc 1
2017-08-08T18:08:10.272182: step 20412, loss 3.38034e-06, acc 1
2017-08-08T18:08:10.590440: step 20413, loss 0.000121758, acc 1
2017-08-08T18:08:11.021243: step 20414, loss 2.17928e-07, acc 1
2017-08-08T18:08:11.386959: step 20415, loss 8.56816e-08, acc 1
2017-08-08T18:08:11.727776: step 20416, loss 2.17385e-05, acc 1
2017-08-08T18:08:11.983914: step 20417, loss 0.000598948, acc 1
2017-08-08T18:08:12.220778: step 20418, loss 1.21306e-05, acc 1
2017-08-08T18:08:12.588829: step 20419, loss 5.06635e-07, acc 1
2017-08-08T18:08:12.814686: step 20420, loss 3.14199e-06, acc 1
2017-08-08T18:08:13.057772: step 20421, loss 1.24045e-05, acc 1
2017-08-08T18:08:13.325610: step 20422, loss 0, acc 1
2017-08-08T18:08:13.812890: step 20423, loss 2.98023e-08, acc 1
2017-08-08T18:08:14.205011: step 20424, loss 5.1978e-06, acc 1
2017-08-08T18:08:14.571029: step 20425, loss 1.67638e-08, acc 1
2017-08-08T18:08:14.884605: step 20426, loss 3.48008e-05, acc 1
2017-08-08T18:08:15.205489: step 20427, loss 1.0841e-05, acc 1
2017-08-08T18:08:15.668499: step 20428, loss 3.07681e-06, acc 1
2017-08-08T18:08:15.909011: step 20429, loss 0.000972487, acc 1
2017-08-08T18:08:16.190305: step 20430, loss 2.84982e-07, acc 1
2017-08-08T18:08:16.462110: step 20431, loss 9.15211e-06, acc 1
2017-08-08T18:08:16.770395: step 20432, loss 3.23018e-05, acc 1
2017-08-08T18:08:17.126521: step 20433, loss 4.28408e-08, acc 1
2017-08-08T18:08:17.445391: step 20434, loss 0.000215931, acc 1
2017-08-08T18:08:17.696459: step 20435, loss 4.11641e-07, acc 1
2017-08-08T18:08:18.048544: step 20436, loss 1.8193e-05, acc 1
2017-08-08T18:08:18.383193: step 20437, loss 5.96045e-08, acc 1
2017-08-08T18:08:18.630199: step 20438, loss 3.66901e-06, acc 1
2017-08-08T18:08:18.844988: step 20439, loss 5.58793e-09, acc 1
2017-08-08T18:08:19.120247: step 20440, loss 0.000391163, acc 1
2017-08-08T18:08:19.577958: step 20441, loss 1.82538e-07, acc 1
2017-08-08T18:08:19.977849: step 20442, loss 2.23711e-05, acc 1
2017-08-08T18:08:20.293846: step 20443, loss 0, acc 1
2017-08-08T18:08:20.534220: step 20444, loss 9.31321e-08, acc 1
2017-08-08T18:08:20.982055: step 20445, loss 8.98844e-06, acc 1
2017-08-08T18:08:21.283342: step 20446, loss 1.13245e-06, acc 1
2017-08-08T18:08:21.555072: step 20447, loss 9.59247e-07, acc 1
2017-08-08T18:08:21.837319: step 20448, loss 4.65661e-08, acc 1
2017-08-08T18:08:22.251860: step 20449, loss 9.81159e-05, acc 1
2017-08-08T18:08:22.673860: step 20450, loss 4.41634e-05, acc 1
2017-08-08T18:08:23.090213: step 20451, loss 7.6367e-07, acc 1
2017-08-08T18:08:23.350069: step 20452, loss 0.00017071, acc 1
2017-08-08T18:08:23.557578: step 20453, loss 7.70768e-06, acc 1
2017-08-08T18:08:23.875530: step 20454, loss 3.37135e-07, acc 1
2017-08-08T18:08:24.129020: step 20455, loss 1.23993e-05, acc 1
2017-08-08T18:08:24.363766: step 20456, loss 4.10473e-06, acc 1
2017-08-08T18:08:24.613409: step 20457, loss 2.27241e-07, acc 1
2017-08-08T18:08:24.942124: step 20458, loss 1.35868e-05, acc 1
2017-08-08T18:08:25.304650: step 20459, loss 0.149, acc 0.984375
2017-08-08T18:08:25.707841: step 20460, loss 5.98174e-05, acc 1
2017-08-08T18:08:26.032595: step 20461, loss 8.97051e-06, acc 1
2017-08-08T18:08:26.295445: step 20462, loss 0.000653146, acc 1
2017-08-08T18:08:26.764772: step 20463, loss 5.73404e-06, acc 1
2017-08-08T18:08:27.014355: step 20464, loss 1.49012e-08, acc 1
2017-08-08T18:08:27.295844: step 20465, loss 4.28408e-08, acc 1
2017-08-08T18:08:27.618056: step 20466, loss 1.09896e-07, acc 1
2017-08-08T18:08:28.063608: step 20467, loss 2.04891e-08, acc 1
2017-08-08T18:08:28.462432: step 20468, loss 0.00109702, acc 1
2017-08-08T18:08:28.822079: step 20469, loss 0, acc 1
2017-08-08T18:08:29.023096: step 20470, loss 0.0428314, acc 0.984375
2017-08-08T18:08:29.246870: step 20471, loss 0.118321, acc 0.96875
2017-08-08T18:08:29.723972: step 20472, loss 6.63343e-06, acc 1
2017-08-08T18:08:29.948581: step 20473, loss 4.02125e-06, acc 1
2017-08-08T18:08:30.147077: step 20474, loss 4.00173e-05, acc 1
2017-08-08T18:08:30.482449: step 20475, loss 0.00765599, acc 1
2017-08-08T18:08:30.720337: step 20476, loss 2.49026e-05, acc 1
2017-08-08T18:08:30.988605: step 20477, loss 1.6622e-05, acc 1
2017-08-08T18:08:31.164827: step 20478, loss 0.000155452, acc 1
2017-08-08T18:08:31.346754: step 20479, loss 1.34105e-06, acc 1
2017-08-08T18:08:31.563842: step 20480, loss 7.18151e-05, acc 1
2017-08-08T18:08:31.947665: step 20481, loss 1.86264e-08, acc 1
2017-08-08T18:08:32.209794: step 20482, loss 4.91665e-06, acc 1
2017-08-08T18:08:32.436306: step 20483, loss 0.00541017, acc 1
2017-08-08T18:08:32.749322: step 20484, loss 5.58793e-09, acc 1
2017-08-08T18:08:33.052017: step 20485, loss 2.44467e-05, acc 1
2017-08-08T18:08:33.350913: step 20486, loss 1.41561e-07, acc 1
2017-08-08T18:08:33.553260: step 20487, loss 2.111e-05, acc 1
2017-08-08T18:08:33.728786: step 20488, loss 2.5257e-05, acc 1
2017-08-08T18:08:34.024017: step 20489, loss 0.000529931, acc 1
2017-08-08T18:08:34.240460: step 20490, loss 7.89872e-05, acc 1
2017-08-08T18:08:34.421369: step 20491, loss 3.80024e-05, acc 1
2017-08-08T18:08:34.689310: step 20492, loss 1.13774e-05, acc 1
2017-08-08T18:08:35.034576: step 20493, loss 8.88798e-06, acc 1
2017-08-08T18:08:35.363771: step 20494, loss 3.46449e-07, acc 1
2017-08-08T18:08:35.635884: step 20495, loss 1.86265e-09, acc 1
2017-08-08T18:08:35.861332: step 20496, loss 1.40763e-05, acc 1
2017-08-08T18:08:36.127136: step 20497, loss 0.000677283, acc 1
2017-08-08T18:08:36.328659: step 20498, loss 1.24991e-05, acc 1
2017-08-08T18:08:36.498982: step 20499, loss 6.51925e-08, acc 1
2017-08-08T18:08:36.687299: step 20500, loss 2.9204e-06, acc 1

Evaluation:
2017-08-08T18:08:37.760612: step 20500, loss 7.06127, acc 0.713884

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-20500

2017-08-08T18:08:38.486880: step 20501, loss 6.63153e-06, acc 1
2017-08-08T18:08:38.711721: step 20502, loss 0.0969191, acc 0.984375
2017-08-08T18:08:39.101353: step 20503, loss 1.58666e-05, acc 1
2017-08-08T18:08:39.450579: step 20504, loss 2.83992e-05, acc 1
2017-08-08T18:08:39.723097: step 20505, loss 1.86264e-08, acc 1
2017-08-08T18:08:39.999927: step 20506, loss 1.89989e-07, acc 1
2017-08-08T18:08:40.369968: step 20507, loss 0, acc 1
2017-08-08T18:08:40.839325: step 20508, loss 5.27982e-06, acc 1
2017-08-08T18:08:41.184448: step 20509, loss 5.83002e-07, acc 1
2017-08-08T18:08:41.435978: step 20510, loss 7.5002e-05, acc 1
2017-08-08T18:08:41.643432: step 20511, loss 2.42144e-08, acc 1
2017-08-08T18:08:41.959565: step 20512, loss 5.75325e-05, acc 1
2017-08-08T18:08:42.226843: step 20513, loss 4.19049e-06, acc 1
2017-08-08T18:08:42.452083: step 20514, loss 2.64494e-07, acc 1
2017-08-08T18:08:42.678250: step 20515, loss 6.0535e-07, acc 1
2017-08-08T18:08:43.009301: step 20516, loss 0.039701, acc 0.984375
2017-08-08T18:08:43.389356: step 20517, loss 2.75089e-06, acc 1
2017-08-08T18:08:43.675660: step 20518, loss 3.12344e-06, acc 1
2017-08-08T18:08:43.951544: step 20519, loss 1.80676e-07, acc 1
2017-08-08T18:08:44.203242: step 20520, loss 7.45058e-09, acc 1
2017-08-08T18:08:44.629357: step 20521, loss 9.01377e-06, acc 1
2017-08-08T18:08:44.960523: step 20522, loss 5.58793e-09, acc 1
2017-08-08T18:08:45.163995: step 20523, loss 0.000260967, acc 1
2017-08-08T18:08:45.339853: step 20524, loss 6.51919e-07, acc 1
2017-08-08T18:08:45.569257: step 20525, loss 2.98023e-08, acc 1
2017-08-08T18:08:45.829339: step 20526, loss 7.15252e-05, acc 1
2017-08-08T18:08:46.114518: step 20527, loss 7.95329e-07, acc 1
2017-08-08T18:08:46.335113: step 20528, loss 0.00653369, acc 1
2017-08-08T18:08:46.506678: step 20529, loss 0.000468173, acc 1
2017-08-08T18:08:46.808747: step 20530, loss 6.48187e-07, acc 1
2017-08-08T18:08:47.117441: step 20531, loss 6.13631e-06, acc 1
2017-08-08T18:08:47.377232: step 20532, loss 1.64032e-05, acc 1
2017-08-08T18:08:47.647301: step 20533, loss 0.134414, acc 0.984375
2017-08-08T18:08:47.945451: step 20534, loss 0.000161562, acc 1
2017-08-08T18:08:48.248457: step 20535, loss 0.000213123, acc 1
2017-08-08T18:08:48.607526: step 20536, loss 0, acc 1
2017-08-08T18:08:48.921702: step 20537, loss 4.45169e-07, acc 1
2017-08-08T18:08:49.179050: step 20538, loss 8.2514e-07, acc 1
2017-08-08T18:08:49.612080: step 20539, loss 0.000124843, acc 1
2017-08-08T18:08:49.940309: step 20540, loss 4.6716e-05, acc 1
2017-08-08T18:08:50.244784: step 20541, loss 1.15484e-07, acc 1
2017-08-08T18:08:50.523976: step 20542, loss 0.00365454, acc 1
2017-08-08T18:08:50.921332: step 20543, loss 3.85411e-05, acc 1
2017-08-08T18:08:51.308134: step 20544, loss 2.57022e-05, acc 1
2017-08-08T18:08:51.801348: step 20545, loss 1.82538e-07, acc 1
2017-08-08T18:08:52.102136: step 20546, loss 3.53902e-08, acc 1
2017-08-08T18:08:52.462341: step 20547, loss 5.2132e-05, acc 1
2017-08-08T18:08:52.873159: step 20548, loss 1.39698e-07, acc 1
2017-08-08T18:08:53.185379: step 20549, loss 0, acc 1
2017-08-08T18:08:53.440131: step 20550, loss 2.78153e-07, acc 1
2017-08-08T18:08:53.787786: step 20551, loss 2.40281e-07, acc 1
2017-08-08T18:08:54.228524: step 20552, loss 7.45058e-09, acc 1
2017-08-08T18:08:54.651786: step 20553, loss 3.35276e-08, acc 1
2017-08-08T18:08:54.905235: step 20554, loss 0, acc 1
2017-08-08T18:08:55.155153: step 20555, loss 0.000334511, acc 1
2017-08-08T18:08:55.561606: step 20556, loss 0.000347524, acc 1
2017-08-08T18:08:55.832255: step 20557, loss 4.43306e-07, acc 1
2017-08-08T18:08:56.057938: step 20558, loss 3.34683e-06, acc 1
2017-08-08T18:08:56.295136: step 20559, loss 3.35276e-08, acc 1
2017-08-08T18:08:56.665732: step 20560, loss 0.000399023, acc 1
2017-08-08T18:08:57.025905: step 20561, loss 7.45058e-09, acc 1
2017-08-08T18:08:57.305383: step 20562, loss 0.00514465, acc 1
2017-08-08T18:08:57.541792: step 20563, loss 1.86265e-09, acc 1
2017-08-08T18:08:57.896997: step 20564, loss 0.00157664, acc 1
2017-08-08T18:08:58.332034: step 20565, loss 0.00746973, acc 1
2017-08-08T18:08:58.612938: step 20566, loss 5.58794e-09, acc 1
2017-08-08T18:08:58.876282: step 20567, loss 9.15336e-06, acc 1
2017-08-08T18:08:59.125714: step 20568, loss 0.000195887, acc 1
2017-08-08T18:08:59.585394: step 20569, loss 4.64491e-05, acc 1
2017-08-08T18:09:00.037377: step 20570, loss 3.23136e-06, acc 1
2017-08-08T18:09:00.416894: step 20571, loss 1.57576e-06, acc 1
2017-08-08T18:09:00.669978: step 20572, loss 6.07233e-05, acc 1
2017-08-08T18:09:00.886063: step 20573, loss 2.872e-05, acc 1
2017-08-08T18:09:01.331665: step 20574, loss 7.20831e-07, acc 1
2017-08-08T18:09:01.695980: step 20575, loss 2.42144e-08, acc 1
2017-08-08T18:09:02.258170: step 20576, loss 8.56815e-08, acc 1
2017-08-08T18:09:02.551584: step 20577, loss 0.0204858, acc 0.984375
2017-08-08T18:09:02.821426: step 20578, loss 0, acc 1
2017-08-08T18:09:03.333402: step 20579, loss 0.000359941, acc 1
2017-08-08T18:09:03.796381: step 20580, loss 3.70261e-06, acc 1
2017-08-08T18:09:04.233847: step 20581, loss 0.00772346, acc 1
2017-08-08T18:09:04.491144: step 20582, loss 0.000181088, acc 1
2017-08-08T18:09:04.769706: step 20583, loss 6.18298e-06, acc 1
2017-08-08T18:09:05.122208: step 20584, loss 0.155579, acc 0.984375
2017-08-08T18:09:05.431957: step 20585, loss 1.11759e-08, acc 1
2017-08-08T18:09:05.671033: step 20586, loss 8.30726e-07, acc 1
2017-08-08T18:09:05.952134: step 20587, loss 6.94757e-07, acc 1
2017-08-08T18:09:06.329324: step 20588, loss 0.0105424, acc 1
2017-08-08T18:09:06.691962: step 20589, loss 3.35276e-08, acc 1
2017-08-08T18:09:07.025175: step 20590, loss 2.77533e-07, acc 1
2017-08-08T18:09:07.290182: step 20591, loss 3.27824e-07, acc 1
2017-08-08T18:09:07.540550: step 20592, loss 3.92596e-06, acc 1
2017-08-08T18:09:07.835149: step 20593, loss 1.86265e-09, acc 1
2017-08-08T18:09:08.228235: step 20594, loss 1.03773e-05, acc 1
2017-08-08T18:09:08.506811: step 20595, loss 6.53775e-07, acc 1
2017-08-08T18:09:08.774963: step 20596, loss 9.23864e-07, acc 1
2017-08-08T18:09:09.127565: step 20597, loss 0.000150866, acc 1
2017-08-08T18:09:09.548934: step 20598, loss 1.6481e-05, acc 1
2017-08-08T18:09:09.885672: step 20599, loss 4.24677e-07, acc 1
2017-08-08T18:09:10.253849: step 20600, loss 1.1049e-05, acc 1

Evaluation:
2017-08-08T18:09:11.137367: step 20600, loss 7.20197, acc 0.707317

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-20600

2017-08-08T18:09:11.627374: step 20601, loss 2.44005e-07, acc 1
2017-08-08T18:09:11.883183: step 20602, loss 1.86265e-09, acc 1
2017-08-08T18:09:12.235947: step 20603, loss 2.23516e-07, acc 1
2017-08-08T18:09:12.684941: step 20604, loss 0, acc 1
2017-08-08T18:09:13.089635: step 20605, loss 4.18114e-05, acc 1
2017-08-08T18:09:13.375596: step 20606, loss 1.86264e-08, acc 1
2017-08-08T18:09:13.690719: step 20607, loss 0.0105929, acc 1
2017-08-08T18:09:13.936698: step 20608, loss 6.15298e-05, acc 1
2017-08-08T18:09:14.131344: step 20609, loss 3.16627e-06, acc 1
2017-08-08T18:09:14.395594: step 20610, loss 0.000465658, acc 1
2017-08-08T18:09:14.650953: step 20611, loss 3.65787e-06, acc 1
2017-08-08T18:09:15.018867: step 20612, loss 9.3626e-06, acc 1
2017-08-08T18:09:15.455242: step 20613, loss 1.3895e-06, acc 1
2017-08-08T18:09:15.844704: step 20614, loss 0.000489969, acc 1
2017-08-08T18:09:16.091041: step 20615, loss 0, acc 1
2017-08-08T18:09:16.345475: step 20616, loss 1.07378e-05, acc 1
2017-08-08T18:09:16.670416: step 20617, loss 3.72529e-09, acc 1
2017-08-08T18:09:16.887384: step 20618, loss 1.32247e-07, acc 1
2017-08-08T18:09:17.115136: step 20619, loss 7.65528e-07, acc 1
2017-08-08T18:09:17.394964: step 20620, loss 0.0223408, acc 0.984375
2017-08-08T18:09:17.733351: step 20621, loss 1.47515e-06, acc 1
2017-08-08T18:09:18.026733: step 20622, loss 5.45749e-07, acc 1
2017-08-08T18:09:18.265436: step 20623, loss 4.79762e-06, acc 1
2017-08-08T18:09:18.438907: step 20624, loss 7.62928e-05, acc 1
2017-08-08T18:09:18.685767: step 20625, loss 3.32079e-06, acc 1
2017-08-08T18:09:19.014012: step 20626, loss 0.000242216, acc 1
2017-08-08T18:09:19.240451: step 20627, loss 1.36723e-05, acc 1
2017-08-08T18:09:19.549530: step 20628, loss 0.0537909, acc 0.984375
2017-08-08T18:09:19.890424: step 20629, loss 2.9502e-06, acc 1
2017-08-08T18:09:20.230873: step 20630, loss 3.72529e-09, acc 1
2017-08-08T18:09:20.565383: step 20631, loss 1.36712e-06, acc 1
2017-08-08T18:09:20.897885: step 20632, loss 9.06805e-06, acc 1
2017-08-08T18:09:21.160933: step 20633, loss 2.95205e-06, acc 1
2017-08-08T18:09:21.537416: step 20634, loss 1.43423e-07, acc 1
2017-08-08T18:09:21.903002: step 20635, loss 0.000500543, acc 1
2017-08-08T18:09:22.143636: step 20636, loss 0.0442857, acc 0.984375
2017-08-08T18:09:22.425007: step 20637, loss 3.53902e-08, acc 1
2017-08-08T18:09:22.816087: step 20638, loss 4.47034e-08, acc 1
2017-08-08T18:09:23.281387: step 20639, loss 1.32247e-07, acc 1
2017-08-08T18:09:23.668674: step 20640, loss 9.31322e-09, acc 1
2017-08-08T18:09:23.954627: step 20641, loss 0.000395404, acc 1
2017-08-08T18:09:24.249952: step 20642, loss 2.35875e-05, acc 1
2017-08-08T18:09:24.669532: step 20643, loss 1.05726e-05, acc 1
2017-08-08T18:09:24.936322: step 20644, loss 6.50674e-06, acc 1
2017-08-08T18:09:25.198419: step 20645, loss 1.60187e-07, acc 1
2017-08-08T18:09:25.456209: step 20646, loss 1.99987e-05, acc 1
2017-08-08T18:09:25.853343: step 20647, loss 1.45097e-06, acc 1
2017-08-08T18:09:26.278663: step 20648, loss 0.000606993, acc 1
2017-08-08T18:09:26.568127: step 20649, loss 1.09896e-07, acc 1
2017-08-08T18:09:26.791164: step 20650, loss 5.93126e-05, acc 1
2017-08-08T18:09:27.001815: step 20651, loss 2.82912e-06, acc 1
2017-08-08T18:09:27.375918: step 20652, loss 0.000424329, acc 1
2017-08-08T18:09:27.614553: step 20653, loss 1.81788e-06, acc 1
2017-08-08T18:09:27.862231: step 20654, loss 4.49766e-06, acc 1
2017-08-08T18:09:28.084791: step 20655, loss 7.46278e-06, acc 1
2017-08-08T18:09:28.490466: step 20656, loss 4.13359e-05, acc 1
2017-08-08T18:09:28.856980: step 20657, loss 2.16611e-05, acc 1
2017-08-08T18:09:29.156960: step 20658, loss 0.000975486, acc 1
2017-08-08T18:09:29.420743: step 20659, loss 4.4455e-06, acc 1
2017-08-08T18:09:29.625714: step 20660, loss 8.06506e-07, acc 1
2017-08-08T18:09:30.012033: step 20661, loss 8.00937e-08, acc 1
2017-08-08T18:09:30.300874: step 20662, loss 1.11759e-08, acc 1
2017-08-08T18:09:30.591890: step 20663, loss 8.0837e-07, acc 1
2017-08-08T18:09:30.866359: step 20664, loss 4.65272e-06, acc 1
2017-08-08T18:09:31.245977: step 20665, loss 9.51896e-05, acc 1
2017-08-08T18:09:31.711769: step 20666, loss 2.04891e-08, acc 1
2017-08-08T18:09:32.071859: step 20667, loss 0.000135127, acc 1
2017-08-08T18:09:32.263621: step 20668, loss 2.12514e-06, acc 1
2017-08-08T18:09:32.459830: step 20669, loss 5.92316e-07, acc 1
2017-08-08T18:09:32.773953: step 20670, loss 2.98023e-08, acc 1
2017-08-08T18:09:32.964132: step 20671, loss 1.62179e-05, acc 1
2017-08-08T18:09:33.233313: step 20672, loss 0, acc 1
2017-08-08T18:09:33.471079: step 20673, loss 4.9021e-06, acc 1
2017-08-08T18:09:33.869412: step 20674, loss 5.14086e-07, acc 1
2017-08-08T18:09:34.229174: step 20675, loss 1.09896e-07, acc 1
2017-08-08T18:09:34.554069: step 20676, loss 0.000312741, acc 1
2017-08-08T18:09:34.757645: step 20677, loss 8.86599e-07, acc 1
2017-08-08T18:09:35.006174: step 20678, loss 4.22194e-05, acc 1
2017-08-08T18:09:35.320651: step 20679, loss 4.96349e-06, acc 1
2017-08-08T18:09:35.560549: step 20680, loss 9.12695e-08, acc 1
2017-08-08T18:09:35.765413: step 20681, loss 0.000161029, acc 1
2017-08-08T18:09:36.129376: step 20682, loss 9.16395e-07, acc 1
2017-08-08T18:09:36.644090: step 20683, loss 4.22471e-05, acc 1
2017-08-08T18:09:37.032773: step 20684, loss 1.71362e-07, acc 1
2017-08-08T18:09:37.268945: step 20685, loss 3.53902e-08, acc 1
2017-08-08T18:09:37.694223: step 20686, loss 4.35853e-07, acc 1
2017-08-08T18:09:38.037383: step 20687, loss 2.94295e-07, acc 1
2017-08-08T18:09:38.320214: step 20688, loss 0.014988, acc 0.984375
2017-08-08T18:09:38.579174: step 20689, loss 0.0099113, acc 1
2017-08-08T18:09:38.883251: step 20690, loss 1.58724e-05, acc 1
2017-08-08T18:09:39.306663: step 20691, loss 1.75269e-06, acc 1
2017-08-08T18:09:39.705687: step 20692, loss 1.63912e-07, acc 1
2017-08-08T18:09:40.101815: step 20693, loss 0.00352681, acc 1
2017-08-08T18:09:40.303663: step 20694, loss 3.24636e-06, acc 1
2017-08-08T18:09:40.514856: step 20695, loss 5.20179e-06, acc 1
2017-08-08T18:09:40.848890: step 20696, loss 2.89242e-06, acc 1
2017-08-08T18:09:41.069008: step 20697, loss 1.62602e-06, acc 1
2017-08-08T18:09:41.326894: step 20698, loss 2.88329e-06, acc 1
2017-08-08T18:09:41.564604: step 20699, loss 0, acc 1
2017-08-08T18:09:41.944068: step 20700, loss 5.96046e-09, acc 1

Evaluation:
2017-08-08T18:09:42.861601: step 20700, loss 7.04262, acc 0.718574

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-20700

2017-08-08T18:09:43.321364: step 20701, loss 0.00034082, acc 1
2017-08-08T18:09:43.773219: step 20702, loss 2.09811e-05, acc 1
2017-08-08T18:09:44.019659: step 20703, loss 6.07211e-07, acc 1
2017-08-08T18:09:44.304380: step 20704, loss 0.00324217, acc 1
2017-08-08T18:09:44.588666: step 20705, loss 1.86265e-09, acc 1
2017-08-08T18:09:44.980532: step 20706, loss 3.72525e-07, acc 1
2017-08-08T18:09:45.343928: step 20707, loss 2.41948e-06, acc 1
2017-08-08T18:09:45.811566: step 20708, loss 1.80108e-06, acc 1
2017-08-08T18:09:46.086784: step 20709, loss 9.03367e-07, acc 1
2017-08-08T18:09:46.299013: step 20710, loss 0.000121282, acc 1
2017-08-08T18:09:46.742749: step 20711, loss 5.38364e-05, acc 1
2017-08-08T18:09:47.040876: step 20712, loss 0, acc 1
2017-08-08T18:09:47.311826: step 20713, loss 2.0972e-06, acc 1
2017-08-08T18:09:47.590139: step 20714, loss 5.90449e-07, acc 1
2017-08-08T18:09:47.986052: step 20715, loss 9.31322e-09, acc 1
2017-08-08T18:09:48.407946: step 20716, loss 0.0288505, acc 0.984375
2017-08-08T18:09:48.814488: step 20717, loss 2.23517e-08, acc 1
2017-08-08T18:09:49.119304: step 20718, loss 6.4155e-06, acc 1
2017-08-08T18:09:49.371487: step 20719, loss 2.68219e-07, acc 1
2017-08-08T18:09:49.737736: step 20720, loss 8.38188e-08, acc 1
2017-08-08T18:09:50.015842: step 20721, loss 3.13463e-06, acc 1
2017-08-08T18:09:50.289769: step 20722, loss 1.09597e-05, acc 1
2017-08-08T18:09:50.566580: step 20723, loss 2.2344e-05, acc 1
2017-08-08T18:09:50.828617: step 20724, loss 9.3132e-08, acc 1
2017-08-08T18:09:51.200813: step 20725, loss 4.54479e-07, acc 1
2017-08-08T18:09:51.475259: step 20726, loss 1.79816e-05, acc 1
2017-08-08T18:09:51.767827: step 20727, loss 0.000112084, acc 1
2017-08-08T18:09:52.030842: step 20728, loss 0, acc 1
2017-08-08T18:09:52.247942: step 20729, loss 0, acc 1
2017-08-08T18:09:52.645033: step 20730, loss 2.94297e-07, acc 1
2017-08-08T18:09:52.917902: step 20731, loss 4.58205e-07, acc 1
2017-08-08T18:09:53.208442: step 20732, loss 4.15365e-07, acc 1
2017-08-08T18:09:53.437024: step 20733, loss 0.0635608, acc 0.984375
2017-08-08T18:09:53.740554: step 20734, loss 4.17122e-05, acc 1
2017-08-08T18:09:54.071728: step 20735, loss 3.96741e-07, acc 1
2017-08-08T18:09:54.365294: step 20736, loss 0.0592949, acc 0.984375
2017-08-08T18:09:54.604370: step 20737, loss 0, acc 1
2017-08-08T18:09:54.812440: step 20738, loss 0.0010841, acc 1
2017-08-08T18:09:55.235221: step 20739, loss 0.000162864, acc 1
2017-08-08T18:09:55.484099: step 20740, loss 7.45056e-08, acc 1
2017-08-08T18:09:55.792137: step 20741, loss 0.000461067, acc 1
2017-08-08T18:09:56.078847: step 20742, loss 1.20044e-05, acc 1
2017-08-08T18:09:56.477335: step 20743, loss 0.00013716, acc 1
2017-08-08T18:09:56.854489: step 20744, loss 0.000563885, acc 1
2017-08-08T18:09:57.169223: step 20745, loss 0.00277995, acc 1
2017-08-08T18:09:57.408351: step 20746, loss 0.00267467, acc 1
2017-08-08T18:09:57.696315: step 20747, loss 6.90758e-06, acc 1
2017-08-08T18:09:58.108852: step 20748, loss 0.0320318, acc 0.984375
2017-08-08T18:09:58.357525: step 20749, loss 3.84389e-05, acc 1
2017-08-08T18:09:58.687534: step 20750, loss 1.6782e-06, acc 1
2017-08-08T18:09:59.052047: step 20751, loss 0.000767658, acc 1
2017-08-08T18:09:59.322579: step 20752, loss 9.53468e-05, acc 1
2017-08-08T18:09:59.631824: step 20753, loss 1.17768e-05, acc 1
2017-08-08T18:10:00.008663: step 20754, loss 0.11651, acc 0.984375
2017-08-08T18:10:00.384159: step 20755, loss 2.99885e-07, acc 1
2017-08-08T18:10:00.778069: step 20756, loss 2.11955e-06, acc 1
2017-08-08T18:10:00.988244: step 20757, loss 0.000207414, acc 1
2017-08-08T18:10:01.400155: step 20758, loss 7.8231e-08, acc 1
2017-08-08T18:10:01.665317: step 20759, loss 0.00547109, acc 1
2017-08-08T18:10:02.142773: step 20760, loss 3.91155e-08, acc 1
2017-08-08T18:10:02.429962: step 20761, loss 2.28474e-05, acc 1
2017-08-08T18:10:02.761393: step 20762, loss 2.2406e-06, acc 1
2017-08-08T18:10:03.239749: step 20763, loss 3.00243e-06, acc 1
2017-08-08T18:10:03.595425: step 20764, loss 0.053412, acc 0.984375
2017-08-08T18:10:04.021380: step 20765, loss 9.31322e-09, acc 1
2017-08-08T18:10:04.344670: step 20766, loss 3.24098e-07, acc 1
2017-08-08T18:10:04.595835: step 20767, loss 1.73226e-07, acc 1
2017-08-08T18:10:05.021384: step 20768, loss 1.76383e-06, acc 1
2017-08-08T18:10:05.262970: step 20769, loss 1.11759e-08, acc 1
2017-08-08T18:10:05.626873: step 20770, loss 3.53902e-08, acc 1
2017-08-08T18:10:05.933994: step 20771, loss 0.000131727, acc 1
2017-08-08T18:10:06.174360: step 20772, loss 3.72529e-08, acc 1
2017-08-08T18:10:06.627556: step 20773, loss 0.100924, acc 0.984375
2017-08-08T18:10:06.917736: step 20774, loss 1.22932e-06, acc 1
2017-08-08T18:10:07.313368: step 20775, loss 1.10451e-06, acc 1
2017-08-08T18:10:07.587531: step 20776, loss 0.0171688, acc 0.984375
2017-08-08T18:10:07.914295: step 20777, loss 3.88697e-06, acc 1
2017-08-08T18:10:08.279159: step 20778, loss 1.02517e-05, acc 1
2017-08-08T18:10:08.530086: step 20779, loss 1.2498e-06, acc 1
2017-08-08T18:10:08.974363: step 20780, loss 0.000788492, acc 1
2017-08-08T18:10:09.269979: step 20781, loss 3.66937e-07, acc 1
2017-08-08T18:10:09.634077: step 20782, loss 3.483e-06, acc 1
2017-08-08T18:10:10.050203: step 20783, loss 5.2154e-08, acc 1
2017-08-08T18:10:10.384649: step 20784, loss 8.94068e-08, acc 1
2017-08-08T18:10:10.767060: step 20785, loss 0.0523015, acc 0.984375
2017-08-08T18:10:11.126033: step 20786, loss 2.0116e-06, acc 1
2017-08-08T18:10:11.402735: step 20787, loss 2.04508e-06, acc 1
2017-08-08T18:10:11.687424: step 20788, loss 9.29533e-06, acc 1
2017-08-08T18:10:12.133399: step 20789, loss 0.000107737, acc 1
2017-08-08T18:10:12.561013: step 20790, loss 0, acc 1
2017-08-08T18:10:12.905353: step 20791, loss 0.000149619, acc 1
2017-08-08T18:10:13.159019: step 20792, loss 6.14672e-08, acc 1
2017-08-08T18:10:13.517709: step 20793, loss 3.57626e-07, acc 1
2017-08-08T18:10:13.861285: step 20794, loss 5.46504e-05, acc 1
2017-08-08T18:10:14.117467: step 20795, loss 1.22187e-06, acc 1
2017-08-08T18:10:14.399264: step 20796, loss 0.0244416, acc 0.984375
2017-08-08T18:10:14.657902: step 20797, loss 0.0284377, acc 0.984375
2017-08-08T18:10:15.020606: step 20798, loss 0.0250992, acc 0.984375
2017-08-08T18:10:15.395285: step 20799, loss 7.45057e-08, acc 1
2017-08-08T18:10:15.749133: step 20800, loss 0.00102767, acc 1

Evaluation:
2017-08-08T18:10:16.502218: step 20800, loss 7.10715, acc 0.71576

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-20800

2017-08-08T18:10:17.173844: step 20801, loss 0, acc 1
2017-08-08T18:10:17.488290: step 20802, loss 5.34336e-05, acc 1
2017-08-08T18:10:17.772043: step 20803, loss 1.16204e-05, acc 1
2017-08-08T18:10:18.127737: step 20804, loss 0.00573495, acc 1
2017-08-08T18:10:18.576210: step 20805, loss 0, acc 1
2017-08-08T18:10:18.910871: step 20806, loss 4.11441e-05, acc 1
2017-08-08T18:10:19.253577: step 20807, loss 0.00664908, acc 1
2017-08-08T18:10:19.500768: step 20808, loss 1.86265e-09, acc 1
2017-08-08T18:10:19.941352: step 20809, loss 0.000147306, acc 1
2017-08-08T18:10:20.257612: step 20810, loss 2.81258e-07, acc 1
2017-08-08T18:10:20.541934: step 20811, loss 5.08606e-06, acc 1
2017-08-08T18:10:20.828726: step 20812, loss 6.59367e-07, acc 1
2017-08-08T18:10:21.225449: step 20813, loss 5.64346e-06, acc 1
2017-08-08T18:10:21.678513: step 20814, loss 3.58911e-06, acc 1
2017-08-08T18:10:22.056135: step 20815, loss 7.04508e-06, acc 1
2017-08-08T18:10:22.347236: step 20816, loss 4.91734e-07, acc 1
2017-08-08T18:10:22.625414: step 20817, loss 1.29637e-06, acc 1
2017-08-08T18:10:23.029961: step 20818, loss 5.27415e-06, acc 1
2017-08-08T18:10:23.290646: step 20819, loss 0, acc 1
2017-08-08T18:10:23.578116: step 20820, loss 1.80854e-06, acc 1
2017-08-08T18:10:23.857525: step 20821, loss 0.000790811, acc 1
2017-08-08T18:10:24.230237: step 20822, loss 2.38303e-05, acc 1
2017-08-08T18:10:24.648135: step 20823, loss 2.05872e-05, acc 1
2017-08-08T18:10:25.019814: step 20824, loss 7.6553e-07, acc 1
2017-08-08T18:10:25.271539: step 20825, loss 1.45286e-07, acc 1
2017-08-08T18:10:25.509593: step 20826, loss 4.91105e-06, acc 1
2017-08-08T18:10:25.969870: step 20827, loss 8.81635e-06, acc 1
2017-08-08T18:10:26.218230: step 20828, loss 2.15686e-06, acc 1
2017-08-08T18:10:26.497439: step 20829, loss 3.72703e-05, acc 1
2017-08-08T18:10:26.759292: step 20830, loss 1.30385e-08, acc 1
2017-08-08T18:10:27.153464: step 20831, loss 1.15484e-07, acc 1
2017-08-08T18:10:27.537370: step 20832, loss 0.00016073, acc 1
2017-08-08T18:10:27.898113: step 20833, loss 2.70081e-07, acc 1
2017-08-08T18:10:28.198222: step 20834, loss 0.000614815, acc 1
2017-08-08T18:10:28.481434: step 20835, loss 7.45058e-09, acc 1
2017-08-08T18:10:28.788551: step 20836, loss 0.0068603, acc 1
2017-08-08T18:10:29.079069: step 20837, loss 1.08404e-05, acc 1
2017-08-08T18:10:29.396922: step 20838, loss 0.000348721, acc 1
2017-08-08T18:10:29.786880: step 20839, loss 5.36433e-07, acc 1
2017-08-08T18:10:30.145942: step 20840, loss 0.021035, acc 0.984375
2017-08-08T18:10:30.517834: step 20841, loss 1.30385e-08, acc 1
2017-08-08T18:10:30.769521: step 20842, loss 1.29346e-05, acc 1
2017-08-08T18:10:31.047322: step 20843, loss 8.38189e-08, acc 1
2017-08-08T18:10:31.465473: step 20844, loss 9.92761e-07, acc 1
2017-08-08T18:10:31.727330: step 20845, loss 7.41677e-05, acc 1
2017-08-08T18:10:31.955498: step 20846, loss 7.56089e-06, acc 1
2017-08-08T18:10:32.196192: step 20847, loss 0.000126636, acc 1
2017-08-08T18:10:32.653371: step 20848, loss 0.000431688, acc 1
2017-08-08T18:10:33.027887: step 20849, loss 2.97056e-05, acc 1
2017-08-08T18:10:33.397338: step 20850, loss 7.62922e-07, acc 1
2017-08-08T18:10:33.747460: step 20851, loss 0.000189146, acc 1
2017-08-08T18:10:34.162386: step 20852, loss 1.86265e-09, acc 1
2017-08-08T18:10:34.558488: step 20853, loss 0, acc 1
2017-08-08T18:10:34.834546: step 20854, loss 0.000261617, acc 1
2017-08-08T18:10:35.163477: step 20855, loss 5.43887e-07, acc 1
2017-08-08T18:10:35.611685: step 20856, loss 3.20213e-05, acc 1
2017-08-08T18:10:35.993413: step 20857, loss 7.87972e-05, acc 1
2017-08-08T18:10:36.331169: step 20858, loss 0.0110182, acc 0.984375
2017-08-08T18:10:36.666332: step 20859, loss 3.44186e-06, acc 1
2017-08-08T18:10:36.967286: step 20860, loss 4.76831e-07, acc 1
2017-08-08T18:10:37.306877: step 20861, loss 1.20138e-06, acc 1
2017-08-08T18:10:37.501373: step 20862, loss 1.99239e-05, acc 1
2017-08-08T18:10:37.768650: step 20863, loss 3.09196e-07, acc 1
2017-08-08T18:10:38.029990: step 20864, loss 1.8316e-05, acc 1
2017-08-08T18:10:38.399986: step 20865, loss 0.000519571, acc 1
2017-08-08T18:10:38.801344: step 20866, loss 9.12695e-08, acc 1
2017-08-08T18:10:39.084696: step 20867, loss 0.000189449, acc 1
2017-08-08T18:10:39.291358: step 20868, loss 0.000917169, acc 1
2017-08-08T18:10:39.501359: step 20869, loss 9.12694e-08, acc 1
2017-08-08T18:10:39.908550: step 20870, loss 1.71362e-07, acc 1
2017-08-08T18:10:40.193289: step 20871, loss 4.86143e-07, acc 1
2017-08-08T18:10:40.399956: step 20872, loss 5.58794e-09, acc 1
2017-08-08T18:10:40.663747: step 20873, loss 5.58793e-09, acc 1
2017-08-08T18:10:41.057797: step 20874, loss 3.41386e-06, acc 1
2017-08-08T18:10:41.497390: step 20875, loss 5.2537e-06, acc 1
2017-08-08T18:10:41.858379: step 20876, loss 0.00118044, acc 1
2017-08-08T18:10:42.129258: step 20877, loss 0.000683798, acc 1
2017-08-08T18:10:42.448056: step 20878, loss 5.2154e-08, acc 1
2017-08-08T18:10:42.836133: step 20879, loss 4.62801e-06, acc 1
2017-08-08T18:10:43.107218: step 20880, loss 2.13085e-05, acc 1
2017-08-08T18:10:43.412938: step 20881, loss 0.0928311, acc 0.984375
2017-08-08T18:10:43.737365: step 20882, loss 8.99759e-05, acc 1
2017-08-08T18:10:44.173715: step 20883, loss 2.55732e-06, acc 1
2017-08-08T18:10:44.522058: step 20884, loss 5.1781e-07, acc 1
2017-08-08T18:10:44.823395: step 20885, loss 0.002711, acc 1
2017-08-08T18:10:45.068205: step 20886, loss 3.53902e-08, acc 1
2017-08-08T18:10:45.414315: step 20887, loss 2.33866e-05, acc 1
2017-08-08T18:10:45.754872: step 20888, loss 1.86265e-09, acc 1
2017-08-08T18:10:46.057540: step 20889, loss 1.86265e-09, acc 1
2017-08-08T18:10:46.325896: step 20890, loss 4.22027e-06, acc 1
2017-08-08T18:10:46.713373: step 20891, loss 0, acc 1
2017-08-08T18:10:47.154190: step 20892, loss 0.000141746, acc 1
2017-08-08T18:10:47.470306: step 20893, loss 9.31322e-09, acc 1
2017-08-08T18:10:47.752134: step 20894, loss 7.45056e-08, acc 1
2017-08-08T18:10:47.985991: step 20895, loss 3.46292e-05, acc 1
2017-08-08T18:10:48.315885: step 20896, loss 3.53902e-08, acc 1
2017-08-08T18:10:48.590496: step 20897, loss 6.51046e-06, acc 1
2017-08-08T18:10:48.844446: step 20898, loss 1.69743e-05, acc 1
2017-08-08T18:10:49.075197: step 20899, loss 2.15827e-05, acc 1
2017-08-08T18:10:49.438162: step 20900, loss 2.2978e-05, acc 1

Evaluation:
2017-08-08T18:10:50.413363: step 20900, loss 7.03659, acc 0.724203

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-20900

2017-08-08T18:10:50.864386: step 20901, loss 1.43417e-06, acc 1
2017-08-08T18:10:51.301374: step 20902, loss 0.00890914, acc 1
2017-08-08T18:10:51.633203: step 20903, loss 6.53652e-06, acc 1
2017-08-08T18:10:51.919918: step 20904, loss 2.22573e-06, acc 1
2017-08-08T18:10:52.193369: step 20905, loss 1.76569e-06, acc 1
2017-08-08T18:10:52.657368: step 20906, loss 5.91857e-06, acc 1
2017-08-08T18:10:53.094150: step 20907, loss 2.98369e-06, acc 1
2017-08-08T18:10:53.429137: step 20908, loss 9.12695e-08, acc 1
2017-08-08T18:10:53.701168: step 20909, loss 8.51206e-07, acc 1
2017-08-08T18:10:53.957585: step 20910, loss 5.76204e-06, acc 1
2017-08-08T18:10:54.341858: step 20911, loss 2.23517e-08, acc 1
2017-08-08T18:10:54.626040: step 20912, loss 0.0265135, acc 0.984375
2017-08-08T18:10:54.884177: step 20913, loss 1.5976e-05, acc 1
2017-08-08T18:10:55.172549: step 20914, loss 3.72529e-09, acc 1
2017-08-08T18:10:55.466940: step 20915, loss 1.11759e-08, acc 1
2017-08-08T18:10:55.901438: step 20916, loss 6.74134e-06, acc 1
2017-08-08T18:10:56.255034: step 20917, loss 8.95676e-06, acc 1
2017-08-08T18:10:56.685929: step 20918, loss 3.72529e-09, acc 1
2017-08-08T18:10:56.954061: step 20919, loss 2.1531e-06, acc 1
2017-08-08T18:10:57.324249: step 20920, loss 8.38188e-08, acc 1
2017-08-08T18:10:57.654135: step 20921, loss 1.56462e-07, acc 1
2017-08-08T18:10:57.914276: step 20922, loss 2.42144e-08, acc 1
2017-08-08T18:10:58.204392: step 20923, loss 5.22626e-06, acc 1
2017-08-08T18:10:58.465500: step 20924, loss 8.12953e-06, acc 1
2017-08-08T18:10:58.704246: step 20925, loss 4.9732e-07, acc 1
2017-08-08T18:10:59.006613: step 20926, loss 8.38188e-08, acc 1
2017-08-08T18:10:59.339360: step 20927, loss 5.90213e-06, acc 1
2017-08-08T18:10:59.646312: step 20928, loss 4.56468e-06, acc 1
2017-08-08T18:10:59.882370: step 20929, loss 8.84731e-07, acc 1
2017-08-08T18:11:00.242823: step 20930, loss 1.86265e-09, acc 1
2017-08-08T18:11:00.508027: step 20931, loss 2.66901e-06, acc 1
2017-08-08T18:11:00.803581: step 20932, loss 0.000699044, acc 1
2017-08-08T18:11:01.085393: step 20933, loss 2.98023e-08, acc 1
2017-08-08T18:11:01.522647: step 20934, loss 2.52307e-05, acc 1
2017-08-08T18:11:01.906599: step 20935, loss 2.23517e-08, acc 1
2017-08-08T18:11:02.284258: step 20936, loss 2.90153e-05, acc 1
2017-08-08T18:11:02.549416: step 20937, loss 0.0346746, acc 0.984375
2017-08-08T18:11:02.850553: step 20938, loss 8.8516e-06, acc 1
2017-08-08T18:11:03.334537: step 20939, loss 0.000261799, acc 1
2017-08-08T18:11:03.748278: step 20940, loss 2.66708e-06, acc 1
2017-08-08T18:11:04.023267: step 20941, loss 0.000762862, acc 1
2017-08-08T18:11:04.436046: step 20942, loss 2.34306e-06, acc 1
2017-08-08T18:11:04.899290: step 20943, loss 0.025729, acc 0.984375
2017-08-08T18:11:05.243567: step 20944, loss 3.57624e-07, acc 1
2017-08-08T18:11:05.527199: step 20945, loss 5.30212e-05, acc 1
2017-08-08T18:11:05.862031: step 20946, loss 4.26542e-07, acc 1
2017-08-08T18:11:06.254388: step 20947, loss 0.000116236, acc 1
2017-08-08T18:11:06.499860: step 20948, loss 0.000177686, acc 1
2017-08-08T18:11:06.776194: step 20949, loss 0, acc 1
2017-08-08T18:11:07.107332: step 20950, loss 4.76618e-06, acc 1
2017-08-08T18:11:07.439322: step 20951, loss 2.53338e-05, acc 1
2017-08-08T18:11:07.796916: step 20952, loss 3.59489e-07, acc 1
2017-08-08T18:11:08.122669: step 20953, loss 4.28408e-08, acc 1
2017-08-08T18:11:08.366384: step 20954, loss 9.29971e-06, acc 1
2017-08-08T18:11:08.743374: step 20955, loss 1.19209e-07, acc 1
2017-08-08T18:11:09.118010: step 20956, loss 3.98242e-05, acc 1
2017-08-08T18:11:09.384056: step 20957, loss 0, acc 1
2017-08-08T18:11:09.659804: step 20958, loss 0, acc 1
2017-08-08T18:11:10.033373: step 20959, loss 4.65661e-08, acc 1
2017-08-08T18:11:10.445368: step 20960, loss 2.98023e-08, acc 1
2017-08-08T18:11:10.824783: step 20961, loss 0.0156302, acc 0.984375
2017-08-08T18:11:11.190268: step 20962, loss 5.15945e-07, acc 1
2017-08-08T18:11:11.414791: step 20963, loss 0.0538522, acc 0.984375
2017-08-08T18:11:11.731519: step 20964, loss 7.71284e-05, acc 1
2017-08-08T18:11:12.096656: step 20965, loss 1.69305e-06, acc 1
2017-08-08T18:11:12.393954: step 20966, loss 4.61199e-05, acc 1
2017-08-08T18:11:12.644922: step 20967, loss 4.8801e-07, acc 1
2017-08-08T18:11:13.109930: step 20968, loss 6.85441e-07, acc 1
2017-08-08T18:11:13.546808: step 20969, loss 2.59074e-06, acc 1
2017-08-08T18:11:13.920449: step 20970, loss 1.26292e-05, acc 1
2017-08-08T18:11:14.195023: step 20971, loss 1.0131e-05, acc 1
2017-08-08T18:11:14.556983: step 20972, loss 3.43993e-06, acc 1
2017-08-08T18:11:14.935918: step 20973, loss 1.11759e-08, acc 1
2017-08-08T18:11:15.222847: step 20974, loss 1.6205e-07, acc 1
2017-08-08T18:11:15.502882: step 20975, loss 0.000112103, acc 1
2017-08-08T18:11:15.776795: step 20976, loss 9.31322e-09, acc 1
2017-08-08T18:11:16.172800: step 20977, loss 3.88715e-06, acc 1
2017-08-08T18:11:16.608661: step 20978, loss 0.00136841, acc 1
2017-08-08T18:11:16.883841: step 20979, loss 0.000578283, acc 1
2017-08-08T18:11:17.140334: step 20980, loss 0.000681191, acc 1
2017-08-08T18:11:17.413417: step 20981, loss 1.14178e-06, acc 1
2017-08-08T18:11:17.746501: step 20982, loss 1.63013e-05, acc 1
2017-08-08T18:11:18.071300: step 20983, loss 4.4517e-07, acc 1
2017-08-08T18:11:18.375842: step 20984, loss 1.11759e-08, acc 1
2017-08-08T18:11:18.677740: step 20985, loss 3.54961e-05, acc 1
2017-08-08T18:11:19.120424: step 20986, loss 2.51456e-07, acc 1
2017-08-08T18:11:19.599246: step 20987, loss 0.0497143, acc 0.984375
2017-08-08T18:11:19.881600: step 20988, loss 3.27977e-06, acc 1
2017-08-08T18:11:20.064148: step 20989, loss 0, acc 1
2017-08-08T18:11:20.426132: step 20990, loss 0, acc 1
2017-08-08T18:11:20.807907: step 20991, loss 0.000123793, acc 1
2017-08-08T18:11:21.104101: step 20992, loss 1.00954e-06, acc 1
2017-08-08T18:11:21.332832: step 20993, loss 4.40655e-06, acc 1
2017-08-08T18:11:21.573037: step 20994, loss 4.14786e-06, acc 1
2017-08-08T18:11:21.994526: step 20995, loss 0.000741744, acc 1
2017-08-08T18:11:22.317332: step 20996, loss 2.42142e-07, acc 1
2017-08-08T18:11:22.566856: step 20997, loss 0.00423031, acc 1
2017-08-08T18:11:22.784855: step 20998, loss 0, acc 1
2017-08-08T18:11:23.022927: step 20999, loss 3.4535e-05, acc 1
2017-08-08T18:11:23.397510: step 21000, loss 1.86353e-06, acc 1

Evaluation:
2017-08-08T18:11:24.013051: step 21000, loss 7.15316, acc 0.712946

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-21000

2017-08-08T18:11:24.631364: step 21001, loss 0, acc 1
2017-08-08T18:11:24.913563: step 21002, loss 0.00361335, acc 1
2017-08-08T18:11:25.217518: step 21003, loss 1.5422e-06, acc 1
2017-08-08T18:11:25.487280: step 21004, loss 3.09196e-07, acc 1
2017-08-08T18:11:25.993736: step 21005, loss 2.8589e-06, acc 1
2017-08-08T18:11:26.271744: step 21006, loss 1.49012e-08, acc 1
2017-08-08T18:11:26.567408: step 21007, loss 5.51339e-07, acc 1
2017-08-08T18:11:26.836858: step 21008, loss 6.70551e-08, acc 1
2017-08-08T18:11:27.289796: step 21009, loss 1.19209e-07, acc 1
2017-08-08T18:11:27.737478: step 21010, loss 1.8719e-06, acc 1
2017-08-08T18:11:28.061933: step 21011, loss 5.15947e-07, acc 1
2017-08-08T18:11:28.307837: step 21012, loss 3.2081e-05, acc 1
2017-08-08T18:11:28.533322: step 21013, loss 4.46564e-05, acc 1
2017-08-08T18:11:28.793369: step 21014, loss 2.62632e-07, acc 1
2017-08-08T18:11:29.131964: step 21015, loss 0.000857155, acc 1
2017-08-08T18:11:29.444978: step 21016, loss 3.73974e-06, acc 1
2017-08-08T18:11:29.768159: step 21017, loss 3.72529e-09, acc 1
2017-08-08T18:11:30.118893: step 21018, loss 1.86265e-09, acc 1
2017-08-08T18:11:30.535159: step 21019, loss 3.72529e-09, acc 1
2017-08-08T18:11:30.964560: step 21020, loss 0.0836794, acc 0.984375
2017-08-08T18:11:31.330531: step 21021, loss 1.41561e-07, acc 1
2017-08-08T18:11:31.552481: step 21022, loss 5.62347e-05, acc 1
2017-08-08T18:11:31.796287: step 21023, loss 1.4156e-07, acc 1
2017-08-08T18:11:32.247736: step 21024, loss 6.3697e-06, acc 1
2017-08-08T18:11:32.534218: step 21025, loss 7.45058e-09, acc 1
2017-08-08T18:11:32.775855: step 21026, loss 2.23517e-08, acc 1
2017-08-08T18:11:33.054519: step 21027, loss 8.56814e-08, acc 1
2017-08-08T18:11:33.473354: step 21028, loss 2.84774e-06, acc 1
2017-08-08T18:11:33.916517: step 21029, loss 7.45046e-07, acc 1
2017-08-08T18:11:34.297264: step 21030, loss 0.000186199, acc 1
2017-08-08T18:11:34.601984: step 21031, loss 9.31322e-09, acc 1
2017-08-08T18:11:34.891377: step 21032, loss 1.63912e-07, acc 1
2017-08-08T18:11:35.353512: step 21033, loss 0.00283808, acc 1
2017-08-08T18:11:35.677787: step 21034, loss 0.00127849, acc 1
2017-08-08T18:11:35.993522: step 21035, loss 0.102232, acc 0.984375
2017-08-08T18:11:36.384767: step 21036, loss 5.68426e-06, acc 1
2017-08-08T18:11:36.755561: step 21037, loss 4.05649e-06, acc 1
2017-08-08T18:11:37.123930: step 21038, loss 2.29651e-06, acc 1
2017-08-08T18:11:37.498565: step 21039, loss 0.000433029, acc 1
2017-08-08T18:11:37.788396: step 21040, loss 5.89426e-06, acc 1
2017-08-08T18:11:38.175148: step 21041, loss 5.27226e-06, acc 1
2017-08-08T18:11:38.557133: step 21042, loss 6.32986e-05, acc 1
2017-08-08T18:11:38.852418: step 21043, loss 3.00422e-05, acc 1
2017-08-08T18:11:39.251919: step 21044, loss 0.000279086, acc 1
2017-08-08T18:11:39.589347: step 21045, loss 8.19562e-08, acc 1
2017-08-08T18:11:39.970133: step 21046, loss 9.77865e-07, acc 1
2017-08-08T18:11:40.212821: step 21047, loss 0.000165389, acc 1
2017-08-08T18:11:40.474932: step 21048, loss 1.39878e-06, acc 1
2017-08-08T18:11:40.757291: step 21049, loss 3.54163e-05, acc 1
2017-08-08T18:11:41.034637: step 21050, loss 1.43577e-05, acc 1
2017-08-08T18:11:41.258433: step 21051, loss 1.02445e-07, acc 1
2017-08-08T18:11:41.587966: step 21052, loss 0.0010737, acc 1
2017-08-08T18:11:42.042334: step 21053, loss 1.86264e-08, acc 1
2017-08-08T18:11:42.403829: step 21054, loss 0.0002203, acc 1
2017-08-08T18:11:42.733592: step 21055, loss 0, acc 1
2017-08-08T18:11:43.016683: step 21056, loss 0.000113874, acc 1
2017-08-08T18:11:43.361114: step 21057, loss 5.58794e-09, acc 1
2017-08-08T18:11:43.635940: step 21058, loss 4.39989e-05, acc 1
2017-08-08T18:11:43.918135: step 21059, loss 4.35853e-07, acc 1
2017-08-08T18:11:44.171008: step 21060, loss 3.22236e-07, acc 1
2017-08-08T18:11:44.547930: step 21061, loss 2.71945e-07, acc 1
2017-08-08T18:11:44.925357: step 21062, loss 8.82246e-05, acc 1
2017-08-08T18:11:45.229401: step 21063, loss 2.40279e-07, acc 1
2017-08-08T18:11:45.471918: step 21064, loss 0.000123819, acc 1
2017-08-08T18:11:45.674033: step 21065, loss 5.28984e-07, acc 1
2017-08-08T18:11:46.078219: step 21066, loss 1.86265e-09, acc 1
2017-08-08T18:11:46.365142: step 21067, loss 0.000127775, acc 1
2017-08-08T18:11:46.663133: step 21068, loss 1.93465e-05, acc 1
2017-08-08T18:11:46.939813: step 21069, loss 0.000224014, acc 1
2017-08-08T18:11:47.291346: step 21070, loss 1.36526e-06, acc 1
2017-08-08T18:11:47.680994: step 21071, loss 9.31322e-09, acc 1
2017-08-08T18:11:47.975906: step 21072, loss 0, acc 1
2017-08-08T18:11:48.266605: step 21073, loss 3.72529e-09, acc 1
2017-08-08T18:11:48.508667: step 21074, loss 4.66377e-06, acc 1
2017-08-08T18:11:48.889935: step 21075, loss 3.76248e-05, acc 1
2017-08-08T18:11:49.121752: step 21076, loss 3.20374e-07, acc 1
2017-08-08T18:11:49.357851: step 21077, loss 2.17542e-06, acc 1
2017-08-08T18:11:49.621969: step 21078, loss 4.0168e-05, acc 1
2017-08-08T18:11:50.021377: step 21079, loss 0.000328035, acc 1
2017-08-08T18:11:50.373360: step 21080, loss 1.52736e-07, acc 1
2017-08-08T18:11:50.678420: step 21081, loss 1.86265e-09, acc 1
2017-08-08T18:11:50.843784: step 21082, loss 1.7695e-07, acc 1
2017-08-08T18:11:51.090758: step 21083, loss 1.67638e-08, acc 1
2017-08-08T18:11:51.406946: step 21084, loss 3.72529e-09, acc 1
2017-08-08T18:11:51.647667: step 21085, loss 9.31322e-09, acc 1
2017-08-08T18:11:51.902904: step 21086, loss 3.64058e-05, acc 1
2017-08-08T18:11:52.152504: step 21087, loss 1.86265e-09, acc 1
2017-08-08T18:11:52.413446: step 21088, loss 4.04189e-07, acc 1
2017-08-08T18:11:52.645288: step 21089, loss 2.51456e-07, acc 1
2017-08-08T18:11:52.893796: step 21090, loss 2.94297e-07, acc 1
2017-08-08T18:11:53.121101: step 21091, loss 5.58794e-09, acc 1
2017-08-08T18:11:53.409273: step 21092, loss 2.53319e-07, acc 1
2017-08-08T18:11:53.661063: step 21093, loss 0.125806, acc 0.984375
2017-08-08T18:11:53.918587: step 21094, loss 0.000273757, acc 1
2017-08-08T18:11:54.221777: step 21095, loss 1.86264e-08, acc 1
2017-08-08T18:11:54.597389: step 21096, loss 2.11583e-06, acc 1
2017-08-08T18:11:55.047378: step 21097, loss 4.18542e-05, acc 1
2017-08-08T18:11:55.447332: step 21098, loss 8.9728e-06, acc 1
2017-08-08T18:11:55.753832: step 21099, loss 5.61993e-05, acc 1
2017-08-08T18:11:55.997214: step 21100, loss 4.09782e-08, acc 1

Evaluation:
2017-08-08T18:11:56.670011: step 21100, loss 7.17884, acc 0.717636

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-21100

2017-08-08T18:11:57.185449: step 21101, loss 0.000213448, acc 1
2017-08-08T18:11:57.553538: step 21102, loss 5.32444e-06, acc 1
2017-08-08T18:11:57.983518: step 21103, loss 1.09334e-06, acc 1
2017-08-08T18:11:58.361451: step 21104, loss 1.67638e-08, acc 1
2017-08-08T18:11:58.615034: step 21105, loss 0.000182456, acc 1
2017-08-08T18:11:58.872172: step 21106, loss 9.66974e-05, acc 1
2017-08-08T18:11:59.241893: step 21107, loss 4.65658e-07, acc 1
2017-08-08T18:11:59.504041: step 21108, loss 0.0738362, acc 0.984375
2017-08-08T18:11:59.715830: step 21109, loss 1.01511e-06, acc 1
2017-08-08T18:12:00.081523: step 21110, loss 3.06317e-05, acc 1
2017-08-08T18:12:00.501985: step 21111, loss 8.38188e-08, acc 1
2017-08-08T18:12:00.901736: step 21112, loss 0.121299, acc 0.984375
2017-08-08T18:12:01.216587: step 21113, loss 0.000769651, acc 1
2017-08-08T18:12:01.491100: step 21114, loss 5.41011e-06, acc 1
2017-08-08T18:12:01.904546: step 21115, loss 7.24029e-06, acc 1
2017-08-08T18:12:02.323911: step 21116, loss 1.57576e-06, acc 1
2017-08-08T18:12:02.621436: step 21117, loss 0.000354538, acc 1
2017-08-08T18:12:02.923531: step 21118, loss 0.00313975, acc 1
2017-08-08T18:12:03.214642: step 21119, loss 9.87201e-08, acc 1
2017-08-08T18:12:03.672125: step 21120, loss 4.70061e-06, acc 1
2017-08-08T18:12:04.082628: step 21121, loss 0.00245665, acc 1
2017-08-08T18:12:04.386154: step 21122, loss 5.55059e-07, acc 1
2017-08-08T18:12:04.623526: step 21123, loss 1.17346e-07, acc 1
2017-08-08T18:12:04.915767: step 21124, loss 1.54223e-06, acc 1
2017-08-08T18:12:05.328385: step 21125, loss 0.000256394, acc 1
2017-08-08T18:12:05.563910: step 21126, loss 3.95403e-06, acc 1
2017-08-08T18:12:05.833468: step 21127, loss 7.20829e-07, acc 1
2017-08-08T18:12:06.107756: step 21128, loss 5.18018e-05, acc 1
2017-08-08T18:12:06.444552: step 21129, loss 7.78037e-06, acc 1
2017-08-08T18:12:06.846030: step 21130, loss 1.65953e-06, acc 1
2017-08-08T18:12:07.182609: step 21131, loss 7.45058e-09, acc 1
2017-08-08T18:12:07.427392: step 21132, loss 3.13824e-06, acc 1
2017-08-08T18:12:07.643609: step 21133, loss 1.0228e-05, acc 1
2017-08-08T18:12:08.077538: step 21134, loss 0.0101587, acc 1
2017-08-08T18:12:08.380323: step 21135, loss 1.58324e-07, acc 1
2017-08-08T18:12:08.647951: step 21136, loss 4.20229e-05, acc 1
2017-08-08T18:12:08.975916: step 21137, loss 2.88708e-07, acc 1
2017-08-08T18:12:09.418689: step 21138, loss 3.61349e-07, acc 1
2017-08-08T18:12:09.808630: step 21139, loss 4.39561e-05, acc 1
2017-08-08T18:12:10.048153: step 21140, loss 0, acc 1
2017-08-08T18:12:10.236771: step 21141, loss 3.16649e-08, acc 1
2017-08-08T18:12:10.514999: step 21142, loss 4.61367e-05, acc 1
2017-08-08T18:12:10.713747: step 21143, loss 0.000134058, acc 1
2017-08-08T18:12:10.967384: step 21144, loss 8.94069e-08, acc 1
2017-08-08T18:12:11.298525: step 21145, loss 0.000470637, acc 1
2017-08-08T18:12:11.548282: step 21146, loss 1.1846e-06, acc 1
2017-08-08T18:12:11.804107: step 21147, loss 4.47028e-07, acc 1
2017-08-08T18:12:12.018192: step 21148, loss 0.168118, acc 0.984375
2017-08-08T18:12:12.359344: step 21149, loss 2.27241e-07, acc 1
2017-08-08T18:12:12.618943: step 21150, loss 3.77496e-08, acc 1
2017-08-08T18:12:12.874562: step 21151, loss 1.86264e-07, acc 1
2017-08-08T18:12:13.109182: step 21152, loss 7.81872e-06, acc 1
2017-08-08T18:12:13.539787: step 21153, loss 0.000103085, acc 1
2017-08-08T18:12:13.941036: step 21154, loss 6.89775e-06, acc 1
2017-08-08T18:12:14.295995: step 21155, loss 0.00346341, acc 1
2017-08-08T18:12:14.526972: step 21156, loss 0.0902158, acc 0.984375
2017-08-08T18:12:14.731323: step 21157, loss 0.0141712, acc 0.984375
2017-08-08T18:12:15.119429: step 21158, loss 1.0137e-05, acc 1
2017-08-08T18:12:15.459630: step 21159, loss 7.56251e-06, acc 1
2017-08-08T18:12:15.698733: step 21160, loss 1.47703e-06, acc 1
2017-08-08T18:12:15.971874: step 21161, loss 8.51217e-07, acc 1
2017-08-08T18:12:16.331833: step 21162, loss 1.82831e-05, acc 1
2017-08-08T18:12:16.731541: step 21163, loss 1.27401e-06, acc 1
2017-08-08T18:12:17.136553: step 21164, loss 8.06373e-06, acc 1
2017-08-08T18:12:17.497451: step 21165, loss 3.72529e-09, acc 1
2017-08-08T18:12:17.715193: step 21166, loss 0.000100964, acc 1
2017-08-08T18:12:18.094744: step 21167, loss 0.00016381, acc 1
2017-08-08T18:12:18.462246: step 21168, loss 7.2643e-08, acc 1
2017-08-08T18:12:18.704395: step 21169, loss 0, acc 1
2017-08-08T18:12:19.011220: step 21170, loss 3.26813e-05, acc 1
2017-08-08T18:12:19.502006: step 21171, loss 0.000170223, acc 1
2017-08-08T18:12:19.867137: step 21172, loss 9.23307e-05, acc 1
2017-08-08T18:12:20.120641: step 21173, loss 2.19125e-05, acc 1
2017-08-08T18:12:20.378072: step 21174, loss 0.0026153, acc 1
2017-08-08T18:12:20.725389: step 21175, loss 5.81659e-05, acc 1
2017-08-08T18:12:20.976433: step 21176, loss 0.000331638, acc 1
2017-08-08T18:12:21.215705: step 21177, loss 4.65661e-08, acc 1
2017-08-08T18:12:21.430112: step 21178, loss 4.20954e-07, acc 1
2017-08-08T18:12:21.740955: step 21179, loss 3.55975e-05, acc 1
2017-08-08T18:12:22.079356: step 21180, loss 1.45286e-07, acc 1
2017-08-08T18:12:22.433671: step 21181, loss 2.08616e-07, acc 1
2017-08-08T18:12:22.718644: step 21182, loss 1.08073e-05, acc 1
2017-08-08T18:12:22.991944: step 21183, loss 0.000739663, acc 1
2017-08-08T18:12:23.282580: step 21184, loss 0.000399821, acc 1
2017-08-08T18:12:23.522937: step 21185, loss 0.000577113, acc 1
2017-08-08T18:12:23.729794: step 21186, loss 6.6827e-05, acc 1
2017-08-08T18:12:24.139443: step 21187, loss 6.1651e-05, acc 1
2017-08-08T18:12:24.556636: step 21188, loss 5.96046e-08, acc 1
2017-08-08T18:12:24.886894: step 21189, loss 7.63668e-07, acc 1
2017-08-08T18:12:25.159933: step 21190, loss 0.000195371, acc 1
2017-08-08T18:12:25.601154: step 21191, loss 1.38575e-06, acc 1
2017-08-08T18:12:25.860899: step 21192, loss 1.14874e-05, acc 1
2017-08-08T18:12:26.083775: step 21193, loss 0, acc 1
2017-08-08T18:12:26.389314: step 21194, loss 1.75088e-07, acc 1
2017-08-08T18:12:26.688617: step 21195, loss 5.02913e-08, acc 1
2017-08-08T18:12:27.005331: step 21196, loss 0, acc 1
2017-08-08T18:12:27.207755: step 21197, loss 6.89177e-08, acc 1
2017-08-08T18:12:27.430733: step 21198, loss 4.09779e-07, acc 1
2017-08-08T18:12:27.813367: step 21199, loss 4.85493e-05, acc 1
2017-08-08T18:12:28.085542: step 21200, loss 1.09896e-07, acc 1

Evaluation:
2017-08-08T18:12:28.735807: step 21200, loss 7.17372, acc 0.716698

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-21200

2017-08-08T18:12:29.268161: step 21201, loss 4.73531e-05, acc 1
2017-08-08T18:12:29.660063: step 21202, loss 0.0391488, acc 0.984375
2017-08-08T18:12:29.937022: step 21203, loss 2.75523e-05, acc 1
2017-08-08T18:12:30.172255: step 21204, loss 0.00380041, acc 1
2017-08-08T18:12:30.502185: step 21205, loss 4.78446e-06, acc 1
2017-08-08T18:12:30.719603: step 21206, loss 3.77298e-05, acc 1
2017-08-08T18:12:30.966354: step 21207, loss 0.0204809, acc 0.984375
2017-08-08T18:12:31.381365: step 21208, loss 3.72529e-08, acc 1
2017-08-08T18:12:31.769373: step 21209, loss 0.0018845, acc 1
2017-08-08T18:12:32.162344: step 21210, loss 0.000173546, acc 1
2017-08-08T18:12:32.406245: step 21211, loss 0.000545386, acc 1
2017-08-08T18:12:32.865355: step 21212, loss 3.16649e-08, acc 1
2017-08-08T18:12:33.141894: step 21213, loss 1.86265e-09, acc 1
2017-08-08T18:12:33.379165: step 21214, loss 3.65084e-05, acc 1
2017-08-08T18:12:33.666823: step 21215, loss 3.72529e-09, acc 1
2017-08-08T18:12:33.937868: step 21216, loss 0.000270285, acc 1
2017-08-08T18:12:34.425573: step 21217, loss 1.34556e-05, acc 1
2017-08-08T18:12:34.790258: step 21218, loss 5.58793e-08, acc 1
2017-08-08T18:12:35.091843: step 21219, loss 9.31322e-09, acc 1
2017-08-08T18:12:35.341212: step 21220, loss 1.1226e-05, acc 1
2017-08-08T18:12:35.769373: step 21221, loss 3.66938e-07, acc 1
2017-08-08T18:12:36.035540: step 21222, loss 2.23318e-06, acc 1
2017-08-08T18:12:36.269703: step 21223, loss 0.00293262, acc 1
2017-08-08T18:12:36.543548: step 21224, loss 0.000301516, acc 1
2017-08-08T18:12:37.011981: step 21225, loss 5.17807e-07, acc 1
2017-08-08T18:12:37.384035: step 21226, loss 1.05857e-05, acc 1
2017-08-08T18:12:37.621357: step 21227, loss 0.000486807, acc 1
2017-08-08T18:12:37.922073: step 21228, loss 3.72529e-09, acc 1
2017-08-08T18:12:38.169721: step 21229, loss 3.35276e-08, acc 1
2017-08-08T18:12:38.603225: step 21230, loss 3.72529e-09, acc 1
2017-08-08T18:12:38.871501: step 21231, loss 0.000761164, acc 1
2017-08-08T18:12:39.130018: step 21232, loss 5.77419e-08, acc 1
2017-08-08T18:12:39.389065: step 21233, loss 0.00731453, acc 1
2017-08-08T18:12:39.777364: step 21234, loss 2.92978e-06, acc 1
2017-08-08T18:12:40.113638: step 21235, loss 6.78043e-06, acc 1
2017-08-08T18:12:40.402643: step 21236, loss 1.52736e-07, acc 1
2017-08-08T18:12:40.678556: step 21237, loss 1.89237e-06, acc 1
2017-08-08T18:12:40.911963: step 21238, loss 4.57218e-06, acc 1
2017-08-08T18:12:41.309373: step 21239, loss 3.63732e-06, acc 1
2017-08-08T18:12:41.602560: step 21240, loss 1.37387e-05, acc 1
2017-08-08T18:12:41.856092: step 21241, loss 7.38715e-05, acc 1
2017-08-08T18:12:42.121133: step 21242, loss 0.000127666, acc 1
2017-08-08T18:12:42.497116: step 21243, loss 0, acc 1
2017-08-08T18:12:42.895737: step 21244, loss 1.32247e-07, acc 1
2017-08-08T18:12:43.274010: step 21245, loss 0.000400937, acc 1
2017-08-08T18:12:43.562934: step 21246, loss 4.02328e-07, acc 1
2017-08-08T18:12:43.803601: step 21247, loss 0.000318761, acc 1
2017-08-08T18:12:44.180312: step 21248, loss 5.62509e-07, acc 1
2017-08-08T18:12:44.410860: step 21249, loss 1.23302e-06, acc 1
2017-08-08T18:12:44.645467: step 21250, loss 1.00583e-07, acc 1
2017-08-08T18:12:44.863892: step 21251, loss 2.66356e-07, acc 1
2017-08-08T18:12:45.293728: step 21252, loss 9.81594e-07, acc 1
2017-08-08T18:12:45.715002: step 21253, loss 3.58334e-06, acc 1
2017-08-08T18:12:46.045854: step 21254, loss 2.01165e-07, acc 1
2017-08-08T18:12:46.342663: step 21255, loss 0.00822569, acc 1
2017-08-08T18:12:46.585204: step 21256, loss 2.77533e-07, acc 1
2017-08-08T18:12:46.961372: step 21257, loss 4.38603e-06, acc 1
2017-08-08T18:12:47.328108: step 21258, loss 7.64243e-06, acc 1
2017-08-08T18:12:47.618131: step 21259, loss 4.95946e-06, acc 1
2017-08-08T18:12:47.891841: step 21260, loss 0.000202474, acc 1
2017-08-08T18:12:48.165388: step 21261, loss 5.02908e-07, acc 1
2017-08-08T18:12:48.657691: step 21262, loss 8.08372e-07, acc 1
2017-08-08T18:12:48.973580: step 21263, loss 1.35973e-07, acc 1
2017-08-08T18:12:49.255084: step 21264, loss 1.30385e-08, acc 1
2017-08-08T18:12:49.504337: step 21265, loss 1.11386e-05, acc 1
2017-08-08T18:12:49.852694: step 21266, loss 0.000213222, acc 1
2017-08-08T18:12:50.224580: step 21267, loss 2.98023e-08, acc 1
2017-08-08T18:12:50.493565: step 21268, loss 0.000888549, acc 1
2017-08-08T18:12:50.778391: step 21269, loss 3.19599e-06, acc 1
2017-08-08T18:12:51.242254: step 21270, loss 3.48311e-07, acc 1
2017-08-08T18:12:51.645970: step 21271, loss 6.46333e-07, acc 1
2017-08-08T18:12:51.960115: step 21272, loss 1.18648e-06, acc 1
2017-08-08T18:12:52.218446: step 21273, loss 1.86264e-07, acc 1
2017-08-08T18:12:52.456041: step 21274, loss 0.0011853, acc 1
2017-08-08T18:12:52.894922: step 21275, loss 2.45105e-06, acc 1
2017-08-08T18:12:53.188375: step 21276, loss 5.25263e-07, acc 1
2017-08-08T18:12:53.499846: step 21277, loss 1.18122e-05, acc 1
2017-08-08T18:12:53.905468: step 21278, loss 3.2258e-06, acc 1
2017-08-08T18:12:54.283189: step 21279, loss 0.00311375, acc 1
2017-08-08T18:12:54.723654: step 21280, loss 0.00309645, acc 1
2017-08-08T18:12:55.048516: step 21281, loss 5.77419e-08, acc 1
2017-08-08T18:12:55.298150: step 21282, loss 1.12576e-05, acc 1
2017-08-08T18:12:55.705189: step 21283, loss 1.21072e-07, acc 1
2017-08-08T18:12:55.923229: step 21284, loss 0.000643385, acc 1
2017-08-08T18:12:56.300011: step 21285, loss 5.58793e-08, acc 1
2017-08-08T18:12:56.626110: step 21286, loss 1.73225e-07, acc 1
2017-08-08T18:12:56.802756: step 21287, loss 2.95192e-05, acc 1
2017-08-08T18:12:57.163519: step 21288, loss 4.3867e-05, acc 1
2017-08-08T18:12:57.478399: step 21289, loss 1.0617e-07, acc 1
2017-08-08T18:12:57.774356: step 21290, loss 0.00010136, acc 1
2017-08-08T18:12:58.060899: step 21291, loss 5.33381e-06, acc 1
2017-08-08T18:12:58.489238: step 21292, loss 0, acc 1
2017-08-08T18:12:58.891118: step 21293, loss 5.77419e-08, acc 1
2017-08-08T18:12:59.300367: step 21294, loss 0.000592611, acc 1
2017-08-08T18:12:59.574153: step 21295, loss 2.21653e-07, acc 1
2017-08-08T18:12:59.917366: step 21296, loss 9.81587e-07, acc 1
2017-08-08T18:13:00.209962: step 21297, loss 1.72658e-06, acc 1
2017-08-08T18:13:00.498417: step 21298, loss 0.000123786, acc 1
2017-08-08T18:13:00.805933: step 21299, loss 2.04891e-08, acc 1
2017-08-08T18:13:01.029311: step 21300, loss 0.0142723, acc 0.983333

Evaluation:
2017-08-08T18:13:01.602360: step 21300, loss 7.20182, acc 0.713884

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-21300

2017-08-08T18:13:02.197485: step 21301, loss 0, acc 1
2017-08-08T18:13:02.555643: step 21302, loss 4.35618e-06, acc 1
2017-08-08T18:13:02.832218: step 21303, loss 2.45868e-07, acc 1
2017-08-08T18:13:03.069527: step 21304, loss 4.21649e-06, acc 1
2017-08-08T18:13:03.384015: step 21305, loss 0.00110196, acc 1
2017-08-08T18:13:03.794393: step 21306, loss 9.31322e-09, acc 1
2017-08-08T18:13:04.180963: step 21307, loss 3.72529e-09, acc 1
2017-08-08T18:13:04.550673: step 21308, loss 0.0315095, acc 0.984375
2017-08-08T18:13:04.800291: step 21309, loss 6.59677e-06, acc 1
2017-08-08T18:13:05.053922: step 21310, loss 9.31322e-09, acc 1
2017-08-08T18:13:05.521260: step 21311, loss 5.2154e-08, acc 1
2017-08-08T18:13:05.803023: step 21312, loss 0.000216778, acc 1
2017-08-08T18:13:06.069539: step 21313, loss 0.00130902, acc 1
2017-08-08T18:13:06.390357: step 21314, loss 5.01531e-06, acc 1
2017-08-08T18:13:06.889012: step 21315, loss 4.03397e-06, acc 1
2017-08-08T18:13:07.281472: step 21316, loss 0.000116256, acc 1
2017-08-08T18:13:07.526566: step 21317, loss 4.13503e-07, acc 1
2017-08-08T18:13:07.785857: step 21318, loss 3.04636e-05, acc 1
2017-08-08T18:13:08.159494: step 21319, loss 7.8231e-08, acc 1
2017-08-08T18:13:08.433256: step 21320, loss 6.52163e-06, acc 1
2017-08-08T18:13:08.762957: step 21321, loss 0.00560352, acc 1
2017-08-08T18:13:09.005259: step 21322, loss 1.45286e-07, acc 1
2017-08-08T18:13:09.408529: step 21323, loss 0.000662436, acc 1
2017-08-08T18:13:09.817961: step 21324, loss 3.5511e-05, acc 1
2017-08-08T18:13:10.116346: step 21325, loss 0, acc 1
2017-08-08T18:13:10.362059: step 21326, loss 1.31095e-05, acc 1
2017-08-08T18:13:10.586197: step 21327, loss 0.00018885, acc 1
2017-08-08T18:13:11.034456: step 21328, loss 6.37843e-06, acc 1
2017-08-08T18:13:11.298293: step 21329, loss 3.24416e-05, acc 1
2017-08-08T18:13:11.571510: step 21330, loss 2.01164e-07, acc 1
2017-08-08T18:13:11.846840: step 21331, loss 0.000243541, acc 1
2017-08-08T18:13:12.185235: step 21332, loss 1.47149e-07, acc 1
2017-08-08T18:13:12.528825: step 21333, loss 1.49011e-07, acc 1
2017-08-08T18:13:12.929372: step 21334, loss 0.00428232, acc 1
2017-08-08T18:13:13.325367: step 21335, loss 2.42144e-08, acc 1
2017-08-08T18:13:13.594779: step 21336, loss 4.47034e-08, acc 1
2017-08-08T18:13:13.970471: step 21337, loss 6.61607e-05, acc 1
2017-08-08T18:13:14.339539: step 21338, loss 6.24544e-05, acc 1
2017-08-08T18:13:14.623620: step 21339, loss 5.02678e-06, acc 1
2017-08-08T18:13:14.889546: step 21340, loss 1.86265e-09, acc 1
2017-08-08T18:13:15.120176: step 21341, loss 0, acc 1
2017-08-08T18:13:15.561365: step 21342, loss 4.00479e-05, acc 1
2017-08-08T18:13:15.969327: step 21343, loss 0.000553553, acc 1
2017-08-08T18:13:16.318053: step 21344, loss 1.35973e-07, acc 1
2017-08-08T18:13:16.575672: step 21345, loss 1.11759e-08, acc 1
2017-08-08T18:13:16.986483: step 21346, loss 0.000216611, acc 1
2017-08-08T18:13:17.279647: step 21347, loss 1.24797e-07, acc 1
2017-08-08T18:13:17.565310: step 21348, loss 1.86265e-09, acc 1
2017-08-08T18:13:17.892958: step 21349, loss 4.54793e-06, acc 1
2017-08-08T18:13:18.209325: step 21350, loss 0.00584979, acc 1
2017-08-08T18:13:18.603210: step 21351, loss 6.70551e-08, acc 1
2017-08-08T18:13:18.942863: step 21352, loss 7.54935e-06, acc 1
2017-08-08T18:13:19.254507: step 21353, loss 2.0489e-07, acc 1
2017-08-08T18:13:19.520842: step 21354, loss 7.45058e-09, acc 1
2017-08-08T18:13:19.742751: step 21355, loss 2.83121e-07, acc 1
2017-08-08T18:13:20.187390: step 21356, loss 5.58794e-09, acc 1
2017-08-08T18:13:20.504766: step 21357, loss 4.54079e-05, acc 1
2017-08-08T18:13:20.783569: step 21358, loss 3.35276e-08, acc 1
2017-08-08T18:13:21.049850: step 21359, loss 7.58083e-07, acc 1
2017-08-08T18:13:21.436274: step 21360, loss 3.11059e-07, acc 1
2017-08-08T18:13:21.836476: step 21361, loss 1.34853e-05, acc 1
2017-08-08T18:13:22.213983: step 21362, loss 2.16797e-06, acc 1
2017-08-08T18:13:22.542403: step 21363, loss 3.72529e-09, acc 1
2017-08-08T18:13:22.894363: step 21364, loss 4.0419e-07, acc 1
2017-08-08T18:13:23.305479: step 21365, loss 0, acc 1
2017-08-08T18:13:23.607276: step 21366, loss 3.15691e-06, acc 1
2017-08-08T18:13:23.838289: step 21367, loss 6.62961e-06, acc 1
2017-08-08T18:13:24.229383: step 21368, loss 8.90319e-07, acc 1
2017-08-08T18:13:24.725724: step 21369, loss 1.30385e-08, acc 1
2017-08-08T18:13:25.079581: step 21370, loss 7.74842e-07, acc 1
2017-08-08T18:13:25.316550: step 21371, loss 5.70613e-06, acc 1
2017-08-08T18:13:25.594015: step 21372, loss 0, acc 1
2017-08-08T18:13:25.952379: step 21373, loss 8.98109e-06, acc 1
2017-08-08T18:13:26.158323: step 21374, loss 4.73106e-07, acc 1
2017-08-08T18:13:26.388003: step 21375, loss 2.00968e-06, acc 1
2017-08-08T18:13:26.717135: step 21376, loss 1.55713e-06, acc 1
2017-08-08T18:13:27.063738: step 21377, loss 0.00293425, acc 1
2017-08-08T18:13:27.395138: step 21378, loss 2.0489e-07, acc 1
2017-08-08T18:13:27.695187: step 21379, loss 1.86265e-09, acc 1
2017-08-08T18:13:27.930038: step 21380, loss 1.52513e-05, acc 1
2017-08-08T18:13:28.253363: step 21381, loss 9.3749e-05, acc 1
2017-08-08T18:13:28.610693: step 21382, loss 1.63376e-05, acc 1
2017-08-08T18:13:28.858827: step 21383, loss 2.42144e-08, acc 1
2017-08-08T18:13:29.038818: step 21384, loss 2.0489e-07, acc 1
2017-08-08T18:13:29.300959: step 21385, loss 0.000175767, acc 1
2017-08-08T18:13:29.643895: step 21386, loss 1.90171e-06, acc 1
2017-08-08T18:13:30.075581: step 21387, loss 3.35633e-06, acc 1
2017-08-08T18:13:30.423734: step 21388, loss 1.67638e-08, acc 1
2017-08-08T18:13:30.737397: step 21389, loss 4.70095e-06, acc 1
2017-08-08T18:13:31.037894: step 21390, loss 0.0086974, acc 1
2017-08-08T18:13:31.483522: step 21391, loss 0.000276849, acc 1
2017-08-08T18:13:31.768056: step 21392, loss 0.000341828, acc 1
2017-08-08T18:13:32.061834: step 21393, loss 3.98604e-07, acc 1
2017-08-08T18:13:32.336425: step 21394, loss 6.90927e-05, acc 1
2017-08-08T18:13:32.744282: step 21395, loss 3.72525e-07, acc 1
2017-08-08T18:13:33.188402: step 21396, loss 7.33542e-06, acc 1
2017-08-08T18:13:33.526352: step 21397, loss 4.20952e-07, acc 1
2017-08-08T18:13:33.832574: step 21398, loss 4.21821e-05, acc 1
2017-08-08T18:13:34.021983: step 21399, loss 1.06171e-07, acc 1
2017-08-08T18:13:34.557144: step 21400, loss 3.74818e-05, acc 1

Evaluation:
2017-08-08T18:13:35.255649: step 21400, loss 7.26471, acc 0.713884

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-21400

2017-08-08T18:13:35.894079: step 21401, loss 1.50961e-05, acc 1
2017-08-08T18:13:36.172873: step 21402, loss 1.25538e-06, acc 1
2017-08-08T18:13:36.462352: step 21403, loss 5.99761e-07, acc 1
2017-08-08T18:13:36.709151: step 21404, loss 4.57988e-05, acc 1
2017-08-08T18:13:37.055215: step 21405, loss 6.2447e-06, acc 1
2017-08-08T18:13:37.422871: step 21406, loss 2.23502e-06, acc 1
2017-08-08T18:13:37.708773: step 21407, loss 1.02599e-05, acc 1
2017-08-08T18:13:37.934389: step 21408, loss 4.37572e-05, acc 1
2017-08-08T18:13:38.398463: step 21409, loss 2.16065e-07, acc 1
2017-08-08T18:13:38.700666: step 21410, loss 4.22435e-05, acc 1
2017-08-08T18:13:39.041485: step 21411, loss 5.63557e-06, acc 1
2017-08-08T18:13:39.382577: step 21412, loss 0, acc 1
2017-08-08T18:13:39.634421: step 21413, loss 1.104e-05, acc 1
2017-08-08T18:13:40.009445: step 21414, loss 4.62517e-05, acc 1
2017-08-08T18:13:40.367914: step 21415, loss 2.16797e-06, acc 1
2017-08-08T18:13:40.670123: step 21416, loss 1.88126e-07, acc 1
2017-08-08T18:13:40.964144: step 21417, loss 0.000164325, acc 1
2017-08-08T18:13:41.376166: step 21418, loss 3.25371e-06, acc 1
2017-08-08T18:13:41.819176: step 21419, loss 2.06059e-05, acc 1
2017-08-08T18:13:42.113311: step 21420, loss 7.45143e-06, acc 1
2017-08-08T18:13:42.357722: step 21421, loss 5.77419e-08, acc 1
2017-08-08T18:13:42.591027: step 21422, loss 0, acc 1
2017-08-08T18:13:42.966666: step 21423, loss 6.94241e-06, acc 1
2017-08-08T18:13:43.190652: step 21424, loss 3.29282e-06, acc 1
2017-08-08T18:13:43.450682: step 21425, loss 2.77139e-06, acc 1
2017-08-08T18:13:43.710930: step 21426, loss 3.96321e-06, acc 1
2017-08-08T18:13:44.055095: step 21427, loss 1.17346e-07, acc 1
2017-08-08T18:13:44.476908: step 21428, loss 0.00760649, acc 1
2017-08-08T18:13:44.870034: step 21429, loss 0.00019868, acc 1
2017-08-08T18:13:45.120150: step 21430, loss 1.86265e-09, acc 1
2017-08-08T18:13:45.337686: step 21431, loss 2.0338e-05, acc 1
2017-08-08T18:13:45.685384: step 21432, loss 1.86265e-09, acc 1
2017-08-08T18:13:45.987815: step 21433, loss 5.69963e-07, acc 1
2017-08-08T18:13:46.258969: step 21434, loss 1.86265e-09, acc 1
2017-08-08T18:13:46.535507: step 21435, loss 0.000226215, acc 1
2017-08-08T18:13:46.773824: step 21436, loss 5.58794e-09, acc 1
2017-08-08T18:13:47.235569: step 21437, loss 1.40328e-05, acc 1
2017-08-08T18:13:47.588695: step 21438, loss 0, acc 1
2017-08-08T18:13:47.975809: step 21439, loss 4.82749e-06, acc 1
2017-08-08T18:13:48.245360: step 21440, loss 3.72529e-08, acc 1
2017-08-08T18:13:48.630135: step 21441, loss 0, acc 1
2017-08-08T18:13:49.035765: step 21442, loss 1.34663e-06, acc 1
2017-08-08T18:13:49.318254: step 21443, loss 1.67638e-08, acc 1
2017-08-08T18:13:49.603373: step 21444, loss 1.86265e-09, acc 1
2017-08-08T18:13:49.962079: step 21445, loss 2.23517e-08, acc 1
2017-08-08T18:13:50.382623: step 21446, loss 4.47034e-08, acc 1
2017-08-08T18:13:50.750481: step 21447, loss 7.24553e-07, acc 1
2017-08-08T18:13:51.045439: step 21448, loss 5.58794e-09, acc 1
2017-08-08T18:13:51.277900: step 21449, loss 1.86265e-09, acc 1
2017-08-08T18:13:51.532417: step 21450, loss 1.48251e-05, acc 1
2017-08-08T18:13:51.948805: step 21451, loss 1.86265e-09, acc 1
2017-08-08T18:13:52.216486: step 21452, loss 1.91851e-07, acc 1
2017-08-08T18:13:52.494259: step 21453, loss 1.28522e-07, acc 1
2017-08-08T18:13:52.780461: step 21454, loss 3.522e-06, acc 1
2017-08-08T18:13:53.245132: step 21455, loss 3.72529e-09, acc 1
2017-08-08T18:13:53.641962: step 21456, loss 1.9836e-06, acc 1
2017-08-08T18:13:53.981404: step 21457, loss 7.45058e-09, acc 1
2017-08-08T18:13:54.271630: step 21458, loss 1.91851e-07, acc 1
2017-08-08T18:13:54.522651: step 21459, loss 3.16649e-08, acc 1
2017-08-08T18:13:54.965249: step 21460, loss 1.13061e-06, acc 1
2017-08-08T18:13:55.255889: step 21461, loss 1.86265e-09, acc 1
2017-08-08T18:13:55.526228: step 21462, loss 0.00105314, acc 1
2017-08-08T18:13:55.789886: step 21463, loss 7.45058e-09, acc 1
2017-08-08T18:13:56.089511: step 21464, loss 0.000192733, acc 1
2017-08-08T18:13:56.381369: step 21465, loss 0.000402611, acc 1
2017-08-08T18:13:56.704858: step 21466, loss 2.42144e-08, acc 1
2017-08-08T18:13:57.067738: step 21467, loss 5.80175e-06, acc 1
2017-08-08T18:13:57.295315: step 21468, loss 1.695e-07, acc 1
2017-08-08T18:13:57.674464: step 21469, loss 5.61673e-06, acc 1
2017-08-08T18:13:58.042314: step 21470, loss 8.95895e-05, acc 1
2017-08-08T18:13:58.332848: step 21471, loss 0, acc 1
2017-08-08T18:13:58.604102: step 21472, loss 5.40167e-08, acc 1
2017-08-08T18:13:58.967576: step 21473, loss 6.71527e-06, acc 1
2017-08-08T18:13:59.450754: step 21474, loss 1.71846e-05, acc 1
2017-08-08T18:13:59.832830: step 21475, loss 2.01165e-07, acc 1
2017-08-08T18:14:00.153241: step 21476, loss 3.93014e-07, acc 1
2017-08-08T18:14:00.385336: step 21477, loss 6.97778e-06, acc 1
2017-08-08T18:14:00.717176: step 21478, loss 0.00011547, acc 1
2017-08-08T18:14:01.122126: step 21479, loss 1.57761e-06, acc 1
2017-08-08T18:14:01.383471: step 21480, loss 1.86265e-09, acc 1
2017-08-08T18:14:01.669927: step 21481, loss 9.10255e-05, acc 1
2017-08-08T18:14:02.045411: step 21482, loss 1.16227e-06, acc 1
2017-08-08T18:14:02.474646: step 21483, loss 4.67646e-06, acc 1
2017-08-08T18:14:02.840876: step 21484, loss 0.000233376, acc 1
2017-08-08T18:14:03.206834: step 21485, loss 4.47034e-08, acc 1
2017-08-08T18:14:03.447509: step 21486, loss 9.872e-08, acc 1
2017-08-08T18:14:03.788239: step 21487, loss 9.58977e-05, acc 1
2017-08-08T18:14:04.067408: step 21488, loss 0, acc 1
2017-08-08T18:14:04.381608: step 21489, loss 8.45618e-07, acc 1
2017-08-08T18:14:04.716134: step 21490, loss 6.14661e-07, acc 1
2017-08-08T18:14:05.029008: step 21491, loss 2.23517e-08, acc 1
2017-08-08T18:14:05.369488: step 21492, loss 0.000167537, acc 1
2017-08-08T18:14:05.864701: step 21493, loss 6.8704e-05, acc 1
2017-08-08T18:14:06.240529: step 21494, loss 1.62049e-07, acc 1
2017-08-08T18:14:06.494166: step 21495, loss 3.21367e-05, acc 1
2017-08-08T18:14:06.922142: step 21496, loss 3.35276e-08, acc 1
2017-08-08T18:14:07.331151: step 21497, loss 5.66234e-07, acc 1
2017-08-08T18:14:07.671739: step 21498, loss 5.63988e-05, acc 1
2017-08-08T18:14:07.959516: step 21499, loss 2.46036e-06, acc 1
2017-08-08T18:14:08.341802: step 21500, loss 1.86265e-09, acc 1

Evaluation:
2017-08-08T18:14:09.444344: step 21500, loss 7.29371, acc 0.710131

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-21500

2017-08-08T18:14:10.044025: step 21501, loss 2.17928e-07, acc 1
2017-08-08T18:14:10.337377: step 21502, loss 3.2146e-06, acc 1
2017-08-08T18:14:10.769926: step 21503, loss 1.30385e-08, acc 1
2017-08-08T18:14:11.093219: step 21504, loss 1.91841e-06, acc 1
2017-08-08T18:14:11.392728: step 21505, loss 1.45759e-05, acc 1
2017-08-08T18:14:11.700901: step 21506, loss 0, acc 1
2017-08-08T18:14:12.029388: step 21507, loss 1.11758e-07, acc 1
2017-08-08T18:14:12.493384: step 21508, loss 2.1234e-07, acc 1
2017-08-08T18:14:12.939603: step 21509, loss 1.16391e-05, acc 1
2017-08-08T18:14:13.268067: step 21510, loss 0, acc 1
2017-08-08T18:14:13.467744: step 21511, loss 4.67835e-05, acc 1
2017-08-08T18:14:13.725478: step 21512, loss 2.64661e-06, acc 1
2017-08-08T18:14:14.152049: step 21513, loss 5.81135e-07, acc 1
2017-08-08T18:14:14.394800: step 21514, loss 4.14343e-05, acc 1
2017-08-08T18:14:14.664942: step 21515, loss 1.86265e-09, acc 1
2017-08-08T18:14:15.010751: step 21516, loss 8.81303e-05, acc 1
2017-08-08T18:14:15.394777: step 21517, loss 5.14758e-06, acc 1
2017-08-08T18:14:15.765404: step 21518, loss 1.67638e-08, acc 1
2017-08-08T18:14:16.086220: step 21519, loss 2.42144e-08, acc 1
2017-08-08T18:14:16.353453: step 21520, loss 6.63093e-07, acc 1
2017-08-08T18:14:16.755681: step 21521, loss 1.31598e-05, acc 1
2017-08-08T18:14:17.216775: step 21522, loss 7.45058e-09, acc 1
2017-08-08T18:14:17.637486: step 21523, loss 1.86264e-07, acc 1
2017-08-08T18:14:17.936413: step 21524, loss 3.34012e-05, acc 1
2017-08-08T18:14:18.409773: step 21525, loss 0.00536236, acc 1
2017-08-08T18:14:19.061955: step 21526, loss 0, acc 1
2017-08-08T18:14:19.451734: step 21527, loss 4.5262e-07, acc 1
2017-08-08T18:14:19.733188: step 21528, loss 0, acc 1
2017-08-08T18:14:20.154854: step 21529, loss 1.86264e-08, acc 1
2017-08-08T18:14:20.650474: step 21530, loss 4.53552e-05, acc 1
2017-08-08T18:14:20.938630: step 21531, loss 3.72529e-09, acc 1
2017-08-08T18:14:21.236940: step 21532, loss 5.58788e-07, acc 1
2017-08-08T18:14:21.607963: step 21533, loss 0.000570202, acc 1
2017-08-08T18:14:22.073386: step 21534, loss 1.08071e-05, acc 1
2017-08-08T18:14:22.517382: step 21535, loss 1.15484e-07, acc 1
2017-08-08T18:14:22.852584: step 21536, loss 0.000122375, acc 1
2017-08-08T18:14:23.169379: step 21537, loss 0.0003451, acc 1
2017-08-08T18:14:23.482351: step 21538, loss 6.41747e-06, acc 1
2017-08-08T18:14:24.017603: step 21539, loss 2.23688e-06, acc 1
2017-08-08T18:14:24.336396: step 21540, loss 1.13621e-07, acc 1
2017-08-08T18:14:24.722166: step 21541, loss 0, acc 1
2017-08-08T18:14:25.090344: step 21542, loss 1.05542e-05, acc 1
2017-08-08T18:14:25.444357: step 21543, loss 4.71245e-07, acc 1
2017-08-08T18:14:25.933049: step 21544, loss 4.68749e-05, acc 1
2017-08-08T18:14:26.327479: step 21545, loss 3.73047e-06, acc 1
2017-08-08T18:14:26.686325: step 21546, loss 0.0053318, acc 1
2017-08-08T18:14:26.919010: step 21547, loss 0.00591946, acc 1
2017-08-08T18:14:27.280027: step 21548, loss 0.000501602, acc 1
2017-08-08T18:14:27.612681: step 21549, loss 5.96046e-08, acc 1
2017-08-08T18:14:27.897528: step 21550, loss 1.52736e-07, acc 1
2017-08-08T18:14:28.227931: step 21551, loss 0.000237594, acc 1
2017-08-08T18:14:28.505795: step 21552, loss 1.49012e-08, acc 1
2017-08-08T18:14:29.010698: step 21553, loss 0.000235222, acc 1
2017-08-08T18:14:29.407199: step 21554, loss 0.000115545, acc 1
2017-08-08T18:14:29.687803: step 21555, loss 1.85813e-05, acc 1
2017-08-08T18:14:29.952503: step 21556, loss 7.26431e-08, acc 1
2017-08-08T18:14:30.296176: step 21557, loss 7.6553e-07, acc 1
2017-08-08T18:14:30.614317: step 21558, loss 0.00123959, acc 1
2017-08-08T18:14:30.872497: step 21559, loss 1.14325e-05, acc 1
2017-08-08T18:14:31.151503: step 21560, loss 9.4247e-07, acc 1
2017-08-08T18:14:31.373300: step 21561, loss 9.31322e-09, acc 1
2017-08-08T18:14:31.781406: step 21562, loss 1.17712e-05, acc 1
2017-08-08T18:14:32.219047: step 21563, loss 0.0023067, acc 1
2017-08-08T18:14:32.475462: step 21564, loss 8.38189e-08, acc 1
2017-08-08T18:14:32.769221: step 21565, loss 0, acc 1
2017-08-08T18:14:33.025553: step 21566, loss 1.92799e-05, acc 1
2017-08-08T18:14:33.446377: step 21567, loss 0.0022436, acc 1
2017-08-08T18:14:33.722460: step 21568, loss 4.15364e-07, acc 1
2017-08-08T18:14:34.025810: step 21569, loss 4.19093e-07, acc 1
2017-08-08T18:14:34.317568: step 21570, loss 9.07086e-07, acc 1
2017-08-08T18:14:34.693371: step 21571, loss 1.11759e-08, acc 1
2017-08-08T18:14:35.217376: step 21572, loss 3.72529e-09, acc 1
2017-08-08T18:14:35.555399: step 21573, loss 0, acc 1
2017-08-08T18:14:35.805677: step 21574, loss 0.00076265, acc 1
2017-08-08T18:14:36.086693: step 21575, loss 0.000761673, acc 1
2017-08-08T18:14:36.424333: step 21576, loss 2.3524e-06, acc 1
2017-08-08T18:14:36.758121: step 21577, loss 0, acc 1
2017-08-08T18:14:37.019178: step 21578, loss 5.96722e-06, acc 1
2017-08-08T18:14:37.274812: step 21579, loss 4.28544e-06, acc 1
2017-08-08T18:14:37.694856: step 21580, loss 2.77532e-07, acc 1
2017-08-08T18:14:38.089381: step 21581, loss 1.5683e-06, acc 1
2017-08-08T18:14:38.505480: step 21582, loss 1.62785e-05, acc 1
2017-08-08T18:14:38.807059: step 21583, loss 5.10357e-07, acc 1
2017-08-08T18:14:39.055298: step 21584, loss 3.32462e-06, acc 1
2017-08-08T18:14:39.423887: step 21585, loss 4.93371e-06, acc 1
2017-08-08T18:14:39.764057: step 21586, loss 1.22934e-07, acc 1
2017-08-08T18:14:39.992768: step 21587, loss 0.00268764, acc 1
2017-08-08T18:14:40.275456: step 21588, loss 9.69048e-06, acc 1
2017-08-08T18:14:40.601490: step 21589, loss 1.06935e-05, acc 1
2017-08-08T18:14:40.957672: step 21590, loss 0.0115609, acc 0.984375
2017-08-08T18:14:41.441741: step 21591, loss 9.31322e-09, acc 1
2017-08-08T18:14:41.723551: step 21592, loss 4.80556e-07, acc 1
2017-08-08T18:14:41.936283: step 21593, loss 1.86265e-09, acc 1
2017-08-08T18:14:42.270855: step 21594, loss 1.37836e-07, acc 1
2017-08-08T18:14:42.576127: step 21595, loss 0.000258926, acc 1
2017-08-08T18:14:42.860786: step 21596, loss 7.38178e-06, acc 1
2017-08-08T18:14:43.126290: step 21597, loss 6.30281e-05, acc 1
2017-08-08T18:14:43.481321: step 21598, loss 1.52916e-06, acc 1
2017-08-08T18:14:43.941370: step 21599, loss 7.19007e-06, acc 1
2017-08-08T18:14:44.348142: step 21600, loss 3.74681e-06, acc 1

Evaluation:
2017-08-08T18:14:45.124768: step 21600, loss 7.30217, acc 0.713884

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-21600

2017-08-08T18:14:45.827098: step 21601, loss 4.32128e-07, acc 1
2017-08-08T18:14:46.114237: step 21602, loss 3.94878e-07, acc 1
2017-08-08T18:14:46.341408: step 21603, loss 5.92316e-07, acc 1
2017-08-08T18:14:46.708797: step 21604, loss 3.44344e-05, acc 1
2017-08-08T18:14:47.232590: step 21605, loss 0.000238075, acc 1
2017-08-08T18:14:47.603622: step 21606, loss 1.00583e-07, acc 1
2017-08-08T18:14:47.859974: step 21607, loss 0, acc 1
2017-08-08T18:14:48.249355: step 21608, loss 0.000443032, acc 1
2017-08-08T18:14:48.553054: step 21609, loss 1.38017e-06, acc 1
2017-08-08T18:14:48.768238: step 21610, loss 1.66112e-05, acc 1
2017-08-08T18:14:48.988349: step 21611, loss 1.00609e-05, acc 1
2017-08-08T18:14:49.339778: step 21612, loss 1.86265e-09, acc 1
2017-08-08T18:14:49.641002: step 21613, loss 3.29685e-07, acc 1
2017-08-08T18:14:49.987160: step 21614, loss 3.01539e-06, acc 1
2017-08-08T18:14:50.289150: step 21615, loss 7.79696e-06, acc 1
2017-08-08T18:14:50.545198: step 21616, loss 3.88499e-06, acc 1
2017-08-08T18:14:50.931031: step 21617, loss 4.47028e-07, acc 1
2017-08-08T18:14:51.218175: step 21618, loss 1.86265e-09, acc 1
2017-08-08T18:14:51.498827: step 21619, loss 6.35156e-07, acc 1
2017-08-08T18:14:51.786690: step 21620, loss 0, acc 1
2017-08-08T18:14:52.205778: step 21621, loss 6.01623e-07, acc 1
2017-08-08T18:14:52.621721: step 21622, loss 1.54677e-05, acc 1
2017-08-08T18:14:52.897829: step 21623, loss 1.86265e-09, acc 1
2017-08-08T18:14:53.103522: step 21624, loss 1.45286e-07, acc 1
2017-08-08T18:14:53.467570: step 21625, loss 3.8403e-06, acc 1
2017-08-08T18:14:53.812705: step 21626, loss 1.30385e-08, acc 1
2017-08-08T18:14:54.043087: step 21627, loss 6.575e-07, acc 1
2017-08-08T18:14:54.290543: step 21628, loss 2.60769e-07, acc 1
2017-08-08T18:14:54.543257: step 21629, loss 2.48559e-05, acc 1
2017-08-08T18:14:54.966263: step 21630, loss 6.90482e-05, acc 1
2017-08-08T18:14:55.352335: step 21631, loss 0.000135209, acc 1
2017-08-08T18:14:55.615752: step 21632, loss 4.52968e-05, acc 1
2017-08-08T18:14:55.854711: step 21633, loss 3.72529e-09, acc 1
2017-08-08T18:14:56.297295: step 21634, loss 1.17343e-06, acc 1
2017-08-08T18:14:56.662479: step 21635, loss 4.5036e-05, acc 1
2017-08-08T18:14:57.131757: step 21636, loss 3.11059e-07, acc 1
2017-08-08T18:14:57.368389: step 21637, loss 4.47034e-08, acc 1
2017-08-08T18:14:57.741382: step 21638, loss 2.64477e-06, acc 1
2017-08-08T18:14:57.979076: step 21639, loss 2.9802e-07, acc 1
2017-08-08T18:14:58.213308: step 21640, loss 2.23517e-08, acc 1
2017-08-08T18:14:58.695994: step 21641, loss 1.67638e-08, acc 1
2017-08-08T18:14:58.953711: step 21642, loss 1.42672e-06, acc 1
2017-08-08T18:14:59.259906: step 21643, loss 1.11011e-06, acc 1
2017-08-08T18:14:59.689637: step 21644, loss 0, acc 1
2017-08-08T18:14:59.933442: step 21645, loss 4.66895e-06, acc 1
2017-08-08T18:15:00.323363: step 21646, loss 1.86264e-08, acc 1
2017-08-08T18:15:00.538243: step 21647, loss 5.11812e-06, acc 1
2017-08-08T18:15:00.936713: step 21648, loss 8.08376e-07, acc 1
2017-08-08T18:15:01.214260: step 21649, loss 0, acc 1
2017-08-08T18:15:01.567173: step 21650, loss 9.67162e-06, acc 1
2017-08-08T18:15:01.982745: step 21651, loss 1.86264e-07, acc 1
2017-08-08T18:15:02.415512: step 21652, loss 0.000155514, acc 1
2017-08-08T18:15:03.033438: step 21653, loss 4.47035e-08, acc 1
2017-08-08T18:15:03.348503: step 21654, loss 9.36883e-07, acc 1
2017-08-08T18:15:03.696094: step 21655, loss 1.89989e-07, acc 1
2017-08-08T18:15:04.161389: step 21656, loss 0, acc 1
2017-08-08T18:15:04.527609: step 21657, loss 0, acc 1
2017-08-08T18:15:04.773421: step 21658, loss 0, acc 1
2017-08-08T18:15:05.185343: step 21659, loss 1.65129e-05, acc 1
2017-08-08T18:15:05.516305: step 21660, loss 2.56286e-06, acc 1
2017-08-08T18:15:05.825433: step 21661, loss 6.83576e-07, acc 1
2017-08-08T18:15:06.267367: step 21662, loss 2.79191e-06, acc 1
2017-08-08T18:15:06.722940: step 21663, loss 3.72486e-06, acc 1
2017-08-08T18:15:07.102485: step 21664, loss 2.62632e-07, acc 1
2017-08-08T18:15:07.485362: step 21665, loss 6.00963e-06, acc 1
2017-08-08T18:15:07.800455: step 21666, loss 0, acc 1
2017-08-08T18:15:08.220213: step 21667, loss 0.00364223, acc 1
2017-08-08T18:15:08.557816: step 21668, loss 8.90503e-05, acc 1
2017-08-08T18:15:08.891628: step 21669, loss 3.35276e-08, acc 1
2017-08-08T18:15:09.218683: step 21670, loss 5.58793e-08, acc 1
2017-08-08T18:15:09.452734: step 21671, loss 3.57028e-06, acc 1
2017-08-08T18:15:09.708430: step 21672, loss 2.01529e-06, acc 1
2017-08-08T18:15:10.101469: step 21673, loss 0, acc 1
2017-08-08T18:15:10.387493: step 21674, loss 6.87308e-07, acc 1
2017-08-08T18:15:10.641708: step 21675, loss 5.58793e-09, acc 1
2017-08-08T18:15:10.936969: step 21676, loss 5.58793e-09, acc 1
2017-08-08T18:15:11.405378: step 21677, loss 5.2154e-08, acc 1
2017-08-08T18:15:11.855047: step 21678, loss 1.11759e-08, acc 1
2017-08-08T18:15:12.193309: step 21679, loss 0.000126512, acc 1
2017-08-08T18:15:12.473973: step 21680, loss 2.39514e-05, acc 1
2017-08-08T18:15:12.738937: step 21681, loss 2.42144e-08, acc 1
2017-08-08T18:15:13.185966: step 21682, loss 8.65819e-05, acc 1
2017-08-08T18:15:13.470464: step 21683, loss 6.87408e-06, acc 1
2017-08-08T18:15:13.707888: step 21684, loss 2.10099e-06, acc 1
2017-08-08T18:15:14.026256: step 21685, loss 0.00497279, acc 1
2017-08-08T18:15:14.421366: step 21686, loss 0.000193967, acc 1
2017-08-08T18:15:14.901404: step 21687, loss 1.00393e-06, acc 1
2017-08-08T18:15:15.281919: step 21688, loss 0.000461725, acc 1
2017-08-08T18:15:15.517235: step 21689, loss 1.37835e-07, acc 1
2017-08-08T18:15:15.764142: step 21690, loss 0, acc 1
2017-08-08T18:15:16.197396: step 21691, loss 5.35728e-05, acc 1
2017-08-08T18:15:16.533818: step 21692, loss 3.72529e-09, acc 1
2017-08-08T18:15:16.823351: step 21693, loss 9.12693e-08, acc 1
2017-08-08T18:15:17.128760: step 21694, loss 8.16359e-05, acc 1
2017-08-08T18:15:17.537206: step 21695, loss 2.8291e-06, acc 1
2017-08-08T18:15:17.960063: step 21696, loss 2.14203e-07, acc 1
2017-08-08T18:15:18.386090: step 21697, loss 9.46197e-07, acc 1
2017-08-08T18:15:18.779243: step 21698, loss 1.12724e-05, acc 1
2017-08-08T18:15:19.161373: step 21699, loss 2.84983e-07, acc 1
2017-08-08T18:15:19.456628: step 21700, loss 3.16646e-07, acc 1

Evaluation:
2017-08-08T18:15:20.205340: step 21700, loss 7.31468, acc 0.709193

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-21700

2017-08-08T18:15:20.653418: step 21701, loss 8.49937e-05, acc 1
2017-08-08T18:15:21.006668: step 21702, loss 6.6656e-06, acc 1
2017-08-08T18:15:21.243931: step 21703, loss 0, acc 1
2017-08-08T18:15:21.447887: step 21704, loss 6.10937e-07, acc 1
2017-08-08T18:15:21.656849: step 21705, loss 3.41954e-06, acc 1
2017-08-08T18:15:22.025529: step 21706, loss 0, acc 1
2017-08-08T18:15:22.333206: step 21707, loss 1.88126e-07, acc 1
2017-08-08T18:15:22.628541: step 21708, loss 7.45058e-09, acc 1
2017-08-08T18:15:22.910594: step 21709, loss 6.98477e-07, acc 1
2017-08-08T18:15:23.231403: step 21710, loss 2.44005e-07, acc 1
2017-08-08T18:15:23.625620: step 21711, loss 1.60369e-06, acc 1
2017-08-08T18:15:24.039286: step 21712, loss 0, acc 1
2017-08-08T18:15:24.418785: step 21713, loss 0, acc 1
2017-08-08T18:15:24.727579: step 21714, loss 1.49012e-08, acc 1
2017-08-08T18:15:24.989328: step 21715, loss 1.86265e-09, acc 1
2017-08-08T18:15:25.419563: step 21716, loss 8.94068e-08, acc 1
2017-08-08T18:15:25.688210: step 21717, loss 4.19093e-07, acc 1
2017-08-08T18:15:25.982571: step 21718, loss 1.89989e-07, acc 1
2017-08-08T18:15:26.405577: step 21719, loss 4.21807e-05, acc 1
2017-08-08T18:15:26.871445: step 21720, loss 1.31466e-05, acc 1
2017-08-08T18:15:27.252521: step 21721, loss 0.000212896, acc 1
2017-08-08T18:15:27.514064: step 21722, loss 2.87942e-05, acc 1
2017-08-08T18:15:27.862363: step 21723, loss 6.15792e-05, acc 1
2017-08-08T18:15:28.256021: step 21724, loss 1.78813e-07, acc 1
2017-08-08T18:15:28.552428: step 21725, loss 5.58794e-09, acc 1
2017-08-08T18:15:28.843460: step 21726, loss 4.7683e-07, acc 1
2017-08-08T18:15:29.219462: step 21727, loss 0.00018161, acc 1
2017-08-08T18:15:29.565810: step 21728, loss 0, acc 1
2017-08-08T18:15:29.876957: step 21729, loss 1.8898e-05, acc 1
2017-08-08T18:15:30.208025: step 21730, loss 6.75812e-06, acc 1
2017-08-08T18:15:30.486012: step 21731, loss 1.11758e-07, acc 1
2017-08-08T18:15:30.906778: step 21732, loss 5.47608e-07, acc 1
2017-08-08T18:15:31.160216: step 21733, loss 5.96041e-07, acc 1
2017-08-08T18:15:31.431474: step 21734, loss 0.000183182, acc 1
2017-08-08T18:15:31.728963: step 21735, loss 1.86265e-09, acc 1
2017-08-08T18:15:32.137404: step 21736, loss 9.31295e-07, acc 1
2017-08-08T18:15:32.531524: step 21737, loss 2.37469e-06, acc 1
2017-08-08T18:15:32.833470: step 21738, loss 5.58793e-09, acc 1
2017-08-08T18:15:33.184326: step 21739, loss 2.29291e-05, acc 1
2017-08-08T18:15:33.378108: step 21740, loss 1.86265e-09, acc 1
2017-08-08T18:15:33.657376: step 21741, loss 1.02445e-07, acc 1
2017-08-08T18:15:33.952234: step 21742, loss 9.9602e-06, acc 1
2017-08-08T18:15:34.182133: step 21743, loss 2.1177e-06, acc 1
2017-08-08T18:15:34.410129: step 21744, loss 7.45058e-09, acc 1
2017-08-08T18:15:34.670945: step 21745, loss 9.31322e-09, acc 1
2017-08-08T18:15:34.937326: step 21746, loss 2.42144e-08, acc 1
2017-08-08T18:15:35.167543: step 21747, loss 5.64475e-06, acc 1
2017-08-08T18:15:35.427386: step 21748, loss 0.000109478, acc 1
2017-08-08T18:15:35.702021: step 21749, loss 1.67638e-08, acc 1
2017-08-08T18:15:35.910963: step 21750, loss 1.19209e-07, acc 1
2017-08-08T18:15:36.144187: step 21751, loss 0.00312661, acc 1
2017-08-08T18:15:36.357327: step 21752, loss 2.77532e-07, acc 1
2017-08-08T18:15:36.564525: step 21753, loss 5.58793e-08, acc 1
2017-08-08T18:15:36.947228: step 21754, loss 2.96158e-07, acc 1
2017-08-08T18:15:37.304831: step 21755, loss 0.0102785, acc 1
2017-08-08T18:15:37.674913: step 21756, loss 4.09782e-08, acc 1
2017-08-08T18:15:37.860258: step 21757, loss 1.86265e-09, acc 1
2017-08-08T18:15:38.044406: step 21758, loss 4.50155e-05, acc 1
2017-08-08T18:15:38.459236: step 21759, loss 0.000168854, acc 1
2017-08-08T18:15:38.726253: step 21760, loss 5.0135e-06, acc 1
2017-08-08T18:15:38.945655: step 21761, loss 2.02084e-06, acc 1
2017-08-08T18:15:39.198541: step 21762, loss 5.77504e-06, acc 1
2017-08-08T18:15:39.533473: step 21763, loss 2.42144e-08, acc 1
2017-08-08T18:15:39.973353: step 21764, loss 1.3411e-07, acc 1
2017-08-08T18:15:40.281978: step 21765, loss 1.80676e-07, acc 1
2017-08-08T18:15:40.526903: step 21766, loss 1.01267e-05, acc 1
2017-08-08T18:15:40.756569: step 21767, loss 2.45868e-07, acc 1
2017-08-08T18:15:41.162919: step 21768, loss 5.2154e-08, acc 1
2017-08-08T18:15:41.521685: step 21769, loss 1.9744e-07, acc 1
2017-08-08T18:15:41.821119: step 21770, loss 4.09781e-08, acc 1
2017-08-08T18:15:42.024343: step 21771, loss 5.69963e-07, acc 1
2017-08-08T18:15:42.414471: step 21772, loss 4.8387e-05, acc 1
2017-08-08T18:15:42.746546: step 21773, loss 4.37716e-07, acc 1
2017-08-08T18:15:42.987346: step 21774, loss 1.06359e-05, acc 1
2017-08-08T18:15:43.175487: step 21775, loss 5.58793e-09, acc 1
2017-08-08T18:15:43.498152: step 21776, loss 8.02779e-07, acc 1
2017-08-08T18:15:43.900724: step 21777, loss 1.56462e-07, acc 1
2017-08-08T18:15:44.172940: step 21778, loss 1.22934e-07, acc 1
2017-08-08T18:15:44.475460: step 21779, loss 0.000383543, acc 1
2017-08-08T18:15:44.731001: step 21780, loss 4.1723e-07, acc 1
2017-08-08T18:15:45.191913: step 21781, loss 1.8247e-05, acc 1
2017-08-08T18:15:45.553456: step 21782, loss 3.53902e-08, acc 1
2017-08-08T18:15:45.810974: step 21783, loss 6.51925e-08, acc 1
2017-08-08T18:15:46.015581: step 21784, loss 1.86265e-09, acc 1
2017-08-08T18:15:46.282471: step 21785, loss 1.1412e-05, acc 1
2017-08-08T18:15:46.605244: step 21786, loss 0.000939152, acc 1
2017-08-08T18:15:46.857617: step 21787, loss 5.58793e-09, acc 1
2017-08-08T18:15:47.144461: step 21788, loss 3.18039e-05, acc 1
2017-08-08T18:15:47.528238: step 21789, loss 1.39698e-07, acc 1
2017-08-08T18:15:47.918472: step 21790, loss 2.79397e-08, acc 1
2017-08-08T18:15:48.302449: step 21791, loss 7.45058e-09, acc 1
2017-08-08T18:15:48.623322: step 21792, loss 3.53902e-08, acc 1
2017-08-08T18:15:48.859612: step 21793, loss 2.94457e-06, acc 1
2017-08-08T18:15:49.210413: step 21794, loss 5.84432e-06, acc 1
2017-08-08T18:15:49.514153: step 21795, loss 0, acc 1
2017-08-08T18:15:49.773932: step 21796, loss 2.00222e-06, acc 1
2017-08-08T18:15:50.084065: step 21797, loss 2.1234e-07, acc 1
2017-08-08T18:15:50.372740: step 21798, loss 0, acc 1
2017-08-08T18:15:50.762052: step 21799, loss 9.68574e-08, acc 1
2017-08-08T18:15:51.183389: step 21800, loss 6.54398e-06, acc 1

Evaluation:
2017-08-08T18:15:52.034323: step 21800, loss 7.31963, acc 0.709193

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-21800

2017-08-08T18:15:52.680705: step 21801, loss 1.86265e-09, acc 1
2017-08-08T18:15:52.956040: step 21802, loss 2.04891e-08, acc 1
2017-08-08T18:15:53.234977: step 21803, loss 1.60198e-05, acc 1
2017-08-08T18:15:53.465729: step 21804, loss 0.00354601, acc 1
2017-08-08T18:15:53.829463: step 21805, loss 1.58324e-07, acc 1
2017-08-08T18:15:54.099831: step 21806, loss 5.21532e-07, acc 1
2017-08-08T18:15:54.471825: step 21807, loss 1.21015e-05, acc 1
2017-08-08T18:15:54.685218: step 21808, loss 2.23502e-06, acc 1
2017-08-08T18:15:54.905504: step 21809, loss 3.98928e-06, acc 1
2017-08-08T18:15:55.206333: step 21810, loss 1.50874e-07, acc 1
2017-08-08T18:15:55.402244: step 21811, loss 2.26861e-06, acc 1
2017-08-08T18:15:55.637317: step 21812, loss 1.79289e-05, acc 1
2017-08-08T18:15:55.880881: step 21813, loss 0.000143204, acc 1
2017-08-08T18:15:56.194983: step 21814, loss 1.09896e-07, acc 1
2017-08-08T18:15:56.536718: step 21815, loss 4.28408e-08, acc 1
2017-08-08T18:15:56.883572: step 21816, loss 1.86264e-08, acc 1
2017-08-08T18:15:57.139876: step 21817, loss 0, acc 1
2017-08-08T18:15:57.449378: step 21818, loss 9.12694e-08, acc 1
2017-08-08T18:15:57.845582: step 21819, loss 1.28147e-06, acc 1
2017-08-08T18:15:58.113663: step 21820, loss 3.80873e-06, acc 1
2017-08-08T18:15:58.404994: step 21821, loss 5.23396e-07, acc 1
2017-08-08T18:15:58.685615: step 21822, loss 3.91155e-08, acc 1
2017-08-08T18:15:59.105148: step 21823, loss 0, acc 1
2017-08-08T18:15:59.477633: step 21824, loss 2.96662e-05, acc 1
2017-08-08T18:15:59.800844: step 21825, loss 6.16109e-05, acc 1
2017-08-08T18:16:00.035930: step 21826, loss 2.42684e-06, acc 1
2017-08-08T18:16:00.228857: step 21827, loss 1.72472e-06, acc 1
2017-08-08T18:16:00.424790: step 21828, loss 0, acc 1
2017-08-08T18:16:00.707485: step 21829, loss 2.45664e-06, acc 1
2017-08-08T18:16:00.921990: step 21830, loss 2.03269e-05, acc 1
2017-08-08T18:16:01.214894: step 21831, loss 7.86017e-07, acc 1
2017-08-08T18:16:01.701254: step 21832, loss 1.86265e-09, acc 1
2017-08-08T18:16:02.147741: step 21833, loss 7.82294e-07, acc 1
2017-08-08T18:16:02.617797: step 21834, loss 2.04891e-08, acc 1
2017-08-08T18:16:02.876161: step 21835, loss 2.9057e-07, acc 1
2017-08-08T18:16:03.224008: step 21836, loss 5.82183e-05, acc 1
2017-08-08T18:16:03.557965: step 21837, loss 0.00159937, acc 1
2017-08-08T18:16:03.843653: step 21838, loss 8.71706e-07, acc 1
2017-08-08T18:16:04.110970: step 21839, loss 7.61803e-07, acc 1
2017-08-08T18:16:04.537555: step 21840, loss 4.68818e-05, acc 1
2017-08-08T18:16:04.946094: step 21841, loss 8.7446e-06, acc 1
2017-08-08T18:16:05.305409: step 21842, loss 0, acc 1
2017-08-08T18:16:05.627551: step 21843, loss 1.30015e-05, acc 1
2017-08-08T18:16:05.894043: step 21844, loss 2.49593e-07, acc 1
2017-08-08T18:16:06.290486: step 21845, loss 1.86265e-09, acc 1
2017-08-08T18:16:06.522893: step 21846, loss 1.41556e-06, acc 1
2017-08-08T18:16:06.789118: step 21847, loss 2.10478e-07, acc 1
2017-08-08T18:16:07.061689: step 21848, loss 3.44586e-07, acc 1
2017-08-08T18:16:07.365362: step 21849, loss 1.10919e-05, acc 1
2017-08-08T18:16:07.724616: step 21850, loss 0, acc 1
2017-08-08T18:16:08.022845: step 21851, loss 1.49012e-08, acc 1
2017-08-08T18:16:08.251499: step 21852, loss 0, acc 1
2017-08-08T18:16:08.442899: step 21853, loss 0, acc 1
2017-08-08T18:16:08.825379: step 21854, loss 2.02834e-06, acc 1
2017-08-08T18:16:09.051942: step 21855, loss 4.09782e-08, acc 1
2017-08-08T18:16:09.372898: step 21856, loss 1.23863e-06, acc 1
2017-08-08T18:16:09.701430: step 21857, loss 1.30385e-07, acc 1
2017-08-08T18:16:10.094469: step 21858, loss 7.92627e-06, acc 1
2017-08-08T18:16:10.494378: step 21859, loss 6.89178e-08, acc 1
2017-08-08T18:16:10.824624: step 21860, loss 0, acc 1
2017-08-08T18:16:11.069210: step 21861, loss 1.82343e-06, acc 1
2017-08-08T18:16:11.287316: step 21862, loss 8.94068e-08, acc 1
2017-08-08T18:16:11.628716: step 21863, loss 6.45302e-05, acc 1
2017-08-08T18:16:11.823621: step 21864, loss 0, acc 1
2017-08-08T18:16:12.027018: step 21865, loss 7.11714e-05, acc 1
2017-08-08T18:16:12.309951: step 21866, loss 7.45058e-09, acc 1
2017-08-08T18:16:12.636635: step 21867, loss 5.64373e-07, acc 1
2017-08-08T18:16:13.005451: step 21868, loss 1.04308e-07, acc 1
2017-08-08T18:16:13.317761: step 21869, loss 2.21653e-07, acc 1
2017-08-08T18:16:13.555277: step 21870, loss 1.23678e-06, acc 1
2017-08-08T18:16:13.739512: step 21871, loss 0.00875996, acc 1
2017-08-08T18:16:14.049847: step 21872, loss 0, acc 1
2017-08-08T18:16:14.430446: step 21873, loss 2.30773e-06, acc 1
2017-08-08T18:16:14.715291: step 21874, loss 4.00466e-07, acc 1
2017-08-08T18:16:15.020957: step 21875, loss 9.31322e-09, acc 1
2017-08-08T18:16:15.431098: step 21876, loss 0.00378646, acc 1
2017-08-08T18:16:15.784287: step 21877, loss 0.000158503, acc 1
2017-08-08T18:16:16.088960: step 21878, loss 1.54599e-07, acc 1
2017-08-08T18:16:16.378133: step 21879, loss 8.94068e-08, acc 1
2017-08-08T18:16:16.685432: step 21880, loss 9.33996e-05, acc 1
2017-08-08T18:16:17.180982: step 21881, loss 2.61503e-06, acc 1
2017-08-08T18:16:17.373206: step 21882, loss 8.94051e-07, acc 1
2017-08-08T18:16:17.602507: step 21883, loss 1.423e-06, acc 1
2017-08-08T18:16:17.913356: step 21884, loss 7.63683e-08, acc 1
2017-08-08T18:16:18.366570: step 21885, loss 2.34692e-07, acc 1
2017-08-08T18:16:18.754682: step 21886, loss 3.14585e-06, acc 1
2017-08-08T18:16:19.123993: step 21887, loss 1.35973e-07, acc 1
2017-08-08T18:16:19.371083: step 21888, loss 9.31322e-09, acc 1
2017-08-08T18:16:19.873610: step 21889, loss 1.81745e-05, acc 1
2017-08-08T18:16:20.136426: step 21890, loss 2.0988e-05, acc 1
2017-08-08T18:16:20.358766: step 21891, loss 6.22111e-07, acc 1
2017-08-08T18:16:20.595813: step 21892, loss 7.45058e-09, acc 1
2017-08-08T18:16:20.889806: step 21893, loss 0, acc 1
2017-08-08T18:16:21.214163: step 21894, loss 1.86265e-09, acc 1
2017-08-08T18:16:21.521698: step 21895, loss 5.5604e-05, acc 1
2017-08-08T18:16:21.726001: step 21896, loss 1.65775e-07, acc 1
2017-08-08T18:16:22.005289: step 21897, loss 0.000248918, acc 1
2017-08-08T18:16:22.358714: step 21898, loss 3.72529e-09, acc 1
2017-08-08T18:16:22.645569: step 21899, loss 2.08044e-06, acc 1
2017-08-08T18:16:22.908370: step 21900, loss 4.01107e-06, acc 1

Evaluation:
2017-08-08T18:16:23.896212: step 21900, loss 7.34528, acc 0.712946

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-21900

2017-08-08T18:16:24.541212: step 21901, loss 1.77319e-06, acc 1
2017-08-08T18:16:24.804474: step 21902, loss 5.21532e-07, acc 1
2017-08-08T18:16:25.101370: step 21903, loss 0.00194033, acc 1
2017-08-08T18:16:25.478512: step 21904, loss 5.88864e-05, acc 1
2017-08-08T18:16:25.750560: step 21905, loss 3.72529e-09, acc 1
2017-08-08T18:16:26.057443: step 21906, loss 7.61804e-07, acc 1
2017-08-08T18:16:26.447679: step 21907, loss 1.86265e-09, acc 1
2017-08-08T18:16:26.917150: step 21908, loss 8.34448e-07, acc 1
2017-08-08T18:16:27.344978: step 21909, loss 1.62605e-06, acc 1
2017-08-08T18:16:27.622286: step 21910, loss 3.72529e-09, acc 1
2017-08-08T18:16:27.863435: step 21911, loss 0, acc 1
2017-08-08T18:16:28.308085: step 21912, loss 3.23265e-05, acc 1
2017-08-08T18:16:28.602324: step 21913, loss 0.000263539, acc 1
2017-08-08T18:16:28.864685: step 21914, loss 6.33299e-08, acc 1
2017-08-08T18:16:29.147611: step 21915, loss 8.90319e-07, acc 1
2017-08-08T18:16:29.569276: step 21916, loss 0.000205959, acc 1
2017-08-08T18:16:29.943067: step 21917, loss 7.26431e-08, acc 1
2017-08-08T18:16:30.312370: step 21918, loss 2.79397e-08, acc 1
2017-08-08T18:16:30.648065: step 21919, loss 5.58793e-09, acc 1
2017-08-08T18:16:30.880783: step 21920, loss 0, acc 1
2017-08-08T18:16:31.308155: step 21921, loss 9.3132e-08, acc 1
2017-08-08T18:16:31.575556: step 21922, loss 2.31213e-05, acc 1
2017-08-08T18:16:31.853834: step 21923, loss 0, acc 1
2017-08-08T18:16:32.127008: step 21924, loss 7.65533e-07, acc 1
2017-08-08T18:16:32.490865: step 21925, loss 1.15484e-07, acc 1
2017-08-08T18:16:32.856365: step 21926, loss 2.6394e-05, acc 1
2017-08-08T18:16:33.252865: step 21927, loss 3.4908e-05, acc 1
2017-08-08T18:16:33.564390: step 21928, loss 5.59545e-05, acc 1
2017-08-08T18:16:33.974639: step 21929, loss 3.29686e-07, acc 1
2017-08-08T18:16:34.292787: step 21930, loss 4.61979e-05, acc 1
2017-08-08T18:16:34.573313: step 21931, loss 0.00260117, acc 1
2017-08-08T18:16:34.881389: step 21932, loss 1.11382e-06, acc 1
2017-08-08T18:16:35.245421: step 21933, loss 1.32247e-07, acc 1
2017-08-08T18:16:35.657616: step 21934, loss 1.12315e-06, acc 1
2017-08-08T18:16:35.941421: step 21935, loss 5.58793e-09, acc 1
2017-08-08T18:16:36.221056: step 21936, loss 1.67638e-08, acc 1
2017-08-08T18:16:36.465379: step 21937, loss 8.32584e-07, acc 1
2017-08-08T18:16:36.850377: step 21938, loss 2.0937e-05, acc 1
2017-08-08T18:16:37.124136: step 21939, loss 0, acc 1
2017-08-08T18:16:37.331406: step 21940, loss 1.17955e-05, acc 1
2017-08-08T18:16:37.597320: step 21941, loss 7.05783e-06, acc 1
2017-08-08T18:16:37.919038: step 21942, loss 5.58793e-09, acc 1
2017-08-08T18:16:38.316928: step 21943, loss 0, acc 1
2017-08-08T18:16:38.625786: step 21944, loss 7.20833e-07, acc 1
2017-08-08T18:16:38.809210: step 21945, loss 0.00136621, acc 1
2017-08-08T18:16:39.208922: step 21946, loss 9.08912e-06, acc 1
2017-08-08T18:16:39.483257: step 21947, loss 0.0242803, acc 0.984375
2017-08-08T18:16:39.766745: step 21948, loss 0.000157716, acc 1
2017-08-08T18:16:40.038240: step 21949, loss 5.89493e-05, acc 1
2017-08-08T18:16:40.533856: step 21950, loss 0.000317062, acc 1
2017-08-08T18:16:40.976838: step 21951, loss 1.1101e-06, acc 1
2017-08-08T18:16:41.306589: step 21952, loss 7.45058e-09, acc 1
2017-08-08T18:16:41.579858: step 21953, loss 2.21653e-07, acc 1
2017-08-08T18:16:41.824156: step 21954, loss 3.25959e-07, acc 1
2017-08-08T18:16:42.199737: step 21955, loss 6.75622e-06, acc 1
2017-08-08T18:16:42.532345: step 21956, loss 5.58794e-09, acc 1
2017-08-08T18:16:42.830650: step 21957, loss 1.274e-06, acc 1
2017-08-08T18:16:43.117400: step 21958, loss 0.00628839, acc 1
2017-08-08T18:16:43.457958: step 21959, loss 1.2538e-05, acc 1
2017-08-08T18:16:43.899066: step 21960, loss 3.4434e-05, acc 1
2017-08-08T18:16:44.309800: step 21961, loss 0, acc 1
2017-08-08T18:16:44.623350: step 21962, loss 2.18287e-06, acc 1
2017-08-08T18:16:44.885533: step 21963, loss 6.22092e-05, acc 1
2017-08-08T18:16:45.254368: step 21964, loss 0.0846554, acc 0.984375
2017-08-08T18:16:45.525895: step 21965, loss 4.58985e-05, acc 1
2017-08-08T18:16:45.828465: step 21966, loss 5.6069e-05, acc 1
2017-08-08T18:16:46.111344: step 21967, loss 2.83801e-05, acc 1
2017-08-08T18:16:46.490415: step 21968, loss 0, acc 1
2017-08-08T18:16:46.922846: step 21969, loss 0, acc 1
2017-08-08T18:16:47.281382: step 21970, loss 6.67989e-06, acc 1
2017-08-08T18:16:47.554570: step 21971, loss 5.83272e-06, acc 1
2017-08-08T18:16:47.796606: step 21972, loss 3.55762e-07, acc 1
2017-08-08T18:16:48.173988: step 21973, loss 8.31961e-05, acc 1
2017-08-08T18:16:48.530054: step 21974, loss 5.58793e-09, acc 1
2017-08-08T18:16:48.816362: step 21975, loss 9.29574e-06, acc 1
2017-08-08T18:16:49.112150: step 21976, loss 0, acc 1
2017-08-08T18:16:49.413421: step 21977, loss 0, acc 1
2017-08-08T18:16:49.849147: step 21978, loss 1.62117e-05, acc 1
2017-08-08T18:16:50.097456: step 21979, loss 1.93522e-06, acc 1
2017-08-08T18:16:50.381394: step 21980, loss 3.2596e-07, acc 1
2017-08-08T18:16:50.641057: step 21981, loss 6.12798e-07, acc 1
2017-08-08T18:16:51.009367: step 21982, loss 0.0147735, acc 0.984375
2017-08-08T18:16:51.353527: step 21983, loss 5.66236e-07, acc 1
2017-08-08T18:16:51.673216: step 21984, loss 6.25397e-06, acc 1
2017-08-08T18:16:52.049448: step 21985, loss 0.000104985, acc 1
2017-08-08T18:16:52.535733: step 21986, loss 5.58794e-09, acc 1
2017-08-08T18:16:52.953830: step 21987, loss 1.11759e-08, acc 1
2017-08-08T18:16:53.226427: step 21988, loss 0, acc 1
2017-08-08T18:16:53.454103: step 21989, loss 6.33293e-07, acc 1
2017-08-08T18:16:53.799163: step 21990, loss 5.58793e-09, acc 1
2017-08-08T18:16:54.100359: step 21991, loss 3.23616e-05, acc 1
2017-08-08T18:16:54.337376: step 21992, loss 2.42144e-08, acc 1
2017-08-08T18:16:54.630851: step 21993, loss 0.000737866, acc 1
2017-08-08T18:16:54.933262: step 21994, loss 0.0241828, acc 0.984375
2017-08-08T18:16:55.242574: step 21995, loss 4.47034e-08, acc 1
2017-08-08T18:16:55.608574: step 21996, loss 1.45286e-07, acc 1
2017-08-08T18:16:55.946902: step 21997, loss 3.89288e-07, acc 1
2017-08-08T18:16:56.277482: step 21998, loss 7.58656e-05, acc 1
2017-08-08T18:16:56.604081: step 21999, loss 3.53902e-08, acc 1
2017-08-08T18:16:57.006283: step 22000, loss 2.80675e-06, acc 1

Evaluation:
2017-08-08T18:16:57.722049: step 22000, loss 7.63938, acc 0.710131

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-22000

2017-08-08T18:16:58.278107: step 22001, loss 2.24883e-05, acc 1
2017-08-08T18:16:58.574699: step 22002, loss 4.82514e-05, acc 1
2017-08-08T18:16:58.873362: step 22003, loss 9.0522e-07, acc 1
2017-08-08T18:16:59.184275: step 22004, loss 9.35036e-07, acc 1
2017-08-08T18:16:59.486089: step 22005, loss 6.24303e-05, acc 1
2017-08-08T18:16:59.907966: step 22006, loss 2.49593e-07, acc 1
2017-08-08T18:17:00.176788: step 22007, loss 2.82364e-06, acc 1
2017-08-08T18:17:00.460780: step 22008, loss 3.1106e-07, acc 1
2017-08-08T18:17:00.747774: step 22009, loss 2.04891e-08, acc 1
2017-08-08T18:17:01.097381: step 22010, loss 0.00101812, acc 1
2017-08-08T18:17:01.497848: step 22011, loss 2.79397e-08, acc 1
2017-08-08T18:17:01.887496: step 22012, loss 0, acc 1
2017-08-08T18:17:02.141732: step 22013, loss 2.60769e-07, acc 1
2017-08-08T18:17:02.498910: step 22014, loss 0.0391456, acc 0.984375
2017-08-08T18:17:02.932868: step 22015, loss 1.05608e-06, acc 1
2017-08-08T18:17:03.270460: step 22016, loss 7.07804e-08, acc 1
2017-08-08T18:17:03.601987: step 22017, loss 7.24672e-06, acc 1
2017-08-08T18:17:04.077765: step 22018, loss 0, acc 1
2017-08-08T18:17:04.457701: step 22019, loss 3.63212e-07, acc 1
2017-08-08T18:17:04.926796: step 22020, loss 7.45056e-08, acc 1
2017-08-08T18:17:05.287450: step 22021, loss 4.06052e-07, acc 1
2017-08-08T18:17:05.707448: step 22022, loss 1.86265e-09, acc 1
2017-08-08T18:17:06.100778: step 22023, loss 2.49575e-06, acc 1
2017-08-08T18:17:06.383033: step 22024, loss 7.2726e-05, acc 1
2017-08-08T18:17:06.660965: step 22025, loss 1.86265e-09, acc 1
2017-08-08T18:17:06.957605: step 22026, loss 2.99246e-05, acc 1
2017-08-08T18:17:07.447643: step 22027, loss 2.45298e-05, acc 1
2017-08-08T18:17:07.901609: step 22028, loss 2.06935e-06, acc 1
2017-08-08T18:17:08.230863: step 22029, loss 3.6783e-06, acc 1
2017-08-08T18:17:08.490923: step 22030, loss 4.95456e-07, acc 1
2017-08-08T18:17:08.747574: step 22031, loss 2.10651e-06, acc 1
2017-08-08T18:17:09.111675: step 22032, loss 3.72529e-09, acc 1
2017-08-08T18:17:09.354723: step 22033, loss 9.31322e-09, acc 1
2017-08-08T18:17:09.614290: step 22034, loss 2.69274e-05, acc 1
2017-08-08T18:17:09.989391: step 22035, loss 5.9045e-07, acc 1
2017-08-08T18:17:10.437384: step 22036, loss 6.77989e-07, acc 1
2017-08-08T18:17:10.845829: step 22037, loss 5.04327e-05, acc 1
2017-08-08T18:17:11.066522: step 22038, loss 5.03612e-05, acc 1
2017-08-08T18:17:11.294903: step 22039, loss 8.56815e-08, acc 1
2017-08-08T18:17:11.696334: step 22040, loss 8.76504e-06, acc 1
2017-08-08T18:17:11.981707: step 22041, loss 2.33634e-05, acc 1
2017-08-08T18:17:12.223598: step 22042, loss 2.18283e-05, acc 1
2017-08-08T18:17:12.529717: step 22043, loss 4.85664e-05, acc 1
2017-08-08T18:17:12.931867: step 22044, loss 0.000118517, acc 1
2017-08-08T18:17:13.333343: step 22045, loss 0.00105895, acc 1
2017-08-08T18:17:13.605552: step 22046, loss 2.42144e-08, acc 1
2017-08-08T18:17:13.824864: step 22047, loss 5.3862e-05, acc 1
2017-08-08T18:17:14.049649: step 22048, loss 1.51057e-06, acc 1
2017-08-08T18:17:14.464164: step 22049, loss 7.03364e-05, acc 1
2017-08-08T18:17:14.741195: step 22050, loss 2.95617e-06, acc 1
2017-08-08T18:17:15.009384: step 22051, loss 3.53902e-08, acc 1
2017-08-08T18:17:15.273754: step 22052, loss 7.20012e-06, acc 1
2017-08-08T18:17:15.523532: step 22053, loss 1.86265e-09, acc 1
2017-08-08T18:17:16.029404: step 22054, loss 0, acc 1
2017-08-08T18:17:16.419339: step 22055, loss 0, acc 1
2017-08-08T18:17:16.744444: step 22056, loss 2.98554e-06, acc 1
2017-08-08T18:17:17.010577: step 22057, loss 1.86265e-09, acc 1
2017-08-08T18:17:17.499239: step 22058, loss 9.46261e-06, acc 1
2017-08-08T18:17:17.789226: step 22059, loss 1.13621e-07, acc 1
2017-08-08T18:17:18.098459: step 22060, loss 6.63972e-05, acc 1
2017-08-08T18:17:18.397332: step 22061, loss 1.01883e-06, acc 1
2017-08-08T18:17:18.836663: step 22062, loss 1.65775e-07, acc 1
2017-08-08T18:17:19.303372: step 22063, loss 9.27003e-05, acc 1
2017-08-08T18:17:19.681589: step 22064, loss 0.00409835, acc 1
2017-08-08T18:17:19.927649: step 22065, loss 1.90725e-06, acc 1
2017-08-08T18:17:20.198213: step 22066, loss 2.09395e-05, acc 1
2017-08-08T18:17:20.595073: step 22067, loss 4.77885e-06, acc 1
2017-08-08T18:17:20.896578: step 22068, loss 9.28092e-06, acc 1
2017-08-08T18:17:21.215816: step 22069, loss 5.58794e-09, acc 1
2017-08-08T18:17:21.502809: step 22070, loss 9.718e-06, acc 1
2017-08-08T18:17:21.953169: step 22071, loss 1.82786e-05, acc 1
2017-08-08T18:17:22.291971: step 22072, loss 4.54479e-07, acc 1
2017-08-08T18:17:22.681281: step 22073, loss 6.89178e-08, acc 1
2017-08-08T18:17:23.074548: step 22074, loss 9.87503e-05, acc 1
2017-08-08T18:17:23.331997: step 22075, loss 0, acc 1
2017-08-08T18:17:23.709424: step 22076, loss 0.000135505, acc 1
2017-08-08T18:17:24.045257: step 22077, loss 2.52315e-05, acc 1
2017-08-08T18:17:24.371400: step 22078, loss 0.0216425, acc 0.984375
2017-08-08T18:17:24.668777: step 22079, loss 5.96046e-08, acc 1
2017-08-08T18:17:24.973060: step 22080, loss 8.19563e-08, acc 1
2017-08-08T18:17:25.462608: step 22081, loss 1.2442e-06, acc 1
2017-08-08T18:17:25.812477: step 22082, loss 4.15367e-07, acc 1
2017-08-08T18:17:26.150002: step 22083, loss 1.86265e-09, acc 1
2017-08-08T18:17:26.404131: step 22084, loss 1.64463e-06, acc 1
2017-08-08T18:17:26.793570: step 22085, loss 0.000455858, acc 1
2017-08-08T18:17:27.162374: step 22086, loss 4.51813e-06, acc 1
2017-08-08T18:17:27.427460: step 22087, loss 3.61351e-07, acc 1
2017-08-08T18:17:27.669900: step 22088, loss 3.72529e-09, acc 1
2017-08-08T18:17:28.057387: step 22089, loss 1.58431e-05, acc 1
2017-08-08T18:17:28.427846: step 22090, loss 1.21286e-05, acc 1
2017-08-08T18:17:28.817529: step 22091, loss 2.24805e-06, acc 1
2017-08-08T18:17:29.092687: step 22092, loss 1.27772e-06, acc 1
2017-08-08T18:17:29.352422: step 22093, loss 4.63796e-07, acc 1
2017-08-08T18:17:29.762145: step 22094, loss 5.2154e-08, acc 1
2017-08-08T18:17:30.039474: step 22095, loss 0.00153909, acc 1
2017-08-08T18:17:30.316299: step 22096, loss 2.47731e-07, acc 1
2017-08-08T18:17:30.574384: step 22097, loss 1.24797e-07, acc 1
2017-08-08T18:17:30.945324: step 22098, loss 7.84997e-06, acc 1
2017-08-08T18:17:31.367683: step 22099, loss 1.08033e-07, acc 1
2017-08-08T18:17:31.700119: step 22100, loss 1.84396e-06, acc 1

Evaluation:
2017-08-08T18:17:32.405945: step 22100, loss 7.55155, acc 0.712946

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-22100

2017-08-08T18:17:33.041899: step 22101, loss 2.81258e-07, acc 1
2017-08-08T18:17:33.347404: step 22102, loss 2.98023e-08, acc 1
2017-08-08T18:17:33.665583: step 22103, loss 6.68195e-05, acc 1
2017-08-08T18:17:34.016237: step 22104, loss 0.0370748, acc 0.984375
2017-08-08T18:17:34.533993: step 22105, loss 0.000139942, acc 1
2017-08-08T18:17:34.951653: step 22106, loss 9.88565e-06, acc 1
2017-08-08T18:17:35.234475: step 22107, loss 1.86264e-08, acc 1
2017-08-08T18:17:35.699571: step 22108, loss 7.22692e-07, acc 1
2017-08-08T18:17:35.978342: step 22109, loss 1.54599e-07, acc 1
2017-08-08T18:17:36.222077: step 22110, loss 2.57043e-07, acc 1
2017-08-08T18:17:36.505421: step 22111, loss 1.51117e-05, acc 1
2017-08-08T18:17:36.801849: step 22112, loss 1.84401e-07, acc 1
2017-08-08T18:17:37.214564: step 22113, loss 4.91802e-05, acc 1
2017-08-08T18:17:37.625510: step 22114, loss 6.42108e-06, acc 1
2017-08-08T18:17:37.945348: step 22115, loss 0.000145935, acc 1
2017-08-08T18:17:38.192594: step 22116, loss 7.06117e-05, acc 1
2017-08-08T18:17:38.470945: step 22117, loss 9.31322e-09, acc 1
2017-08-08T18:17:38.817715: step 22118, loss 1.26659e-07, acc 1
2017-08-08T18:17:39.101431: step 22119, loss 0, acc 1
2017-08-08T18:17:39.337923: step 22120, loss 1.18647e-06, acc 1
2017-08-08T18:17:39.717487: step 22121, loss 3.11297e-05, acc 1
2017-08-08T18:17:40.086585: step 22122, loss 0.000472692, acc 1
2017-08-08T18:17:40.406294: step 22123, loss 1.03375e-06, acc 1
2017-08-08T18:17:40.617358: step 22124, loss 4.85323e-05, acc 1
2017-08-08T18:17:40.865399: step 22125, loss 5.2154e-08, acc 1
2017-08-08T18:17:41.211388: step 22126, loss 3.72529e-09, acc 1
2017-08-08T18:17:41.460606: step 22127, loss 4.32832e-06, acc 1
2017-08-08T18:17:41.727757: step 22128, loss 0.0563008, acc 0.96875
2017-08-08T18:17:41.973049: step 22129, loss 1.0356e-06, acc 1
2017-08-08T18:17:42.253334: step 22130, loss 5.77419e-08, acc 1
2017-08-08T18:17:42.694940: step 22131, loss 2.53117e-05, acc 1
2017-08-08T18:17:43.037544: step 22132, loss 8.84732e-07, acc 1
2017-08-08T18:17:43.359421: step 22133, loss 1.60187e-07, acc 1
2017-08-08T18:17:43.636351: step 22134, loss 3.41088e-05, acc 1
2017-08-08T18:17:43.894281: step 22135, loss 0, acc 1
2017-08-08T18:17:44.329475: step 22136, loss 0, acc 1
2017-08-08T18:17:44.563649: step 22137, loss 4.07914e-07, acc 1
2017-08-08T18:17:44.845330: step 22138, loss 4.88005e-07, acc 1
2017-08-08T18:17:45.116944: step 22139, loss 0.000283174, acc 1
2017-08-08T18:17:45.519311: step 22140, loss 3.73049e-06, acc 1
2017-08-08T18:17:45.885310: step 22141, loss 1.2107e-06, acc 1
2017-08-08T18:17:46.227000: step 22142, loss 2.64493e-07, acc 1
2017-08-08T18:17:46.512193: step 22143, loss 2.63362e-06, acc 1
2017-08-08T18:17:46.776403: step 22144, loss 0, acc 1
2017-08-08T18:17:47.157366: step 22145, loss 5.77419e-08, acc 1
2017-08-08T18:17:47.494196: step 22146, loss 7.45058e-09, acc 1
2017-08-08T18:17:47.739720: step 22147, loss 0, acc 1
2017-08-08T18:17:47.982683: step 22148, loss 1.86265e-09, acc 1
2017-08-08T18:17:48.269444: step 22149, loss 0.150552, acc 0.984375
2017-08-08T18:17:48.732649: step 22150, loss 2.6077e-08, acc 1
2017-08-08T18:17:49.118188: step 22151, loss 0.00146231, acc 1
2017-08-08T18:17:49.528190: step 22152, loss 1.16856e-05, acc 1
2017-08-08T18:17:49.798417: step 22153, loss 4.33993e-07, acc 1
2017-08-08T18:17:50.087007: step 22154, loss 0.000352762, acc 1
2017-08-08T18:17:50.501721: step 22155, loss 1.97439e-07, acc 1
2017-08-08T18:17:50.794009: step 22156, loss 7.45058e-09, acc 1
2017-08-08T18:17:51.082924: step 22157, loss 1.71054e-05, acc 1
2017-08-08T18:17:51.374011: step 22158, loss 1.67638e-08, acc 1
2017-08-08T18:17:51.723250: step 22159, loss 1.76107e-05, acc 1
2017-08-08T18:17:52.141013: step 22160, loss 7.43178e-07, acc 1
2017-08-08T18:17:52.539874: step 22161, loss 3.72529e-09, acc 1
2017-08-08T18:17:52.887238: step 22162, loss 0.00049267, acc 1
2017-08-08T18:17:53.118530: step 22163, loss 2.98023e-08, acc 1
2017-08-08T18:17:53.419915: step 22164, loss 6.3329e-07, acc 1
2017-08-08T18:17:53.783731: step 22165, loss 5.33791e-06, acc 1
2017-08-08T18:17:54.064527: step 22166, loss 3.7045e-05, acc 1
2017-08-08T18:17:54.348603: step 22167, loss 5.21826e-06, acc 1
2017-08-08T18:17:54.709383: step 22168, loss 7.4991e-06, acc 1
2017-08-08T18:17:55.162709: step 22169, loss 4.18859e-06, acc 1
2017-08-08T18:17:55.583013: step 22170, loss 0.00141181, acc 1
2017-08-08T18:17:55.897782: step 22171, loss 1.86265e-09, acc 1
2017-08-08T18:17:56.135300: step 22172, loss 0.0033237, acc 1
2017-08-08T18:17:56.433375: step 22173, loss 2.17964e-05, acc 1
2017-08-08T18:17:56.841268: step 22174, loss 1.88442e-05, acc 1
2017-08-08T18:17:57.095856: step 22175, loss 0.0355614, acc 0.984375
2017-08-08T18:17:57.427222: step 22176, loss 0, acc 1
2017-08-08T18:17:57.737346: step 22177, loss 5.90448e-07, acc 1
2017-08-08T18:17:58.206682: step 22178, loss 1.23513e-05, acc 1
2017-08-08T18:17:58.533356: step 22179, loss 1.44353e-06, acc 1
2017-08-08T18:17:58.877298: step 22180, loss 0, acc 1
2017-08-08T18:17:59.133933: step 22181, loss 0.000125943, acc 1
2017-08-08T18:17:59.436512: step 22182, loss 0.000900986, acc 1
2017-08-08T18:17:59.851022: step 22183, loss 2.9802e-07, acc 1
2017-08-08T18:18:00.139010: step 22184, loss 3.85564e-07, acc 1
2017-08-08T18:18:00.388190: step 22185, loss 0.20136, acc 0.984375
2017-08-08T18:18:00.805383: step 22186, loss 3.72529e-09, acc 1
2017-08-08T18:18:01.245348: step 22187, loss 1.67638e-08, acc 1
2017-08-08T18:18:01.651458: step 22188, loss 6.04048e-05, acc 1
2017-08-08T18:18:01.978885: step 22189, loss 0.00265338, acc 1
2017-08-08T18:18:02.270891: step 22190, loss 5.53197e-07, acc 1
2017-08-08T18:18:02.675775: step 22191, loss 1.04308e-07, acc 1
2017-08-08T18:18:03.061525: step 22192, loss 0.00477148, acc 1
2017-08-08T18:18:03.407370: step 22193, loss 0.0361983, acc 0.984375
2017-08-08T18:18:03.712685: step 22194, loss 0, acc 1
2017-08-08T18:18:04.021113: step 22195, loss 1.86265e-09, acc 1
2017-08-08T18:18:04.465717: step 22196, loss 1.67638e-08, acc 1
2017-08-08T18:18:04.852993: step 22197, loss 2.86845e-07, acc 1
2017-08-08T18:18:05.190933: step 22198, loss 2.56093e-06, acc 1
2017-08-08T18:18:05.418465: step 22199, loss 5.2154e-08, acc 1
2017-08-08T18:18:05.709404: step 22200, loss 0, acc 1

Evaluation:
2017-08-08T18:18:06.497051: step 22200, loss 7.60629, acc 0.711069

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-22200

2017-08-08T18:18:07.101907: step 22201, loss 0.000197626, acc 1
2017-08-08T18:18:07.504561: step 22202, loss 4.43306e-07, acc 1
2017-08-08T18:18:07.863068: step 22203, loss 1.06171e-07, acc 1
2017-08-08T18:18:08.136811: step 22204, loss 3.18857e-06, acc 1
2017-08-08T18:18:08.469762: step 22205, loss 9.31322e-09, acc 1
2017-08-08T18:18:08.846529: step 22206, loss 1.13621e-07, acc 1
2017-08-08T18:18:09.125645: step 22207, loss 0.000359639, acc 1
2017-08-08T18:18:09.420300: step 22208, loss 2.57043e-07, acc 1
2017-08-08T18:18:09.734950: step 22209, loss 5.58794e-09, acc 1
2017-08-08T18:18:10.104401: step 22210, loss 1.99302e-07, acc 1
2017-08-08T18:18:10.475031: step 22211, loss 0, acc 1
2017-08-08T18:18:10.820235: step 22212, loss 7.46903e-07, acc 1
2017-08-08T18:18:11.054972: step 22213, loss 3.04246e-05, acc 1
2017-08-08T18:18:11.411007: step 22214, loss 0.000946866, acc 1
2017-08-08T18:18:11.746414: step 22215, loss 6.6495e-07, acc 1
2017-08-08T18:18:12.020530: step 22216, loss 0, acc 1
2017-08-08T18:18:12.301688: step 22217, loss 3.71763e-06, acc 1
2017-08-08T18:18:12.589542: step 22218, loss 8.76056e-05, acc 1
2017-08-08T18:18:12.953375: step 22219, loss 2.91487e-06, acc 1
2017-08-08T18:18:13.437186: step 22220, loss 0, acc 1
2017-08-08T18:18:13.804709: step 22221, loss 2.00553e-05, acc 1
2017-08-08T18:18:14.047085: step 22222, loss 5.45749e-07, acc 1
2017-08-08T18:18:14.414785: step 22223, loss 1.02445e-07, acc 1
2017-08-08T18:18:14.623436: step 22224, loss 0.00522632, acc 1
2017-08-08T18:18:14.863474: step 22225, loss 0.000465417, acc 1
2017-08-08T18:18:15.131261: step 22226, loss 6.38297e-05, acc 1
2017-08-08T18:18:15.580705: step 22227, loss 0.0012575, acc 1
2017-08-08T18:18:15.904550: step 22228, loss 0.000111818, acc 1
2017-08-08T18:18:16.166085: step 22229, loss 1.32057e-06, acc 1
2017-08-08T18:18:16.393500: step 22230, loss 5.71822e-07, acc 1
2017-08-08T18:18:16.729364: step 22231, loss 7.84154e-07, acc 1
2017-08-08T18:18:17.097114: step 22232, loss 0.000638174, acc 1
2017-08-08T18:18:17.404159: step 22233, loss 1.45286e-07, acc 1
2017-08-08T18:18:17.667705: step 22234, loss 7.76714e-07, acc 1
2017-08-08T18:18:17.950995: step 22235, loss 3.72529e-09, acc 1
2017-08-08T18:18:18.315256: step 22236, loss 1.02445e-07, acc 1
2017-08-08T18:18:18.698611: step 22237, loss 0.0341655, acc 0.984375
2017-08-08T18:18:19.049027: step 22238, loss 1.86265e-09, acc 1
2017-08-08T18:18:19.339005: step 22239, loss 1.06171e-07, acc 1
2017-08-08T18:18:19.673184: step 22240, loss 0.00016162, acc 1
2017-08-08T18:18:20.025880: step 22241, loss 1.86265e-09, acc 1
2017-08-08T18:18:20.315882: step 22242, loss 7.28771e-05, acc 1
2017-08-08T18:18:20.611105: step 22243, loss 0.000214349, acc 1
2017-08-08T18:18:20.923699: step 22244, loss 6.14672e-08, acc 1
2017-08-08T18:18:21.216022: step 22245, loss 6.21499e-06, acc 1
2017-08-08T18:18:21.620555: step 22246, loss 0.0594749, acc 0.984375
2017-08-08T18:18:21.860459: step 22247, loss 6.72347e-06, acc 1
2017-08-08T18:18:22.079254: step 22248, loss 0.000990009, acc 1
2017-08-08T18:18:22.485371: step 22249, loss 5.14378e-06, acc 1
2017-08-08T18:18:22.778773: step 22250, loss 3.55762e-07, acc 1
2017-08-08T18:18:23.048669: step 22251, loss 3.53902e-08, acc 1
2017-08-08T18:18:23.320251: step 22252, loss 1.09522e-06, acc 1
2017-08-08T18:18:23.708005: step 22253, loss 7.3762e-06, acc 1
2017-08-08T18:18:24.058093: step 22254, loss 3.06004e-06, acc 1
2017-08-08T18:18:24.269405: step 22255, loss 2.6077e-08, acc 1
2017-08-08T18:18:24.487488: step 22256, loss 3.91526e-05, acc 1
2017-08-08T18:18:24.793986: step 22257, loss 1.30111e-05, acc 1
2017-08-08T18:18:25.032070: step 22258, loss 3.04079e-05, acc 1
2017-08-08T18:18:25.249369: step 22259, loss 7.32003e-07, acc 1
2017-08-08T18:18:25.503055: step 22260, loss 1.14473e-05, acc 1
2017-08-08T18:18:25.841310: step 22261, loss 0.000155597, acc 1
2017-08-08T18:18:26.185868: step 22262, loss 3.76835e-05, acc 1
2017-08-08T18:18:26.419305: step 22263, loss 2.79397e-08, acc 1
2017-08-08T18:18:26.644529: step 22264, loss 1.00583e-07, acc 1
2017-08-08T18:18:26.952017: step 22265, loss 1.26841e-06, acc 1
2017-08-08T18:18:27.340041: step 22266, loss 2.42144e-08, acc 1
2017-08-08T18:18:27.634590: step 22267, loss 3.16649e-08, acc 1
2017-08-08T18:18:27.978806: step 22268, loss 0, acc 1
2017-08-08T18:18:28.391253: step 22269, loss 2.03325e-05, acc 1
2017-08-08T18:18:28.812701: step 22270, loss 0, acc 1
2017-08-08T18:18:29.100923: step 22271, loss 5.90387e-05, acc 1
2017-08-08T18:18:29.352919: step 22272, loss 1.49011e-07, acc 1
2017-08-08T18:18:29.715169: step 22273, loss 5.1967e-07, acc 1
2017-08-08T18:18:30.098736: step 22274, loss 1.47149e-07, acc 1
2017-08-08T18:18:30.337185: step 22275, loss 0.00014799, acc 1
2017-08-08T18:18:30.565322: step 22276, loss 6.96183e-06, acc 1
2017-08-08T18:18:30.901897: step 22277, loss 2.01594e-05, acc 1
2017-08-08T18:18:31.219554: step 22278, loss 1.63912e-07, acc 1
2017-08-08T18:18:31.465179: step 22279, loss 0.000115742, acc 1
2017-08-08T18:18:31.667727: step 22280, loss 1.60608e-05, acc 1
2017-08-08T18:18:32.036471: step 22281, loss 0.000629599, acc 1
2017-08-08T18:18:32.312668: step 22282, loss 0.000168331, acc 1
2017-08-08T18:18:32.616789: step 22283, loss 6.33389e-06, acc 1
2017-08-08T18:18:32.908295: step 22284, loss 3.35276e-08, acc 1
2017-08-08T18:18:33.353365: step 22285, loss 0, acc 1
2017-08-08T18:18:33.782631: step 22286, loss 6.53775e-07, acc 1
2017-08-08T18:18:34.152388: step 22287, loss 0.000948476, acc 1
2017-08-08T18:18:34.448285: step 22288, loss 1.30385e-08, acc 1
2017-08-08T18:18:34.684817: step 22289, loss 0, acc 1
2017-08-08T18:18:34.973390: step 22290, loss 1.3411e-07, acc 1
2017-08-08T18:18:35.374895: step 22291, loss 0.000512202, acc 1
2017-08-08T18:18:35.641509: step 22292, loss 0.000916547, acc 1
2017-08-08T18:18:35.906502: step 22293, loss 2.29104e-07, acc 1
2017-08-08T18:18:36.198884: step 22294, loss 7.99055e-07, acc 1
2017-08-08T18:18:36.614599: step 22295, loss 5.12219e-07, acc 1
2017-08-08T18:18:37.079203: step 22296, loss 2.07671e-06, acc 1
2017-08-08T18:18:37.481495: step 22297, loss 1.30385e-08, acc 1
2017-08-08T18:18:37.759244: step 22298, loss 2.79372e-06, acc 1
2017-08-08T18:18:38.021485: step 22299, loss 0.0321585, acc 0.984375
2017-08-08T18:18:38.494231: step 22300, loss 2.25378e-07, acc 1

Evaluation:
2017-08-08T18:18:39.183195: step 22300, loss 7.66808, acc 0.719512

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-22300

2017-08-08T18:18:39.747121: step 22301, loss 5.64516e-05, acc 1
2017-08-08T18:18:40.131728: step 22302, loss 0.00040863, acc 1
2017-08-08T18:18:40.515341: step 22303, loss 6.97965e-06, acc 1
2017-08-08T18:18:40.801700: step 22304, loss 1.46959e-06, acc 1
2017-08-08T18:18:41.066012: step 22305, loss 0.000124257, acc 1
2017-08-08T18:18:41.495515: step 22306, loss 3.72529e-09, acc 1
2017-08-08T18:18:41.845025: step 22307, loss 1.11759e-08, acc 1
2017-08-08T18:18:42.130394: step 22308, loss 9.49947e-08, acc 1
2017-08-08T18:18:42.494680: step 22309, loss 1.71543e-06, acc 1
2017-08-08T18:18:42.808515: step 22310, loss 3.72529e-09, acc 1
2017-08-08T18:18:43.184243: step 22311, loss 1.70051e-06, acc 1
2017-08-08T18:18:43.538568: step 22312, loss 5.28986e-07, acc 1
2017-08-08T18:18:43.891325: step 22313, loss 0, acc 1
2017-08-08T18:18:44.150532: step 22314, loss 1.777e-05, acc 1
2017-08-08T18:18:44.470739: step 22315, loss 3.85823e-05, acc 1
2017-08-08T18:18:44.869569: step 22316, loss 0, acc 1
2017-08-08T18:18:45.095374: step 22317, loss 0.0205103, acc 0.984375
2017-08-08T18:18:45.341779: step 22318, loss 9.1777e-06, acc 1
2017-08-08T18:18:45.681838: step 22319, loss 1.86265e-09, acc 1
2017-08-08T18:18:46.148029: step 22320, loss 1.30385e-08, acc 1
2017-08-08T18:18:46.469410: step 22321, loss 1.67638e-08, acc 1
2017-08-08T18:18:46.677503: step 22322, loss 7.67976e-06, acc 1
2017-08-08T18:18:47.131542: step 22323, loss 7.46909e-07, acc 1
2017-08-08T18:18:47.314189: step 22324, loss 0.00334448, acc 1
2017-08-08T18:18:47.596315: step 22325, loss 0.0186099, acc 0.984375
2017-08-08T18:18:47.847881: step 22326, loss 2.06877e-05, acc 1
2017-08-08T18:18:48.135343: step 22327, loss 0.000374772, acc 1
2017-08-08T18:18:48.523495: step 22328, loss 8.00936e-08, acc 1
2017-08-08T18:18:48.803646: step 22329, loss 4.4088e-05, acc 1
2017-08-08T18:18:49.023454: step 22330, loss 2.6077e-08, acc 1
2017-08-08T18:18:49.313568: step 22331, loss 3.48708e-05, acc 1
2017-08-08T18:18:49.577712: step 22332, loss 1.72151e-05, acc 1
2017-08-08T18:18:49.791882: step 22333, loss 1.30385e-08, acc 1
2017-08-08T18:18:49.995141: step 22334, loss 6.53797e-05, acc 1
2017-08-08T18:18:50.309507: step 22335, loss 5.38183e-05, acc 1
2017-08-08T18:18:50.550611: step 22336, loss 8.13956e-07, acc 1
2017-08-08T18:18:50.761226: step 22337, loss 3.72529e-09, acc 1
2017-08-08T18:18:50.946107: step 22338, loss 0.160368, acc 0.984375
2017-08-08T18:18:51.113226: step 22339, loss 3.72529e-08, acc 1
2017-08-08T18:18:51.416412: step 22340, loss 7.39461e-07, acc 1
2017-08-08T18:18:51.628837: step 22341, loss 3.72529e-09, acc 1
2017-08-08T18:18:51.843784: step 22342, loss 9.31322e-09, acc 1
2017-08-08T18:18:52.080614: step 22343, loss 1.86265e-09, acc 1
2017-08-08T18:18:52.401662: step 22344, loss 1.78488e-05, acc 1
2017-08-08T18:18:52.662537: step 22345, loss 1.42831e-05, acc 1
2017-08-08T18:18:52.925624: step 22346, loss 0.000855302, acc 1
2017-08-08T18:18:53.091705: step 22347, loss 2.04891e-08, acc 1
2017-08-08T18:18:53.253340: step 22348, loss 8.41893e-07, acc 1
2017-08-08T18:18:53.517019: step 22349, loss 7.39461e-07, acc 1
2017-08-08T18:18:53.675377: step 22350, loss 5.41353e-06, acc 1
2017-08-08T18:18:53.951072: step 22351, loss 2.25736e-06, acc 1
2017-08-08T18:18:54.271994: step 22352, loss 0, acc 1
2017-08-08T18:18:54.672564: step 22353, loss 0.000914927, acc 1
2017-08-08T18:18:54.925330: step 22354, loss 2.34692e-07, acc 1
2017-08-08T18:18:55.197767: step 22355, loss 0.000103806, acc 1
2017-08-08T18:18:55.425854: step 22356, loss 1.86265e-09, acc 1
2017-08-08T18:18:55.611373: step 22357, loss 4.88496e-06, acc 1
2017-08-08T18:18:55.908268: step 22358, loss 1.33361e-06, acc 1
2017-08-08T18:18:56.104713: step 22359, loss 6.89178e-08, acc 1
2017-08-08T18:18:56.327729: step 22360, loss 0.00258458, acc 1
2017-08-08T18:18:56.589453: step 22361, loss 6.18643e-05, acc 1
2017-08-08T18:18:56.984779: step 22362, loss 4.47034e-08, acc 1
2017-08-08T18:18:57.373432: step 22363, loss 1.86265e-09, acc 1
2017-08-08T18:18:57.683182: step 22364, loss 2.84983e-07, acc 1
2017-08-08T18:18:57.904843: step 22365, loss 3.82729e-06, acc 1
2017-08-08T18:18:58.216559: step 22366, loss 9.31322e-09, acc 1
2017-08-08T18:18:58.566395: step 22367, loss 3.35276e-08, acc 1
2017-08-08T18:18:58.791080: step 22368, loss 1.86265e-09, acc 1
2017-08-08T18:18:59.060482: step 22369, loss 2.19173e-05, acc 1
2017-08-08T18:18:59.350124: step 22370, loss 5.58793e-09, acc 1
2017-08-08T18:18:59.737430: step 22371, loss 3.79978e-07, acc 1
2017-08-08T18:18:59.994310: step 22372, loss 3.72298e-06, acc 1
2017-08-08T18:19:00.263409: step 22373, loss 0.00066928, acc 1
2017-08-08T18:19:00.500737: step 22374, loss 0.00236808, acc 1
2017-08-08T18:19:00.711475: step 22375, loss 2.88709e-07, acc 1
2017-08-08T18:19:01.077129: step 22376, loss 0.000153792, acc 1
2017-08-08T18:19:01.276000: step 22377, loss 3.47538e-05, acc 1
2017-08-08T18:19:01.483722: step 22378, loss 8.38897e-05, acc 1
2017-08-08T18:19:01.790727: step 22379, loss 7.63683e-08, acc 1
2017-08-08T18:19:02.092559: step 22380, loss 9.20435e-06, acc 1
2017-08-08T18:19:02.539652: step 22381, loss 2.90935e-05, acc 1
2017-08-08T18:19:02.925055: step 22382, loss 3.38207e-05, acc 1
2017-08-08T18:19:03.308414: step 22383, loss 8.1023e-07, acc 1
2017-08-08T18:19:03.599620: step 22384, loss 1.31876e-05, acc 1
2017-08-08T18:19:03.902626: step 22385, loss 0, acc 1
2017-08-08T18:19:04.304192: step 22386, loss 7.58543e-05, acc 1
2017-08-08T18:19:04.553220: step 22387, loss 2.44005e-07, acc 1
2017-08-08T18:19:04.834497: step 22388, loss 2.622e-05, acc 1
2017-08-08T18:19:05.088827: step 22389, loss 0, acc 1
2017-08-08T18:19:05.566166: step 22390, loss 0, acc 1
2017-08-08T18:19:05.954649: step 22391, loss 9.29433e-07, acc 1
2017-08-08T18:19:06.283085: step 22392, loss 1.11759e-08, acc 1
2017-08-08T18:19:06.562172: step 22393, loss 5.50895e-05, acc 1
2017-08-08T18:19:06.852992: step 22394, loss 6.39073e-05, acc 1
2017-08-08T18:19:07.090932: step 22395, loss 0.00100034, acc 1
2017-08-08T18:19:07.316467: step 22396, loss 3.72529e-08, acc 1
2017-08-08T18:19:07.532859: step 22397, loss 1.3411e-07, acc 1
2017-08-08T18:19:07.898987: step 22398, loss 1.30385e-08, acc 1
2017-08-08T18:19:08.300709: step 22399, loss 9.68572e-08, acc 1
2017-08-08T18:19:08.633052: step 22400, loss 1.21025e-05, acc 1

Evaluation:
2017-08-08T18:19:09.364064: step 22400, loss 7.71747, acc 0.706379

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-22400

2017-08-08T18:19:09.900313: step 22401, loss 7.82122e-06, acc 1
2017-08-08T18:19:10.119021: step 22402, loss 0.00329998, acc 1
2017-08-08T18:19:10.336722: step 22403, loss 1.09896e-07, acc 1
2017-08-08T18:19:10.625706: step 22404, loss 1.53103e-06, acc 1
2017-08-08T18:19:10.873837: step 22405, loss 7.45058e-09, acc 1
2017-08-08T18:19:11.132045: step 22406, loss 2.98023e-08, acc 1
2017-08-08T18:19:11.367982: step 22407, loss 1.86265e-09, acc 1
2017-08-08T18:19:11.633062: step 22408, loss 3.67834e-06, acc 1
2017-08-08T18:19:11.933376: step 22409, loss 1.39469e-05, acc 1
2017-08-08T18:19:12.144072: step 22410, loss 1.11759e-08, acc 1
2017-08-08T18:19:12.386688: step 22411, loss 0, acc 1
2017-08-08T18:19:12.738189: step 22412, loss 2.37214e-05, acc 1
2017-08-08T18:19:13.049350: step 22413, loss 0.000715392, acc 1
2017-08-08T18:19:13.358824: step 22414, loss 0, acc 1
2017-08-08T18:19:13.541821: step 22415, loss 3.72529e-09, acc 1
2017-08-08T18:19:13.846430: step 22416, loss 6.46326e-07, acc 1
2017-08-08T18:19:14.128176: step 22417, loss 1.35973e-07, acc 1
2017-08-08T18:19:14.328563: step 22418, loss 9.31322e-09, acc 1
2017-08-08T18:19:14.568521: step 22419, loss 1.86265e-09, acc 1
2017-08-08T18:19:14.815850: step 22420, loss 5.43053e-06, acc 1
2017-08-08T18:19:15.153384: step 22421, loss 7.69262e-07, acc 1
2017-08-08T18:19:15.534974: step 22422, loss 6.14672e-08, acc 1
2017-08-08T18:19:15.895803: step 22423, loss 5.58793e-09, acc 1
2017-08-08T18:19:16.144080: step 22424, loss 3.35276e-08, acc 1
2017-08-08T18:19:16.510005: step 22425, loss 0.0010619, acc 1
2017-08-08T18:19:16.808790: step 22426, loss 6.18843e-06, acc 1
2017-08-08T18:19:17.054172: step 22427, loss 1.38575e-06, acc 1
2017-08-08T18:19:17.294897: step 22428, loss 4.07834e-05, acc 1
2017-08-08T18:19:17.538927: step 22429, loss 1.26659e-07, acc 1
2017-08-08T18:19:17.861375: step 22430, loss 3.58898e-06, acc 1
2017-08-08T18:19:18.105032: step 22431, loss 5.58793e-09, acc 1
2017-08-08T18:19:18.427869: step 22432, loss 2.22093e-05, acc 1
2017-08-08T18:19:18.645672: step 22433, loss 0, acc 1
2017-08-08T18:19:19.058334: step 22434, loss 1.67638e-08, acc 1
2017-08-08T18:19:19.272228: step 22435, loss 0.000398177, acc 1
2017-08-08T18:19:19.470770: step 22436, loss 1.88127e-07, acc 1
2017-08-08T18:19:19.671238: step 22437, loss 0.000241086, acc 1
2017-08-08T18:19:19.984906: step 22438, loss 9.72271e-07, acc 1
2017-08-08T18:19:20.333351: step 22439, loss 3.72529e-09, acc 1
2017-08-08T18:19:20.681217: step 22440, loss 1.38388e-06, acc 1
2017-08-08T18:19:20.964804: step 22441, loss 1.03314e-05, acc 1
2017-08-08T18:19:21.265442: step 22442, loss 1.76519e-05, acc 1
2017-08-08T18:19:21.688456: step 22443, loss 8.00937e-08, acc 1
2017-08-08T18:19:21.912862: step 22444, loss 3.72529e-09, acc 1
2017-08-08T18:19:22.147099: step 22445, loss 5.08498e-07, acc 1
2017-08-08T18:19:22.343645: step 22446, loss 6.07849e-06, acc 1
2017-08-08T18:19:22.625383: step 22447, loss 1.30385e-08, acc 1
2017-08-08T18:19:22.969402: step 22448, loss 2.04891e-08, acc 1
2017-08-08T18:19:23.281211: step 22449, loss 0, acc 1
2017-08-08T18:19:23.546043: step 22450, loss 1.11759e-07, acc 1
2017-08-08T18:19:23.778641: step 22451, loss 9.31322e-09, acc 1
2017-08-08T18:19:24.039139: step 22452, loss 0.000148978, acc 1
2017-08-08T18:19:24.427707: step 22453, loss 7.07804e-08, acc 1
2017-08-08T18:19:24.736902: step 22454, loss 3.688e-07, acc 1
2017-08-08T18:19:24.944980: step 22455, loss 3.61542e-05, acc 1
2017-08-08T18:19:25.199406: step 22456, loss 1.08961e-06, acc 1
2017-08-08T18:19:25.507209: step 22457, loss 9.31045e-06, acc 1
2017-08-08T18:19:25.866627: step 22458, loss 3.72529e-09, acc 1
2017-08-08T18:19:26.209454: step 22459, loss 9.9276e-07, acc 1
2017-08-08T18:19:26.429812: step 22460, loss 5.61453e-05, acc 1
2017-08-08T18:19:26.685403: step 22461, loss 1.14567e-05, acc 1
2017-08-08T18:19:27.032861: step 22462, loss 4.00987e-06, acc 1
2017-08-08T18:19:27.358454: step 22463, loss 0, acc 1
2017-08-08T18:19:27.662554: step 22464, loss 5.40166e-08, acc 1
2017-08-08T18:19:27.902150: step 22465, loss 4.88009e-07, acc 1
2017-08-08T18:19:28.249960: step 22466, loss 3.88333e-06, acc 1
2017-08-08T18:19:28.631737: step 22467, loss 2.04891e-08, acc 1
2017-08-08T18:19:28.920060: step 22468, loss 7.90122e-06, acc 1
2017-08-08T18:19:29.141804: step 22469, loss 0.0075996, acc 1
2017-08-08T18:19:29.335925: step 22470, loss 0, acc 1
2017-08-08T18:19:29.757870: step 22471, loss 0.0930557, acc 0.984375
2017-08-08T18:19:29.976270: step 22472, loss 2.99636e-05, acc 1
2017-08-08T18:19:30.185618: step 22473, loss 2.42144e-08, acc 1
2017-08-08T18:19:30.393323: step 22474, loss 0, acc 1
2017-08-08T18:19:30.653338: step 22475, loss 0.00166934, acc 1
2017-08-08T18:19:30.876637: step 22476, loss 0.000330417, acc 1
2017-08-08T18:19:31.150209: step 22477, loss 3.54729e-05, acc 1
2017-08-08T18:19:31.346712: step 22478, loss 2.04891e-08, acc 1
2017-08-08T18:19:31.570600: step 22479, loss 0.000240388, acc 1
2017-08-08T18:19:31.888945: step 22480, loss 1.01884e-06, acc 1
2017-08-08T18:19:32.068357: step 22481, loss 0.00013591, acc 1
2017-08-08T18:19:32.259343: step 22482, loss 5.14084e-07, acc 1
2017-08-08T18:19:32.525952: step 22483, loss 1.88127e-07, acc 1
2017-08-08T18:19:32.883513: step 22484, loss 9.03357e-07, acc 1
2017-08-08T18:19:33.250486: step 22485, loss 0, acc 1
2017-08-08T18:19:33.578107: step 22486, loss 4.80559e-07, acc 1
2017-08-08T18:19:33.849194: step 22487, loss 1.86265e-09, acc 1
2017-08-08T18:19:34.141351: step 22488, loss 5.58794e-09, acc 1
2017-08-08T18:19:34.391614: step 22489, loss 0.00129043, acc 1
2017-08-08T18:19:34.620611: step 22490, loss 5.03314e-05, acc 1
2017-08-08T18:19:34.816878: step 22491, loss 9.31322e-09, acc 1
2017-08-08T18:19:35.017447: step 22492, loss 0.00412598, acc 1
2017-08-08T18:19:35.297781: step 22493, loss 4.92221e-06, acc 1
2017-08-08T18:19:35.491501: step 22494, loss 0.000499985, acc 1
2017-08-08T18:19:35.768399: step 22495, loss 0.0151204, acc 0.984375
2017-08-08T18:19:36.090851: step 22496, loss 2.00997e-05, acc 1
2017-08-08T18:19:36.514601: step 22497, loss 1.53479e-06, acc 1
2017-08-08T18:19:36.947578: step 22498, loss 6.80888e-05, acc 1
2017-08-08T18:19:37.395944: step 22499, loss 5.55938e-06, acc 1
2017-08-08T18:19:37.828854: step 22500, loss 0.0140624, acc 0.983333

Evaluation:
2017-08-08T18:19:38.898667: step 22500, loss 7.60537, acc 0.721388

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-22500

2017-08-08T18:19:39.452116: step 22501, loss 2.28903e-06, acc 1
2017-08-08T18:19:39.723363: step 22502, loss 8.60521e-07, acc 1
2017-08-08T18:19:39.941103: step 22503, loss 1.86265e-09, acc 1
2017-08-08T18:19:40.320803: step 22504, loss 5.96045e-08, acc 1
2017-08-08T18:19:40.622854: step 22505, loss 6.15864e-06, acc 1
2017-08-08T18:19:40.884094: step 22506, loss 1.67816e-06, acc 1
2017-08-08T18:19:41.117638: step 22507, loss 1.0329e-05, acc 1
2017-08-08T18:19:41.389991: step 22508, loss 1.30385e-08, acc 1
2017-08-08T18:19:41.716328: step 22509, loss 2.18326e-05, acc 1
2017-08-08T18:19:41.973526: step 22510, loss 7.45058e-09, acc 1
2017-08-08T18:19:42.178636: step 22511, loss 5.58794e-09, acc 1
2017-08-08T18:19:42.456590: step 22512, loss 7.45058e-09, acc 1
2017-08-08T18:19:42.721455: step 22513, loss 4.79744e-06, acc 1
2017-08-08T18:19:42.948387: step 22514, loss 0, acc 1
2017-08-08T18:19:43.167592: step 22515, loss 0.0541808, acc 0.984375
2017-08-08T18:19:43.454235: step 22516, loss 0.000571853, acc 1
2017-08-08T18:19:43.651616: step 22517, loss 2.65606e-05, acc 1
2017-08-08T18:19:43.816341: step 22518, loss 2.42143e-07, acc 1
2017-08-08T18:19:44.036120: step 22519, loss 7.376e-07, acc 1
2017-08-08T18:19:44.434119: step 22520, loss 1.12464e-05, acc 1
2017-08-08T18:19:44.872945: step 22521, loss 8.12101e-07, acc 1
2017-08-08T18:19:45.273187: step 22522, loss 0.00785549, acc 1
2017-08-08T18:19:45.530121: step 22523, loss 0.000126281, acc 1
2017-08-08T18:19:45.783189: step 22524, loss 0, acc 1
2017-08-08T18:19:46.252186: step 22525, loss 3.41572e-06, acc 1
2017-08-08T18:19:46.552553: step 22526, loss 2.94295e-07, acc 1
2017-08-08T18:19:46.881421: step 22527, loss 1.03001e-06, acc 1
2017-08-08T18:19:47.102579: step 22528, loss 0.00918764, acc 1
2017-08-08T18:19:47.352986: step 22529, loss 1.98738e-06, acc 1
2017-08-08T18:19:47.681622: step 22530, loss 0.000275114, acc 1
2017-08-08T18:19:47.930646: step 22531, loss 6.33299e-08, acc 1
2017-08-08T18:19:48.196620: step 22532, loss 0.000841377, acc 1
2017-08-08T18:19:48.419559: step 22533, loss 8.05017e-06, acc 1
2017-08-08T18:19:48.791206: step 22534, loss 3.72529e-09, acc 1
2017-08-08T18:19:49.010682: step 22535, loss 0.000196565, acc 1
2017-08-08T18:19:49.369954: step 22536, loss 2.09594e-05, acc 1
2017-08-08T18:19:49.549622: step 22537, loss 9.40613e-05, acc 1
2017-08-08T18:19:49.752417: step 22538, loss 9.87199e-08, acc 1
2017-08-08T18:19:50.044239: step 22539, loss 4.63361e-06, acc 1
2017-08-08T18:19:50.242816: step 22540, loss 3.25588e-05, acc 1
2017-08-08T18:19:50.504074: step 22541, loss 1.30385e-08, acc 1
2017-08-08T18:19:50.728098: step 22542, loss 0.257046, acc 0.984375
2017-08-08T18:19:50.928485: step 22543, loss 8.56584e-06, acc 1
2017-08-08T18:19:51.242803: step 22544, loss 3.79779e-05, acc 1
2017-08-08T18:19:51.414766: step 22545, loss 0.000340872, acc 1
2017-08-08T18:19:51.746764: step 22546, loss 3.21931e-05, acc 1
2017-08-08T18:19:52.047691: step 22547, loss 3.93926e-05, acc 1
2017-08-08T18:19:52.308250: step 22548, loss 6.53775e-07, acc 1
2017-08-08T18:19:52.717642: step 22549, loss 0.000111098, acc 1
2017-08-08T18:19:52.956623: step 22550, loss 7.44597e-06, acc 1
2017-08-08T18:19:53.201334: step 22551, loss 9.31322e-09, acc 1
2017-08-08T18:19:53.429407: step 22552, loss 2.79397e-08, acc 1
2017-08-08T18:19:53.677381: step 22553, loss 5.58794e-09, acc 1
2017-08-08T18:19:54.005014: step 22554, loss 2.23517e-08, acc 1
2017-08-08T18:19:54.346819: step 22555, loss 0, acc 1
2017-08-08T18:19:54.706142: step 22556, loss 0, acc 1
2017-08-08T18:19:54.916906: step 22557, loss 0.0846822, acc 0.984375
2017-08-08T18:19:55.286460: step 22558, loss 5.2154e-08, acc 1
2017-08-08T18:19:55.465924: step 22559, loss 1.66884e-06, acc 1
2017-08-08T18:19:55.677371: step 22560, loss 2.40643e-06, acc 1
2017-08-08T18:19:56.032575: step 22561, loss 0, acc 1
2017-08-08T18:19:56.256455: step 22562, loss 1.93219e-05, acc 1
2017-08-08T18:19:56.489870: step 22563, loss 4.79396e-05, acc 1
2017-08-08T18:19:56.782567: step 22564, loss 1.05582e-05, acc 1
2017-08-08T18:19:57.121401: step 22565, loss 4.28408e-08, acc 1
2017-08-08T18:19:57.415322: step 22566, loss 2.26486e-06, acc 1
2017-08-08T18:19:57.614761: step 22567, loss 4.28408e-08, acc 1
2017-08-08T18:19:57.812961: step 22568, loss 4.32828e-06, acc 1
2017-08-08T18:19:58.084200: step 22569, loss 6.89177e-08, acc 1
2017-08-08T18:19:58.266641: step 22570, loss 1.10824e-06, acc 1
2017-08-08T18:19:58.441842: step 22571, loss 1.60187e-07, acc 1
2017-08-08T18:19:58.649308: step 22572, loss 5.71619e-05, acc 1
2017-08-08T18:19:58.900836: step 22573, loss 0.000260485, acc 1
2017-08-08T18:19:59.126469: step 22574, loss 7.60296e-05, acc 1
2017-08-08T18:19:59.364011: step 22575, loss 1.86265e-09, acc 1
2017-08-08T18:19:59.586035: step 22576, loss 2.1202e-05, acc 1
2017-08-08T18:19:59.827337: step 22577, loss 0.00018589, acc 1
2017-08-08T18:19:59.989042: step 22578, loss 1.91852e-07, acc 1
2017-08-08T18:20:00.159404: step 22579, loss 6.68682e-07, acc 1
2017-08-08T18:20:00.340826: step 22580, loss 2.79744e-06, acc 1
2017-08-08T18:20:00.632139: step 22581, loss 0.00147893, acc 1
2017-08-08T18:20:00.883886: step 22582, loss 5.58793e-09, acc 1
2017-08-08T18:20:01.078410: step 22583, loss 0, acc 1
2017-08-08T18:20:01.239948: step 22584, loss 1.46868e-05, acc 1
2017-08-08T18:20:01.469380: step 22585, loss 0.00129651, acc 1
2017-08-08T18:20:01.906177: step 22586, loss 3.42673e-05, acc 1
2017-08-08T18:20:02.169959: step 22587, loss 2.01164e-07, acc 1
2017-08-08T18:20:02.482063: step 22588, loss 1.74706e-06, acc 1
2017-08-08T18:20:02.728198: step 22589, loss 2.15641e-05, acc 1
2017-08-08T18:20:03.050906: step 22590, loss 0.000242156, acc 1
2017-08-08T18:20:03.479465: step 22591, loss 2.8312e-07, acc 1
2017-08-08T18:20:03.872660: step 22592, loss 2.77533e-07, acc 1
2017-08-08T18:20:04.175344: step 22593, loss 0, acc 1
2017-08-08T18:20:04.408053: step 22594, loss 8.62354e-06, acc 1
2017-08-08T18:20:04.657393: step 22595, loss 0.201647, acc 0.984375
2017-08-08T18:20:04.990202: step 22596, loss 9.8559e-06, acc 1
2017-08-08T18:20:05.237812: step 22597, loss 3.83477e-05, acc 1
2017-08-08T18:20:05.513536: step 22598, loss 4.56344e-07, acc 1
2017-08-08T18:20:05.805384: step 22599, loss 3.58535e-06, acc 1
2017-08-08T18:20:06.241350: step 22600, loss 6.56802e-05, acc 1

Evaluation:
2017-08-08T18:20:06.975680: step 22600, loss 7.9379, acc 0.706379

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-22600

2017-08-08T18:20:07.341327: step 22601, loss 5.58794e-09, acc 1
2017-08-08T18:20:07.673354: step 22602, loss 0.000147772, acc 1
2017-08-08T18:20:07.927378: step 22603, loss 1.19209e-07, acc 1
2017-08-08T18:20:08.235330: step 22604, loss 2.51819e-06, acc 1
2017-08-08T18:20:08.516100: step 22605, loss 2.23517e-08, acc 1
2017-08-08T18:20:08.865658: step 22606, loss 1.50909e-05, acc 1
2017-08-08T18:20:09.225903: step 22607, loss 2.30041e-05, acc 1
2017-08-08T18:20:09.550638: step 22608, loss 5.58793e-09, acc 1
2017-08-08T18:20:09.796088: step 22609, loss 8.7542e-07, acc 1
2017-08-08T18:20:09.995505: step 22610, loss 6.89177e-08, acc 1
2017-08-08T18:20:10.251044: step 22611, loss 6.20912e-06, acc 1
2017-08-08T18:20:10.617028: step 22612, loss 6.31426e-07, acc 1
2017-08-08T18:20:10.899271: step 22613, loss 0.000260149, acc 1
2017-08-08T18:20:11.181843: step 22614, loss 3.86824e-06, acc 1
2017-08-08T18:20:11.460978: step 22615, loss 2.42144e-08, acc 1
2017-08-08T18:20:11.921985: step 22616, loss 2.25379e-07, acc 1
2017-08-08T18:20:12.321237: step 22617, loss 1.11759e-08, acc 1
2017-08-08T18:20:12.650402: step 22618, loss 0, acc 1
2017-08-08T18:20:12.879531: step 22619, loss 0.000482052, acc 1
2017-08-08T18:20:13.179758: step 22620, loss 3.93014e-07, acc 1
2017-08-08T18:20:13.393415: step 22621, loss 1.52369e-05, acc 1
2017-08-08T18:20:13.618874: step 22622, loss 6.85304e-06, acc 1
2017-08-08T18:20:13.872452: step 22623, loss 9.31322e-09, acc 1
2017-08-08T18:20:14.157392: step 22624, loss 0.000164846, acc 1
2017-08-08T18:20:14.610358: step 22625, loss 0, acc 1
2017-08-08T18:20:15.042214: step 22626, loss 0.00630606, acc 1
2017-08-08T18:20:15.412711: step 22627, loss 1.63864e-05, acc 1
2017-08-08T18:20:15.642406: step 22628, loss 2.57594e-06, acc 1
2017-08-08T18:20:15.849317: step 22629, loss 6.69382e-05, acc 1
2017-08-08T18:20:16.177284: step 22630, loss 0.000159944, acc 1
2017-08-08T18:20:16.429938: step 22631, loss 1.86264e-08, acc 1
2017-08-08T18:20:16.622230: step 22632, loss 4.47035e-08, acc 1
2017-08-08T18:20:16.887947: step 22633, loss 4.22778e-06, acc 1
2017-08-08T18:20:17.273355: step 22634, loss 0, acc 1
2017-08-08T18:20:17.625747: step 22635, loss 9.68573e-08, acc 1
2017-08-08T18:20:17.881309: step 22636, loss 5.58793e-09, acc 1
2017-08-08T18:20:18.118931: step 22637, loss 1.67624e-05, acc 1
2017-08-08T18:20:18.385373: step 22638, loss 1.80676e-07, acc 1
2017-08-08T18:20:18.840614: step 22639, loss 2.34613e-05, acc 1
2017-08-08T18:20:19.120900: step 22640, loss 5.23395e-07, acc 1
2017-08-08T18:20:19.354715: step 22641, loss 3.20931e-05, acc 1
2017-08-08T18:20:19.614855: step 22642, loss 4.47034e-08, acc 1
2017-08-08T18:20:19.954066: step 22643, loss 9.83462e-07, acc 1
2017-08-08T18:20:20.369448: step 22644, loss 1.08961e-06, acc 1
2017-08-08T18:20:20.697783: step 22645, loss 7.24964e-06, acc 1
2017-08-08T18:20:20.934504: step 22646, loss 1.86265e-09, acc 1
2017-08-08T18:20:21.116109: step 22647, loss 6.61228e-07, acc 1
2017-08-08T18:20:21.338700: step 22648, loss 9.49947e-08, acc 1
2017-08-08T18:20:21.657544: step 22649, loss 1.6018e-05, acc 1
2017-08-08T18:20:21.851062: step 22650, loss 5.96046e-09, acc 1
2017-08-08T18:20:22.062006: step 22651, loss 4.91733e-07, acc 1
2017-08-08T18:20:22.302261: step 22652, loss 3.14784e-07, acc 1
2017-08-08T18:20:22.681396: step 22653, loss 2.1382e-06, acc 1
2017-08-08T18:20:23.072887: step 22654, loss 1.06824e-05, acc 1
2017-08-08T18:20:23.299634: step 22655, loss 0.00018262, acc 1
2017-08-08T18:20:23.575420: step 22656, loss 0.000465873, acc 1
2017-08-08T18:20:23.892764: step 22657, loss 0.000100984, acc 1
2017-08-08T18:20:24.083897: step 22658, loss 5.58793e-09, acc 1
2017-08-08T18:20:24.354656: step 22659, loss 0.000177379, acc 1
2017-08-08T18:20:24.737416: step 22660, loss 7.99057e-07, acc 1
2017-08-08T18:20:25.093235: step 22661, loss 1.30385e-08, acc 1
2017-08-08T18:20:25.341773: step 22662, loss 1.01325e-06, acc 1
2017-08-08T18:20:25.644359: step 22663, loss 1.17952e-05, acc 1
2017-08-08T18:20:26.015697: step 22664, loss 2.11288e-05, acc 1
2017-08-08T18:20:26.262823: step 22665, loss 4.34676e-05, acc 1
2017-08-08T18:20:26.482910: step 22666, loss 3.72529e-08, acc 1
2017-08-08T18:20:26.739178: step 22667, loss 2.6077e-08, acc 1
2017-08-08T18:20:27.033398: step 22668, loss 1.93714e-07, acc 1
2017-08-08T18:20:27.321448: step 22669, loss 2.44005e-07, acc 1
2017-08-08T18:20:27.701816: step 22670, loss 0.0861346, acc 0.984375
2017-08-08T18:20:27.947302: step 22671, loss 5.82563e-06, acc 1
2017-08-08T18:20:28.198947: step 22672, loss 1.02445e-07, acc 1
2017-08-08T18:20:28.597352: step 22673, loss 6.41364e-06, acc 1
2017-08-08T18:20:28.962739: step 22674, loss 0.00132678, acc 1
2017-08-08T18:20:29.247835: step 22675, loss 0.000120181, acc 1
2017-08-08T18:20:29.470542: step 22676, loss 0, acc 1
2017-08-08T18:20:29.809375: step 22677, loss 2.04891e-08, acc 1
2017-08-08T18:20:30.160425: step 22678, loss 4.62986e-06, acc 1
2017-08-08T18:20:30.457180: step 22679, loss 1.30385e-07, acc 1
2017-08-08T18:20:30.716405: step 22680, loss 6.65383e-06, acc 1
2017-08-08T18:20:30.998704: step 22681, loss 0, acc 1
2017-08-08T18:20:31.227374: step 22682, loss 1.67638e-07, acc 1
2017-08-08T18:20:31.409658: step 22683, loss 3.22333e-05, acc 1
2017-08-08T18:20:31.584797: step 22684, loss 7.83956e-05, acc 1
2017-08-08T18:20:31.921346: step 22685, loss 9.98356e-07, acc 1
2017-08-08T18:20:32.224662: step 22686, loss 1.15293e-06, acc 1
2017-08-08T18:20:32.545112: step 22687, loss 2.48524e-05, acc 1
2017-08-08T18:20:32.767708: step 22688, loss 1.80666e-06, acc 1
2017-08-08T18:20:32.962462: step 22689, loss 1.86265e-09, acc 1
2017-08-08T18:20:33.277428: step 22690, loss 5.58794e-09, acc 1
2017-08-08T18:20:33.480898: step 22691, loss 4.53116e-06, acc 1
2017-08-08T18:20:33.701061: step 22692, loss 1.69523e-05, acc 1
2017-08-08T18:20:33.919420: step 22693, loss 1.4826e-06, acc 1
2017-08-08T18:20:34.208733: step 22694, loss 9.31322e-09, acc 1
2017-08-08T18:20:34.461363: step 22695, loss 9.6111e-07, acc 1
2017-08-08T18:20:34.717028: step 22696, loss 5.70853e-05, acc 1
2017-08-08T18:20:34.990734: step 22697, loss 3.40284e-06, acc 1
2017-08-08T18:20:35.178021: step 22698, loss 1.1721e-05, acc 1
2017-08-08T18:20:35.400612: step 22699, loss 7.58668e-05, acc 1
2017-08-08T18:20:35.847195: step 22700, loss 2.19963e-06, acc 1

Evaluation:
2017-08-08T18:20:36.477462: step 22700, loss 7.79203, acc 0.717636

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-22700

2017-08-08T18:20:36.939459: step 22701, loss 4.60067e-07, acc 1
2017-08-08T18:20:37.253553: step 22702, loss 9.9173e-06, acc 1
2017-08-08T18:20:37.536366: step 22703, loss 9.38763e-07, acc 1
2017-08-08T18:20:37.800449: step 22704, loss 0.0140985, acc 0.984375
2017-08-08T18:20:38.008710: step 22705, loss 6.57502e-07, acc 1
2017-08-08T18:20:38.400240: step 22706, loss 4.84959e-06, acc 1
2017-08-08T18:20:38.588977: step 22707, loss 1.39507e-06, acc 1
2017-08-08T18:20:38.794810: step 22708, loss 5.75434e-05, acc 1
2017-08-08T18:20:39.056614: step 22709, loss 0, acc 1
2017-08-08T18:20:39.361163: step 22710, loss 5.2154e-08, acc 1
2017-08-08T18:20:39.677985: step 22711, loss 1.67638e-08, acc 1
2017-08-08T18:20:39.975444: step 22712, loss 6.21278e-05, acc 1
2017-08-08T18:20:40.214838: step 22713, loss 6.96849e-06, acc 1
2017-08-08T18:20:40.447438: step 22714, loss 1.47148e-07, acc 1
2017-08-08T18:20:40.782525: step 22715, loss 2.3449e-06, acc 1
2017-08-08T18:20:41.069921: step 22716, loss 1.65208e-06, acc 1
2017-08-08T18:20:41.362061: step 22717, loss 5.58784e-07, acc 1
2017-08-08T18:20:41.608787: step 22718, loss 4.10435e-05, acc 1
2017-08-08T18:20:42.031118: step 22719, loss 3.72529e-09, acc 1
2017-08-08T18:20:42.369649: step 22720, loss 6.64955e-07, acc 1
2017-08-08T18:20:42.728843: step 22721, loss 0, acc 1
2017-08-08T18:20:43.010139: step 22722, loss 3.32296e-05, acc 1
2017-08-08T18:20:43.273393: step 22723, loss 1.64279e-06, acc 1
2017-08-08T18:20:43.689011: step 22724, loss 1.4081e-06, acc 1
2017-08-08T18:20:43.958948: step 22725, loss 1.65581e-06, acc 1
2017-08-08T18:20:44.266071: step 22726, loss 2.40449e-06, acc 1
2017-08-08T18:20:44.537975: step 22727, loss 0, acc 1
2017-08-08T18:20:44.857630: step 22728, loss 2.58907e-07, acc 1
2017-08-08T18:20:45.272477: step 22729, loss 1.57945e-06, acc 1
2017-08-08T18:20:45.606619: step 22730, loss 1.17346e-07, acc 1
2017-08-08T18:20:45.837207: step 22731, loss 2.08615e-07, acc 1
2017-08-08T18:20:46.180141: step 22732, loss 3.72525e-07, acc 1
2017-08-08T18:20:46.569378: step 22733, loss 0.0759278, acc 0.984375
2017-08-08T18:20:46.827672: step 22734, loss 4.60939e-06, acc 1
2017-08-08T18:20:47.128146: step 22735, loss 2.85517e-06, acc 1
2017-08-08T18:20:47.381624: step 22736, loss 1.30385e-08, acc 1
2017-08-08T18:20:47.733403: step 22737, loss 0, acc 1
2017-08-08T18:20:48.139594: step 22738, loss 2.59452e-06, acc 1
2017-08-08T18:20:48.573699: step 22739, loss 1.49012e-08, acc 1
2017-08-08T18:20:48.896124: step 22740, loss 2.04891e-08, acc 1
2017-08-08T18:20:49.142221: step 22741, loss 7.67709e-06, acc 1
2017-08-08T18:20:49.607046: step 22742, loss 1.11941e-06, acc 1
2017-08-08T18:20:49.852104: step 22743, loss 4.81812e-05, acc 1
2017-08-08T18:20:50.071299: step 22744, loss 3.72529e-09, acc 1
2017-08-08T18:20:50.414540: step 22745, loss 1.11759e-08, acc 1
2017-08-08T18:20:50.865954: step 22746, loss 5.47613e-07, acc 1
2017-08-08T18:20:51.243514: step 22747, loss 1.15334e-05, acc 1
2017-08-08T18:20:51.579547: step 22748, loss 4.28408e-08, acc 1
2017-08-08T18:20:51.844968: step 22749, loss 1.97987e-06, acc 1
2017-08-08T18:20:52.183830: step 22750, loss 0, acc 1
2017-08-08T18:20:52.579972: step 22751, loss 2.50822e-05, acc 1
2017-08-08T18:20:52.786032: step 22752, loss 1.59248e-06, acc 1
2017-08-08T18:20:53.017548: step 22753, loss 1.18089e-06, acc 1
2017-08-08T18:20:53.387724: step 22754, loss 3.35276e-08, acc 1
2017-08-08T18:20:53.760837: step 22755, loss 0, acc 1
2017-08-08T18:20:54.091589: step 22756, loss 4.09781e-08, acc 1
2017-08-08T18:20:54.449955: step 22757, loss 0.000461301, acc 1
2017-08-08T18:20:54.688143: step 22758, loss 3.20715e-06, acc 1
2017-08-08T18:20:54.946307: step 22759, loss 6.575e-07, acc 1
2017-08-08T18:20:55.312583: step 22760, loss 2.66356e-07, acc 1
2017-08-08T18:20:55.593625: step 22761, loss 5.94178e-05, acc 1
2017-08-08T18:20:55.888308: step 22762, loss 1.61226e-05, acc 1
2017-08-08T18:20:56.311061: step 22763, loss 7.11515e-07, acc 1
2017-08-08T18:20:56.683342: step 22764, loss 1.11759e-08, acc 1
2017-08-08T18:20:56.976956: step 22765, loss 8.94187e-06, acc 1
2017-08-08T18:20:57.222863: step 22766, loss 3.13767e-05, acc 1
2017-08-08T18:20:57.565640: step 22767, loss 0.00222387, acc 1
2017-08-08T18:20:57.806080: step 22768, loss 0.00252297, acc 1
2017-08-08T18:20:58.032181: step 22769, loss 4.7683e-07, acc 1
2017-08-08T18:20:58.320626: step 22770, loss 9.31322e-09, acc 1
2017-08-08T18:20:58.672232: step 22771, loss 4.82352e-06, acc 1
2017-08-08T18:20:59.080471: step 22772, loss 9.31322e-09, acc 1
2017-08-08T18:20:59.511887: step 22773, loss 6.79877e-05, acc 1
2017-08-08T18:20:59.816388: step 22774, loss 2.91104e-06, acc 1
2017-08-08T18:21:00.046273: step 22775, loss 8.46956e-06, acc 1
2017-08-08T18:21:00.471506: step 22776, loss 1.00583e-07, acc 1
2017-08-08T18:21:00.774541: step 22777, loss 9.6669e-07, acc 1
2017-08-08T18:21:01.068617: step 22778, loss 4.15365e-07, acc 1
2017-08-08T18:21:01.361392: step 22779, loss 2.17352e-05, acc 1
2017-08-08T18:21:01.785378: step 22780, loss 3.71186e-06, acc 1
2017-08-08T18:21:02.227418: step 22781, loss 3.35276e-08, acc 1
2017-08-08T18:21:02.545319: step 22782, loss 0.00125296, acc 1
2017-08-08T18:21:02.806454: step 22783, loss 0, acc 1
2017-08-08T18:21:03.101095: step 22784, loss 1.21999e-06, acc 1
2017-08-08T18:21:03.507443: step 22785, loss 1.11759e-08, acc 1
2017-08-08T18:21:03.695423: step 22786, loss 5.6037e-06, acc 1
2017-08-08T18:21:03.911001: step 22787, loss 5.8486e-07, acc 1
2017-08-08T18:21:04.127334: step 22788, loss 1.30385e-08, acc 1
2017-08-08T18:21:04.528639: step 22789, loss 0, acc 1
2017-08-08T18:21:04.947828: step 22790, loss 2.3859e-06, acc 1
2017-08-08T18:21:05.268337: step 22791, loss 1.37835e-07, acc 1
2017-08-08T18:21:05.497973: step 22792, loss 7.07804e-08, acc 1
2017-08-08T18:21:05.840897: step 22793, loss 1.25725e-06, acc 1
2017-08-08T18:21:06.172940: step 22794, loss 0.000123462, acc 1
2017-08-08T18:21:06.449625: step 22795, loss 0, acc 1
2017-08-08T18:21:06.744506: step 22796, loss 3.72529e-09, acc 1
2017-08-08T18:21:07.032946: step 22797, loss 1.95577e-07, acc 1
2017-08-08T18:21:07.501407: step 22798, loss 2.23517e-08, acc 1
2017-08-08T18:21:07.953113: step 22799, loss 5.58794e-09, acc 1
2017-08-08T18:21:08.258636: step 22800, loss 1.2293e-05, acc 1

Evaluation:
2017-08-08T18:21:08.791912: step 22800, loss 7.90934, acc 0.705441

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-22800

2017-08-08T18:21:09.319269: step 22801, loss 0, acc 1
2017-08-08T18:21:09.562229: step 22802, loss 0.000195727, acc 1
2017-08-08T18:21:09.997051: step 22803, loss 2.04891e-08, acc 1
2017-08-08T18:21:10.465734: step 22804, loss 1.17346e-07, acc 1
2017-08-08T18:21:10.817625: step 22805, loss 6.78982e-06, acc 1
2017-08-08T18:21:11.094922: step 22806, loss 1.86265e-09, acc 1
2017-08-08T18:21:11.394561: step 22807, loss 7.82309e-08, acc 1
2017-08-08T18:21:11.711254: step 22808, loss 0.00100832, acc 1
2017-08-08T18:21:11.911774: step 22809, loss 0, acc 1
2017-08-08T18:21:12.137386: step 22810, loss 2.55819e-05, acc 1
2017-08-08T18:21:12.347352: step 22811, loss 1.32802e-06, acc 1
2017-08-08T18:21:12.650738: step 22812, loss 2.6077e-08, acc 1
2017-08-08T18:21:12.983975: step 22813, loss 0, acc 1
2017-08-08T18:21:13.337505: step 22814, loss 1.86265e-09, acc 1
2017-08-08T18:21:13.651489: step 22815, loss 0.00285754, acc 1
2017-08-08T18:21:13.841381: step 22816, loss 9.31322e-09, acc 1
2017-08-08T18:21:14.191597: step 22817, loss 1.49012e-08, acc 1
2017-08-08T18:21:14.568933: step 22818, loss 1.49012e-08, acc 1
2017-08-08T18:21:14.875412: step 22819, loss 2.65369e-05, acc 1
2017-08-08T18:21:15.146009: step 22820, loss 9.70297e-06, acc 1
2017-08-08T18:21:15.505185: step 22821, loss 1.40538e-05, acc 1
2017-08-08T18:21:15.973372: step 22822, loss 0.000196626, acc 1
2017-08-08T18:21:16.391793: step 22823, loss 0.000320934, acc 1
2017-08-08T18:21:16.605744: step 22824, loss 0, acc 1
2017-08-08T18:21:16.791338: step 22825, loss 0, acc 1
2017-08-08T18:21:17.133349: step 22826, loss 2.04891e-08, acc 1
2017-08-08T18:21:17.322785: step 22827, loss 3.72529e-09, acc 1
2017-08-08T18:21:17.517390: step 22828, loss 8.7542e-07, acc 1
2017-08-08T18:21:17.767892: step 22829, loss 0.000208358, acc 1
2017-08-08T18:21:18.121363: step 22830, loss 3.07334e-07, acc 1
2017-08-08T18:21:18.440084: step 22831, loss 2.81609e-06, acc 1
2017-08-08T18:21:18.788386: step 22832, loss 0, acc 1
2017-08-08T18:21:19.105377: step 22833, loss 1.06359e-05, acc 1
2017-08-08T18:21:19.372238: step 22834, loss 3.09549e-06, acc 1
2017-08-08T18:21:19.589091: step 22835, loss 1.25353e-06, acc 1
2017-08-08T18:21:19.852903: step 22836, loss 1.63461e-05, acc 1
2017-08-08T18:21:20.140109: step 22837, loss 1.88126e-07, acc 1
2017-08-08T18:21:20.367264: step 22838, loss 6.79858e-07, acc 1
2017-08-08T18:21:20.575955: step 22839, loss 2.7567e-07, acc 1
2017-08-08T18:21:20.929355: step 22840, loss 7.86234e-05, acc 1
2017-08-08T18:21:21.340224: step 22841, loss 2.27107e-05, acc 1
2017-08-08T18:21:21.590995: step 22842, loss 1.0561e-06, acc 1
2017-08-08T18:21:21.795231: step 22843, loss 0.0119595, acc 0.984375
2017-08-08T18:21:22.064733: step 22844, loss 2.04891e-08, acc 1
2017-08-08T18:21:22.289107: step 22845, loss 3.35276e-08, acc 1
2017-08-08T18:21:22.534393: step 22846, loss 0, acc 1
2017-08-08T18:21:22.722689: step 22847, loss 8.00935e-08, acc 1
2017-08-08T18:21:23.009348: step 22848, loss 9.87199e-08, acc 1
2017-08-08T18:21:23.317215: step 22849, loss 5.2712e-07, acc 1
2017-08-08T18:21:23.650273: step 22850, loss 3.72529e-09, acc 1
2017-08-08T18:21:23.875538: step 22851, loss 5.58794e-09, acc 1
2017-08-08T18:21:24.131949: step 22852, loss 3.72529e-09, acc 1
2017-08-08T18:21:24.508119: step 22853, loss 1.41741e-06, acc 1
2017-08-08T18:21:24.826737: step 22854, loss 8.25134e-07, acc 1
2017-08-08T18:21:25.108097: step 22855, loss 1.28522e-07, acc 1
2017-08-08T18:21:25.345423: step 22856, loss 7.87884e-07, acc 1
2017-08-08T18:21:25.689167: step 22857, loss 1.83788e-05, acc 1
2017-08-08T18:21:26.013257: step 22858, loss 4.73106e-07, acc 1
2017-08-08T18:21:26.296763: step 22859, loss 4.61929e-07, acc 1
2017-08-08T18:21:26.474906: step 22860, loss 6.5005e-07, acc 1
2017-08-08T18:21:26.745058: step 22861, loss 1.86265e-09, acc 1
2017-08-08T18:21:27.024507: step 22862, loss 0, acc 1
2017-08-08T18:21:27.214712: step 22863, loss 2.84586e-06, acc 1
2017-08-08T18:21:27.458668: step 22864, loss 3.66527e-06, acc 1
2017-08-08T18:21:27.665280: step 22865, loss 5.19406e-06, acc 1
2017-08-08T18:21:28.052124: step 22866, loss 7.07805e-08, acc 1
2017-08-08T18:21:28.414176: step 22867, loss 0.0267724, acc 0.984375
2017-08-08T18:21:28.734197: step 22868, loss 1.75363e-05, acc 1
2017-08-08T18:21:28.951470: step 22869, loss 0.000364801, acc 1
2017-08-08T18:21:29.164765: step 22870, loss 1.03187e-06, acc 1
2017-08-08T18:21:29.485321: step 22871, loss 1.05609e-06, acc 1
2017-08-08T18:21:29.762807: step 22872, loss 2.84982e-07, acc 1
2017-08-08T18:21:29.961446: step 22873, loss 7.63683e-08, acc 1
2017-08-08T18:21:30.227637: step 22874, loss 7.07804e-08, acc 1
2017-08-08T18:21:30.451465: step 22875, loss 9.3132e-08, acc 1
2017-08-08T18:21:30.838868: step 22876, loss 0.000193396, acc 1
2017-08-08T18:21:31.161527: step 22877, loss 6.83132e-06, acc 1
2017-08-08T18:21:31.525425: step 22878, loss 3.07741e-05, acc 1
2017-08-08T18:21:31.705135: step 22879, loss 3.72529e-09, acc 1
2017-08-08T18:21:31.885670: step 22880, loss 5.94175e-07, acc 1
2017-08-08T18:21:32.235599: step 22881, loss 2.04891e-08, acc 1
2017-08-08T18:21:32.596522: step 22882, loss 1.11759e-08, acc 1
2017-08-08T18:21:32.901125: step 22883, loss 1.86265e-09, acc 1
2017-08-08T18:21:33.134206: step 22884, loss 5.4111e-05, acc 1
2017-08-08T18:21:33.593799: step 22885, loss 6.5812e-06, acc 1
2017-08-08T18:21:33.932506: step 22886, loss 1.02961e-05, acc 1
2017-08-08T18:21:34.242993: step 22887, loss 8.75442e-08, acc 1
2017-08-08T18:21:34.503878: step 22888, loss 0.000111309, acc 1
2017-08-08T18:21:34.803132: step 22889, loss 0.00307997, acc 1
2017-08-08T18:21:35.124476: step 22890, loss 4.28404e-07, acc 1
2017-08-08T18:21:35.408025: step 22891, loss 3.83601e-05, acc 1
2017-08-08T18:21:35.695951: step 22892, loss 1.91658e-06, acc 1
2017-08-08T18:21:35.989376: step 22893, loss 2.04891e-08, acc 1
2017-08-08T18:21:36.441406: step 22894, loss 0.000210088, acc 1
2017-08-08T18:21:36.754330: step 22895, loss 1.50874e-07, acc 1
2017-08-08T18:21:37.036594: step 22896, loss 4.0228e-06, acc 1
2017-08-08T18:21:37.243109: step 22897, loss 2.04891e-08, acc 1
2017-08-08T18:21:37.617562: step 22898, loss 1.02445e-07, acc 1
2017-08-08T18:21:37.808891: step 22899, loss 5.22826e-05, acc 1
2017-08-08T18:21:38.001065: step 22900, loss 1.86265e-09, acc 1

Evaluation:
2017-08-08T18:21:38.563943: step 22900, loss 7.78347, acc 0.716698

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-22900

2017-08-08T18:21:39.116170: step 22901, loss 4.19973e-06, acc 1
2017-08-08T18:21:39.502856: step 22902, loss 1.94749e-05, acc 1
2017-08-08T18:21:39.751773: step 22903, loss 2.10023e-05, acc 1
2017-08-08T18:21:40.015731: step 22904, loss 5.9231e-07, acc 1
2017-08-08T18:21:40.382784: step 22905, loss 1.695e-07, acc 1
2017-08-08T18:21:40.726113: step 22906, loss 1.86264e-08, acc 1
2017-08-08T18:21:40.992310: step 22907, loss 1.65775e-07, acc 1
2017-08-08T18:21:41.229423: step 22908, loss 7.48766e-07, acc 1
2017-08-08T18:21:41.571197: step 22909, loss 1.50122e-06, acc 1
2017-08-08T18:21:42.056559: step 22910, loss 7.45058e-09, acc 1
2017-08-08T18:21:42.442586: step 22911, loss 4.37147e-05, acc 1
2017-08-08T18:21:42.748192: step 22912, loss 0.00013074, acc 1
2017-08-08T18:21:43.009397: step 22913, loss 0.000225734, acc 1
2017-08-08T18:21:43.381200: step 22914, loss 6.14672e-08, acc 1
2017-08-08T18:21:43.693820: step 22915, loss 3.31368e-05, acc 1
2017-08-08T18:21:43.981611: step 22916, loss 5.58794e-09, acc 1
2017-08-08T18:21:44.224284: step 22917, loss 7.1596e-05, acc 1
2017-08-08T18:21:44.565249: step 22918, loss 5.58793e-09, acc 1
2017-08-08T18:21:44.973369: step 22919, loss 0, acc 1
2017-08-08T18:21:45.321729: step 22920, loss 1.08033e-07, acc 1
2017-08-08T18:21:45.655435: step 22921, loss 1.96014e-05, acc 1
2017-08-08T18:21:46.021631: step 22922, loss 1.09597e-05, acc 1
2017-08-08T18:21:46.236522: step 22923, loss 2.11026e-06, acc 1
2017-08-08T18:21:46.497267: step 22924, loss 3.72529e-09, acc 1
2017-08-08T18:21:46.693878: step 22925, loss 0, acc 1
2017-08-08T18:21:47.036244: step 22926, loss 9.33872e-05, acc 1
2017-08-08T18:21:47.412617: step 22927, loss 8.49882e-05, acc 1
2017-08-08T18:21:47.793012: step 22928, loss 3.45065e-05, acc 1
2017-08-08T18:21:48.059477: step 22929, loss 0.00024471, acc 1
2017-08-08T18:21:48.268664: step 22930, loss 4.91486e-05, acc 1
2017-08-08T18:21:48.571387: step 22931, loss 6.38874e-07, acc 1
2017-08-08T18:21:48.734834: step 22932, loss 1.8048e-06, acc 1
2017-08-08T18:21:48.907489: step 22933, loss 6.49928e-06, acc 1
2017-08-08T18:21:49.140938: step 22934, loss 1.86265e-09, acc 1
2017-08-08T18:21:49.402512: step 22935, loss 9.31321e-08, acc 1
2017-08-08T18:21:49.672754: step 22936, loss 1.11759e-08, acc 1
2017-08-08T18:21:49.865723: step 22937, loss 0.00111475, acc 1
2017-08-08T18:21:50.186281: step 22938, loss 1.86265e-09, acc 1
2017-08-08T18:21:50.473390: step 22939, loss 5.40166e-08, acc 1
2017-08-08T18:21:50.733741: step 22940, loss 0.0118699, acc 0.984375
2017-08-08T18:21:50.942768: step 22941, loss 0, acc 1
2017-08-08T18:21:51.211852: step 22942, loss 1.3094e-06, acc 1
2017-08-08T18:21:51.525334: step 22943, loss 7.02545e-05, acc 1
2017-08-08T18:21:51.821798: step 22944, loss 1.86265e-09, acc 1
2017-08-08T18:21:52.014537: step 22945, loss 1.86265e-09, acc 1
2017-08-08T18:21:52.171379: step 22946, loss 1.32248e-07, acc 1
2017-08-08T18:21:52.383320: step 22947, loss 8.38189e-08, acc 1
2017-08-08T18:21:52.593517: step 22948, loss 2.74146e-05, acc 1
2017-08-08T18:21:52.760909: step 22949, loss 9.71998e-06, acc 1
2017-08-08T18:21:52.942511: step 22950, loss 1.07288e-07, acc 1
2017-08-08T18:21:53.173201: step 22951, loss 1.49011e-07, acc 1
2017-08-08T18:21:53.453324: step 22952, loss 3.22236e-07, acc 1
2017-08-08T18:21:53.741395: step 22953, loss 0.000694281, acc 1
2017-08-08T18:21:53.956919: step 22954, loss 1.73225e-07, acc 1
2017-08-08T18:21:54.164729: step 22955, loss 3.72529e-09, acc 1
2017-08-08T18:21:54.497503: step 22956, loss 1.11759e-08, acc 1
2017-08-08T18:21:54.742515: step 22957, loss 1.86265e-09, acc 1
2017-08-08T18:21:54.954640: step 22958, loss 8.7916e-07, acc 1
2017-08-08T18:21:55.187546: step 22959, loss 1.86265e-09, acc 1
2017-08-08T18:21:55.462190: step 22960, loss 7.67392e-07, acc 1
2017-08-08T18:21:55.786918: step 22961, loss 0, acc 1
2017-08-08T18:21:56.093171: step 22962, loss 1.47237e-05, acc 1
2017-08-08T18:21:56.391992: step 22963, loss 0.00558215, acc 1
2017-08-08T18:21:56.666080: step 22964, loss 0.000443026, acc 1
2017-08-08T18:21:56.903076: step 22965, loss 1.11941e-06, acc 1
2017-08-08T18:21:57.137240: step 22966, loss 0, acc 1
2017-08-08T18:21:57.332864: step 22967, loss 2.50835e-05, acc 1
2017-08-08T18:21:57.527541: step 22968, loss 4.99883e-06, acc 1
2017-08-08T18:21:57.813557: step 22969, loss 1.86264e-08, acc 1
2017-08-08T18:21:58.059202: step 22970, loss 1.38929e-05, acc 1
2017-08-08T18:21:58.316610: step 22971, loss 1.86265e-09, acc 1
2017-08-08T18:21:58.535557: step 22972, loss 1.33732e-06, acc 1
2017-08-08T18:21:58.739777: step 22973, loss 6.03486e-07, acc 1
2017-08-08T18:21:59.087356: step 22974, loss 8.38414e-05, acc 1
2017-08-08T18:21:59.309673: step 22975, loss 4.28408e-08, acc 1
2017-08-08T18:21:59.535748: step 22976, loss 5.77409e-07, acc 1
2017-08-08T18:21:59.793696: step 22977, loss 3.11017e-05, acc 1
2017-08-08T18:22:00.160162: step 22978, loss 0, acc 1
2017-08-08T18:22:00.505313: step 22979, loss 4.2613e-06, acc 1
2017-08-08T18:22:00.771952: step 22980, loss 5.19234e-06, acc 1
2017-08-08T18:22:00.994063: step 22981, loss 1.86264e-08, acc 1
2017-08-08T18:22:01.217405: step 22982, loss 0, acc 1
2017-08-08T18:22:01.619131: step 22983, loss 0, acc 1
2017-08-08T18:22:01.907589: step 22984, loss 8.5859e-05, acc 1
2017-08-08T18:22:02.182116: step 22985, loss 4.22815e-07, acc 1
2017-08-08T18:22:02.568983: step 22986, loss 1.49011e-07, acc 1
2017-08-08T18:22:02.976545: step 22987, loss 2.84969e-06, acc 1
2017-08-08T18:22:03.301682: step 22988, loss 3.8929e-07, acc 1
2017-08-08T18:22:03.618583: step 22989, loss 1.52916e-06, acc 1
2017-08-08T18:22:03.866543: step 22990, loss 2.17928e-07, acc 1
2017-08-08T18:22:04.118112: step 22991, loss 0, acc 1
2017-08-08T18:22:04.453443: step 22992, loss 1.34811e-05, acc 1
2017-08-08T18:22:04.696759: step 22993, loss 1.67638e-08, acc 1
2017-08-08T18:22:04.926100: step 22994, loss 0.000131962, acc 1
2017-08-08T18:22:05.194063: step 22995, loss 0, acc 1
2017-08-08T18:22:05.522345: step 22996, loss 2.04891e-08, acc 1
2017-08-08T18:22:05.881365: step 22997, loss 0, acc 1
2017-08-08T18:22:06.209166: step 22998, loss 7.8231e-08, acc 1
2017-08-08T18:22:06.487548: step 22999, loss 1.50874e-07, acc 1
2017-08-08T18:22:06.683769: step 23000, loss 0, acc 1

Evaluation:
2017-08-08T18:22:07.342097: step 23000, loss 7.75649, acc 0.727017

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-23000

2017-08-08T18:22:07.705569: step 23001, loss 5.40167e-08, acc 1
2017-08-08T18:22:07.991774: step 23002, loss 0, acc 1
2017-08-08T18:22:08.268206: step 23003, loss 0.00126631, acc 1
2017-08-08T18:22:08.742485: step 23004, loss 1.58131e-06, acc 1
2017-08-08T18:22:09.053370: step 23005, loss 5.79892e-05, acc 1
2017-08-08T18:22:09.265972: step 23006, loss 3.40727e-05, acc 1
2017-08-08T18:22:09.469568: step 23007, loss 1.44177e-05, acc 1
2017-08-08T18:22:09.700721: step 23008, loss 0.00018424, acc 1
2017-08-08T18:22:10.104263: step 23009, loss 0.000782916, acc 1
2017-08-08T18:22:10.414095: step 23010, loss 7.66922e-05, acc 1
2017-08-08T18:22:10.742730: step 23011, loss 1.19209e-07, acc 1
2017-08-08T18:22:10.979167: step 23012, loss 4.09782e-08, acc 1
2017-08-08T18:22:11.336598: step 23013, loss 1.86265e-09, acc 1
2017-08-08T18:22:11.643059: step 23014, loss 0, acc 1
2017-08-08T18:22:11.862506: step 23015, loss 3.25961e-07, acc 1
2017-08-08T18:22:12.040405: step 23016, loss 5.58793e-08, acc 1
2017-08-08T18:22:12.219863: step 23017, loss 0.0546165, acc 0.984375
2017-08-08T18:22:12.496372: step 23018, loss 1.86265e-09, acc 1
2017-08-08T18:22:12.641247: step 23019, loss 0, acc 1
2017-08-08T18:22:12.832754: step 23020, loss 6.73212e-06, acc 1
2017-08-08T18:22:13.053199: step 23021, loss 7.69495e-06, acc 1
2017-08-08T18:22:13.360592: step 23022, loss 0.000102535, acc 1
2017-08-08T18:22:13.679680: step 23023, loss 1.30385e-08, acc 1
2017-08-08T18:22:13.975150: step 23024, loss 1.15199e-05, acc 1
2017-08-08T18:22:14.202220: step 23025, loss 9.93033e-06, acc 1
2017-08-08T18:22:14.382704: step 23026, loss 5.58793e-09, acc 1
2017-08-08T18:22:14.678505: step 23027, loss 0.000168203, acc 1
2017-08-08T18:22:15.009414: step 23028, loss 2.29104e-07, acc 1
2017-08-08T18:22:15.241934: step 23029, loss 2.71945e-07, acc 1
2017-08-08T18:22:15.526609: step 23030, loss 5.96046e-08, acc 1
2017-08-08T18:22:15.945758: step 23031, loss 2.75089e-06, acc 1
2017-08-08T18:22:16.346946: step 23032, loss 4.11219e-06, acc 1
2017-08-08T18:22:16.633832: step 23033, loss 1.16515e-05, acc 1
2017-08-08T18:22:16.897687: step 23034, loss 0.000576977, acc 1
2017-08-08T18:22:17.148927: step 23035, loss 0.000844405, acc 1
2017-08-08T18:22:17.648255: step 23036, loss 7.97195e-06, acc 1
2017-08-08T18:22:17.890084: step 23037, loss 1.00583e-07, acc 1
2017-08-08T18:22:18.099418: step 23038, loss 4.40849e-06, acc 1
2017-08-08T18:22:18.329304: step 23039, loss 1.30385e-08, acc 1
2017-08-08T18:22:18.597613: step 23040, loss 6.63089e-07, acc 1
2017-08-08T18:22:19.013974: step 23041, loss 3.53902e-08, acc 1
2017-08-08T18:22:19.391713: step 23042, loss 9.96251e-06, acc 1
2017-08-08T18:22:19.677214: step 23043, loss 0.00126343, acc 1
2017-08-08T18:22:19.903061: step 23044, loss 3.53902e-08, acc 1
2017-08-08T18:22:20.272817: step 23045, loss 1.49012e-08, acc 1
2017-08-08T18:22:20.550125: step 23046, loss 1.11759e-08, acc 1
2017-08-08T18:22:20.753006: step 23047, loss 1.43885e-05, acc 1
2017-08-08T18:22:20.999804: step 23048, loss 1.24797e-07, acc 1
2017-08-08T18:22:21.277690: step 23049, loss 0, acc 1
2017-08-08T18:22:21.768470: step 23050, loss 0.13306, acc 0.984375
2017-08-08T18:22:22.114490: step 23051, loss 7.09653e-07, acc 1
2017-08-08T18:22:22.450915: step 23052, loss 1.21547e-05, acc 1
2017-08-08T18:22:22.714059: step 23053, loss 8.63941e-05, acc 1
2017-08-08T18:22:23.041506: step 23054, loss 2.0489e-07, acc 1
2017-08-08T18:22:23.499393: step 23055, loss 1.30385e-08, acc 1
2017-08-08T18:22:23.788976: step 23056, loss 0.0333515, acc 0.984375
2017-08-08T18:22:24.074463: step 23057, loss 9.31322e-09, acc 1
2017-08-08T18:22:24.381352: step 23058, loss 5.58794e-09, acc 1
2017-08-08T18:22:24.862004: step 23059, loss 0.000312401, acc 1
2017-08-08T18:22:25.261835: step 23060, loss 0, acc 1
2017-08-08T18:22:25.536854: step 23061, loss 1.22934e-07, acc 1
2017-08-08T18:22:25.737860: step 23062, loss 2.16424e-06, acc 1
2017-08-08T18:22:26.061871: step 23063, loss 2.08976e-06, acc 1
2017-08-08T18:22:26.486373: step 23064, loss 2.04891e-08, acc 1
2017-08-08T18:22:26.793465: step 23065, loss 5.58793e-09, acc 1
2017-08-08T18:22:27.157380: step 23066, loss 9.16395e-07, acc 1
2017-08-08T18:22:27.502878: step 23067, loss 1.29692e-05, acc 1
2017-08-08T18:22:27.773364: step 23068, loss 6.51925e-08, acc 1
2017-08-08T18:22:27.974480: step 23069, loss 1.67638e-08, acc 1
2017-08-08T18:22:28.171527: step 23070, loss 0, acc 1
2017-08-08T18:22:28.407972: step 23071, loss 1.01699e-06, acc 1
2017-08-08T18:22:28.815452: step 23072, loss 1.5432e-05, acc 1
2017-08-08T18:22:29.074646: step 23073, loss 0.000412732, acc 1
2017-08-08T18:22:29.363680: step 23074, loss 5.64371e-07, acc 1
2017-08-08T18:22:29.593682: step 23075, loss 0.0117842, acc 0.984375
2017-08-08T18:22:29.995323: step 23076, loss 2.12341e-07, acc 1
2017-08-08T18:22:30.324883: step 23077, loss 0.00159815, acc 1
2017-08-08T18:22:30.658521: step 23078, loss 1.84401e-07, acc 1
2017-08-08T18:22:30.849031: step 23079, loss 2.36554e-07, acc 1
2017-08-08T18:22:31.021379: step 23080, loss 1.6186e-06, acc 1
2017-08-08T18:22:31.318981: step 23081, loss 4.57773e-06, acc 1
2017-08-08T18:22:31.525971: step 23082, loss 1.36901e-06, acc 1
2017-08-08T18:22:31.740290: step 23083, loss 6.39503e-06, acc 1
2017-08-08T18:22:31.910335: step 23084, loss 9.83379e-05, acc 1
2017-08-08T18:22:32.193454: step 23085, loss 7.07789e-07, acc 1
2017-08-08T18:22:32.445802: step 23086, loss 1.30385e-07, acc 1
2017-08-08T18:22:32.733333: step 23087, loss 1.89607e-06, acc 1
2017-08-08T18:22:32.954933: step 23088, loss 2.66523e-06, acc 1
2017-08-08T18:22:33.172845: step 23089, loss 0.000236541, acc 1
2017-08-08T18:22:33.529478: step 23090, loss 2.18853e-06, acc 1
2017-08-08T18:22:33.737857: step 23091, loss 4.97319e-07, acc 1
2017-08-08T18:22:33.950038: step 23092, loss 3.53902e-08, acc 1
2017-08-08T18:22:34.140562: step 23093, loss 3.27822e-07, acc 1
2017-08-08T18:22:34.461477: step 23094, loss 1.31723e-05, acc 1
2017-08-08T18:22:34.892734: step 23095, loss 5.05247e-05, acc 1
2017-08-08T18:22:35.237394: step 23096, loss 8.49353e-07, acc 1
2017-08-08T18:22:35.444538: step 23097, loss 5.59752e-05, acc 1
2017-08-08T18:22:35.633578: step 23098, loss 3.68828e-05, acc 1
2017-08-08T18:22:35.961093: step 23099, loss 1.86265e-09, acc 1
2017-08-08T18:22:36.275404: step 23100, loss 0, acc 1

Evaluation:
2017-08-08T18:22:36.918113: step 23100, loss 7.95325, acc 0.723265

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-23100

2017-08-08T18:22:37.579244: step 23101, loss 5.2154e-08, acc 1
2017-08-08T18:22:37.861375: step 23102, loss 1.09895e-05, acc 1
2017-08-08T18:22:38.233173: step 23103, loss 1.49592e-05, acc 1
2017-08-08T18:22:38.456950: step 23104, loss 9.31322e-09, acc 1
2017-08-08T18:22:38.642106: step 23105, loss 3.9539e-06, acc 1
2017-08-08T18:22:38.977380: step 23106, loss 0.00027949, acc 1
2017-08-08T18:22:39.218778: step 23107, loss 5.64472e-06, acc 1
2017-08-08T18:22:39.464150: step 23108, loss 6.96616e-07, acc 1
2017-08-08T18:22:39.690886: step 23109, loss 0.000123349, acc 1
2017-08-08T18:22:40.028371: step 23110, loss 1.66701e-06, acc 1
2017-08-08T18:22:40.465645: step 23111, loss 0, acc 1
2017-08-08T18:22:40.883685: step 23112, loss 9.44307e-05, acc 1
2017-08-08T18:22:41.229885: step 23113, loss 1.22557e-06, acc 1
2017-08-08T18:22:41.494103: step 23114, loss 5.58794e-09, acc 1
2017-08-08T18:22:41.865113: step 23115, loss 7.6189e-06, acc 1
2017-08-08T18:22:42.227345: step 23116, loss 1.8067e-06, acc 1
2017-08-08T18:22:42.519727: step 23117, loss 0, acc 1
2017-08-08T18:22:42.799981: step 23118, loss 3.53902e-08, acc 1
2017-08-08T18:22:43.137336: step 23119, loss 1.11759e-08, acc 1
2017-08-08T18:22:43.429352: step 23120, loss 1.30385e-07, acc 1
2017-08-08T18:22:43.789369: step 23121, loss 2.36554e-07, acc 1
2017-08-08T18:22:44.058611: step 23122, loss 8.38188e-08, acc 1
2017-08-08T18:22:44.287757: step 23123, loss 0.000114288, acc 1
2017-08-08T18:22:44.632832: step 23124, loss 1.91841e-06, acc 1
2017-08-08T18:22:45.032412: step 23125, loss 0, acc 1
2017-08-08T18:22:45.316968: step 23126, loss 6.12799e-07, acc 1
2017-08-08T18:22:45.608706: step 23127, loss 1.52826e-05, acc 1
2017-08-08T18:22:45.879073: step 23128, loss 2.90546e-06, acc 1
2017-08-08T18:22:46.217575: step 23129, loss 1.39655e-05, acc 1
2017-08-08T18:22:46.652692: step 23130, loss 3.0619e-06, acc 1
2017-08-08T18:22:47.117401: step 23131, loss 4.66907e-06, acc 1
2017-08-08T18:22:47.467488: step 23132, loss 2.96158e-07, acc 1
2017-08-08T18:22:47.730328: step 23133, loss 8.19563e-08, acc 1
2017-08-08T18:22:48.050485: step 23134, loss 9.03358e-07, acc 1
2017-08-08T18:22:48.300777: step 23135, loss 1.1842e-05, acc 1
2017-08-08T18:22:48.559476: step 23136, loss 1.82538e-07, acc 1
2017-08-08T18:22:48.834158: step 23137, loss 5.58794e-09, acc 1
2017-08-08T18:22:49.177406: step 23138, loss 2.06752e-07, acc 1
2017-08-08T18:22:49.434100: step 23139, loss 6.51924e-08, acc 1
2017-08-08T18:22:49.796595: step 23140, loss 8.73557e-07, acc 1
2017-08-08T18:22:50.047425: step 23141, loss 5.2154e-08, acc 1
2017-08-08T18:22:50.293626: step 23142, loss 6.28708e-06, acc 1
2017-08-08T18:22:50.625871: step 23143, loss 0.0517506, acc 0.984375
2017-08-08T18:22:51.089860: step 23144, loss 2.8106e-06, acc 1
2017-08-08T18:22:51.319308: step 23145, loss 6.2351e-06, acc 1
2017-08-08T18:22:51.593576: step 23146, loss 0.000427921, acc 1
2017-08-08T18:22:51.789270: step 23147, loss 1.20208e-05, acc 1
2017-08-08T18:22:52.050185: step 23148, loss 3.91155e-08, acc 1
2017-08-08T18:22:52.269484: step 23149, loss 2.71944e-07, acc 1
2017-08-08T18:22:52.503769: step 23150, loss 0.0296299, acc 0.984375
2017-08-08T18:22:52.732351: step 23151, loss 5.2154e-08, acc 1
2017-08-08T18:22:52.914282: step 23152, loss 1.30385e-08, acc 1
2017-08-08T18:22:53.153941: step 23153, loss 2.14203e-07, acc 1
2017-08-08T18:22:53.408545: step 23154, loss 3.48834e-06, acc 1
2017-08-08T18:22:53.603461: step 23155, loss 2.1234e-07, acc 1
2017-08-08T18:22:53.802465: step 23156, loss 0.000251217, acc 1
2017-08-08T18:22:54.056261: step 23157, loss 3.06934e-06, acc 1
2017-08-08T18:22:54.529377: step 23158, loss 6.98483e-07, acc 1
2017-08-08T18:22:54.881361: step 23159, loss 1.86265e-09, acc 1
2017-08-08T18:22:55.173900: step 23160, loss 2.90386e-05, acc 1
2017-08-08T18:22:55.445377: step 23161, loss 0, acc 1
2017-08-08T18:22:55.770276: step 23162, loss 3.53902e-08, acc 1
2017-08-08T18:22:56.023983: step 23163, loss 3.07333e-07, acc 1
2017-08-08T18:22:56.265197: step 23164, loss 5.40166e-08, acc 1
2017-08-08T18:22:56.502082: step 23165, loss 4.13502e-07, acc 1
2017-08-08T18:22:56.847392: step 23166, loss 1.24979e-06, acc 1
2017-08-08T18:22:57.208997: step 23167, loss 5.02914e-08, acc 1
2017-08-08T18:22:57.487504: step 23168, loss 4.93597e-07, acc 1
2017-08-08T18:22:57.728672: step 23169, loss 1.56461e-07, acc 1
2017-08-08T18:22:57.996831: step 23170, loss 0.01844, acc 0.984375
2017-08-08T18:22:58.367632: step 23171, loss 2.04891e-08, acc 1
2017-08-08T18:22:58.544137: step 23172, loss 0.000108467, acc 1
2017-08-08T18:22:58.812847: step 23173, loss 1.67638e-08, acc 1
2017-08-08T18:22:59.114703: step 23174, loss 1.86265e-09, acc 1
2017-08-08T18:22:59.493239: step 23175, loss 0, acc 1
2017-08-08T18:22:59.817050: step 23176, loss 0.02304, acc 0.984375
2017-08-08T18:23:00.182929: step 23177, loss 2.92974e-06, acc 1
2017-08-08T18:23:00.397299: step 23178, loss 1.65775e-07, acc 1
2017-08-08T18:23:00.733361: step 23179, loss 2.12416e-05, acc 1
2017-08-08T18:23:01.024297: step 23180, loss 6.19579e-06, acc 1
2017-08-08T18:23:01.347716: step 23181, loss 0.000285368, acc 1
2017-08-08T18:23:01.699674: step 23182, loss 2.23517e-08, acc 1
2017-08-08T18:23:02.105423: step 23183, loss 5.58793e-08, acc 1
2017-08-08T18:23:02.453356: step 23184, loss 2.23517e-08, acc 1
2017-08-08T18:23:02.797154: step 23185, loss 4.17941e-05, acc 1
2017-08-08T18:23:03.034990: step 23186, loss 1.95577e-07, acc 1
2017-08-08T18:23:03.307720: step 23187, loss 6.00201e-05, acc 1
2017-08-08T18:23:03.710982: step 23188, loss 7.74841e-07, acc 1
2017-08-08T18:23:03.982681: step 23189, loss 0.00088095, acc 1
2017-08-08T18:23:04.273571: step 23190, loss 8.73374e-06, acc 1
2017-08-08T18:23:04.568860: step 23191, loss 8.00917e-07, acc 1
2017-08-08T18:23:04.969587: step 23192, loss 5.81905e-05, acc 1
2017-08-08T18:23:05.315596: step 23193, loss 5.56921e-07, acc 1
2017-08-08T18:23:05.673377: step 23194, loss 8.90031e-05, acc 1
2017-08-08T18:23:05.989878: step 23195, loss 0, acc 1
2017-08-08T18:23:06.213000: step 23196, loss 5.27133e-05, acc 1
2017-08-08T18:23:06.706153: step 23197, loss 3.02278e-06, acc 1
2017-08-08T18:23:07.004939: step 23198, loss 4.18045e-05, acc 1
2017-08-08T18:23:07.299366: step 23199, loss 9.31322e-09, acc 1
2017-08-08T18:23:07.547894: step 23200, loss 0, acc 1

Evaluation:
2017-08-08T18:23:08.291837: step 23200, loss 7.90331, acc 0.722326

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-23200

2017-08-08T18:23:08.704643: step 23201, loss 0.000147096, acc 1
2017-08-08T18:23:08.868904: step 23202, loss 5.2154e-08, acc 1
2017-08-08T18:23:09.035622: step 23203, loss 3.72529e-09, acc 1
2017-08-08T18:23:09.311107: step 23204, loss 1.43423e-07, acc 1
2017-08-08T18:23:09.554027: step 23205, loss 5.23397e-07, acc 1
2017-08-08T18:23:09.789612: step 23206, loss 5.2154e-08, acc 1
2017-08-08T18:23:10.073658: step 23207, loss 4.09782e-08, acc 1
2017-08-08T18:23:10.433317: step 23208, loss 1.67638e-08, acc 1
2017-08-08T18:23:10.883290: step 23209, loss 9.31295e-07, acc 1
2017-08-08T18:23:11.177273: step 23210, loss 5.58793e-09, acc 1
2017-08-08T18:23:11.454393: step 23211, loss 5.58793e-09, acc 1
2017-08-08T18:23:11.716923: step 23212, loss 1.06171e-07, acc 1
2017-08-08T18:23:12.070114: step 23213, loss 0.000127343, acc 1
2017-08-08T18:23:12.325065: step 23214, loss 1.30385e-08, acc 1
2017-08-08T18:23:12.533863: step 23215, loss 1.86265e-09, acc 1
2017-08-08T18:23:12.732544: step 23216, loss 1.65208e-06, acc 1
2017-08-08T18:23:13.026041: step 23217, loss 0.000908134, acc 1
2017-08-08T18:23:13.362193: step 23218, loss 5.58794e-09, acc 1
2017-08-08T18:23:13.730781: step 23219, loss 0, acc 1
2017-08-08T18:23:14.055884: step 23220, loss 3.35276e-08, acc 1
2017-08-08T18:23:14.317906: step 23221, loss 2.74911e-05, acc 1
2017-08-08T18:23:14.761585: step 23222, loss 3.2656e-05, acc 1
2017-08-08T18:23:15.016032: step 23223, loss 0.0226709, acc 0.984375
2017-08-08T18:23:15.277653: step 23224, loss 5.58793e-08, acc 1
2017-08-08T18:23:15.563920: step 23225, loss 0.00638173, acc 1
2017-08-08T18:23:16.013523: step 23226, loss 7.2643e-08, acc 1
2017-08-08T18:23:16.265338: step 23227, loss 1.49564e-06, acc 1
2017-08-08T18:23:16.601516: step 23228, loss 3.22544e-05, acc 1
2017-08-08T18:23:16.859077: step 23229, loss 0, acc 1
2017-08-08T18:23:17.132618: step 23230, loss 0, acc 1
2017-08-08T18:23:17.407542: step 23231, loss 9.31322e-09, acc 1
2017-08-08T18:23:17.681313: step 23232, loss 1.06355e-06, acc 1
2017-08-08T18:23:17.868458: step 23233, loss 6.89177e-08, acc 1
2017-08-08T18:23:18.076393: step 23234, loss 1.11759e-08, acc 1
2017-08-08T18:23:18.313313: step 23235, loss 9.31322e-09, acc 1
2017-08-08T18:23:18.702049: step 23236, loss 2.4028e-07, acc 1
2017-08-08T18:23:18.938264: step 23237, loss 6.27415e-06, acc 1
2017-08-08T18:23:19.147307: step 23238, loss 0.000291131, acc 1
2017-08-08T18:23:19.484232: step 23239, loss 0, acc 1
2017-08-08T18:23:19.900928: step 23240, loss 0.000215917, acc 1
2017-08-08T18:23:20.257353: step 23241, loss 1.11759e-08, acc 1
2017-08-08T18:23:20.605092: step 23242, loss 3.1665e-08, acc 1
2017-08-08T18:23:20.879667: step 23243, loss 1.86265e-09, acc 1
2017-08-08T18:23:21.244433: step 23244, loss 1.81043e-06, acc 1
2017-08-08T18:23:21.671148: step 23245, loss 4.27131e-05, acc 1
2017-08-08T18:23:21.945820: step 23246, loss 1.3634e-06, acc 1
2017-08-08T18:23:22.158138: step 23247, loss 0, acc 1
2017-08-08T18:23:22.359699: step 23248, loss 0.000236297, acc 1
2017-08-08T18:23:22.672199: step 23249, loss 3.72529e-09, acc 1
2017-08-08T18:23:22.874578: step 23250, loss 5.96046e-09, acc 1
2017-08-08T18:23:23.084546: step 23251, loss 3.72529e-09, acc 1
2017-08-08T18:23:23.300614: step 23252, loss 1.39698e-07, acc 1
2017-08-08T18:23:23.581219: step 23253, loss 1.86265e-09, acc 1
2017-08-08T18:23:23.913276: step 23254, loss 5.03679e-05, acc 1
2017-08-08T18:23:24.123069: step 23255, loss 0.0566898, acc 0.984375
2017-08-08T18:23:24.312312: step 23256, loss 4.95456e-07, acc 1
2017-08-08T18:23:24.648003: step 23257, loss 0.000237222, acc 1
2017-08-08T18:23:24.860813: step 23258, loss 3.02071e-05, acc 1
2017-08-08T18:23:25.070209: step 23259, loss 1.43423e-07, acc 1
2017-08-08T18:23:25.321622: step 23260, loss 1.21072e-07, acc 1
2017-08-08T18:23:25.774031: step 23261, loss 6.63399e-05, acc 1
2017-08-08T18:23:26.154963: step 23262, loss 3.72529e-09, acc 1
2017-08-08T18:23:26.434515: step 23263, loss 3.72529e-09, acc 1
2017-08-08T18:23:26.684518: step 23264, loss 1.09521e-06, acc 1
2017-08-08T18:23:26.915296: step 23265, loss 1.86265e-09, acc 1
2017-08-08T18:23:27.225366: step 23266, loss 1.68479e-05, acc 1
2017-08-08T18:23:27.499662: step 23267, loss 2.04891e-08, acc 1
2017-08-08T18:23:27.716170: step 23268, loss 1.86265e-09, acc 1
2017-08-08T18:23:27.934852: step 23269, loss 5.82774e-05, acc 1
2017-08-08T18:23:28.224664: step 23270, loss 0.0180886, acc 0.984375
2017-08-08T18:23:28.582770: step 23271, loss 2.6077e-08, acc 1
2017-08-08T18:23:28.860391: step 23272, loss 4.14209e-06, acc 1
2017-08-08T18:23:29.067284: step 23273, loss 5.4202e-07, acc 1
2017-08-08T18:23:29.304682: step 23274, loss 1.26659e-07, acc 1
2017-08-08T18:23:29.634684: step 23275, loss 1.76945e-06, acc 1
2017-08-08T18:23:29.821384: step 23276, loss 0.051876, acc 0.984375
2017-08-08T18:23:30.038035: step 23277, loss 0, acc 1
2017-08-08T18:23:30.267079: step 23278, loss 1.86265e-09, acc 1
2017-08-08T18:23:30.457144: step 23279, loss 1.53849e-06, acc 1
2017-08-08T18:23:30.754493: step 23280, loss 9.31322e-09, acc 1
2017-08-08T18:23:31.132678: step 23281, loss 2.20717e-05, acc 1
2017-08-08T18:23:31.459759: step 23282, loss 0.0227606, acc 0.984375
2017-08-08T18:23:31.699104: step 23283, loss 7.94034e-06, acc 1
2017-08-08T18:23:31.951939: step 23284, loss 7.3014e-07, acc 1
2017-08-08T18:23:32.300880: step 23285, loss 5.02914e-08, acc 1
2017-08-08T18:23:32.526140: step 23286, loss 2.31696e-06, acc 1
2017-08-08T18:23:32.820688: step 23287, loss 7.82309e-08, acc 1
2017-08-08T18:23:33.207963: step 23288, loss 4.3399e-07, acc 1
2017-08-08T18:23:33.643048: step 23289, loss 1.81349e-05, acc 1
2017-08-08T18:23:34.144110: step 23290, loss 4.25336e-05, acc 1
2017-08-08T18:23:34.385264: step 23291, loss 0, acc 1
2017-08-08T18:23:34.595019: step 23292, loss 3.67837e-06, acc 1
2017-08-08T18:23:34.958120: step 23293, loss 0, acc 1
2017-08-08T18:23:35.188345: step 23294, loss 2.03574e-06, acc 1
2017-08-08T18:23:35.510130: step 23295, loss 0, acc 1
2017-08-08T18:23:35.785357: step 23296, loss 0.0120574, acc 0.984375
2017-08-08T18:23:36.172407: step 23297, loss 3.99808e-05, acc 1
2017-08-08T18:23:36.463232: step 23298, loss 7.45058e-09, acc 1
2017-08-08T18:23:36.701846: step 23299, loss 5.01044e-07, acc 1
2017-08-08T18:23:36.887518: step 23300, loss 7.41316e-07, acc 1

Evaluation:
2017-08-08T18:23:37.534012: step 23300, loss 7.94359, acc 0.724203

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-23300

2017-08-08T18:23:38.006552: step 23301, loss 2.42144e-08, acc 1
2017-08-08T18:23:38.262391: step 23302, loss 0, acc 1
2017-08-08T18:23:38.648683: step 23303, loss 1.86265e-09, acc 1
2017-08-08T18:23:39.011854: step 23304, loss 7.83955e-05, acc 1
2017-08-08T18:23:39.327680: step 23305, loss 2.68219e-07, acc 1
2017-08-08T18:23:39.577015: step 23306, loss 1.86265e-09, acc 1
2017-08-08T18:23:39.925590: step 23307, loss 6.8172e-07, acc 1
2017-08-08T18:23:40.268309: step 23308, loss 0.00094305, acc 1
2017-08-08T18:23:40.507391: step 23309, loss 7.7653e-06, acc 1
2017-08-08T18:23:40.764266: step 23310, loss 3.72529e-09, acc 1
2017-08-08T18:23:41.138401: step 23311, loss 2.32829e-07, acc 1
2017-08-08T18:23:41.433003: step 23312, loss 2.12893e-06, acc 1
2017-08-08T18:23:41.752556: step 23313, loss 3.90261e-05, acc 1
2017-08-08T18:23:42.017253: step 23314, loss 2.79397e-08, acc 1
2017-08-08T18:23:42.284464: step 23315, loss 1.60187e-07, acc 1
2017-08-08T18:23:42.663825: step 23316, loss 0, acc 1
2017-08-08T18:23:42.865342: step 23317, loss 1.11347e-05, acc 1
2017-08-08T18:23:43.072551: step 23318, loss 8.64252e-07, acc 1
2017-08-08T18:23:43.390020: step 23319, loss 0.00064811, acc 1
2017-08-08T18:23:43.758486: step 23320, loss 1.60187e-07, acc 1
2017-08-08T18:23:43.998126: step 23321, loss 1.37835e-07, acc 1
2017-08-08T18:23:44.240618: step 23322, loss 0.000991678, acc 1
2017-08-08T18:23:44.413734: step 23323, loss 6.24668e-06, acc 1
2017-08-08T18:23:44.656560: step 23324, loss 2.75281e-06, acc 1
2017-08-08T18:23:44.969656: step 23325, loss 0.000364495, acc 1
2017-08-08T18:23:45.225519: step 23326, loss 1.19577e-06, acc 1
2017-08-08T18:23:45.500404: step 23327, loss 7.8231e-08, acc 1
2017-08-08T18:23:45.757377: step 23328, loss 1.86265e-09, acc 1
2017-08-08T18:23:46.058340: step 23329, loss 4.65661e-08, acc 1
2017-08-08T18:23:46.333714: step 23330, loss 0, acc 1
2017-08-08T18:23:46.555799: step 23331, loss 0, acc 1
2017-08-08T18:23:46.799482: step 23332, loss 1.31355e-05, acc 1
2017-08-08T18:23:47.209281: step 23333, loss 0.0130422, acc 0.984375
2017-08-08T18:23:47.531184: step 23334, loss 0, acc 1
2017-08-08T18:23:47.760445: step 23335, loss 2.13872e-05, acc 1
2017-08-08T18:23:47.983938: step 23336, loss 3.51607e-05, acc 1
2017-08-08T18:23:48.277332: step 23337, loss 9.61761e-06, acc 1
2017-08-08T18:23:48.622197: step 23338, loss 0, acc 1
2017-08-08T18:23:48.958158: step 23339, loss 7.45058e-09, acc 1
2017-08-08T18:23:49.197950: step 23340, loss 4.47034e-08, acc 1
2017-08-08T18:23:49.578564: step 23341, loss 0, acc 1
2017-08-08T18:23:49.937027: step 23342, loss 1.35973e-07, acc 1
2017-08-08T18:23:50.242101: step 23343, loss 9.75996e-07, acc 1
2017-08-08T18:23:50.633203: step 23344, loss 3.85422e-05, acc 1
2017-08-08T18:23:51.036361: step 23345, loss 1.86265e-09, acc 1
2017-08-08T18:23:51.538882: step 23346, loss 2.19492e-05, acc 1
2017-08-08T18:23:52.100487: step 23347, loss 1.15484e-07, acc 1
2017-08-08T18:23:52.513987: step 23348, loss 2.70433e-06, acc 1
2017-08-08T18:23:52.860864: step 23349, loss 3.7439e-07, acc 1
2017-08-08T18:23:53.125374: step 23350, loss 3.63212e-07, acc 1
2017-08-08T18:23:53.509615: step 23351, loss 0.0122907, acc 0.984375
2017-08-08T18:23:53.756441: step 23352, loss 1.30385e-08, acc 1
2017-08-08T18:23:54.090733: step 23353, loss 1.83798e-05, acc 1
2017-08-08T18:23:54.548121: step 23354, loss 8.19562e-08, acc 1
2017-08-08T18:23:55.025436: step 23355, loss 9.31321e-08, acc 1
2017-08-08T18:23:55.576614: step 23356, loss 2.44005e-07, acc 1
2017-08-08T18:23:56.016837: step 23357, loss 1.86265e-09, acc 1
2017-08-08T18:23:56.394341: step 23358, loss 5.75322e-05, acc 1
2017-08-08T18:23:56.780965: step 23359, loss 4.00671e-05, acc 1
2017-08-08T18:23:57.297713: step 23360, loss 3.82731e-06, acc 1
2017-08-08T18:23:57.665691: step 23361, loss 5.40166e-08, acc 1
2017-08-08T18:23:58.043925: step 23362, loss 3.16649e-08, acc 1
2017-08-08T18:23:58.429234: step 23363, loss 0.00227243, acc 1
2017-08-08T18:23:58.853486: step 23364, loss 4.22817e-07, acc 1
2017-08-08T18:23:59.233517: step 23365, loss 3.35276e-08, acc 1
2017-08-08T18:23:59.581452: step 23366, loss 0.0169756, acc 0.984375
2017-08-08T18:23:59.858370: step 23367, loss 1.14923e-06, acc 1
2017-08-08T18:24:00.193858: step 23368, loss 1.56462e-07, acc 1
2017-08-08T18:24:00.567053: step 23369, loss 0.0250719, acc 0.984375
2017-08-08T18:24:00.909862: step 23370, loss 5.75638e-06, acc 1
2017-08-08T18:24:01.274218: step 23371, loss 0, acc 1
2017-08-08T18:24:01.709994: step 23372, loss 0, acc 1
2017-08-08T18:24:02.225869: step 23373, loss 1.86265e-09, acc 1
2017-08-08T18:24:02.683897: step 23374, loss 2.2682e-05, acc 1
2017-08-08T18:24:03.176282: step 23375, loss 6.92564e-06, acc 1
2017-08-08T18:24:03.572047: step 23376, loss 1.40627e-06, acc 1
2017-08-08T18:24:03.929033: step 23377, loss 1.84975e-05, acc 1
2017-08-08T18:24:04.397120: step 23378, loss 3.72529e-09, acc 1
2017-08-08T18:24:04.707945: step 23379, loss 0, acc 1
2017-08-08T18:24:04.976038: step 23380, loss 2.6113e-06, acc 1
2017-08-08T18:24:05.310226: step 23381, loss 2.23517e-08, acc 1
2017-08-08T18:24:05.683593: step 23382, loss 0, acc 1
2017-08-08T18:24:06.170893: step 23383, loss 0, acc 1
2017-08-08T18:24:06.702603: step 23384, loss 0, acc 1
2017-08-08T18:24:07.106409: step 23385, loss 6.07211e-07, acc 1
2017-08-08T18:24:07.346225: step 23386, loss 2.04891e-08, acc 1
2017-08-08T18:24:07.655017: step 23387, loss 1.78247e-06, acc 1
2017-08-08T18:24:07.913258: step 23388, loss 3.37135e-07, acc 1
2017-08-08T18:24:08.164267: step 23389, loss 2.54976e-06, acc 1
2017-08-08T18:24:08.397331: step 23390, loss 1.86265e-09, acc 1
2017-08-08T18:24:08.750965: step 23391, loss 1.75088e-07, acc 1
2017-08-08T18:24:09.109703: step 23392, loss 6.43598e-06, acc 1
2017-08-08T18:24:09.426332: step 23393, loss 1.86265e-09, acc 1
2017-08-08T18:24:09.659405: step 23394, loss 0, acc 1
2017-08-08T18:24:09.847112: step 23395, loss 1.47148e-07, acc 1
2017-08-08T18:24:10.234230: step 23396, loss 1.44049e-05, acc 1
2017-08-08T18:24:10.494745: step 23397, loss 1.49751e-06, acc 1
2017-08-08T18:24:10.760858: step 23398, loss 5.58793e-09, acc 1
2017-08-08T18:24:11.042719: step 23399, loss 0.000718117, acc 1
2017-08-08T18:24:11.462707: step 23400, loss 0, acc 1

Evaluation:
2017-08-08T18:24:12.513368: step 23400, loss 8.08905, acc 0.713884

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-23400

2017-08-08T18:24:12.980746: step 23401, loss 2.19792e-07, acc 1
2017-08-08T18:24:13.213297: step 23402, loss 1.5571e-06, acc 1
2017-08-08T18:24:13.401742: step 23403, loss 3.7156e-06, acc 1
2017-08-08T18:24:13.584714: step 23404, loss 5.77313e-06, acc 1
2017-08-08T18:24:13.968931: step 23405, loss 0.00140509, acc 1
2017-08-08T18:24:14.343575: step 23406, loss 0.000978418, acc 1
2017-08-08T18:24:14.669838: step 23407, loss 0, acc 1
2017-08-08T18:24:14.974281: step 23408, loss 5.19672e-07, acc 1
2017-08-08T18:24:15.185198: step 23409, loss 5.96045e-08, acc 1
2017-08-08T18:24:15.530645: step 23410, loss 9.164e-07, acc 1
2017-08-08T18:24:15.801147: step 23411, loss 1.78431e-06, acc 1
2017-08-08T18:24:16.010434: step 23412, loss 3.45855e-06, acc 1
2017-08-08T18:24:16.236639: step 23413, loss 0.0305312, acc 0.984375
2017-08-08T18:24:16.461350: step 23414, loss 8.82899e-06, acc 1
2017-08-08T18:24:16.713343: step 23415, loss 8.80631e-05, acc 1
2017-08-08T18:24:17.061635: step 23416, loss 6.46327e-07, acc 1
2017-08-08T18:24:17.349338: step 23417, loss 7.51614e-05, acc 1
2017-08-08T18:24:17.556505: step 23418, loss 3.35631e-06, acc 1
2017-08-08T18:24:17.789450: step 23419, loss 2.94084e-06, acc 1
2017-08-08T18:24:18.044905: step 23420, loss 1.28522e-07, acc 1
2017-08-08T18:24:18.250379: step 23421, loss 2.42144e-08, acc 1
2017-08-08T18:24:18.478318: step 23422, loss 0, acc 1
2017-08-08T18:24:18.839313: step 23423, loss 3.16649e-08, acc 1
2017-08-08T18:24:19.121370: step 23424, loss 3.91155e-08, acc 1
2017-08-08T18:24:19.380988: step 23425, loss 5.12146e-06, acc 1
2017-08-08T18:24:19.556121: step 23426, loss 8.19562e-08, acc 1
2017-08-08T18:24:19.742412: step 23427, loss 0.000532524, acc 1
2017-08-08T18:24:20.081324: step 23428, loss 9.31322e-09, acc 1
2017-08-08T18:24:20.302985: step 23429, loss 0, acc 1
2017-08-08T18:24:20.680294: step 23430, loss 0, acc 1
2017-08-08T18:24:20.928952: step 23431, loss 1.86265e-09, acc 1
2017-08-08T18:24:21.245367: step 23432, loss 1.09021e-05, acc 1
2017-08-08T18:24:21.616961: step 23433, loss 6.38876e-07, acc 1
2017-08-08T18:24:21.830568: step 23434, loss 1.30385e-08, acc 1
2017-08-08T18:24:22.289371: step 23435, loss 0.000390147, acc 1
2017-08-08T18:24:22.564588: step 23436, loss 6.27699e-07, acc 1
2017-08-08T18:24:22.861413: step 23437, loss 2.42873e-06, acc 1
2017-08-08T18:24:23.138989: step 23438, loss 9.22742e-05, acc 1
2017-08-08T18:24:23.386584: step 23439, loss 0.000157414, acc 1
2017-08-08T18:24:23.839398: step 23440, loss 9.31322e-09, acc 1
2017-08-08T18:24:24.119825: step 23441, loss 0.0059578, acc 1
2017-08-08T18:24:24.469005: step 23442, loss 8.81025e-07, acc 1
2017-08-08T18:24:24.689535: step 23443, loss 3.01161e-06, acc 1
2017-08-08T18:24:25.015720: step 23444, loss 7.37838e-05, acc 1
2017-08-08T18:24:25.353031: step 23445, loss 1.86265e-09, acc 1
2017-08-08T18:24:25.629362: step 23446, loss 2.1234e-07, acc 1
2017-08-08T18:24:26.012670: step 23447, loss 0.000137154, acc 1
2017-08-08T18:24:26.280420: step 23448, loss 7.17205e-05, acc 1
2017-08-08T18:24:26.633387: step 23449, loss 0.00239123, acc 1
2017-08-08T18:24:26.842604: step 23450, loss 3.75652e-06, acc 1
2017-08-08T18:24:27.101594: step 23451, loss 8.75441e-08, acc 1
2017-08-08T18:24:27.531902: step 23452, loss 2.91717e-05, acc 1
2017-08-08T18:24:27.749956: step 23453, loss 1.02445e-07, acc 1
2017-08-08T18:24:28.021183: step 23454, loss 1.03808e-05, acc 1
2017-08-08T18:24:28.326231: step 23455, loss 1.49012e-08, acc 1
2017-08-08T18:24:28.620242: step 23456, loss 0, acc 1
2017-08-08T18:24:28.958500: step 23457, loss 3.72529e-09, acc 1
2017-08-08T18:24:29.147422: step 23458, loss 0.00498072, acc 1
2017-08-08T18:24:29.454947: step 23459, loss 3.72529e-09, acc 1
2017-08-08T18:24:29.738437: step 23460, loss 0.000118129, acc 1
2017-08-08T18:24:30.013833: step 23461, loss 7.45058e-09, acc 1
2017-08-08T18:24:30.424748: step 23462, loss 1.65775e-07, acc 1
2017-08-08T18:24:30.666077: step 23463, loss 1.49012e-08, acc 1
2017-08-08T18:24:30.913181: step 23464, loss 0.0011043, acc 1
2017-08-08T18:24:31.266648: step 23465, loss 1.30385e-08, acc 1
2017-08-08T18:24:31.457388: step 23466, loss 2.8312e-07, acc 1
2017-08-08T18:24:31.672273: step 23467, loss 0, acc 1
2017-08-08T18:24:31.904830: step 23468, loss 5.58793e-08, acc 1
2017-08-08T18:24:32.261551: step 23469, loss 0, acc 1
2017-08-08T18:24:32.527004: step 23470, loss 4.79176e-05, acc 1
2017-08-08T18:24:32.793490: step 23471, loss 0, acc 1
2017-08-08T18:24:32.977982: step 23472, loss 1.49011e-07, acc 1
2017-08-08T18:24:33.231383: step 23473, loss 3.72529e-08, acc 1
2017-08-08T18:24:33.566971: step 23474, loss 1.86264e-08, acc 1
2017-08-08T18:24:33.754743: step 23475, loss 3.52021e-06, acc 1
2017-08-08T18:24:33.969286: step 23476, loss 1.26843e-06, acc 1
2017-08-08T18:24:34.205393: step 23477, loss 0, acc 1
2017-08-08T18:24:34.564175: step 23478, loss 1.90911e-06, acc 1
2017-08-08T18:24:34.891545: step 23479, loss 1.86265e-09, acc 1
2017-08-08T18:24:35.267486: step 23480, loss 5.29973e-05, acc 1
2017-08-08T18:24:35.502197: step 23481, loss 8.38188e-08, acc 1
2017-08-08T18:24:35.693753: step 23482, loss 0, acc 1
2017-08-08T18:24:35.995472: step 23483, loss 6.95027e-06, acc 1
2017-08-08T18:24:36.175329: step 23484, loss 1.80109e-06, acc 1
2017-08-08T18:24:36.348355: step 23485, loss 1.86265e-09, acc 1
2017-08-08T18:24:36.531205: step 23486, loss 3.72529e-09, acc 1
2017-08-08T18:24:36.722277: step 23487, loss 2.6077e-08, acc 1
2017-08-08T18:24:36.997198: step 23488, loss 0, acc 1
2017-08-08T18:24:37.323838: step 23489, loss 1.63912e-07, acc 1
2017-08-08T18:24:37.662662: step 23490, loss 7.45057e-08, acc 1
2017-08-08T18:24:37.868696: step 23491, loss 2.17928e-07, acc 1
2017-08-08T18:24:38.108784: step 23492, loss 1.35036e-06, acc 1
2017-08-08T18:24:38.428744: step 23493, loss 9.23917e-06, acc 1
2017-08-08T18:24:38.658027: step 23494, loss 3.50994e-05, acc 1
2017-08-08T18:24:38.863839: step 23495, loss 2.58515e-06, acc 1
2017-08-08T18:24:39.189865: step 23496, loss 0.0179732, acc 0.984375
2017-08-08T18:24:39.449867: step 23497, loss 0, acc 1
2017-08-08T18:24:39.713447: step 23498, loss 6.9289e-07, acc 1
2017-08-08T18:24:39.969371: step 23499, loss 9.12694e-08, acc 1
2017-08-08T18:24:40.193698: step 23500, loss 0.000153824, acc 1

Evaluation:
2017-08-08T18:24:40.869247: step 23500, loss 7.97248, acc 0.716698

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-23500

2017-08-08T18:24:41.224096: step 23501, loss 0.000137673, acc 1
2017-08-08T18:24:41.549409: step 23502, loss 8.84754e-05, acc 1
2017-08-08T18:24:41.850517: step 23503, loss 1.32429e-06, acc 1
2017-08-08T18:24:42.181838: step 23504, loss 0.000381959, acc 1
2017-08-08T18:24:42.450903: step 23505, loss 1.09792e-05, acc 1
2017-08-08T18:24:42.660791: step 23506, loss 3.72529e-09, acc 1
2017-08-08T18:24:43.044433: step 23507, loss 1.12072e-05, acc 1
2017-08-08T18:24:43.322168: step 23508, loss 1.86264e-08, acc 1
2017-08-08T18:24:43.528254: step 23509, loss 2.36538e-06, acc 1
2017-08-08T18:24:43.894432: step 23510, loss 0, acc 1
2017-08-08T18:24:44.233537: step 23511, loss 5.02913e-08, acc 1
2017-08-08T18:24:44.493304: step 23512, loss 1.36212e-05, acc 1
2017-08-08T18:24:44.713875: step 23513, loss 2.18287e-06, acc 1
2017-08-08T18:24:45.040530: step 23514, loss 8.56814e-08, acc 1
2017-08-08T18:24:45.409155: step 23515, loss 1.11759e-08, acc 1
2017-08-08T18:24:45.667536: step 23516, loss 1.86265e-09, acc 1
2017-08-08T18:24:45.918513: step 23517, loss 0, acc 1
2017-08-08T18:24:46.181571: step 23518, loss 0, acc 1
2017-08-08T18:24:46.589200: step 23519, loss 7.86016e-07, acc 1
2017-08-08T18:24:46.964789: step 23520, loss 0, acc 1
2017-08-08T18:24:47.277450: step 23521, loss 1.91851e-07, acc 1
2017-08-08T18:24:47.591540: step 23522, loss 3.15702e-06, acc 1
2017-08-08T18:24:47.844934: step 23523, loss 6.65603e-06, acc 1
2017-08-08T18:24:48.203028: step 23524, loss 1.14765e-05, acc 1
2017-08-08T18:24:48.503500: step 23525, loss 2.80305e-06, acc 1
2017-08-08T18:24:48.762406: step 23526, loss 2.4474e-06, acc 1
2017-08-08T18:24:49.044263: step 23527, loss 1.30385e-08, acc 1
2017-08-08T18:24:49.299233: step 23528, loss 8.81007e-07, acc 1
2017-08-08T18:24:49.680317: step 23529, loss 1.86265e-09, acc 1
2017-08-08T18:24:50.157478: step 23530, loss 1.99743e-05, acc 1
2017-08-08T18:24:50.552466: step 23531, loss 6.7799e-07, acc 1
2017-08-08T18:24:50.839729: step 23532, loss 0, acc 1
2017-08-08T18:24:51.077755: step 23533, loss 4.43305e-07, acc 1
2017-08-08T18:24:51.485372: step 23534, loss 1.12315e-06, acc 1
2017-08-08T18:24:51.710866: step 23535, loss 1.695e-07, acc 1
2017-08-08T18:24:51.886957: step 23536, loss 1.77128e-06, acc 1
2017-08-08T18:24:52.060675: step 23537, loss 0, acc 1
2017-08-08T18:24:52.384667: step 23538, loss 0, acc 1
2017-08-08T18:24:52.624471: step 23539, loss 0.00613972, acc 1
2017-08-08T18:24:52.843363: step 23540, loss 2.29104e-07, acc 1
2017-08-08T18:24:53.199088: step 23541, loss 4.47034e-08, acc 1
2017-08-08T18:24:53.533300: step 23542, loss 0, acc 1
2017-08-08T18:24:53.800490: step 23543, loss 1.90165e-06, acc 1
2017-08-08T18:24:54.063647: step 23544, loss 0, acc 1
2017-08-08T18:24:54.388923: step 23545, loss 3.72529e-09, acc 1
2017-08-08T18:24:54.658092: step 23546, loss 0, acc 1
2017-08-08T18:24:54.958189: step 23547, loss 2.65857e-05, acc 1
2017-08-08T18:24:55.201424: step 23548, loss 2.68393e-06, acc 1
2017-08-08T18:24:55.413386: step 23549, loss 0.000184504, acc 1
2017-08-08T18:24:55.673623: step 23550, loss 0.000153912, acc 1
2017-08-08T18:24:55.993351: step 23551, loss 2.23517e-08, acc 1
2017-08-08T18:24:56.229632: step 23552, loss 0, acc 1
2017-08-08T18:24:56.509945: step 23553, loss 8.00936e-08, acc 1
2017-08-08T18:24:56.849765: step 23554, loss 1.39698e-07, acc 1
2017-08-08T18:24:57.163170: step 23555, loss 7.04071e-07, acc 1
2017-08-08T18:24:57.391308: step 23556, loss 1.78813e-07, acc 1
2017-08-08T18:24:57.565973: step 23557, loss 1.32247e-07, acc 1
2017-08-08T18:24:57.721455: step 23558, loss 1.71362e-07, acc 1
2017-08-08T18:24:57.988146: step 23559, loss 3.27822e-07, acc 1
2017-08-08T18:24:58.163511: step 23560, loss 1.86265e-09, acc 1
2017-08-08T18:24:58.338943: step 23561, loss 0, acc 1
2017-08-08T18:24:58.508816: step 23562, loss 1.05353e-05, acc 1
2017-08-08T18:24:58.880532: step 23563, loss 1.95012e-06, acc 1
2017-08-08T18:24:59.141372: step 23564, loss 3.18134e-05, acc 1
2017-08-08T18:24:59.393375: step 23565, loss 7.07804e-08, acc 1
2017-08-08T18:24:59.599682: step 23566, loss 1.86265e-09, acc 1
2017-08-08T18:24:59.809374: step 23567, loss 0.000106716, acc 1
2017-08-08T18:25:00.086189: step 23568, loss 4.39373e-06, acc 1
2017-08-08T18:25:00.271843: step 23569, loss 0, acc 1
2017-08-08T18:25:00.472505: step 23570, loss 1.86264e-08, acc 1
2017-08-08T18:25:00.742680: step 23571, loss 3.7625e-07, acc 1
2017-08-08T18:25:00.978132: step 23572, loss 8.36305e-07, acc 1
2017-08-08T18:25:01.437191: step 23573, loss 6.34586e-05, acc 1
2017-08-08T18:25:01.853312: step 23574, loss 2.83658e-06, acc 1
2017-08-08T18:25:02.223370: step 23575, loss 4.11641e-07, acc 1
2017-08-08T18:25:02.510744: step 23576, loss 2.00609e-05, acc 1
2017-08-08T18:25:02.925889: step 23577, loss 3.56858e-06, acc 1
2017-08-08T18:25:03.312513: step 23578, loss 3.72529e-09, acc 1
2017-08-08T18:25:03.554257: step 23579, loss 4.98497e-05, acc 1
2017-08-08T18:25:03.871493: step 23580, loss 1.12259e-05, acc 1
2017-08-08T18:25:04.247794: step 23581, loss 4.13502e-07, acc 1
2017-08-08T18:25:04.730027: step 23582, loss 0, acc 1
2017-08-08T18:25:05.162780: step 23583, loss 1.56461e-07, acc 1
2017-08-08T18:25:05.512756: step 23584, loss 0.000301856, acc 1
2017-08-08T18:25:05.746712: step 23585, loss 1.59434e-06, acc 1
2017-08-08T18:25:06.180072: step 23586, loss 0.00016798, acc 1
2017-08-08T18:25:06.432467: step 23587, loss 2.68205e-06, acc 1
2017-08-08T18:25:06.698963: step 23588, loss 0.00258906, acc 1
2017-08-08T18:25:07.046950: step 23589, loss 1.0803e-06, acc 1
2017-08-08T18:25:07.423357: step 23590, loss 0, acc 1
2017-08-08T18:25:07.758537: step 23591, loss 1.11758e-07, acc 1
2017-08-08T18:25:07.993591: step 23592, loss 0.00011102, acc 1
2017-08-08T18:25:08.331346: step 23593, loss 0.000420295, acc 1
2017-08-08T18:25:08.640192: step 23594, loss 4.92025e-05, acc 1
2017-08-08T18:25:08.884132: step 23595, loss 4.71365e-06, acc 1
2017-08-08T18:25:09.155973: step 23596, loss 3.72529e-09, acc 1
2017-08-08T18:25:09.437047: step 23597, loss 3.12921e-07, acc 1
2017-08-08T18:25:09.795673: step 23598, loss 2.61357e-05, acc 1
2017-08-08T18:25:10.258136: step 23599, loss 3.03592e-06, acc 1
2017-08-08T18:25:10.527164: step 23600, loss 2.53868e-06, acc 1

Evaluation:
2017-08-08T18:25:10.921903: step 23600, loss 7.96626, acc 0.71576

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-23600

2017-08-08T18:25:11.539724: step 23601, loss 1.21072e-07, acc 1
2017-08-08T18:25:11.817956: step 23602, loss 3.91155e-08, acc 1
2017-08-08T18:25:12.081779: step 23603, loss 9.25091e-06, acc 1
2017-08-08T18:25:12.339918: step 23604, loss 4.93597e-07, acc 1
2017-08-08T18:25:12.593419: step 23605, loss 1.40009e-05, acc 1
2017-08-08T18:25:12.911588: step 23606, loss 2.6077e-08, acc 1
2017-08-08T18:25:13.169477: step 23607, loss 9.87199e-08, acc 1
2017-08-08T18:25:13.495244: step 23608, loss 0, acc 1
2017-08-08T18:25:13.760888: step 23609, loss 3.10065e-05, acc 1
2017-08-08T18:25:14.143060: step 23610, loss 2.03395e-06, acc 1
2017-08-08T18:25:14.498244: step 23611, loss 2.04891e-08, acc 1
2017-08-08T18:25:14.820061: step 23612, loss 3.59673e-05, acc 1
2017-08-08T18:25:15.152228: step 23613, loss 0.000114985, acc 1
2017-08-08T18:25:15.582955: step 23614, loss 4.7304e-06, acc 1
2017-08-08T18:25:16.101711: step 23615, loss 0.000877977, acc 1
2017-08-08T18:25:16.576598: step 23616, loss 1.35973e-07, acc 1
2017-08-08T18:25:16.877618: step 23617, loss 7.47106e-05, acc 1
2017-08-08T18:25:17.127248: step 23618, loss 2.30836e-05, acc 1
2017-08-08T18:25:17.431691: step 23619, loss 5.59077e-06, acc 1
2017-08-08T18:25:17.847899: step 23620, loss 1.86265e-09, acc 1
2017-08-08T18:25:18.141773: step 23621, loss 3.88873e-06, acc 1
2017-08-08T18:25:18.448149: step 23622, loss 0, acc 1
2017-08-08T18:25:18.708796: step 23623, loss 0.0229364, acc 0.984375
2017-08-08T18:25:19.053390: step 23624, loss 1.93714e-07, acc 1
2017-08-08T18:25:19.435367: step 23625, loss 0.00116086, acc 1
2017-08-08T18:25:19.893974: step 23626, loss 0, acc 1
2017-08-08T18:25:20.186893: step 23627, loss 9.49946e-08, acc 1
2017-08-08T18:25:20.413692: step 23628, loss 2.7007e-06, acc 1
2017-08-08T18:25:20.679853: step 23629, loss 3.25929e-06, acc 1
2017-08-08T18:25:21.176782: step 23630, loss 0.00023508, acc 1
2017-08-08T18:25:21.443950: step 23631, loss 0.00107598, acc 1
2017-08-08T18:25:21.645321: step 23632, loss 0.00687133, acc 1
2017-08-08T18:25:21.882847: step 23633, loss 6.10936e-07, acc 1
2017-08-08T18:25:22.268130: step 23634, loss 0.0652078, acc 0.984375
2017-08-08T18:25:22.623294: step 23635, loss 1.01816e-05, acc 1
2017-08-08T18:25:22.982557: step 23636, loss 2.28082e-05, acc 1
2017-08-08T18:25:23.159997: step 23637, loss 0, acc 1
2017-08-08T18:25:23.353276: step 23638, loss 0.000177255, acc 1
2017-08-08T18:25:23.732294: step 23639, loss 0.000228672, acc 1
2017-08-08T18:25:23.936282: step 23640, loss 2.95772e-06, acc 1
2017-08-08T18:25:24.220393: step 23641, loss 0, acc 1
2017-08-08T18:25:24.490321: step 23642, loss 0.000197967, acc 1
2017-08-08T18:25:24.740957: step 23643, loss 0.000131159, acc 1
2017-08-08T18:25:25.171414: step 23644, loss 4.07706e-06, acc 1
2017-08-08T18:25:25.604703: step 23645, loss 0, acc 1
2017-08-08T18:25:25.964162: step 23646, loss 1.86265e-09, acc 1
2017-08-08T18:25:26.207487: step 23647, loss 8.88468e-07, acc 1
2017-08-08T18:25:26.479704: step 23648, loss 0, acc 1
2017-08-08T18:25:26.882274: step 23649, loss 0, acc 1
2017-08-08T18:25:27.170554: step 23650, loss 2.4883e-06, acc 1
2017-08-08T18:25:27.454378: step 23651, loss 2.42144e-08, acc 1
2017-08-08T18:25:27.736989: step 23652, loss 3.72529e-09, acc 1
2017-08-08T18:25:28.127364: step 23653, loss 1.86264e-08, acc 1
2017-08-08T18:25:28.539181: step 23654, loss 2.14203e-07, acc 1
2017-08-08T18:25:28.941711: step 23655, loss 1.21442e-06, acc 1
2017-08-08T18:25:29.210514: step 23656, loss 9.12488e-06, acc 1
2017-08-08T18:25:29.406630: step 23657, loss 9.31322e-09, acc 1
2017-08-08T18:25:29.690850: step 23658, loss 1.65953e-06, acc 1
2017-08-08T18:25:29.886120: step 23659, loss 6.54955e-06, acc 1
2017-08-08T18:25:30.092064: step 23660, loss 6.4032e-06, acc 1
2017-08-08T18:25:30.298833: step 23661, loss 9.75538e-06, acc 1
2017-08-08T18:25:30.565341: step 23662, loss 1.56641e-06, acc 1
2017-08-08T18:25:30.936678: step 23663, loss 5.07497e-06, acc 1
2017-08-08T18:25:31.232958: step 23664, loss 6.16973e-06, acc 1
2017-08-08T18:25:31.494592: step 23665, loss 8.9219e-07, acc 1
2017-08-08T18:25:31.729342: step 23666, loss 2.31621e-05, acc 1
2017-08-08T18:25:32.096846: step 23667, loss 5.58793e-09, acc 1
2017-08-08T18:25:32.414914: step 23668, loss 0.000710151, acc 1
2017-08-08T18:25:32.704691: step 23669, loss 1.30385e-08, acc 1
2017-08-08T18:25:33.002005: step 23670, loss 1.08589e-06, acc 1
2017-08-08T18:25:33.336385: step 23671, loss 3.93156e-06, acc 1
2017-08-08T18:25:33.757344: step 23672, loss 1.49166e-05, acc 1
2017-08-08T18:25:34.083311: step 23673, loss 2.11362e-05, acc 1
2017-08-08T18:25:34.365418: step 23674, loss 0, acc 1
2017-08-08T18:25:34.572958: step 23675, loss 1.47148e-07, acc 1
2017-08-08T18:25:34.772153: step 23676, loss 0.000283975, acc 1
2017-08-08T18:25:35.074701: step 23677, loss 3.05817e-06, acc 1
2017-08-08T18:25:35.299679: step 23678, loss 3.72529e-09, acc 1
2017-08-08T18:25:35.618283: step 23679, loss 7.63683e-08, acc 1
2017-08-08T18:25:36.007884: step 23680, loss 2.81257e-07, acc 1
2017-08-08T18:25:36.349450: step 23681, loss 2.17731e-06, acc 1
2017-08-08T18:25:36.623807: step 23682, loss 1.46958e-06, acc 1
2017-08-08T18:25:36.893025: step 23683, loss 4.19091e-07, acc 1
2017-08-08T18:25:37.147506: step 23684, loss 8.23267e-07, acc 1
2017-08-08T18:25:37.404778: step 23685, loss 0.00292362, acc 1
2017-08-08T18:25:37.725532: step 23686, loss 0.000167652, acc 1
2017-08-08T18:25:38.108197: step 23687, loss 0, acc 1
2017-08-08T18:25:38.360569: step 23688, loss 0, acc 1
2017-08-08T18:25:38.545058: step 23689, loss 6.03491e-07, acc 1
2017-08-08T18:25:38.985285: step 23690, loss 0.0489754, acc 0.984375
2017-08-08T18:25:39.415385: step 23691, loss 1.09896e-07, acc 1
2017-08-08T18:25:39.720847: step 23692, loss 3.16649e-08, acc 1
2017-08-08T18:25:39.988848: step 23693, loss 3.11059e-07, acc 1
2017-08-08T18:25:40.270096: step 23694, loss 0.00020633, acc 1
2017-08-08T18:25:40.556462: step 23695, loss 2.74902e-06, acc 1
2017-08-08T18:25:40.751629: step 23696, loss 5.9976e-07, acc 1
2017-08-08T18:25:40.974108: step 23697, loss 4.1909e-07, acc 1
2017-08-08T18:25:41.197352: step 23698, loss 2.60935e-06, acc 1
2017-08-08T18:25:41.541386: step 23699, loss 2.04891e-08, acc 1
2017-08-08T18:25:41.952953: step 23700, loss 1.56958e-07, acc 1

Evaluation:
2017-08-08T18:25:42.812173: step 23700, loss 8.111, acc 0.724203

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-23700

2017-08-08T18:25:43.409544: step 23701, loss 0, acc 1
2017-08-08T18:25:43.644780: step 23702, loss 3.74389e-07, acc 1
2017-08-08T18:25:43.893141: step 23703, loss 4.39401e-05, acc 1
2017-08-08T18:25:44.151343: step 23704, loss 5.58794e-09, acc 1
2017-08-08T18:25:44.437324: step 23705, loss 9.84474e-06, acc 1
2017-08-08T18:25:44.748539: step 23706, loss 3.16069e-05, acc 1
2017-08-08T18:25:44.993750: step 23707, loss 6.65723e-05, acc 1
2017-08-08T18:25:45.293528: step 23708, loss 0.00017352, acc 1
2017-08-08T18:25:45.575452: step 23709, loss 1.22744e-06, acc 1
2017-08-08T18:25:45.798190: step 23710, loss 1.14996e-05, acc 1
2017-08-08T18:25:46.136477: step 23711, loss 3.16649e-08, acc 1
2017-08-08T18:25:46.469677: step 23712, loss 0, acc 1
2017-08-08T18:25:46.755647: step 23713, loss 2.45867e-07, acc 1
2017-08-08T18:25:47.019520: step 23714, loss 1.86265e-09, acc 1
2017-08-08T18:25:47.485354: step 23715, loss 3.61686e-06, acc 1
2017-08-08T18:25:47.961899: step 23716, loss 5.96045e-08, acc 1
2017-08-08T18:25:48.183598: step 23717, loss 7.45058e-09, acc 1
2017-08-08T18:25:48.427918: step 23718, loss 9.97988e-06, acc 1
2017-08-08T18:25:48.864005: step 23719, loss 8.85264e-05, acc 1
2017-08-08T18:25:49.171321: step 23720, loss 0.000256944, acc 1
2017-08-08T18:25:49.437106: step 23721, loss 1.86265e-09, acc 1
2017-08-08T18:25:49.772388: step 23722, loss 0.000997474, acc 1
2017-08-08T18:25:50.065218: step 23723, loss 3.35276e-08, acc 1
2017-08-08T18:25:50.357768: step 23724, loss 2.29104e-07, acc 1
2017-08-08T18:25:50.772421: step 23725, loss 0, acc 1
2017-08-08T18:25:51.043640: step 23726, loss 4.62432e-06, acc 1
2017-08-08T18:25:51.368173: step 23727, loss 9.22866e-06, acc 1
2017-08-08T18:25:51.596211: step 23728, loss 1.60871e-05, acc 1
2017-08-08T18:25:51.851147: step 23729, loss 8.86233e-06, acc 1
2017-08-08T18:25:52.196749: step 23730, loss 3.61349e-07, acc 1
2017-08-08T18:25:52.555100: step 23731, loss 0.000309364, acc 1
2017-08-08T18:25:52.858393: step 23732, loss 2.47034e-05, acc 1
2017-08-08T18:25:53.118239: step 23733, loss 5.41259e-05, acc 1
2017-08-08T18:25:53.374426: step 23734, loss 7.17102e-07, acc 1
2017-08-08T18:25:53.756010: step 23735, loss 3.72529e-09, acc 1
2017-08-08T18:25:54.027075: step 23736, loss 2.98023e-08, acc 1
2017-08-08T18:25:54.348194: step 23737, loss 2.08416e-06, acc 1
2017-08-08T18:25:54.801386: step 23738, loss 3.96739e-07, acc 1
2017-08-08T18:25:55.168034: step 23739, loss 9.69951e-06, acc 1
2017-08-08T18:25:55.636647: step 23740, loss 1.59361e-05, acc 1
2017-08-08T18:25:55.896425: step 23741, loss 0.00048718, acc 1
2017-08-08T18:25:56.126529: step 23742, loss 0.000320141, acc 1
2017-08-08T18:25:56.502789: step 23743, loss 0.000668806, acc 1
2017-08-08T18:25:56.846690: step 23744, loss 9.10208e-06, acc 1
2017-08-08T18:25:57.148182: step 23745, loss 2.04891e-08, acc 1
2017-08-08T18:25:57.385355: step 23746, loss 0.000823243, acc 1
2017-08-08T18:25:57.752531: step 23747, loss 7.82309e-08, acc 1
2017-08-08T18:25:58.105364: step 23748, loss 4.28408e-08, acc 1
2017-08-08T18:25:58.405362: step 23749, loss 5.69334e-06, acc 1
2017-08-08T18:25:58.642539: step 23750, loss 2.04891e-08, acc 1
2017-08-08T18:25:58.839530: step 23751, loss 0, acc 1
2017-08-08T18:25:59.171778: step 23752, loss 6.38935e-05, acc 1
2017-08-08T18:25:59.386535: step 23753, loss 8.51457e-05, acc 1
2017-08-08T18:25:59.798361: step 23754, loss 5.30594e-06, acc 1
2017-08-08T18:26:00.111935: step 23755, loss 3.35276e-08, acc 1
2017-08-08T18:26:00.355219: step 23756, loss 1.01165e-05, acc 1
2017-08-08T18:26:00.730514: step 23757, loss 1.07383e-05, acc 1
2017-08-08T18:26:01.092025: step 23758, loss 5.00973e-06, acc 1
2017-08-08T18:26:01.391613: step 23759, loss 8.23268e-07, acc 1
2017-08-08T18:26:01.753232: step 23760, loss 6.48084e-06, acc 1
2017-08-08T18:26:02.184461: step 23761, loss 0, acc 1
2017-08-08T18:26:02.547091: step 23762, loss 0.000923012, acc 1
2017-08-08T18:26:02.946305: step 23763, loss 0.000448366, acc 1
2017-08-08T18:26:03.234137: step 23764, loss 8.53069e-07, acc 1
2017-08-08T18:26:03.491897: step 23765, loss 0.000749133, acc 1
2017-08-08T18:26:03.838474: step 23766, loss 1.86264e-08, acc 1
2017-08-08T18:26:04.248081: step 23767, loss 1.0768e-05, acc 1
2017-08-08T18:26:04.533035: step 23768, loss 1.36659e-05, acc 1
2017-08-08T18:26:04.823900: step 23769, loss 5.32652e-06, acc 1
2017-08-08T18:26:05.198632: step 23770, loss 1.79915e-05, acc 1
2017-08-08T18:26:05.575263: step 23771, loss 1.18646e-06, acc 1
2017-08-08T18:26:05.962831: step 23772, loss 7.87888e-07, acc 1
2017-08-08T18:26:06.234992: step 23773, loss 3.05472e-07, acc 1
2017-08-08T18:26:06.507581: step 23774, loss 8.19562e-08, acc 1
2017-08-08T18:26:06.855424: step 23775, loss 1.90537e-06, acc 1
2017-08-08T18:26:07.059987: step 23776, loss 0, acc 1
2017-08-08T18:26:07.320466: step 23777, loss 0, acc 1
2017-08-08T18:26:07.746729: step 23778, loss 1.55337e-06, acc 1
2017-08-08T18:26:08.205090: step 23779, loss 2.53318e-07, acc 1
2017-08-08T18:26:08.581026: step 23780, loss 0, acc 1
2017-08-08T18:26:08.888266: step 23781, loss 1.86265e-09, acc 1
2017-08-08T18:26:09.145409: step 23782, loss 6.89177e-08, acc 1
2017-08-08T18:26:09.481403: step 23783, loss 1.43423e-07, acc 1
2017-08-08T18:26:09.746677: step 23784, loss 0.000443717, acc 1
2017-08-08T18:26:10.020773: step 23785, loss 0, acc 1
2017-08-08T18:26:10.281651: step 23786, loss 6.70538e-07, acc 1
2017-08-08T18:26:10.648579: step 23787, loss 4.93455e-05, acc 1
2017-08-08T18:26:11.001573: step 23788, loss 7.45058e-09, acc 1
2017-08-08T18:26:11.261470: step 23789, loss 1.30385e-08, acc 1
2017-08-08T18:26:11.470585: step 23790, loss 4.84402e-05, acc 1
2017-08-08T18:26:11.769770: step 23791, loss 0.000268127, acc 1
2017-08-08T18:26:12.137029: step 23792, loss 7.45058e-09, acc 1
2017-08-08T18:26:12.422028: step 23793, loss 3.16649e-08, acc 1
2017-08-08T18:26:12.661207: step 23794, loss 3.67942e-05, acc 1
2017-08-08T18:26:12.912097: step 23795, loss 3.23136e-06, acc 1
2017-08-08T18:26:13.369256: step 23796, loss 7.35729e-07, acc 1
2017-08-08T18:26:13.779035: step 23797, loss 4.61929e-07, acc 1
2017-08-08T18:26:14.233413: step 23798, loss 1.775e-06, acc 1
2017-08-08T18:26:14.488028: step 23799, loss 3.24097e-07, acc 1
2017-08-08T18:26:14.788517: step 23800, loss 2.12141e-06, acc 1

Evaluation:
2017-08-08T18:26:15.430411: step 23800, loss 8.13612, acc 0.724203

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-23800

2017-08-08T18:26:15.985451: step 23801, loss 1.21342e-05, acc 1
2017-08-08T18:26:16.372905: step 23802, loss 0.00388577, acc 1
2017-08-08T18:26:16.616683: step 23803, loss 3.72529e-09, acc 1
2017-08-08T18:26:16.835453: step 23804, loss 1.69123e-06, acc 1
2017-08-08T18:26:17.213362: step 23805, loss 1.00583e-07, acc 1
2017-08-08T18:26:17.568813: step 23806, loss 0, acc 1
2017-08-08T18:26:17.790004: step 23807, loss 5.58784e-07, acc 1
2017-08-08T18:26:18.025433: step 23808, loss 1.49012e-08, acc 1
2017-08-08T18:26:18.269386: step 23809, loss 0, acc 1
2017-08-08T18:26:18.635434: step 23810, loss 5.77419e-08, acc 1
2017-08-08T18:26:18.893022: step 23811, loss 6.84047e-05, acc 1
2017-08-08T18:26:19.162222: step 23812, loss 8.77281e-07, acc 1
2017-08-08T18:26:19.473340: step 23813, loss 3.7102e-06, acc 1
2017-08-08T18:26:19.738919: step 23814, loss 4.56342e-07, acc 1
2017-08-08T18:26:19.993777: step 23815, loss 9.31322e-09, acc 1
2017-08-08T18:26:20.217603: step 23816, loss 2.04891e-08, acc 1
2017-08-08T18:26:20.506886: step 23817, loss 1.2591e-06, acc 1
2017-08-08T18:26:20.870355: step 23818, loss 5.58794e-09, acc 1
2017-08-08T18:26:21.161237: step 23819, loss 4.47031e-07, acc 1
2017-08-08T18:26:21.325390: step 23820, loss 0, acc 1
2017-08-08T18:26:21.525317: step 23821, loss 0.108291, acc 0.984375
2017-08-08T18:26:21.960062: step 23822, loss 9.31322e-09, acc 1
2017-08-08T18:26:22.236719: step 23823, loss 6.01623e-07, acc 1
2017-08-08T18:26:22.490014: step 23824, loss 7.98971e-05, acc 1
2017-08-08T18:26:22.728432: step 23825, loss 1.83465e-06, acc 1
2017-08-08T18:26:23.089570: step 23826, loss 2.8589e-06, acc 1
2017-08-08T18:26:23.489964: step 23827, loss 2.38443e-05, acc 1
2017-08-08T18:26:23.885735: step 23828, loss 1.30385e-08, acc 1
2017-08-08T18:26:24.107993: step 23829, loss 0, acc 1
2017-08-08T18:26:24.299064: step 23830, loss 5.2154e-08, acc 1
2017-08-08T18:26:24.721376: step 23831, loss 3.43248e-06, acc 1
2017-08-08T18:26:25.030887: step 23832, loss 5.02913e-08, acc 1
2017-08-08T18:26:25.273141: step 23833, loss 2.79396e-07, acc 1
2017-08-08T18:26:25.568535: step 23834, loss 8.94068e-08, acc 1
2017-08-08T18:26:25.913343: step 23835, loss 3.72529e-09, acc 1
2017-08-08T18:26:26.289363: step 23836, loss 0.000227635, acc 1
2017-08-08T18:26:26.582376: step 23837, loss 9.31322e-09, acc 1
2017-08-08T18:26:26.803227: step 23838, loss 3.02667e-05, acc 1
2017-08-08T18:26:26.995321: step 23839, loss 0, acc 1
2017-08-08T18:26:27.310892: step 23840, loss 6.81996e-06, acc 1
2017-08-08T18:26:27.537372: step 23841, loss 2.78187e-05, acc 1
2017-08-08T18:26:27.754212: step 23842, loss 9.31321e-08, acc 1
2017-08-08T18:26:28.019997: step 23843, loss 4.11639e-07, acc 1
2017-08-08T18:26:28.261410: step 23844, loss 4.52142e-05, acc 1
2017-08-08T18:26:28.648262: step 23845, loss 0, acc 1
2017-08-08T18:26:29.085712: step 23846, loss 1.90351e-06, acc 1
2017-08-08T18:26:29.423360: step 23847, loss 1.45096e-06, acc 1
2017-08-08T18:26:29.644721: step 23848, loss 1.30385e-08, acc 1
2017-08-08T18:26:29.929328: step 23849, loss 4.13835e-05, acc 1
2017-08-08T18:26:30.208054: step 23850, loss 2.1855e-08, acc 1
2017-08-08T18:26:30.451037: step 23851, loss 0.000438704, acc 1
2017-08-08T18:26:30.733541: step 23852, loss 8.56795e-07, acc 1
2017-08-08T18:26:31.049923: step 23853, loss 0, acc 1
2017-08-08T18:26:31.484398: step 23854, loss 0.00561305, acc 1
2017-08-08T18:26:31.849692: step 23855, loss 0, acc 1
2017-08-08T18:26:32.197549: step 23856, loss 0.000223133, acc 1
2017-08-08T18:26:32.426422: step 23857, loss 0, acc 1
2017-08-08T18:26:32.796202: step 23858, loss 2.42144e-08, acc 1
2017-08-08T18:26:33.105056: step 23859, loss 1.695e-07, acc 1
2017-08-08T18:26:33.329089: step 23860, loss 4.25519e-05, acc 1
2017-08-08T18:26:33.578281: step 23861, loss 2.77533e-07, acc 1
2017-08-08T18:26:33.975794: step 23862, loss 3.91155e-08, acc 1
2017-08-08T18:26:34.383563: step 23863, loss 3.72529e-08, acc 1
2017-08-08T18:26:34.659088: step 23864, loss 0.035396, acc 0.984375
2017-08-08T18:26:34.965746: step 23865, loss 2.05869e-05, acc 1
2017-08-08T18:26:35.180702: step 23866, loss 1.50874e-07, acc 1
2017-08-08T18:26:35.565397: step 23867, loss 6.44771e-05, acc 1
2017-08-08T18:26:35.904704: step 23868, loss 3.44739e-06, acc 1
2017-08-08T18:26:36.165351: step 23869, loss 1.67638e-08, acc 1
2017-08-08T18:26:36.465990: step 23870, loss 0.00362476, acc 1
2017-08-08T18:26:36.836759: step 23871, loss 5.89975e-06, acc 1
2017-08-08T18:26:37.273362: step 23872, loss 2.79e-06, acc 1
2017-08-08T18:26:37.689377: step 23873, loss 0, acc 1
2017-08-08T18:26:37.993666: step 23874, loss 0, acc 1
2017-08-08T18:26:38.233333: step 23875, loss 6.14665e-07, acc 1
2017-08-08T18:26:38.558951: step 23876, loss 0, acc 1
2017-08-08T18:26:38.758259: step 23877, loss 3.05816e-06, acc 1
2017-08-08T18:26:39.065118: step 23878, loss 1.49012e-08, acc 1
2017-08-08T18:26:39.475428: step 23879, loss 9.66695e-07, acc 1
2017-08-08T18:26:39.876237: step 23880, loss 1.45442e-05, acc 1
2017-08-08T18:26:40.253556: step 23881, loss 4.37715e-07, acc 1
2017-08-08T18:26:40.574797: step 23882, loss 4.84287e-08, acc 1
2017-08-08T18:26:40.829513: step 23883, loss 1.14233e-05, acc 1
2017-08-08T18:26:41.267867: step 23884, loss 3.38998e-07, acc 1
2017-08-08T18:26:41.498198: step 23885, loss 0, acc 1
2017-08-08T18:26:41.717628: step 23886, loss 1.11759e-08, acc 1
2017-08-08T18:26:41.947918: step 23887, loss 0.000457591, acc 1
2017-08-08T18:26:42.255394: step 23888, loss 0, acc 1
2017-08-08T18:26:42.685382: step 23889, loss 1.61646e-05, acc 1
2017-08-08T18:26:43.101403: step 23890, loss 1.369e-06, acc 1
2017-08-08T18:26:43.402414: step 23891, loss 0, acc 1
2017-08-08T18:26:43.668111: step 23892, loss 2.75669e-07, acc 1
2017-08-08T18:26:44.023017: step 23893, loss 3.04674e-05, acc 1
2017-08-08T18:26:44.258464: step 23894, loss 2.6077e-08, acc 1
2017-08-08T18:26:44.554687: step 23895, loss 2.15453e-05, acc 1
2017-08-08T18:26:44.828728: step 23896, loss 3.72529e-09, acc 1
2017-08-08T18:26:45.182944: step 23897, loss 5.40167e-08, acc 1
2017-08-08T18:26:45.563641: step 23898, loss 6.1415e-05, acc 1
2017-08-08T18:26:45.956201: step 23899, loss 0, acc 1
2017-08-08T18:26:46.281710: step 23900, loss 2.09726e-06, acc 1

Evaluation:
2017-08-08T18:26:47.159122: step 23900, loss 8.1086, acc 0.713884

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-23900

2017-08-08T18:26:47.525154: step 23901, loss 9.12669e-07, acc 1
2017-08-08T18:26:47.743399: step 23902, loss 1.86265e-09, acc 1
2017-08-08T18:26:48.062913: step 23903, loss 0, acc 1
2017-08-08T18:26:48.469334: step 23904, loss 0, acc 1
2017-08-08T18:26:48.700097: step 23905, loss 6.63095e-07, acc 1
2017-08-08T18:26:48.886102: step 23906, loss 0, acc 1
2017-08-08T18:26:49.088634: step 23907, loss 1.99302e-07, acc 1
2017-08-08T18:26:49.425362: step 23908, loss 2.04891e-08, acc 1
2017-08-08T18:26:49.647444: step 23909, loss 8.08369e-07, acc 1
2017-08-08T18:26:49.840337: step 23910, loss 0, acc 1
2017-08-08T18:26:50.052833: step 23911, loss 0.000367734, acc 1
2017-08-08T18:26:50.273327: step 23912, loss 6.16523e-07, acc 1
2017-08-08T18:26:50.574622: step 23913, loss 3.06748e-06, acc 1
2017-08-08T18:26:50.862279: step 23914, loss 0.000139176, acc 1
2017-08-08T18:26:51.090397: step 23915, loss 1.49012e-08, acc 1
2017-08-08T18:26:51.286505: step 23916, loss 0, acc 1
2017-08-08T18:26:51.733359: step 23917, loss 4.70521e-05, acc 1
2017-08-08T18:26:52.032279: step 23918, loss 4.50754e-07, acc 1
2017-08-08T18:26:52.330606: step 23919, loss 2.29105e-07, acc 1
2017-08-08T18:26:52.585742: step 23920, loss 1.86265e-09, acc 1
2017-08-08T18:26:52.855067: step 23921, loss 1.99139e-05, acc 1
2017-08-08T18:26:53.277365: step 23922, loss 5.95952e-05, acc 1
2017-08-08T18:26:53.563331: step 23923, loss 3.98556e-06, acc 1
2017-08-08T18:26:53.872736: step 23924, loss 3.72529e-09, acc 1
2017-08-08T18:26:54.088618: step 23925, loss 7.93477e-07, acc 1
2017-08-08T18:26:54.437379: step 23926, loss 7.45058e-09, acc 1
2017-08-08T18:26:54.806644: step 23927, loss 0, acc 1
2017-08-08T18:26:55.066131: step 23928, loss 1.10974e-05, acc 1
2017-08-08T18:26:55.357811: step 23929, loss 7.85256e-05, acc 1
2017-08-08T18:26:55.739487: step 23930, loss 0, acc 1
2017-08-08T18:26:56.052481: step 23931, loss 0, acc 1
2017-08-08T18:26:56.413353: step 23932, loss 1.19209e-07, acc 1
2017-08-08T18:26:56.701592: step 23933, loss 4.18777e-05, acc 1
2017-08-08T18:26:56.898474: step 23934, loss 9.68572e-08, acc 1
2017-08-08T18:26:57.244490: step 23935, loss 4.30265e-07, acc 1
2017-08-08T18:26:57.540905: step 23936, loss 0.000620377, acc 1
2017-08-08T18:26:57.804168: step 23937, loss 0, acc 1
2017-08-08T18:26:58.052537: step 23938, loss 2.42144e-08, acc 1
2017-08-08T18:26:58.293324: step 23939, loss 6.61206e-05, acc 1
2017-08-08T18:26:58.524608: step 23940, loss 3.57626e-07, acc 1
2017-08-08T18:26:58.735294: step 23941, loss 1.8327e-05, acc 1
2017-08-08T18:26:58.911225: step 23942, loss 5.32708e-07, acc 1
2017-08-08T18:26:59.104054: step 23943, loss 0, acc 1
2017-08-08T18:26:59.338494: step 23944, loss 4.28408e-08, acc 1
2017-08-08T18:26:59.519677: step 23945, loss 1.49005e-06, acc 1
2017-08-08T18:26:59.696076: step 23946, loss 0.000130297, acc 1
2017-08-08T18:26:59.904515: step 23947, loss 4.84287e-08, acc 1
2017-08-08T18:27:00.224730: step 23948, loss 0, acc 1
2017-08-08T18:27:00.624108: step 23949, loss 3.72529e-09, acc 1
2017-08-08T18:27:00.868958: step 23950, loss 8.41898e-07, acc 1
2017-08-08T18:27:01.070261: step 23951, loss 8.50455e-06, acc 1
2017-08-08T18:27:01.276715: step 23952, loss 1.86265e-09, acc 1
2017-08-08T18:27:01.737797: step 23953, loss 8.38189e-08, acc 1
2017-08-08T18:27:02.010397: step 23954, loss 0.000853877, acc 1
2017-08-08T18:27:02.274809: step 23955, loss 7.09882e-06, acc 1
2017-08-08T18:27:02.565888: step 23956, loss 2.5345e-05, acc 1
2017-08-08T18:27:02.907965: step 23957, loss 7.45058e-09, acc 1
2017-08-08T18:27:03.336731: step 23958, loss 7.82309e-08, acc 1
2017-08-08T18:27:03.726151: step 23959, loss 1.08963e-06, acc 1
2017-08-08T18:27:04.048004: step 23960, loss 5.47609e-07, acc 1
2017-08-08T18:27:04.259618: step 23961, loss 0, acc 1
2017-08-08T18:27:04.592800: step 23962, loss 1.19209e-07, acc 1
2017-08-08T18:27:04.924415: step 23963, loss 1.67638e-08, acc 1
2017-08-08T18:27:05.191570: step 23964, loss 8.38189e-08, acc 1
2017-08-08T18:27:05.450538: step 23965, loss 6.70551e-08, acc 1
2017-08-08T18:27:05.795867: step 23966, loss 5.58794e-09, acc 1
2017-08-08T18:27:06.158129: step 23967, loss 8.45091e-06, acc 1
2017-08-08T18:27:06.556393: step 23968, loss 1.86265e-09, acc 1
2017-08-08T18:27:06.918913: step 23969, loss 1.56461e-07, acc 1
2017-08-08T18:27:07.159370: step 23970, loss 5.02906e-07, acc 1
2017-08-08T18:27:07.565818: step 23971, loss 0.000108379, acc 1
2017-08-08T18:27:07.813570: step 23972, loss 0.00107648, acc 1
2017-08-08T18:27:08.042647: step 23973, loss 1.09896e-07, acc 1
2017-08-08T18:27:08.275826: step 23974, loss 7.56993e-05, acc 1
2017-08-08T18:27:08.630821: step 23975, loss 0, acc 1
2017-08-08T18:27:08.906837: step 23976, loss 9.18015e-06, acc 1
2017-08-08T18:27:09.179677: step 23977, loss 0.000308589, acc 1
2017-08-08T18:27:09.345448: step 23978, loss 2.03201e-06, acc 1
2017-08-08T18:27:09.537337: step 23979, loss 0, acc 1
2017-08-08T18:27:09.838542: step 23980, loss 4.09782e-08, acc 1
2017-08-08T18:27:10.031871: step 23981, loss 0, acc 1
2017-08-08T18:27:10.267936: step 23982, loss 2.23517e-08, acc 1
2017-08-08T18:27:10.460639: step 23983, loss 6.70551e-08, acc 1
2017-08-08T18:27:10.676489: step 23984, loss 1.70358e-05, acc 1
2017-08-08T18:27:10.983946: step 23985, loss 9.90897e-07, acc 1
2017-08-08T18:27:11.217338: step 23986, loss 7.45058e-09, acc 1
2017-08-08T18:27:11.412771: step 23987, loss 4.67519e-07, acc 1
2017-08-08T18:27:11.692307: step 23988, loss 6.68932e-06, acc 1
2017-08-08T18:27:11.951654: step 23989, loss 1.91852e-07, acc 1
2017-08-08T18:27:12.175876: step 23990, loss 1.88107e-05, acc 1
2017-08-08T18:27:12.406524: step 23991, loss 1.36183e-05, acc 1
2017-08-08T18:27:12.630328: step 23992, loss 0, acc 1
2017-08-08T18:27:12.997786: step 23993, loss 4.58352e-06, acc 1
2017-08-08T18:27:13.280300: step 23994, loss 7.45056e-08, acc 1
2017-08-08T18:27:13.534615: step 23995, loss 2.6077e-08, acc 1
2017-08-08T18:27:13.708872: step 23996, loss 5.58793e-09, acc 1
2017-08-08T18:27:13.999076: step 23997, loss 8.75441e-08, acc 1
2017-08-08T18:27:14.467025: step 23998, loss 3.41969e-06, acc 1
2017-08-08T18:27:14.741639: step 23999, loss 0, acc 1
2017-08-08T18:27:15.040235: step 24000, loss 5.96046e-09, acc 1

Evaluation:
2017-08-08T18:27:15.933741: step 24000, loss 8.10867, acc 0.716698

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-24000

2017-08-08T18:27:16.558304: step 24001, loss 2.36554e-07, acc 1
2017-08-08T18:27:16.831694: step 24002, loss 2.60749e-06, acc 1
2017-08-08T18:27:17.053623: step 24003, loss 8.00936e-08, acc 1
2017-08-08T18:27:17.297382: step 24004, loss 1.30385e-08, acc 1
2017-08-08T18:27:17.664390: step 24005, loss 9.31322e-09, acc 1
2017-08-08T18:27:17.962462: step 24006, loss 2.83473e-06, acc 1
2017-08-08T18:27:18.237500: step 24007, loss 6.70552e-08, acc 1
2017-08-08T18:27:18.734249: step 24008, loss 1.11759e-08, acc 1
2017-08-08T18:27:19.082278: step 24009, loss 2.3002e-06, acc 1
2017-08-08T18:27:19.438837: step 24010, loss 1.29698e-05, acc 1
2017-08-08T18:27:19.615770: step 24011, loss 1.0505e-06, acc 1
2017-08-08T18:27:19.905368: step 24012, loss 1.30385e-08, acc 1
2017-08-08T18:27:20.225871: step 24013, loss 0.000365705, acc 1
2017-08-08T18:27:20.499615: step 24014, loss 0, acc 1
2017-08-08T18:27:20.714522: step 24015, loss 1.86265e-09, acc 1
2017-08-08T18:27:21.017379: step 24016, loss 6.02321e-05, acc 1
2017-08-08T18:27:21.304133: step 24017, loss 8.19562e-08, acc 1
2017-08-08T18:27:21.583786: step 24018, loss 1.49012e-08, acc 1
2017-08-08T18:27:21.798390: step 24019, loss 1.00583e-07, acc 1
2017-08-08T18:27:21.993299: step 24020, loss 1.91101e-06, acc 1
2017-08-08T18:27:22.265286: step 24021, loss 8.43178e-06, acc 1
2017-08-08T18:27:22.430782: step 24022, loss 4.8924e-06, acc 1
2017-08-08T18:27:22.602552: step 24023, loss 2.34117e-06, acc 1
2017-08-08T18:27:22.817412: step 24024, loss 1.49012e-08, acc 1
2017-08-08T18:27:23.039272: step 24025, loss 5.96364e-05, acc 1
2017-08-08T18:27:23.302336: step 24026, loss 3.09169e-06, acc 1
2017-08-08T18:27:23.585949: step 24027, loss 9.29433e-07, acc 1
2017-08-08T18:27:23.860207: step 24028, loss 1.86265e-09, acc 1
2017-08-08T18:27:24.262602: step 24029, loss 3.91155e-08, acc 1
2017-08-08T18:27:24.614454: step 24030, loss 2.10841e-06, acc 1
2017-08-08T18:27:24.942219: step 24031, loss 1.49012e-08, acc 1
2017-08-08T18:27:25.215672: step 24032, loss 2.39707e-06, acc 1
2017-08-08T18:27:25.513419: step 24033, loss 1.86265e-09, acc 1
2017-08-08T18:27:25.860510: step 24034, loss 1.11759e-08, acc 1
2017-08-08T18:27:26.223762: step 24035, loss 6.14672e-08, acc 1
2017-08-08T18:27:26.438339: step 24036, loss 3.72529e-09, acc 1
2017-08-08T18:27:26.639079: step 24037, loss 4.6616e-06, acc 1
2017-08-08T18:27:27.034854: step 24038, loss 7.45057e-08, acc 1
2017-08-08T18:27:27.306519: step 24039, loss 0, acc 1
2017-08-08T18:27:27.514908: step 24040, loss 2.01165e-07, acc 1
2017-08-08T18:27:27.759114: step 24041, loss 1.00022e-06, acc 1
2017-08-08T18:27:28.018675: step 24042, loss 1.19209e-07, acc 1
2017-08-08T18:27:28.393396: step 24043, loss 3.35273e-07, acc 1
2017-08-08T18:27:28.635567: step 24044, loss 3.25961e-07, acc 1
2017-08-08T18:27:28.857972: step 24045, loss 0, acc 1
2017-08-08T18:27:29.133355: step 24046, loss 0, acc 1
2017-08-08T18:27:29.403582: step 24047, loss 0, acc 1
2017-08-08T18:27:29.666057: step 24048, loss 5.96045e-08, acc 1
2017-08-08T18:27:29.927361: step 24049, loss 4.2812e-05, acc 1
2017-08-08T18:27:30.248028: step 24050, loss 9.49947e-08, acc 1
2017-08-08T18:27:30.538015: step 24051, loss 3.72529e-09, acc 1
2017-08-08T18:27:30.977945: step 24052, loss 3.72529e-09, acc 1
2017-08-08T18:27:31.277602: step 24053, loss 1.67638e-08, acc 1
2017-08-08T18:27:31.477830: step 24054, loss 2.66356e-07, acc 1
2017-08-08T18:27:31.808294: step 24055, loss 2.56279e-06, acc 1
2017-08-08T18:27:31.995235: step 24056, loss 2.86845e-07, acc 1
2017-08-08T18:27:32.186760: step 24057, loss 0, acc 1
2017-08-08T18:27:32.426153: step 24058, loss 4.74941e-05, acc 1
2017-08-08T18:27:32.682786: step 24059, loss 1.87267e-05, acc 1
2017-08-08T18:27:32.915269: step 24060, loss 0.000165786, acc 1
2017-08-08T18:27:33.190050: step 24061, loss 3.87426e-07, acc 1
2017-08-08T18:27:33.418191: step 24062, loss 3.91155e-08, acc 1
2017-08-08T18:27:33.575746: step 24063, loss 0, acc 1
2017-08-08T18:27:33.827704: step 24064, loss 1.13367e-05, acc 1
2017-08-08T18:27:34.138988: step 24065, loss 7.63683e-08, acc 1
2017-08-08T18:27:34.342628: step 24066, loss 3.63006e-06, acc 1
2017-08-08T18:27:34.565692: step 24067, loss 1.15484e-07, acc 1
2017-08-08T18:27:34.918411: step 24068, loss 1.86265e-09, acc 1
2017-08-08T18:27:35.360638: step 24069, loss 3.72529e-09, acc 1
2017-08-08T18:27:35.759929: step 24070, loss 1.598e-05, acc 1
2017-08-08T18:27:36.090297: step 24071, loss 8.01026e-05, acc 1
2017-08-08T18:27:36.323373: step 24072, loss 2.23517e-08, acc 1
2017-08-08T18:27:36.592581: step 24073, loss 2.33192e-06, acc 1
2017-08-08T18:27:37.009032: step 24074, loss 2.87997e-05, acc 1
2017-08-08T18:27:37.199014: step 24075, loss 3.11255e-05, acc 1
2017-08-08T18:27:37.417101: step 24076, loss 0, acc 1
2017-08-08T18:27:37.626337: step 24077, loss 1.60117e-05, acc 1
2017-08-08T18:27:37.991332: step 24078, loss 5.58794e-09, acc 1
2017-08-08T18:27:38.282630: step 24079, loss 0.0970771, acc 0.984375
2017-08-08T18:27:38.580139: step 24080, loss 1.3501e-05, acc 1
2017-08-08T18:27:38.848143: step 24081, loss 0.000313612, acc 1
2017-08-08T18:27:39.069607: step 24082, loss 0.00201041, acc 1
2017-08-08T18:27:39.489394: step 24083, loss 3.8827e-05, acc 1
2017-08-08T18:27:39.791459: step 24084, loss 5.49474e-07, acc 1
2017-08-08T18:27:40.035459: step 24085, loss 2.76467e-05, acc 1
2017-08-08T18:27:40.312620: step 24086, loss 5.36433e-07, acc 1
2017-08-08T18:27:40.697372: step 24087, loss 1.86265e-09, acc 1
2017-08-08T18:27:41.098203: step 24088, loss 1.52734e-06, acc 1
2017-08-08T18:27:41.394553: step 24089, loss 6.14672e-08, acc 1
2017-08-08T18:27:41.636268: step 24090, loss 0, acc 1
2017-08-08T18:27:42.047148: step 24091, loss 9.12693e-08, acc 1
2017-08-08T18:27:42.351209: step 24092, loss 0, acc 1
2017-08-08T18:27:42.612377: step 24093, loss 4.44561e-06, acc 1
2017-08-08T18:27:42.816216: step 24094, loss 2.08416e-06, acc 1
2017-08-08T18:27:43.069601: step 24095, loss 2.08615e-07, acc 1
2017-08-08T18:27:43.441360: step 24096, loss 3.68388e-06, acc 1
2017-08-08T18:27:43.730947: step 24097, loss 0.0428839, acc 0.984375
2017-08-08T18:27:43.965629: step 24098, loss 1.89824e-05, acc 1
2017-08-08T18:27:44.337705: step 24099, loss 1.80676e-07, acc 1
2017-08-08T18:27:44.625984: step 24100, loss 3.7625e-07, acc 1

Evaluation:
2017-08-08T18:27:45.271440: step 24100, loss 8.27857, acc 0.727017

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-24100

2017-08-08T18:27:45.881416: step 24101, loss 0.000826054, acc 1
2017-08-08T18:27:46.106651: step 24102, loss 2.23517e-08, acc 1
2017-08-08T18:27:46.334543: step 24103, loss 8.19562e-08, acc 1
2017-08-08T18:27:46.545325: step 24104, loss 2.9802e-07, acc 1
2017-08-08T18:27:46.880483: step 24105, loss 1.40065e-06, acc 1
2017-08-08T18:27:47.071247: step 24106, loss 1.86265e-09, acc 1
2017-08-08T18:27:47.249182: step 24107, loss 1.89606e-06, acc 1
2017-08-08T18:27:47.435890: step 24108, loss 5.18134e-06, acc 1
2017-08-08T18:27:47.641323: step 24109, loss 1.02256e-06, acc 1
2017-08-08T18:27:48.017369: step 24110, loss 0, acc 1
2017-08-08T18:27:48.304072: step 24111, loss 9.31322e-09, acc 1
2017-08-08T18:27:48.601929: step 24112, loss 2.24627e-06, acc 1
2017-08-08T18:27:48.835310: step 24113, loss 3.72529e-09, acc 1
2017-08-08T18:27:49.297290: step 24114, loss 1.09375e-05, acc 1
2017-08-08T18:27:49.605846: step 24115, loss 5.02682e-05, acc 1
2017-08-08T18:27:49.889728: step 24116, loss 5.2154e-08, acc 1
2017-08-08T18:27:50.174587: step 24117, loss 1.22458e-05, acc 1
2017-08-08T18:27:50.605703: step 24118, loss 6.01521e-06, acc 1
2017-08-08T18:27:50.993366: step 24119, loss 9.31322e-09, acc 1
2017-08-08T18:27:51.431584: step 24120, loss 0, acc 1
2017-08-08T18:27:51.717325: step 24121, loss 3.79189e-06, acc 1
2017-08-08T18:27:51.952572: step 24122, loss 8.54555e-06, acc 1
2017-08-08T18:27:52.437010: step 24123, loss 2.06368e-06, acc 1
2017-08-08T18:27:52.794328: step 24124, loss 3.74387e-07, acc 1
2017-08-08T18:27:53.089729: step 24125, loss 0, acc 1
2017-08-08T18:27:53.373375: step 24126, loss 1.67638e-08, acc 1
2017-08-08T18:27:53.833904: step 24127, loss 3.05471e-07, acc 1
2017-08-08T18:27:54.202170: step 24128, loss 9.31322e-09, acc 1
2017-08-08T18:27:54.546620: step 24129, loss 1.86265e-09, acc 1
2017-08-08T18:27:54.857400: step 24130, loss 1.3889e-05, acc 1
2017-08-08T18:27:55.135637: step 24131, loss 1.86265e-09, acc 1
2017-08-08T18:27:55.569916: step 24132, loss 0.110157, acc 0.984375
2017-08-08T18:27:55.838528: step 24133, loss 2.49215e-05, acc 1
2017-08-08T18:27:56.125368: step 24134, loss 2.04891e-08, acc 1
2017-08-08T18:27:56.481456: step 24135, loss 0.00242526, acc 1
2017-08-08T18:27:56.971238: step 24136, loss 0.0231195, acc 0.984375
2017-08-08T18:27:57.402679: step 24137, loss 7.45058e-09, acc 1
2017-08-08T18:27:57.645265: step 24138, loss 1.57578e-06, acc 1
2017-08-08T18:27:57.887480: step 24139, loss 3.84594e-06, acc 1
2017-08-08T18:27:58.312756: step 24140, loss 3.58743e-05, acc 1
2017-08-08T18:27:58.656438: step 24141, loss 3.41199e-06, acc 1
2017-08-08T18:27:58.840281: step 24142, loss 3.24098e-07, acc 1
2017-08-08T18:27:59.063098: step 24143, loss 1.67638e-08, acc 1
2017-08-08T18:27:59.374482: step 24144, loss 1.36898e-06, acc 1
2017-08-08T18:27:59.657377: step 24145, loss 1.75088e-07, acc 1
2017-08-08T18:27:59.975706: step 24146, loss 0.000175734, acc 1
2017-08-08T18:28:00.186563: step 24147, loss 1.86265e-09, acc 1
2017-08-08T18:28:00.386995: step 24148, loss 3.53902e-08, acc 1
2017-08-08T18:28:00.762700: step 24149, loss 0, acc 1
2017-08-08T18:28:00.999870: step 24150, loss 9.96193e-05, acc 1
2017-08-08T18:28:01.273583: step 24151, loss 8.88457e-07, acc 1
2017-08-08T18:28:01.532828: step 24152, loss 2.59445e-06, acc 1
2017-08-08T18:28:01.857870: step 24153, loss 2.6077e-08, acc 1
2017-08-08T18:28:02.248877: step 24154, loss 1.71098e-05, acc 1
2017-08-08T18:28:02.614449: step 24155, loss 1.22371e-06, acc 1
2017-08-08T18:28:02.968984: step 24156, loss 1.16429e-05, acc 1
2017-08-08T18:28:03.250422: step 24157, loss 3.74916e-06, acc 1
2017-08-08T18:28:03.502218: step 24158, loss 4.28408e-08, acc 1
2017-08-08T18:28:03.983660: step 24159, loss 0.000102514, acc 1
2017-08-08T18:28:04.289956: step 24160, loss 0, acc 1
2017-08-08T18:28:04.590831: step 24161, loss 0, acc 1
2017-08-08T18:28:04.836935: step 24162, loss 0, acc 1
2017-08-08T18:28:05.216872: step 24163, loss 9.68573e-08, acc 1
2017-08-08T18:28:05.576496: step 24164, loss 0.000306182, acc 1
2017-08-08T18:28:05.921915: step 24165, loss 2.23517e-08, acc 1
2017-08-08T18:28:06.135778: step 24166, loss 1.13621e-07, acc 1
2017-08-08T18:28:06.395131: step 24167, loss 2.2704e-06, acc 1
2017-08-08T18:28:06.822510: step 24168, loss 3.72529e-09, acc 1
2017-08-08T18:28:07.078771: step 24169, loss 2.48644e-06, acc 1
2017-08-08T18:28:07.338910: step 24170, loss 2.77976e-05, acc 1
2017-08-08T18:28:07.569444: step 24171, loss 2.16347e-05, acc 1
2017-08-08T18:28:08.017118: step 24172, loss 1.67638e-08, acc 1
2017-08-08T18:28:08.390994: step 24173, loss 0.00307975, acc 1
2017-08-08T18:28:08.793462: step 24174, loss 2.6077e-08, acc 1
2017-08-08T18:28:09.070549: step 24175, loss 1.37644e-06, acc 1
2017-08-08T18:28:09.294897: step 24176, loss 4.7974e-05, acc 1
2017-08-08T18:28:09.747252: step 24177, loss 5.58793e-09, acc 1
2017-08-08T18:28:10.028758: step 24178, loss 1.67638e-08, acc 1
2017-08-08T18:28:10.273906: step 24179, loss 8.95918e-07, acc 1
2017-08-08T18:28:10.534278: step 24180, loss 0, acc 1
2017-08-08T18:28:10.838984: step 24181, loss 1.17809e-05, acc 1
2017-08-08T18:28:11.135528: step 24182, loss 4.5293e-06, acc 1
2017-08-08T18:28:11.407940: step 24183, loss 3.32538e-05, acc 1
2017-08-08T18:28:11.612693: step 24184, loss 1.86265e-09, acc 1
2017-08-08T18:28:11.826553: step 24185, loss 1.11759e-08, acc 1
2017-08-08T18:28:12.094194: step 24186, loss 2.50515e-06, acc 1
2017-08-08T18:28:12.256404: step 24187, loss 1.86999e-06, acc 1
2017-08-08T18:28:12.455560: step 24188, loss 1.71354e-06, acc 1
2017-08-08T18:28:12.698281: step 24189, loss 1.86265e-09, acc 1
2017-08-08T18:28:13.097388: step 24190, loss 1.29076e-06, acc 1
2017-08-08T18:28:13.438184: step 24191, loss 0.000257562, acc 1
2017-08-08T18:28:13.721355: step 24192, loss 7.45058e-09, acc 1
2017-08-08T18:28:14.069164: step 24193, loss 5.71828e-07, acc 1
2017-08-08T18:28:14.278656: step 24194, loss 2.51437e-06, acc 1
2017-08-08T18:28:14.612031: step 24195, loss 1.02445e-07, acc 1
2017-08-08T18:28:14.827757: step 24196, loss 3.53707e-05, acc 1
2017-08-08T18:28:15.063436: step 24197, loss 0, acc 1
2017-08-08T18:28:15.246925: step 24198, loss 0.000617597, acc 1
2017-08-08T18:28:15.560019: step 24199, loss 2.17928e-07, acc 1
2017-08-08T18:28:15.930692: step 24200, loss 1.22086e-05, acc 1

Evaluation:
2017-08-08T18:28:16.423562: step 24200, loss 8.23998, acc 0.716698

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-24200

2017-08-08T18:28:16.874500: step 24201, loss 1.49012e-08, acc 1
2017-08-08T18:28:17.086377: step 24202, loss 2.98197e-06, acc 1
2017-08-08T18:28:17.318077: step 24203, loss 2.19032e-06, acc 1
2017-08-08T18:28:17.555384: step 24204, loss 1.39698e-07, acc 1
2017-08-08T18:28:17.925639: step 24205, loss 2.02971e-05, acc 1
2017-08-08T18:28:18.231365: step 24206, loss 3.72529e-09, acc 1
2017-08-08T18:28:18.479232: step 24207, loss 1.86263e-07, acc 1
2017-08-08T18:28:18.696539: step 24208, loss 5.58793e-09, acc 1
2017-08-08T18:28:18.994137: step 24209, loss 1.52736e-07, acc 1
2017-08-08T18:28:19.221795: step 24210, loss 4.50142e-06, acc 1
2017-08-08T18:28:19.440241: step 24211, loss 4.65661e-08, acc 1
2017-08-08T18:28:19.684231: step 24212, loss 4.47034e-08, acc 1
2017-08-08T18:28:19.969417: step 24213, loss 0.0449963, acc 0.984375
2017-08-08T18:28:20.299526: step 24214, loss 2.0489e-07, acc 1
2017-08-08T18:28:20.577792: step 24215, loss 0.000402913, acc 1
2017-08-08T18:28:20.875042: step 24216, loss 1.86265e-09, acc 1
2017-08-08T18:28:21.114629: step 24217, loss 9.31322e-09, acc 1
2017-08-08T18:28:21.527648: step 24218, loss 0.000169535, acc 1
2017-08-08T18:28:21.840042: step 24219, loss 0, acc 1
2017-08-08T18:28:22.151614: step 24220, loss 5.58793e-09, acc 1
2017-08-08T18:28:22.338711: step 24221, loss 6.16786e-06, acc 1
2017-08-08T18:28:22.685407: step 24222, loss 0.00964357, acc 1
2017-08-08T18:28:23.101611: step 24223, loss 0, acc 1
2017-08-08T18:28:23.392635: step 24224, loss 2.6077e-08, acc 1
2017-08-08T18:28:23.625779: step 24225, loss 0.062502, acc 0.984375
2017-08-08T18:28:23.916595: step 24226, loss 1.39507e-06, acc 1
2017-08-08T18:28:24.276307: step 24227, loss 1.11759e-08, acc 1
2017-08-08T18:28:24.552311: step 24228, loss 1.88126e-07, acc 1
2017-08-08T18:28:24.809847: step 24229, loss 1.86265e-09, acc 1
2017-08-08T18:28:25.133662: step 24230, loss 1.10658e-05, acc 1
2017-08-08T18:28:25.477896: step 24231, loss 8.58658e-07, acc 1
2017-08-08T18:28:25.752446: step 24232, loss 4.73606e-06, acc 1
2017-08-08T18:28:26.025779: step 24233, loss 5.43029e-05, acc 1
2017-08-08T18:28:26.250261: step 24234, loss 7.63683e-08, acc 1
2017-08-08T18:28:26.637352: step 24235, loss 2.95028e-06, acc 1
2017-08-08T18:28:26.879438: step 24236, loss 7.54153e-05, acc 1
2017-08-08T18:28:27.101375: step 24237, loss 8.11451e-06, acc 1
2017-08-08T18:28:27.360410: step 24238, loss 6.0509e-06, acc 1
2017-08-08T18:28:27.696177: step 24239, loss 1.86264e-08, acc 1
2017-08-08T18:28:28.003761: step 24240, loss 4.60069e-07, acc 1
2017-08-08T18:28:28.230261: step 24241, loss 7.45058e-09, acc 1
2017-08-08T18:28:28.450041: step 24242, loss 9.77818e-05, acc 1
2017-08-08T18:28:28.675841: step 24243, loss 0.00413138, acc 1
2017-08-08T18:28:28.997294: step 24244, loss 1.11759e-08, acc 1
2017-08-08T18:28:29.281524: step 24245, loss 2.0489e-07, acc 1
2017-08-08T18:28:29.489216: step 24246, loss 6.62962e-06, acc 1
2017-08-08T18:28:29.683557: step 24247, loss 0.000245928, acc 1
2017-08-08T18:28:29.964289: step 24248, loss 1.11759e-08, acc 1
2017-08-08T18:28:30.222202: step 24249, loss 4.11639e-07, acc 1
2017-08-08T18:28:30.427100: step 24250, loss 1.56461e-07, acc 1
2017-08-08T18:28:30.608147: step 24251, loss 0, acc 1
2017-08-08T18:28:30.837954: step 24252, loss 1.14735e-06, acc 1
2017-08-08T18:28:31.052185: step 24253, loss 7.91606e-07, acc 1
2017-08-08T18:28:31.218862: step 24254, loss 1.86395e-05, acc 1
2017-08-08T18:28:31.392843: step 24255, loss 4.47034e-08, acc 1
2017-08-08T18:28:31.557590: step 24256, loss 0, acc 1
2017-08-08T18:28:31.837766: step 24257, loss 0, acc 1
2017-08-08T18:28:32.112631: step 24258, loss 7.56073e-05, acc 1
2017-08-08T18:28:32.362802: step 24259, loss 0, acc 1
2017-08-08T18:28:32.554824: step 24260, loss 1.91569e-05, acc 1
2017-08-08T18:28:32.905509: step 24261, loss 5.2154e-08, acc 1
2017-08-08T18:28:33.117433: step 24262, loss 2.56654e-06, acc 1
2017-08-08T18:28:33.333484: step 24263, loss 3.04887e-06, acc 1
2017-08-08T18:28:33.550054: step 24264, loss 3.1106e-07, acc 1
2017-08-08T18:28:33.880162: step 24265, loss 0, acc 1
2017-08-08T18:28:34.216987: step 24266, loss 1.86265e-09, acc 1
2017-08-08T18:28:34.479460: step 24267, loss 0.000201235, acc 1
2017-08-08T18:28:34.726523: step 24268, loss 8.51206e-07, acc 1
2017-08-08T18:28:34.935496: step 24269, loss 9.31322e-09, acc 1
2017-08-08T18:28:35.256753: step 24270, loss 1.86265e-09, acc 1
2017-08-08T18:28:35.476668: step 24271, loss 0.000336154, acc 1
2017-08-08T18:28:35.704360: step 24272, loss 1.86265e-09, acc 1
2017-08-08T18:28:36.017767: step 24273, loss 7.30148e-07, acc 1
2017-08-08T18:28:36.384036: step 24274, loss 5.06186e-06, acc 1
2017-08-08T18:28:36.665325: step 24275, loss 1.30385e-08, acc 1
2017-08-08T18:28:36.904445: step 24276, loss 0.186199, acc 0.984375
2017-08-08T18:28:37.189347: step 24277, loss 0, acc 1
2017-08-08T18:28:37.404631: step 24278, loss 0, acc 1
2017-08-08T18:28:37.586171: step 24279, loss 5.71822e-07, acc 1
2017-08-08T18:28:37.758285: step 24280, loss 0, acc 1
2017-08-08T18:28:37.945447: step 24281, loss 6.50116e-06, acc 1
2017-08-08T18:28:38.241446: step 24282, loss 0.000202704, acc 1
2017-08-08T18:28:38.455602: step 24283, loss 0, acc 1
2017-08-08T18:28:38.692372: step 24284, loss 2.95112e-05, acc 1
2017-08-08T18:28:38.867215: step 24285, loss 1.6205e-07, acc 1
2017-08-08T18:28:39.052554: step 24286, loss 1.89393e-05, acc 1
2017-08-08T18:28:39.387210: step 24287, loss 2.67267e-06, acc 1
2017-08-08T18:28:39.595631: step 24288, loss 0.000252985, acc 1
2017-08-08T18:28:39.808974: step 24289, loss 4.61348e-05, acc 1
2017-08-08T18:28:40.006192: step 24290, loss 3.57627e-07, acc 1
2017-08-08T18:28:40.264831: step 24291, loss 5.58793e-09, acc 1
2017-08-08T18:28:40.713318: step 24292, loss 1.0058e-06, acc 1
2017-08-08T18:28:40.981316: step 24293, loss 0, acc 1
2017-08-08T18:28:41.175213: step 24294, loss 0, acc 1
2017-08-08T18:28:41.412626: step 24295, loss 6.71526e-06, acc 1
2017-08-08T18:28:41.775040: step 24296, loss 3.72529e-09, acc 1
2017-08-08T18:28:42.074217: step 24297, loss 1.99104e-06, acc 1
2017-08-08T18:28:42.372759: step 24298, loss 0.0633596, acc 0.984375
2017-08-08T18:28:42.851548: step 24299, loss 1.18737e-05, acc 1
2017-08-08T18:28:43.262826: step 24300, loss 1.21196e-07, acc 1

Evaluation:
2017-08-08T18:28:44.023830: step 24300, loss 8.2552, acc 0.726079

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-24300

2017-08-08T18:28:44.563925: step 24301, loss 5.58794e-09, acc 1
2017-08-08T18:28:44.905676: step 24302, loss 9.49288e-06, acc 1
2017-08-08T18:28:45.193861: step 24303, loss 0.00995783, acc 1
2017-08-08T18:28:45.424766: step 24304, loss 0, acc 1
2017-08-08T18:28:45.773363: step 24305, loss 6.29568e-07, acc 1
2017-08-08T18:28:46.065062: step 24306, loss 0.000395838, acc 1
2017-08-08T18:28:46.331224: step 24307, loss 7.26431e-08, acc 1
2017-08-08T18:28:46.567806: step 24308, loss 3.16649e-08, acc 1
2017-08-08T18:28:46.802862: step 24309, loss 9.90896e-07, acc 1
2017-08-08T18:28:47.231871: step 24310, loss 0.0437002, acc 0.984375
2017-08-08T18:28:47.498072: step 24311, loss 0.000760574, acc 1
2017-08-08T18:28:47.818159: step 24312, loss 0.000164669, acc 1
2017-08-08T18:28:48.106010: step 24313, loss 3.35276e-08, acc 1
2017-08-08T18:28:48.501534: step 24314, loss 2.04891e-08, acc 1
2017-08-08T18:28:48.817866: step 24315, loss 3.72529e-09, acc 1
2017-08-08T18:28:49.184613: step 24316, loss 2.23517e-08, acc 1
2017-08-08T18:28:49.539818: step 24317, loss 5.58793e-09, acc 1
2017-08-08T18:28:49.797480: step 24318, loss 0, acc 1
2017-08-08T18:28:50.267297: step 24319, loss 0.00138618, acc 1
2017-08-08T18:28:50.562536: step 24320, loss 1.30385e-08, acc 1
2017-08-08T18:28:50.925844: step 24321, loss 0, acc 1
2017-08-08T18:28:51.189647: step 24322, loss 2.73806e-07, acc 1
2017-08-08T18:28:51.394661: step 24323, loss 0, acc 1
2017-08-08T18:28:51.798847: step 24324, loss 1.80676e-07, acc 1
2017-08-08T18:28:52.078693: step 24325, loss 2.3092e-05, acc 1
2017-08-08T18:28:52.449452: step 24326, loss 3.72529e-09, acc 1
2017-08-08T18:28:52.672433: step 24327, loss 8.84731e-07, acc 1
2017-08-08T18:28:52.871083: step 24328, loss 1.6502e-05, acc 1
2017-08-08T18:28:53.157472: step 24329, loss 0, acc 1
2017-08-08T18:28:53.330011: step 24330, loss 2.13817e-06, acc 1
2017-08-08T18:28:53.614559: step 24331, loss 3.56106e-05, acc 1
2017-08-08T18:28:53.983001: step 24332, loss 3.47717e-06, acc 1
2017-08-08T18:28:54.256194: step 24333, loss 2.3002e-06, acc 1
2017-08-08T18:28:54.489223: step 24334, loss 2.79397e-08, acc 1
2017-08-08T18:28:54.708116: step 24335, loss 5.08233e-06, acc 1
2017-08-08T18:28:54.907339: step 24336, loss 1.26739e-05, acc 1
2017-08-08T18:28:55.059534: step 24337, loss 7.00339e-07, acc 1
2017-08-08T18:28:55.330436: step 24338, loss 1.30385e-08, acc 1
2017-08-08T18:28:55.613857: step 24339, loss 4.91731e-07, acc 1
2017-08-08T18:28:55.964021: step 24340, loss 0.00644495, acc 1
2017-08-08T18:28:56.289049: step 24341, loss 0.000412786, acc 1
2017-08-08T18:28:56.548208: step 24342, loss 5.58794e-09, acc 1
2017-08-08T18:28:56.819515: step 24343, loss 0, acc 1
2017-08-08T18:28:57.041370: step 24344, loss 3.72529e-09, acc 1
2017-08-08T18:28:57.399534: step 24345, loss 1.28522e-07, acc 1
2017-08-08T18:28:57.638500: step 24346, loss 7.59247e-06, acc 1
2017-08-08T18:28:57.898058: step 24347, loss 1.21416e-05, acc 1
2017-08-08T18:28:58.309638: step 24348, loss 8.75441e-08, acc 1
2017-08-08T18:28:58.579736: step 24349, loss 3.59486e-07, acc 1
2017-08-08T18:28:58.969743: step 24350, loss 4.23508e-06, acc 1
2017-08-08T18:28:59.290139: step 24351, loss 1.9687e-06, acc 1
2017-08-08T18:28:59.622379: step 24352, loss 6.70551e-08, acc 1
2017-08-08T18:28:59.994984: step 24353, loss 4.8976e-05, acc 1
2017-08-08T18:29:00.284200: step 24354, loss 2.34692e-07, acc 1
2017-08-08T18:29:00.569372: step 24355, loss 1.06656e-05, acc 1
2017-08-08T18:29:01.008340: step 24356, loss 7.45058e-09, acc 1
2017-08-08T18:29:01.287949: step 24357, loss 3.72529e-09, acc 1
2017-08-08T18:29:01.563805: step 24358, loss 1.86265e-09, acc 1
2017-08-08T18:29:01.844513: step 24359, loss 1.67638e-08, acc 1
2017-08-08T18:29:02.325008: step 24360, loss 7.33868e-07, acc 1
2017-08-08T18:29:02.762840: step 24361, loss 3.72529e-09, acc 1
2017-08-08T18:29:03.190723: step 24362, loss 4.78693e-07, acc 1
2017-08-08T18:29:03.460984: step 24363, loss 1.27893e-05, acc 1
2017-08-08T18:29:03.869779: step 24364, loss 0, acc 1
2017-08-08T18:29:04.141646: step 24365, loss 2.15687e-06, acc 1
2017-08-08T18:29:04.423340: step 24366, loss 4.09735e-06, acc 1
2017-08-08T18:29:04.682733: step 24367, loss 1.67638e-08, acc 1
2017-08-08T18:29:05.168357: step 24368, loss 1.30385e-08, acc 1
2017-08-08T18:29:05.569690: step 24369, loss 0, acc 1
2017-08-08T18:29:05.849357: step 24370, loss 2.09347e-06, acc 1
2017-08-08T18:29:06.037006: step 24371, loss 9.82981e-06, acc 1
2017-08-08T18:29:06.303391: step 24372, loss 1.30385e-08, acc 1
2017-08-08T18:29:06.596916: step 24373, loss 4.65661e-08, acc 1
2017-08-08T18:29:06.794302: step 24374, loss 0, acc 1
2017-08-08T18:29:07.002538: step 24375, loss 8.5379e-06, acc 1
2017-08-08T18:29:07.206041: step 24376, loss 0.000217529, acc 1
2017-08-08T18:29:07.488138: step 24377, loss 2.84983e-07, acc 1
2017-08-08T18:29:07.767512: step 24378, loss 0.000247204, acc 1
2017-08-08T18:29:08.033706: step 24379, loss 0.000886658, acc 1
2017-08-08T18:29:08.253015: step 24380, loss 1.86264e-08, acc 1
2017-08-08T18:29:08.466143: step 24381, loss 4.11639e-07, acc 1
2017-08-08T18:29:08.885577: step 24382, loss 1.25351e-06, acc 1
2017-08-08T18:29:09.181357: step 24383, loss 2.26814e-05, acc 1
2017-08-08T18:29:09.430021: step 24384, loss 7.43206e-06, acc 1
2017-08-08T18:29:09.680455: step 24385, loss 5.58793e-09, acc 1
2017-08-08T18:29:10.121262: step 24386, loss 0, acc 1
2017-08-08T18:29:10.536781: step 24387, loss 9.31322e-09, acc 1
2017-08-08T18:29:10.871761: step 24388, loss 2.97252e-06, acc 1
2017-08-08T18:29:11.132158: step 24389, loss 0.00365559, acc 1
2017-08-08T18:29:11.351907: step 24390, loss 0, acc 1
2017-08-08T18:29:11.734628: step 24391, loss 0.000320769, acc 1
2017-08-08T18:29:11.997845: step 24392, loss 1.67638e-08, acc 1
2017-08-08T18:29:12.249027: step 24393, loss 2.47091e-05, acc 1
2017-08-08T18:29:12.453382: step 24394, loss 1.6205e-07, acc 1
2017-08-08T18:29:12.815448: step 24395, loss 1.0794e-05, acc 1
2017-08-08T18:29:13.153773: step 24396, loss 1.30385e-08, acc 1
2017-08-08T18:29:13.387111: step 24397, loss 0.000544744, acc 1
2017-08-08T18:29:13.585045: step 24398, loss 2.04891e-08, acc 1
2017-08-08T18:29:13.898909: step 24399, loss 5.73685e-07, acc 1
2017-08-08T18:29:14.200927: step 24400, loss 0, acc 1

Evaluation:
2017-08-08T18:29:14.852014: step 24400, loss 8.22604, acc 0.719512

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-24400

2017-08-08T18:29:15.365307: step 24401, loss 0, acc 1
2017-08-08T18:29:15.654729: step 24402, loss 0, acc 1
2017-08-08T18:29:15.945616: step 24403, loss 0.00541125, acc 1
2017-08-08T18:29:16.187244: step 24404, loss 2.09348e-06, acc 1
2017-08-08T18:29:16.587212: step 24405, loss 2.60768e-07, acc 1
2017-08-08T18:29:16.818626: step 24406, loss 8.38188e-08, acc 1
2017-08-08T18:29:17.104085: step 24407, loss 4.33786e-06, acc 1
2017-08-08T18:29:17.412805: step 24408, loss 2.98023e-08, acc 1
2017-08-08T18:29:17.822057: step 24409, loss 7.44716e-06, acc 1
2017-08-08T18:29:18.130015: step 24410, loss 1.47863e-05, acc 1
2017-08-08T18:29:18.411396: step 24411, loss 3.29653e-06, acc 1
2017-08-08T18:29:18.695915: step 24412, loss 0, acc 1
2017-08-08T18:29:18.929798: step 24413, loss 1.12575e-05, acc 1
2017-08-08T18:29:19.237402: step 24414, loss 1.86265e-09, acc 1
2017-08-08T18:29:19.534573: step 24415, loss 0, acc 1
2017-08-08T18:29:19.760940: step 24416, loss 0, acc 1
2017-08-08T18:29:20.011006: step 24417, loss 1.84126e-05, acc 1
2017-08-08T18:29:20.294878: step 24418, loss 0, acc 1
2017-08-08T18:29:20.696843: step 24419, loss 2.5518e-07, acc 1
2017-08-08T18:29:21.104225: step 24420, loss 1.49011e-07, acc 1
2017-08-08T18:29:21.399621: step 24421, loss 0.000122263, acc 1
2017-08-08T18:29:21.717180: step 24422, loss 4.02859e-06, acc 1
2017-08-08T18:29:22.167660: step 24423, loss 5.02906e-07, acc 1
2017-08-08T18:29:22.438192: step 24424, loss 2.56862e-05, acc 1
2017-08-08T18:29:22.713248: step 24425, loss 1.64839e-06, acc 1
2017-08-08T18:29:22.982966: step 24426, loss 2.91865e-05, acc 1
2017-08-08T18:29:23.368430: step 24427, loss 7.82309e-08, acc 1
2017-08-08T18:29:23.735758: step 24428, loss 1.17346e-07, acc 1
2017-08-08T18:29:24.118527: step 24429, loss 6.44462e-07, acc 1
2017-08-08T18:29:24.346929: step 24430, loss 0.000820299, acc 1
2017-08-08T18:29:24.562792: step 24431, loss 1.50456e-05, acc 1
2017-08-08T18:29:25.001743: step 24432, loss 0.000157468, acc 1
2017-08-08T18:29:25.307515: step 24433, loss 1.49012e-08, acc 1
2017-08-08T18:29:25.593613: step 24434, loss 8.38188e-08, acc 1
2017-08-08T18:29:25.897376: step 24435, loss 1.03187e-06, acc 1
2017-08-08T18:29:26.297323: step 24436, loss 0.000135411, acc 1
2017-08-08T18:29:26.681028: step 24437, loss 1.35973e-07, acc 1
2017-08-08T18:29:27.101393: step 24438, loss 4.22205e-06, acc 1
2017-08-08T18:29:27.422335: step 24439, loss 5.58794e-09, acc 1
2017-08-08T18:29:27.669024: step 24440, loss 0, acc 1
2017-08-08T18:29:28.113806: step 24441, loss 0, acc 1
2017-08-08T18:29:28.445404: step 24442, loss 5.2154e-08, acc 1
2017-08-08T18:29:28.770167: step 24443, loss 2.37977e-05, acc 1
2017-08-08T18:29:29.139038: step 24444, loss 0, acc 1
2017-08-08T18:29:29.575468: step 24445, loss 9.0336e-07, acc 1
2017-08-08T18:29:30.016151: step 24446, loss 0, acc 1
2017-08-08T18:29:30.323914: step 24447, loss 0.000469171, acc 1
2017-08-08T18:29:30.560695: step 24448, loss 4.31615e-05, acc 1
2017-08-08T18:29:30.857476: step 24449, loss 0.0061693, acc 1
2017-08-08T18:29:31.265168: step 24450, loss 1.39077e-08, acc 1
2017-08-08T18:29:31.509385: step 24451, loss 4.74968e-07, acc 1
2017-08-08T18:29:31.823839: step 24452, loss 0, acc 1
2017-08-08T18:29:32.222876: step 24453, loss 0, acc 1
2017-08-08T18:29:32.612736: step 24454, loss 0, acc 1
2017-08-08T18:29:32.915819: step 24455, loss 2.99301e-05, acc 1
2017-08-08T18:29:33.112578: step 24456, loss 1.11759e-08, acc 1
2017-08-08T18:29:33.334552: step 24457, loss 0.000108007, acc 1
2017-08-08T18:29:33.739464: step 24458, loss 2.04891e-08, acc 1
2017-08-08T18:29:33.978741: step 24459, loss 9.31322e-09, acc 1
2017-08-08T18:29:34.189940: step 24460, loss 1.92723e-05, acc 1
2017-08-08T18:29:34.367545: step 24461, loss 1.86265e-09, acc 1
2017-08-08T18:29:34.653394: step 24462, loss 9.15981e-06, acc 1
2017-08-08T18:29:34.953907: step 24463, loss 0, acc 1
2017-08-08T18:29:35.203268: step 24464, loss 1.63912e-07, acc 1
2017-08-08T18:29:35.403498: step 24465, loss 0, acc 1
2017-08-08T18:29:35.701125: step 24466, loss 0.0439421, acc 0.984375
2017-08-08T18:29:35.951915: step 24467, loss 0, acc 1
2017-08-08T18:29:36.172617: step 24468, loss 0.00174557, acc 1
2017-08-08T18:29:36.419331: step 24469, loss 1.86265e-09, acc 1
2017-08-08T18:29:36.779834: step 24470, loss 1.51427e-06, acc 1
2017-08-08T18:29:37.181400: step 24471, loss 0, acc 1
2017-08-08T18:29:37.449752: step 24472, loss 6.83576e-07, acc 1
2017-08-08T18:29:37.651740: step 24473, loss 1.37292e-05, acc 1
2017-08-08T18:29:37.877585: step 24474, loss 7.45058e-09, acc 1
2017-08-08T18:29:38.187546: step 24475, loss 5.77314e-06, acc 1
2017-08-08T18:29:38.400363: step 24476, loss 2.48533e-05, acc 1
2017-08-08T18:29:38.620435: step 24477, loss 3.70662e-07, acc 1
2017-08-08T18:29:38.805693: step 24478, loss 6.40299e-06, acc 1
2017-08-08T18:29:39.152254: step 24479, loss 3.72529e-09, acc 1
2017-08-08T18:29:39.453777: step 24480, loss 1.01521e-05, acc 1
2017-08-08T18:29:39.670450: step 24481, loss 1.49012e-08, acc 1
2017-08-08T18:29:39.833469: step 24482, loss 2.42144e-08, acc 1
2017-08-08T18:29:40.049379: step 24483, loss 1.34105e-06, acc 1
2017-08-08T18:29:40.389715: step 24484, loss 9.34954e-06, acc 1
2017-08-08T18:29:40.664195: step 24485, loss 0.00584465, acc 1
2017-08-08T18:29:40.970133: step 24486, loss 1.30385e-08, acc 1
2017-08-08T18:29:41.227025: step 24487, loss 6.44466e-07, acc 1
2017-08-08T18:29:41.601279: step 24488, loss 1.86264e-08, acc 1
2017-08-08T18:29:41.853349: step 24489, loss 4.08074e-06, acc 1
2017-08-08T18:29:42.140889: step 24490, loss 2.98023e-08, acc 1
2017-08-08T18:29:42.345377: step 24491, loss 0.187441, acc 0.984375
2017-08-08T18:29:42.593281: step 24492, loss 0, acc 1
2017-08-08T18:29:42.797706: step 24493, loss 0.00862294, acc 1
2017-08-08T18:29:43.010694: step 24494, loss 1.3411e-07, acc 1
2017-08-08T18:29:43.213407: step 24495, loss 7.45058e-09, acc 1
2017-08-08T18:29:43.431752: step 24496, loss 0.000268811, acc 1
2017-08-08T18:29:43.793547: step 24497, loss 3.31548e-07, acc 1
2017-08-08T18:29:44.185733: step 24498, loss 2.04891e-08, acc 1
2017-08-08T18:29:44.447929: step 24499, loss 2.6077e-08, acc 1
2017-08-08T18:29:44.655251: step 24500, loss 0, acc 1

Evaluation:
2017-08-08T18:29:45.276341: step 24500, loss 8.2208, acc 0.714822

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-24500

2017-08-08T18:29:45.760794: step 24501, loss 2.60768e-07, acc 1
2017-08-08T18:29:45.983113: step 24502, loss 7.40229e-06, acc 1
2017-08-08T18:29:46.301550: step 24503, loss 3.36348e-05, acc 1
2017-08-08T18:29:46.549735: step 24504, loss 3.72529e-09, acc 1
2017-08-08T18:29:46.864056: step 24505, loss 0, acc 1
2017-08-08T18:29:47.110642: step 24506, loss 1.30385e-08, acc 1
2017-08-08T18:29:47.331447: step 24507, loss 5.58793e-09, acc 1
2017-08-08T18:29:47.533367: step 24508, loss 4.09781e-08, acc 1
2017-08-08T18:29:47.930855: step 24509, loss 0.000147179, acc 1
2017-08-08T18:29:48.136145: step 24510, loss 2.19433e-05, acc 1
2017-08-08T18:29:48.323268: step 24511, loss 1.44009e-05, acc 1
2017-08-08T18:29:48.588624: step 24512, loss 0, acc 1
2017-08-08T18:29:48.924394: step 24513, loss 5.62509e-07, acc 1
2017-08-08T18:29:49.203974: step 24514, loss 8.00935e-08, acc 1
2017-08-08T18:29:49.446381: step 24515, loss 4.57039e-06, acc 1
2017-08-08T18:29:49.621418: step 24516, loss 1.86265e-09, acc 1
2017-08-08T18:29:50.079035: step 24517, loss 8.67925e-05, acc 1
2017-08-08T18:29:50.294803: step 24518, loss 1.3411e-07, acc 1
2017-08-08T18:29:50.507968: step 24519, loss 3.72529e-09, acc 1
2017-08-08T18:29:50.734404: step 24520, loss 9.82005e-05, acc 1
2017-08-08T18:29:51.113700: step 24521, loss 5.58794e-09, acc 1
2017-08-08T18:29:51.477469: step 24522, loss 1.86265e-09, acc 1
2017-08-08T18:29:51.691490: step 24523, loss 0, acc 1
2017-08-08T18:29:51.883649: step 24524, loss 1.99483e-06, acc 1
2017-08-08T18:29:52.077160: step 24525, loss 1.04308e-07, acc 1
2017-08-08T18:29:52.499184: step 24526, loss 0.00674228, acc 1
2017-08-08T18:29:52.690721: step 24527, loss 0, acc 1
2017-08-08T18:29:52.901913: step 24528, loss 0.000365848, acc 1
2017-08-08T18:29:53.100549: step 24529, loss 1.63912e-07, acc 1
2017-08-08T18:29:53.382136: step 24530, loss 2.96319e-06, acc 1
2017-08-08T18:29:53.701320: step 24531, loss 1.30385e-08, acc 1
2017-08-08T18:29:54.031152: step 24532, loss 9.12695e-08, acc 1
2017-08-08T18:29:54.212243: step 24533, loss 5.92315e-07, acc 1
2017-08-08T18:29:54.385533: step 24534, loss 9.31322e-09, acc 1
2017-08-08T18:29:54.762773: step 24535, loss 2.44739e-05, acc 1
2017-08-08T18:29:54.983592: step 24536, loss 0, acc 1
2017-08-08T18:29:55.224439: step 24537, loss 2.25379e-07, acc 1
2017-08-08T18:29:55.442451: step 24538, loss 1.54599e-07, acc 1
2017-08-08T18:29:55.696081: step 24539, loss 3.4086e-07, acc 1
2017-08-08T18:29:56.065185: step 24540, loss 1.86265e-09, acc 1
2017-08-08T18:29:56.500425: step 24541, loss 6.4775e-05, acc 1
2017-08-08T18:29:56.746534: step 24542, loss 0, acc 1
2017-08-08T18:29:57.113805: step 24543, loss 9.31322e-09, acc 1
2017-08-08T18:29:57.507784: step 24544, loss 6.2651e-06, acc 1
2017-08-08T18:29:57.783315: step 24545, loss 0, acc 1
2017-08-08T18:29:58.073010: step 24546, loss 5.58793e-09, acc 1
2017-08-08T18:29:58.338563: step 24547, loss 0.000498375, acc 1
2017-08-08T18:29:58.797361: step 24548, loss 5.87932e-06, acc 1
2017-08-08T18:29:59.203190: step 24549, loss 3.27824e-07, acc 1
2017-08-08T18:29:59.585497: step 24550, loss 0.019535, acc 0.984375
2017-08-08T18:29:59.858120: step 24551, loss 7.86016e-07, acc 1
2017-08-08T18:30:00.223229: step 24552, loss 4.59119e-05, acc 1
2017-08-08T18:30:00.624572: step 24553, loss 1.00253e-05, acc 1
2017-08-08T18:30:00.907527: step 24554, loss 2.79397e-08, acc 1
2017-08-08T18:30:01.190199: step 24555, loss 4.69381e-07, acc 1
2017-08-08T18:30:01.622523: step 24556, loss 1.49012e-08, acc 1
2017-08-08T18:30:02.237410: step 24557, loss 2.84037e-06, acc 1
2017-08-08T18:30:02.703866: step 24558, loss 0.000222016, acc 1
2017-08-08T18:30:03.095952: step 24559, loss 0, acc 1
2017-08-08T18:30:03.388653: step 24560, loss 0, acc 1
2017-08-08T18:30:03.839643: step 24561, loss 1.42113e-06, acc 1
2017-08-08T18:30:04.306996: step 24562, loss 5.06933e-06, acc 1
2017-08-08T18:30:04.616221: step 24563, loss 0.000315309, acc 1
2017-08-08T18:30:04.883688: step 24564, loss 3.53902e-08, acc 1
2017-08-08T18:30:05.221419: step 24565, loss 0, acc 1
2017-08-08T18:30:05.573338: step 24566, loss 7.54247e-05, acc 1
2017-08-08T18:30:05.985399: step 24567, loss 8.04651e-07, acc 1
2017-08-08T18:30:06.297879: step 24568, loss 8.3819e-08, acc 1
2017-08-08T18:30:06.512315: step 24569, loss 3.861e-05, acc 1
2017-08-08T18:30:06.726775: step 24570, loss 2.95625e-05, acc 1
2017-08-08T18:30:07.109921: step 24571, loss 1.08536e-05, acc 1
2017-08-08T18:30:07.397479: step 24572, loss 2.51251e-06, acc 1
2017-08-08T18:30:07.704049: step 24573, loss 0, acc 1
2017-08-08T18:30:08.034971: step 24574, loss 1.99302e-07, acc 1
2017-08-08T18:30:08.450551: step 24575, loss 1.78813e-07, acc 1
2017-08-08T18:30:08.831162: step 24576, loss 7.45058e-09, acc 1
2017-08-08T18:30:09.162640: step 24577, loss 0, acc 1
2017-08-08T18:30:09.527876: step 24578, loss 1.08182e-05, acc 1
2017-08-08T18:30:09.731278: step 24579, loss 1.70742e-05, acc 1
2017-08-08T18:30:10.014699: step 24580, loss 1.86265e-09, acc 1
2017-08-08T18:30:10.324922: step 24581, loss 1.86265e-09, acc 1
2017-08-08T18:30:10.579922: step 24582, loss 5.58794e-09, acc 1
2017-08-08T18:30:10.879617: step 24583, loss 5.44957e-05, acc 1
2017-08-08T18:30:11.135862: step 24584, loss 1.67638e-08, acc 1
2017-08-08T18:30:11.588436: step 24585, loss 0, acc 1
2017-08-08T18:30:11.885026: step 24586, loss 3.72529e-09, acc 1
2017-08-08T18:30:12.233646: step 24587, loss 1.86264e-08, acc 1
2017-08-08T18:30:12.445137: step 24588, loss 5.44495e-05, acc 1
2017-08-08T18:30:12.659709: step 24589, loss 4.47034e-08, acc 1
2017-08-08T18:30:13.019692: step 24590, loss 0.000145337, acc 1
2017-08-08T18:30:13.224670: step 24591, loss 4.28408e-08, acc 1
2017-08-08T18:30:13.435604: step 24592, loss 4.09805e-05, acc 1
2017-08-08T18:30:13.637258: step 24593, loss 0, acc 1
2017-08-08T18:30:13.986925: step 24594, loss 2.86846e-07, acc 1
2017-08-08T18:30:14.358176: step 24595, loss 1.82538e-07, acc 1
2017-08-08T18:30:14.716003: step 24596, loss 5.40167e-08, acc 1
2017-08-08T18:30:15.037963: step 24597, loss 2.57396e-06, acc 1
2017-08-08T18:30:15.296476: step 24598, loss 3.72529e-09, acc 1
2017-08-08T18:30:15.623674: step 24599, loss 1.21626e-06, acc 1
2017-08-08T18:30:15.862874: step 24600, loss 8.52526e-06, acc 1

Evaluation:
2017-08-08T18:30:16.508187: step 24600, loss 8.30015, acc 0.707317

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-24600

2017-08-08T18:30:17.056082: step 24601, loss 1.07716e-05, acc 1
2017-08-08T18:30:17.496551: step 24602, loss 5.97529e-05, acc 1
2017-08-08T18:30:17.829890: step 24603, loss 0, acc 1
2017-08-08T18:30:18.076962: step 24604, loss 1.86265e-09, acc 1
2017-08-08T18:30:18.315262: step 24605, loss 1.30385e-08, acc 1
2017-08-08T18:30:18.680319: step 24606, loss 1.68935e-06, acc 1
2017-08-08T18:30:18.977647: step 24607, loss 0, acc 1
2017-08-08T18:30:19.298351: step 24608, loss 0.000123586, acc 1
2017-08-08T18:30:19.512849: step 24609, loss 7.02814e-06, acc 1
2017-08-08T18:30:19.785355: step 24610, loss 0.000404997, acc 1
2017-08-08T18:30:20.133967: step 24611, loss 7.49065e-06, acc 1
2017-08-08T18:30:20.470561: step 24612, loss 8.41875e-06, acc 1
2017-08-08T18:30:20.746863: step 24613, loss 1.5422e-06, acc 1
2017-08-08T18:30:20.947326: step 24614, loss 0.0699257, acc 0.984375
2017-08-08T18:30:21.404407: step 24615, loss 3.86461e-06, acc 1
2017-08-08T18:30:21.634027: step 24616, loss 8.64244e-07, acc 1
2017-08-08T18:30:21.933121: step 24617, loss 6.25837e-07, acc 1
2017-08-08T18:30:22.213826: step 24618, loss 1.67638e-08, acc 1
2017-08-08T18:30:22.562655: step 24619, loss 7.02067e-06, acc 1
2017-08-08T18:30:22.898668: step 24620, loss 5.77419e-08, acc 1
2017-08-08T18:30:23.261391: step 24621, loss 7.45058e-09, acc 1
2017-08-08T18:30:23.526133: step 24622, loss 1.4156e-07, acc 1
2017-08-08T18:30:23.765913: step 24623, loss 2.49593e-07, acc 1
2017-08-08T18:30:24.189451: step 24624, loss 2.51455e-07, acc 1
2017-08-08T18:30:24.445945: step 24625, loss 0, acc 1
2017-08-08T18:30:24.702747: step 24626, loss 0.000203512, acc 1
2017-08-08T18:30:24.934395: step 24627, loss 1.22372e-06, acc 1
2017-08-08T18:30:25.269056: step 24628, loss 3.72529e-09, acc 1
2017-08-08T18:30:25.593699: step 24629, loss 1.75088e-07, acc 1
2017-08-08T18:30:26.009685: step 24630, loss 5.58784e-07, acc 1
2017-08-08T18:30:26.280568: step 24631, loss 0.0111334, acc 0.984375
2017-08-08T18:30:26.541312: step 24632, loss 1.86264e-08, acc 1
2017-08-08T18:30:26.937369: step 24633, loss 2.01127e-05, acc 1
2017-08-08T18:30:27.180732: step 24634, loss 2.52947e-05, acc 1
2017-08-08T18:30:27.466107: step 24635, loss 0, acc 1
2017-08-08T18:30:27.757323: step 24636, loss 7.45058e-09, acc 1
2017-08-08T18:30:28.166159: step 24637, loss 8.58656e-07, acc 1
2017-08-08T18:30:28.596708: step 24638, loss 4.17227e-07, acc 1
2017-08-08T18:30:28.854878: step 24639, loss 5.96045e-08, acc 1
2017-08-08T18:30:29.048998: step 24640, loss 0, acc 1
2017-08-08T18:30:29.442550: step 24641, loss 0.0323339, acc 0.984375
2017-08-08T18:30:29.652617: step 24642, loss 1.66325e-06, acc 1
2017-08-08T18:30:29.859911: step 24643, loss 7.45056e-08, acc 1
2017-08-08T18:30:30.063254: step 24644, loss 0.000109098, acc 1
2017-08-08T18:30:30.344222: step 24645, loss 5.16634e-06, acc 1
2017-08-08T18:30:30.627149: step 24646, loss 0, acc 1
2017-08-08T18:30:31.023587: step 24647, loss 2.04891e-08, acc 1
2017-08-08T18:30:31.330765: step 24648, loss 5.58793e-09, acc 1
2017-08-08T18:30:31.571148: step 24649, loss 0, acc 1
2017-08-08T18:30:31.821704: step 24650, loss 6.33298e-08, acc 1
2017-08-08T18:30:32.101824: step 24651, loss 0, acc 1
2017-08-08T18:30:32.355124: step 24652, loss 1.67638e-08, acc 1
2017-08-08T18:30:32.628440: step 24653, loss 1.28333e-06, acc 1
2017-08-08T18:30:32.965473: step 24654, loss 2.64287e-06, acc 1
2017-08-08T18:30:33.306115: step 24655, loss 6.34598e-05, acc 1
2017-08-08T18:30:33.623100: step 24656, loss 2.48039e-05, acc 1
2017-08-08T18:30:33.816748: step 24657, loss 4.89872e-07, acc 1
2017-08-08T18:30:33.972656: step 24658, loss 2.9057e-07, acc 1
2017-08-08T18:30:34.221306: step 24659, loss 0.000380637, acc 1
2017-08-08T18:30:34.393766: step 24660, loss 0, acc 1
2017-08-08T18:30:34.569031: step 24661, loss 7.45058e-09, acc 1
2017-08-08T18:30:34.759259: step 24662, loss 8.82869e-07, acc 1
2017-08-08T18:30:35.010912: step 24663, loss 3.79371e-05, acc 1
2017-08-08T18:30:35.232824: step 24664, loss 0, acc 1
2017-08-08T18:30:35.422732: step 24665, loss 2.08615e-07, acc 1
2017-08-08T18:30:35.596574: step 24666, loss 5.32708e-07, acc 1
2017-08-08T18:30:35.772294: step 24667, loss 2.90571e-07, acc 1
2017-08-08T18:30:36.037493: step 24668, loss 3.16649e-08, acc 1
2017-08-08T18:30:36.204842: step 24669, loss 7.82488e-06, acc 1
2017-08-08T18:30:36.422686: step 24670, loss 1.24792e-06, acc 1
2017-08-08T18:30:36.631235: step 24671, loss 0, acc 1
2017-08-08T18:30:36.972439: step 24672, loss 4.05014e-05, acc 1
2017-08-08T18:30:37.245134: step 24673, loss 5.58793e-08, acc 1
2017-08-08T18:30:37.485062: step 24674, loss 0.0583807, acc 0.984375
2017-08-08T18:30:37.700468: step 24675, loss 8.3819e-08, acc 1
2017-08-08T18:30:37.912559: step 24676, loss 5.58793e-09, acc 1
2017-08-08T18:30:38.263375: step 24677, loss 1.86265e-09, acc 1
2017-08-08T18:30:38.552518: step 24678, loss 0.000137967, acc 1
2017-08-08T18:30:38.836242: step 24679, loss 2.98023e-08, acc 1
2017-08-08T18:30:39.114157: step 24680, loss 4.47034e-08, acc 1
2017-08-08T18:30:39.502979: step 24681, loss 6.89177e-08, acc 1
2017-08-08T18:30:39.848294: step 24682, loss 6.18386e-07, acc 1
2017-08-08T18:30:40.159902: step 24683, loss 9.38153e-06, acc 1
2017-08-08T18:30:40.374860: step 24684, loss 1.51509e-05, acc 1
2017-08-08T18:30:40.674914: step 24685, loss 1.86264e-07, acc 1
2017-08-08T18:30:40.939314: step 24686, loss 0, acc 1
2017-08-08T18:30:41.163600: step 24687, loss 0.0217093, acc 0.984375
2017-08-08T18:30:41.364234: step 24688, loss 1.90912e-06, acc 1
2017-08-08T18:30:41.689373: step 24689, loss 4.47034e-08, acc 1
2017-08-08T18:30:42.010774: step 24690, loss 0, acc 1
2017-08-08T18:30:42.297676: step 24691, loss 0.000194673, acc 1
2017-08-08T18:30:42.503959: step 24692, loss 1.86264e-08, acc 1
2017-08-08T18:30:42.771967: step 24693, loss 1.04308e-07, acc 1
2017-08-08T18:30:43.157882: step 24694, loss 1.69204e-05, acc 1
2017-08-08T18:30:43.408604: step 24695, loss 3.08782e-05, acc 1
2017-08-08T18:30:43.636455: step 24696, loss 4.54481e-07, acc 1
2017-08-08T18:30:44.076078: step 24697, loss 7.2643e-08, acc 1
2017-08-08T18:30:44.493522: step 24698, loss 0, acc 1
2017-08-08T18:30:44.901360: step 24699, loss 3.53902e-08, acc 1
2017-08-08T18:30:45.164620: step 24700, loss 1.22934e-07, acc 1

Evaluation:
2017-08-08T18:30:45.796318: step 24700, loss 8.28539, acc 0.719512

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-24700

2017-08-08T18:30:46.216457: step 24701, loss 0, acc 1
2017-08-08T18:30:46.474531: step 24702, loss 9.164e-07, acc 1
2017-08-08T18:30:46.685744: step 24703, loss 3.72529e-09, acc 1
2017-08-08T18:30:47.008371: step 24704, loss 1.54616e-05, acc 1
2017-08-08T18:30:47.333348: step 24705, loss 2.42143e-07, acc 1
2017-08-08T18:30:47.632889: step 24706, loss 0, acc 1
2017-08-08T18:30:47.836514: step 24707, loss 5.58793e-09, acc 1
2017-08-08T18:30:48.173415: step 24708, loss 1.82538e-07, acc 1
2017-08-08T18:30:48.381937: step 24709, loss 9.61109e-07, acc 1
2017-08-08T18:30:48.613335: step 24710, loss 4.38827e-05, acc 1
2017-08-08T18:30:48.841030: step 24711, loss 1.49011e-07, acc 1
2017-08-08T18:30:49.234738: step 24712, loss 1.11758e-07, acc 1
2017-08-08T18:30:49.526651: step 24713, loss 1.26758e-05, acc 1
2017-08-08T18:30:49.815105: step 24714, loss 2.75669e-07, acc 1
2017-08-08T18:30:50.049833: step 24715, loss 0, acc 1
2017-08-08T18:30:50.352729: step 24716, loss 1.86265e-09, acc 1
2017-08-08T18:30:50.741367: step 24717, loss 7.33873e-07, acc 1
2017-08-08T18:30:50.942405: step 24718, loss 1.07419e-05, acc 1
2017-08-08T18:30:51.180047: step 24719, loss 1.86265e-09, acc 1
2017-08-08T18:30:51.467207: step 24720, loss 8.28377e-06, acc 1
2017-08-08T18:30:51.781405: step 24721, loss 2.44005e-07, acc 1
2017-08-08T18:30:51.991574: step 24722, loss 5.04984e-05, acc 1
2017-08-08T18:30:52.211210: step 24723, loss 0.000107353, acc 1
2017-08-08T18:30:52.402579: step 24724, loss 3.85563e-07, acc 1
2017-08-08T18:30:52.644158: step 24725, loss 9.87199e-08, acc 1
2017-08-08T18:30:52.938021: step 24726, loss 9.3132e-08, acc 1
2017-08-08T18:30:53.123255: step 24727, loss 1.79926e-06, acc 1
2017-08-08T18:30:53.361516: step 24728, loss 0.000141583, acc 1
2017-08-08T18:30:53.780401: step 24729, loss 4.41198e-06, acc 1
2017-08-08T18:30:54.167730: step 24730, loss 3.63074e-05, acc 1
2017-08-08T18:30:54.512869: step 24731, loss 3.23591e-05, acc 1
2017-08-08T18:30:54.839131: step 24732, loss 7.11555e-06, acc 1
2017-08-08T18:30:55.080884: step 24733, loss 9.91612e-06, acc 1
2017-08-08T18:30:55.417390: step 24734, loss 4.28408e-08, acc 1
2017-08-08T18:30:55.794283: step 24735, loss 0.000178695, acc 1
2017-08-08T18:30:56.048747: step 24736, loss 5.58793e-09, acc 1
2017-08-08T18:30:56.296122: step 24737, loss 3.72529e-09, acc 1
2017-08-08T18:30:56.625146: step 24738, loss 0, acc 1
2017-08-08T18:30:56.965394: step 24739, loss 3.24111e-05, acc 1
2017-08-08T18:30:57.206073: step 24740, loss 2.44732e-06, acc 1
2017-08-08T18:30:57.366655: step 24741, loss 5.58794e-09, acc 1
2017-08-08T18:30:57.585349: step 24742, loss 2.62631e-07, acc 1
2017-08-08T18:30:57.888237: step 24743, loss 1.41182e-06, acc 1
2017-08-08T18:30:58.135131: step 24744, loss 0.000762065, acc 1
2017-08-08T18:30:58.387067: step 24745, loss 5.02913e-08, acc 1
2017-08-08T18:30:58.642841: step 24746, loss 1.86265e-09, acc 1
2017-08-08T18:30:58.925051: step 24747, loss 0, acc 1
2017-08-08T18:30:59.132271: step 24748, loss 2.71944e-07, acc 1
2017-08-08T18:30:59.350809: step 24749, loss 2.79397e-08, acc 1
2017-08-08T18:30:59.552657: step 24750, loss 7.94728e-09, acc 1
2017-08-08T18:30:59.875736: step 24751, loss 3.12921e-07, acc 1
2017-08-08T18:31:00.161816: step 24752, loss 1.62256e-05, acc 1
2017-08-08T18:31:00.435043: step 24753, loss 8.17682e-05, acc 1
2017-08-08T18:31:00.705116: step 24754, loss 3.72529e-09, acc 1
2017-08-08T18:31:01.130249: step 24755, loss 0, acc 1
2017-08-08T18:31:01.555038: step 24756, loss 4.46287e-05, acc 1
2017-08-08T18:31:01.912255: step 24757, loss 1.86264e-08, acc 1
2017-08-08T18:31:02.193008: step 24758, loss 1.626e-06, acc 1
2017-08-08T18:31:02.589370: step 24759, loss 0, acc 1
2017-08-08T18:31:02.968687: step 24760, loss 2.43803e-06, acc 1
2017-08-08T18:31:03.288086: step 24761, loss 0, acc 1
2017-08-08T18:31:03.608144: step 24762, loss 6.78076e-05, acc 1
2017-08-08T18:31:03.944250: step 24763, loss 0.000374612, acc 1
2017-08-08T18:31:04.357307: step 24764, loss 7.65535e-07, acc 1
2017-08-08T18:31:04.742372: step 24765, loss 0, acc 1
2017-08-08T18:31:05.076576: step 24766, loss 2.37469e-06, acc 1
2017-08-08T18:31:05.339091: step 24767, loss 4.73976e-06, acc 1
2017-08-08T18:31:05.708466: step 24768, loss 4.28408e-08, acc 1
2017-08-08T18:31:06.085854: step 24769, loss 0.00048544, acc 1
2017-08-08T18:31:06.324470: step 24770, loss 1.37013e-05, acc 1
2017-08-08T18:31:06.589924: step 24771, loss 2.50648e-05, acc 1
2017-08-08T18:31:06.822784: step 24772, loss 5.58793e-09, acc 1
2017-08-08T18:31:07.155854: step 24773, loss 1.86264e-08, acc 1
2017-08-08T18:31:07.563101: step 24774, loss 4.73601e-06, acc 1
2017-08-08T18:31:07.854571: step 24775, loss 2.79395e-07, acc 1
2017-08-08T18:31:08.055993: step 24776, loss 0, acc 1
2017-08-08T18:31:08.399449: step 24777, loss 0, acc 1
2017-08-08T18:31:08.654874: step 24778, loss 1.14362e-06, acc 1
2017-08-08T18:31:08.892876: step 24779, loss 0, acc 1
2017-08-08T18:31:09.128919: step 24780, loss 6.70551e-08, acc 1
2017-08-08T18:31:09.500421: step 24781, loss 7.07804e-08, acc 1
2017-08-08T18:31:09.725362: step 24782, loss 1.49012e-08, acc 1
2017-08-08T18:31:09.959059: step 24783, loss 1.7695e-07, acc 1
2017-08-08T18:31:10.203419: step 24784, loss 5.58793e-09, acc 1
2017-08-08T18:31:10.626129: step 24785, loss 1.17158e-06, acc 1
2017-08-08T18:31:10.833379: step 24786, loss 2.35248e-05, acc 1
2017-08-08T18:31:11.070601: step 24787, loss 2.782e-05, acc 1
2017-08-08T18:31:11.364375: step 24788, loss 2.86845e-07, acc 1
2017-08-08T18:31:11.625349: step 24789, loss 7.3759e-07, acc 1
2017-08-08T18:31:11.919547: step 24790, loss 1.30385e-08, acc 1
2017-08-08T18:31:12.174538: step 24791, loss 2.0935e-06, acc 1
2017-08-08T18:31:12.340039: step 24792, loss 7.69643e-06, acc 1
2017-08-08T18:31:12.567643: step 24793, loss 0, acc 1
2017-08-08T18:31:12.818965: step 24794, loss 3.72529e-09, acc 1
2017-08-08T18:31:13.043082: step 24795, loss 7.56216e-07, acc 1
2017-08-08T18:31:13.319972: step 24796, loss 0.000847438, acc 1
2017-08-08T18:31:13.619827: step 24797, loss 1.62775e-05, acc 1
2017-08-08T18:31:14.076801: step 24798, loss 3.72529e-09, acc 1
2017-08-08T18:31:14.429917: step 24799, loss 1.22428e-05, acc 1
2017-08-08T18:31:14.813759: step 24800, loss 1.37835e-07, acc 1

Evaluation:
2017-08-08T18:31:15.534654: step 24800, loss 8.32749, acc 0.716698

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-24800

2017-08-08T18:31:16.138494: step 24801, loss 2.98023e-08, acc 1
2017-08-08T18:31:16.441808: step 24802, loss 1.45286e-07, acc 1
2017-08-08T18:31:16.743885: step 24803, loss 0.00561878, acc 1
2017-08-08T18:31:17.044475: step 24804, loss 4.44179e-06, acc 1
2017-08-08T18:31:17.457319: step 24805, loss 3.35276e-08, acc 1
2017-08-08T18:31:17.910841: step 24806, loss 1.30385e-08, acc 1
2017-08-08T18:31:18.136727: step 24807, loss 0, acc 1
2017-08-08T18:31:18.375316: step 24808, loss 2.4028e-07, acc 1
2017-08-08T18:31:18.859721: step 24809, loss 2.99884e-07, acc 1
2017-08-08T18:31:19.146807: step 24810, loss 9.49946e-08, acc 1
2017-08-08T18:31:19.463724: step 24811, loss 5.02914e-08, acc 1
2017-08-08T18:31:19.753165: step 24812, loss 2.04891e-08, acc 1
2017-08-08T18:31:20.005275: step 24813, loss 1.62787e-06, acc 1
2017-08-08T18:31:20.392019: step 24814, loss 7.45058e-09, acc 1
2017-08-08T18:31:20.673389: step 24815, loss 1.86264e-08, acc 1
2017-08-08T18:31:20.933524: step 24816, loss 6.2857e-05, acc 1
2017-08-08T18:31:21.127964: step 24817, loss 3.72529e-09, acc 1
2017-08-08T18:31:21.472487: step 24818, loss 0.00440691, acc 1
2017-08-08T18:31:21.673864: step 24819, loss 1.72516e-05, acc 1
2017-08-08T18:31:21.898225: step 24820, loss 0, acc 1
2017-08-08T18:31:22.084536: step 24821, loss 2.75669e-07, acc 1
2017-08-08T18:31:22.340115: step 24822, loss 3.82751e-06, acc 1
2017-08-08T18:31:22.654314: step 24823, loss 1.86265e-09, acc 1
2017-08-08T18:31:22.887063: step 24824, loss 2.48271e-06, acc 1
2017-08-08T18:31:23.087895: step 24825, loss 1.11011e-06, acc 1
2017-08-08T18:31:23.373745: step 24826, loss 3.14784e-07, acc 1
2017-08-08T18:31:23.680310: step 24827, loss 0, acc 1
2017-08-08T18:31:23.960290: step 24828, loss 0, acc 1
2017-08-08T18:31:24.211556: step 24829, loss 5.58793e-09, acc 1
2017-08-08T18:31:24.549366: step 24830, loss 1.67638e-08, acc 1
2017-08-08T18:31:24.853408: step 24831, loss 3.72529e-09, acc 1
2017-08-08T18:31:25.192203: step 24832, loss 0, acc 1
2017-08-08T18:31:25.512298: step 24833, loss 0, acc 1
2017-08-08T18:31:25.736926: step 24834, loss 1.13621e-07, acc 1
2017-08-08T18:31:25.993371: step 24835, loss 1.18648e-06, acc 1
2017-08-08T18:31:26.434462: step 24836, loss 0.00180144, acc 1
2017-08-08T18:31:26.675062: step 24837, loss 8.78553e-06, acc 1
2017-08-08T18:31:26.921128: step 24838, loss 4.89869e-07, acc 1
2017-08-08T18:31:27.206780: step 24839, loss 0.00160395, acc 1
2017-08-08T18:31:27.658785: step 24840, loss 6.76397e-05, acc 1
2017-08-08T18:31:28.051782: step 24841, loss 0.00159816, acc 1
2017-08-08T18:31:28.405547: step 24842, loss 5.58794e-09, acc 1
2017-08-08T18:31:28.680079: step 24843, loss 1.63912e-07, acc 1
2017-08-08T18:31:28.953107: step 24844, loss 0.000139508, acc 1
2017-08-08T18:31:29.375842: step 24845, loss 3.01746e-07, acc 1
2017-08-08T18:31:29.675042: step 24846, loss 1.93294e-05, acc 1
2017-08-08T18:31:29.943882: step 24847, loss 5.97899e-07, acc 1
2017-08-08T18:31:30.209407: step 24848, loss 1.67638e-08, acc 1
2017-08-08T18:31:30.510463: step 24849, loss 7.74841e-07, acc 1
2017-08-08T18:31:30.954019: step 24850, loss 0, acc 1
2017-08-08T18:31:31.316990: step 24851, loss 3.05579e-05, acc 1
2017-08-08T18:31:31.545162: step 24852, loss 2.99883e-07, acc 1
2017-08-08T18:31:31.757329: step 24853, loss 2.25737e-05, acc 1
2017-08-08T18:31:32.065390: step 24854, loss 0, acc 1
2017-08-08T18:31:32.354111: step 24855, loss 1.11759e-08, acc 1
2017-08-08T18:31:32.640047: step 24856, loss 3.22767e-06, acc 1
2017-08-08T18:31:32.952776: step 24857, loss 3.67296e-06, acc 1
2017-08-08T18:31:33.348606: step 24858, loss 3.72529e-09, acc 1
2017-08-08T18:31:33.699652: step 24859, loss 2.43622e-06, acc 1
2017-08-08T18:31:33.973823: step 24860, loss 9.31322e-09, acc 1
2017-08-08T18:31:34.227249: step 24861, loss 1.09896e-07, acc 1
2017-08-08T18:31:34.665125: step 24862, loss 3.6709e-05, acc 1
2017-08-08T18:31:34.914804: step 24863, loss 6.33299e-08, acc 1
2017-08-08T18:31:35.184325: step 24864, loss 7.9887e-06, acc 1
2017-08-08T18:31:35.430804: step 24865, loss 2.29104e-07, acc 1
2017-08-08T18:31:35.759337: step 24866, loss 0.000672915, acc 1
2017-08-08T18:31:36.025476: step 24867, loss 7.45058e-09, acc 1
2017-08-08T18:31:36.246657: step 24868, loss 0, acc 1
2017-08-08T18:31:36.423008: step 24869, loss 1.58324e-07, acc 1
2017-08-08T18:31:36.645810: step 24870, loss 0, acc 1
2017-08-08T18:31:36.947683: step 24871, loss 8.19563e-08, acc 1
2017-08-08T18:31:37.243628: step 24872, loss 1.86265e-09, acc 1
2017-08-08T18:31:37.519965: step 24873, loss 1.86265e-09, acc 1
2017-08-08T18:31:37.807809: step 24874, loss 1.49011e-07, acc 1
2017-08-08T18:31:38.223037: step 24875, loss 0, acc 1
2017-08-08T18:31:38.614968: step 24876, loss 5.69959e-07, acc 1
2017-08-08T18:31:38.850350: step 24877, loss 0, acc 1
2017-08-08T18:31:39.062897: step 24878, loss 9.31322e-09, acc 1
2017-08-08T18:31:39.249396: step 24879, loss 0, acc 1
2017-08-08T18:31:39.484392: step 24880, loss 1.16225e-06, acc 1
2017-08-08T18:31:39.765272: step 24881, loss 0.00029243, acc 1
2017-08-08T18:31:40.018435: step 24882, loss 1.12129e-06, acc 1
2017-08-08T18:31:40.295996: step 24883, loss 1.05748e-05, acc 1
2017-08-08T18:31:40.638294: step 24884, loss 3.03585e-06, acc 1
2017-08-08T18:31:41.069667: step 24885, loss 3.72529e-08, acc 1
2017-08-08T18:31:41.434191: step 24886, loss 1.11759e-08, acc 1
2017-08-08T18:31:41.779424: step 24887, loss 3.35276e-08, acc 1
2017-08-08T18:31:42.016265: step 24888, loss 0, acc 1
2017-08-08T18:31:42.233472: step 24889, loss 1.46875e-05, acc 1
2017-08-08T18:31:42.620587: step 24890, loss 1.14307e-05, acc 1
2017-08-08T18:31:42.966324: step 24891, loss 9.4482e-06, acc 1
2017-08-08T18:31:43.266723: step 24892, loss 5.3404e-05, acc 1
2017-08-08T18:31:43.581559: step 24893, loss 4.69875e-06, acc 1
2017-08-08T18:31:44.001957: step 24894, loss 0.000341752, acc 1
2017-08-08T18:31:44.341028: step 24895, loss 8.32587e-07, acc 1
2017-08-08T18:31:44.625362: step 24896, loss 4.50148e-05, acc 1
2017-08-08T18:31:44.908124: step 24897, loss 2.18659e-06, acc 1
2017-08-08T18:31:45.138418: step 24898, loss 5.58793e-09, acc 1
2017-08-08T18:31:45.373321: step 24899, loss 1.17471e-05, acc 1
2017-08-08T18:31:45.717372: step 24900, loss 0, acc 1

Evaluation:
2017-08-08T18:31:46.242158: step 24900, loss 8.30984, acc 0.722326

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-24900

2017-08-08T18:31:46.669342: step 24901, loss 6.89178e-08, acc 1
2017-08-08T18:31:46.962763: step 24902, loss 3.72529e-09, acc 1
2017-08-08T18:31:47.181322: step 24903, loss 0, acc 1
2017-08-08T18:31:47.372832: step 24904, loss 3.40862e-07, acc 1
2017-08-08T18:31:47.549336: step 24905, loss 0, acc 1
2017-08-08T18:31:47.864074: step 24906, loss 1.11759e-08, acc 1
2017-08-08T18:31:48.073396: step 24907, loss 8.19563e-08, acc 1
2017-08-08T18:31:48.312000: step 24908, loss 0.005032, acc 1
2017-08-08T18:31:48.755922: step 24909, loss 1.15484e-07, acc 1
2017-08-08T18:31:49.130583: step 24910, loss 5.86464e-06, acc 1
2017-08-08T18:31:49.554080: step 24911, loss 2.24145e-05, acc 1
2017-08-08T18:31:49.788171: step 24912, loss 4.66735e-06, acc 1
2017-08-08T18:31:50.113465: step 24913, loss 1.86265e-09, acc 1
2017-08-08T18:31:50.467604: step 24914, loss 0, acc 1
2017-08-08T18:31:50.678366: step 24915, loss 1.32247e-07, acc 1
2017-08-08T18:31:50.947913: step 24916, loss 0, acc 1
2017-08-08T18:31:51.167785: step 24917, loss 1.39692e-06, acc 1
2017-08-08T18:31:51.471811: step 24918, loss 1.04308e-07, acc 1
2017-08-08T18:31:51.835655: step 24919, loss 2.36555e-07, acc 1
2017-08-08T18:31:52.166436: step 24920, loss 2.10477e-07, acc 1
2017-08-08T18:31:52.392793: step 24921, loss 4.24681e-07, acc 1
2017-08-08T18:31:52.694312: step 24922, loss 3.91155e-08, acc 1
2017-08-08T18:31:52.973572: step 24923, loss 1.86265e-09, acc 1
2017-08-08T18:31:53.167765: step 24924, loss 7.45058e-09, acc 1
2017-08-08T18:31:53.448863: step 24925, loss 4.43304e-07, acc 1
2017-08-08T18:31:53.897691: step 24926, loss 7.63683e-08, acc 1
2017-08-08T18:31:54.378277: step 24927, loss 9.31322e-09, acc 1
2017-08-08T18:31:54.784360: step 24928, loss 6.46325e-07, acc 1
2017-08-08T18:31:55.020241: step 24929, loss 0, acc 1
2017-08-08T18:31:55.442989: step 24930, loss 0.000330327, acc 1
2017-08-08T18:31:55.792412: step 24931, loss 1.19778e-05, acc 1
2017-08-08T18:31:56.071589: step 24932, loss 7.45057e-08, acc 1
2017-08-08T18:31:56.310964: step 24933, loss 1.31503e-05, acc 1
2017-08-08T18:31:56.641396: step 24934, loss 1.68747e-06, acc 1
2017-08-08T18:31:57.017402: step 24935, loss 2.14376e-06, acc 1
2017-08-08T18:31:57.395006: step 24936, loss 2.16238e-06, acc 1
2017-08-08T18:31:57.699903: step 24937, loss 1.86265e-09, acc 1
2017-08-08T18:31:57.864028: step 24938, loss 1.86254e-06, acc 1
2017-08-08T18:31:58.089321: step 24939, loss 3.18294e-06, acc 1
2017-08-08T18:31:58.414509: step 24940, loss 1.75088e-07, acc 1
2017-08-08T18:31:58.595644: step 24941, loss 7.26415e-07, acc 1
2017-08-08T18:31:58.814369: step 24942, loss 1.11216e-05, acc 1
2017-08-08T18:31:59.115447: step 24943, loss 0.00289916, acc 1
2017-08-08T18:31:59.489384: step 24944, loss 0, acc 1
2017-08-08T18:31:59.897994: step 24945, loss 2.6077e-08, acc 1
2017-08-08T18:32:00.260741: step 24946, loss 9.3295e-06, acc 1
2017-08-08T18:32:00.548744: step 24947, loss 0.00255951, acc 1
2017-08-08T18:32:01.020436: step 24948, loss 0, acc 1
2017-08-08T18:32:01.357138: step 24949, loss 1.90165e-06, acc 1
2017-08-08T18:32:01.723577: step 24950, loss 4.26542e-07, acc 1
2017-08-08T18:32:02.069049: step 24951, loss 0, acc 1
2017-08-08T18:32:02.437698: step 24952, loss 2.61067e-05, acc 1
2017-08-08T18:32:02.849669: step 24953, loss 7.45058e-09, acc 1
2017-08-08T18:32:03.185477: step 24954, loss 0, acc 1
2017-08-08T18:32:03.506946: step 24955, loss 4.08985e-06, acc 1
2017-08-08T18:32:03.841600: step 24956, loss 1.86265e-09, acc 1
2017-08-08T18:32:04.241132: step 24957, loss 0.00110314, acc 1
2017-08-08T18:32:04.494573: step 24958, loss 2.19591e-06, acc 1
2017-08-08T18:32:04.941593: step 24959, loss 1.23488e-06, acc 1
2017-08-08T18:32:05.314591: step 24960, loss 1.62791e-06, acc 1
2017-08-08T18:32:05.545522: step 24961, loss 2.36543e-06, acc 1
2017-08-08T18:32:05.917374: step 24962, loss 0.000147322, acc 1
2017-08-08T18:32:06.196981: step 24963, loss 3.72529e-09, acc 1
2017-08-08T18:32:06.496463: step 24964, loss 0.000134943, acc 1
2017-08-08T18:32:06.765230: step 24965, loss 2.50494e-05, acc 1
2017-08-08T18:32:07.199582: step 24966, loss 4.02842e-06, acc 1
2017-08-08T18:32:07.520066: step 24967, loss 6.98476e-07, acc 1
2017-08-08T18:32:07.799719: step 24968, loss 2.6077e-08, acc 1
2017-08-08T18:32:07.986115: step 24969, loss 0.00012739, acc 1
2017-08-08T18:32:08.161840: step 24970, loss 0, acc 1
2017-08-08T18:32:08.452519: step 24971, loss 3.72529e-09, acc 1
2017-08-08T18:32:08.725128: step 24972, loss 1.49012e-08, acc 1
2017-08-08T18:32:08.982070: step 24973, loss 1.37085e-06, acc 1
2017-08-08T18:32:09.231963: step 24974, loss 5.58793e-09, acc 1
2017-08-08T18:32:09.537567: step 24975, loss 2.60943e-05, acc 1
2017-08-08T18:32:09.771419: step 24976, loss 5.46957e-05, acc 1
2017-08-08T18:32:09.986072: step 24977, loss 1.95577e-07, acc 1
2017-08-08T18:32:10.140672: step 24978, loss 1.30385e-08, acc 1
2017-08-08T18:32:10.399265: step 24979, loss 0.00131632, acc 1
2017-08-08T18:32:10.626581: step 24980, loss 2.23517e-08, acc 1
2017-08-08T18:32:10.831323: step 24981, loss 2.79397e-08, acc 1
2017-08-08T18:32:11.005962: step 24982, loss 7.05976e-06, acc 1
2017-08-08T18:32:11.238890: step 24983, loss 3.72529e-09, acc 1
2017-08-08T18:32:11.645569: step 24984, loss 5.96046e-08, acc 1
2017-08-08T18:32:11.901487: step 24985, loss 7.80429e-07, acc 1
2017-08-08T18:32:12.067023: step 24986, loss 1.04491e-06, acc 1
2017-08-08T18:32:12.281741: step 24987, loss 1.66134e-05, acc 1
2017-08-08T18:32:12.522352: step 24988, loss 1.86265e-09, acc 1
2017-08-08T18:32:12.737630: step 24989, loss 6.28389e-05, acc 1
2017-08-08T18:32:12.930216: step 24990, loss 1.06171e-07, acc 1
2017-08-08T18:32:13.128712: step 24991, loss 3.02406e-05, acc 1
2017-08-08T18:32:13.457766: step 24992, loss 2.79397e-08, acc 1
2017-08-08T18:32:13.821195: step 24993, loss 1.21071e-07, acc 1
2017-08-08T18:32:14.090848: step 24994, loss 0.00140866, acc 1
2017-08-08T18:32:14.340718: step 24995, loss 3.31549e-07, acc 1
2017-08-08T18:32:14.533358: step 24996, loss 1.10825e-05, acc 1
2017-08-08T18:32:14.880022: step 24997, loss 3.72529e-09, acc 1
2017-08-08T18:32:15.111685: step 24998, loss 2.34692e-07, acc 1
2017-08-08T18:32:15.313792: step 24999, loss 5.24088e-05, acc 1
2017-08-08T18:32:15.493009: step 25000, loss 3.1167e-05, acc 1

Evaluation:
2017-08-08T18:32:16.186874: step 25000, loss 8.29366, acc 0.722326

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-25000

2017-08-08T18:32:16.573968: step 25001, loss 9.31322e-09, acc 1
2017-08-08T18:32:16.937818: step 25002, loss 4.4455e-06, acc 1
2017-08-08T18:32:17.205152: step 25003, loss 0, acc 1
2017-08-08T18:32:17.467114: step 25004, loss 1.21254e-06, acc 1
2017-08-08T18:32:17.669586: step 25005, loss 1.86265e-09, acc 1
2017-08-08T18:32:17.972259: step 25006, loss 0, acc 1
2017-08-08T18:32:18.197366: step 25007, loss 3.1665e-08, acc 1
2017-08-08T18:32:18.403074: step 25008, loss 5.40166e-08, acc 1
2017-08-08T18:32:18.611258: step 25009, loss 1.13787e-05, acc 1
2017-08-08T18:32:18.777394: step 25010, loss 1.1905e-05, acc 1
2017-08-08T18:32:19.129774: step 25011, loss 1.65775e-07, acc 1
2017-08-08T18:32:19.383630: step 25012, loss 9.85313e-07, acc 1
2017-08-08T18:32:19.607196: step 25013, loss 9.92758e-07, acc 1
2017-08-08T18:32:19.828873: step 25014, loss 5.4202e-07, acc 1
2017-08-08T18:32:20.125454: step 25015, loss 1.67638e-08, acc 1
2017-08-08T18:32:20.406016: step 25016, loss 5.2154e-08, acc 1
2017-08-08T18:32:20.733297: step 25017, loss 1.63529e-05, acc 1
2017-08-08T18:32:21.017226: step 25018, loss 8.68616e-06, acc 1
2017-08-08T18:32:21.347407: step 25019, loss 5.58793e-09, acc 1
2017-08-08T18:32:21.590339: step 25020, loss 1.70612e-06, acc 1
2017-08-08T18:32:21.849281: step 25021, loss 7.45056e-08, acc 1
2017-08-08T18:32:22.099712: step 25022, loss 2.25404e-05, acc 1
2017-08-08T18:32:22.389449: step 25023, loss 9.31322e-09, acc 1
2017-08-08T18:32:22.609105: step 25024, loss 9.31322e-09, acc 1
2017-08-08T18:32:22.877701: step 25025, loss 7.71059e-05, acc 1
2017-08-08T18:32:23.163188: step 25026, loss 0.000114584, acc 1
2017-08-08T18:32:23.373463: step 25027, loss 2.23517e-08, acc 1
2017-08-08T18:32:23.670137: step 25028, loss 3.87425e-07, acc 1
2017-08-08T18:32:23.901653: step 25029, loss 7.78566e-07, acc 1
2017-08-08T18:32:24.131190: step 25030, loss 2.79397e-08, acc 1
2017-08-08T18:32:24.355830: step 25031, loss 5.02913e-08, acc 1
2017-08-08T18:32:24.637840: step 25032, loss 4.32128e-07, acc 1
2017-08-08T18:32:25.029496: step 25033, loss 1.61483e-06, acc 1
2017-08-08T18:32:25.350383: step 25034, loss 1.15572e-05, acc 1
2017-08-08T18:32:25.773371: step 25035, loss 3.72529e-08, acc 1
2017-08-08T18:32:25.973255: step 25036, loss 5.01344e-06, acc 1
2017-08-08T18:32:26.354977: step 25037, loss 8.36567e-05, acc 1
2017-08-08T18:32:26.592370: step 25038, loss 1.01928e-05, acc 1
2017-08-08T18:32:26.803270: step 25039, loss 5.71822e-07, acc 1
2017-08-08T18:32:27.085376: step 25040, loss 1.08033e-07, acc 1
2017-08-08T18:32:27.413401: step 25041, loss 1.03561e-06, acc 1
2017-08-08T18:32:27.685985: step 25042, loss 1.49012e-08, acc 1
2017-08-08T18:32:27.962075: step 25043, loss 5.58793e-09, acc 1
2017-08-08T18:32:28.185290: step 25044, loss 3.65074e-07, acc 1
2017-08-08T18:32:28.431279: step 25045, loss 2.01712e-06, acc 1
2017-08-08T18:32:28.807740: step 25046, loss 0.000210533, acc 1
2017-08-08T18:32:29.067667: step 25047, loss 0, acc 1
2017-08-08T18:32:29.273040: step 25048, loss 1.60552e-06, acc 1
2017-08-08T18:32:29.633130: step 25049, loss 3.72529e-09, acc 1
2017-08-08T18:32:30.001384: step 25050, loss 0, acc 1
2017-08-08T18:32:30.307421: step 25051, loss 0, acc 1
2017-08-08T18:32:30.516299: step 25052, loss 1.86265e-09, acc 1
2017-08-08T18:32:30.707807: step 25053, loss 1.9899e-05, acc 1
2017-08-08T18:32:31.049709: step 25054, loss 0, acc 1
2017-08-08T18:32:31.260166: step 25055, loss 4.28405e-07, acc 1
2017-08-08T18:32:31.489011: step 25056, loss 1.40624e-06, acc 1
2017-08-08T18:32:31.726718: step 25057, loss 3.53902e-08, acc 1
2017-08-08T18:32:32.063668: step 25058, loss 3.53902e-08, acc 1
2017-08-08T18:32:32.314993: step 25059, loss 1.86265e-09, acc 1
2017-08-08T18:32:32.557629: step 25060, loss 1.54406e-06, acc 1
2017-08-08T18:32:32.737580: step 25061, loss 3.72529e-09, acc 1
2017-08-08T18:32:32.957459: step 25062, loss 2.88501e-06, acc 1
2017-08-08T18:32:33.249000: step 25063, loss 0.000356408, acc 1
2017-08-08T18:32:33.417947: step 25064, loss 0, acc 1
2017-08-08T18:32:33.624968: step 25065, loss 1.23929e-05, acc 1
2017-08-08T18:32:33.836920: step 25066, loss 7.04065e-07, acc 1
2017-08-08T18:32:34.101320: step 25067, loss 1.67638e-08, acc 1
2017-08-08T18:32:34.413659: step 25068, loss 0.0425978, acc 0.984375
2017-08-08T18:32:34.640433: step 25069, loss 3.72529e-09, acc 1
2017-08-08T18:32:34.847001: step 25070, loss 1.68709e-05, acc 1
2017-08-08T18:32:35.044099: step 25071, loss 3.72529e-09, acc 1
2017-08-08T18:32:35.297190: step 25072, loss 1.86265e-09, acc 1
2017-08-08T18:32:35.507166: step 25073, loss 1.89989e-07, acc 1
2017-08-08T18:32:35.722643: step 25074, loss 3.83702e-07, acc 1
2017-08-08T18:32:35.971320: step 25075, loss 5.58794e-09, acc 1
2017-08-08T18:32:36.324800: step 25076, loss 0, acc 1
2017-08-08T18:32:36.671302: step 25077, loss 0, acc 1
2017-08-08T18:32:36.943106: step 25078, loss 2.18039e-05, acc 1
2017-08-08T18:32:37.127520: step 25079, loss 3.07334e-07, acc 1
2017-08-08T18:32:37.303070: step 25080, loss 3.91155e-08, acc 1
2017-08-08T18:32:37.661908: step 25081, loss 1.49012e-08, acc 1
2017-08-08T18:32:37.871906: step 25082, loss 7.82292e-07, acc 1
2017-08-08T18:32:38.162138: step 25083, loss 0.000318797, acc 1
2017-08-08T18:32:38.410169: step 25084, loss 0.000599086, acc 1
2017-08-08T18:32:38.753713: step 25085, loss 1.67638e-08, acc 1
2017-08-08T18:32:39.073380: step 25086, loss 0, acc 1
2017-08-08T18:32:39.416725: step 25087, loss 5.58794e-09, acc 1
2017-08-08T18:32:39.651608: step 25088, loss 4.26541e-07, acc 1
2017-08-08T18:32:39.838616: step 25089, loss 9.57479e-06, acc 1
2017-08-08T18:32:40.133354: step 25090, loss 0, acc 1
2017-08-08T18:32:40.347519: step 25091, loss 0.000110431, acc 1
2017-08-08T18:32:40.587737: step 25092, loss 3.16649e-08, acc 1
2017-08-08T18:32:40.780855: step 25093, loss 6.3515e-07, acc 1
2017-08-08T18:32:41.077212: step 25094, loss 9.74564e-05, acc 1
2017-08-08T18:32:41.352448: step 25095, loss 1.71363e-07, acc 1
2017-08-08T18:32:41.746923: step 25096, loss 2.87949e-06, acc 1
2017-08-08T18:32:42.038022: step 25097, loss 9.90241e-06, acc 1
2017-08-08T18:32:42.252293: step 25098, loss 8.38189e-08, acc 1
2017-08-08T18:32:42.681356: step 25099, loss 0.0659779, acc 0.984375
2017-08-08T18:32:42.999929: step 25100, loss 1.35968e-06, acc 1

Evaluation:
2017-08-08T18:32:43.751048: step 25100, loss 8.70174, acc 0.707317

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-25100

2017-08-08T18:32:44.437475: step 25101, loss 3.72529e-09, acc 1
2017-08-08T18:32:44.767384: step 25102, loss 0, acc 1
2017-08-08T18:32:44.991908: step 25103, loss 2.31225e-05, acc 1
2017-08-08T18:32:45.363386: step 25104, loss 3.69691e-06, acc 1
2017-08-08T18:32:45.607945: step 25105, loss 1.43417e-06, acc 1
2017-08-08T18:32:45.829783: step 25106, loss 2.4883e-06, acc 1
2017-08-08T18:32:46.036126: step 25107, loss 9.31322e-09, acc 1
2017-08-08T18:32:46.413436: step 25108, loss 3.21459e-06, acc 1
2017-08-08T18:32:46.828221: step 25109, loss 1.0204e-05, acc 1
2017-08-08T18:32:47.110065: step 25110, loss 1.08588e-06, acc 1
2017-08-08T18:32:47.296640: step 25111, loss 7.07804e-08, acc 1
2017-08-08T18:32:47.581382: step 25112, loss 4.79558e-06, acc 1
2017-08-08T18:32:47.858797: step 25113, loss 2.36554e-07, acc 1
2017-08-08T18:32:48.062985: step 25114, loss 0.000146426, acc 1
2017-08-08T18:32:48.262140: step 25115, loss 4.07915e-07, acc 1
2017-08-08T18:32:48.601328: step 25116, loss 1.75088e-07, acc 1
2017-08-08T18:32:48.969339: step 25117, loss 7.82309e-08, acc 1
2017-08-08T18:32:49.226247: step 25118, loss 2.23395e-05, acc 1
2017-08-08T18:32:49.465383: step 25119, loss 6.81716e-07, acc 1
2017-08-08T18:32:49.769394: step 25120, loss 0, acc 1
2017-08-08T18:32:50.016661: step 25121, loss 2.92434e-07, acc 1
2017-08-08T18:32:50.232364: step 25122, loss 0, acc 1
2017-08-08T18:32:50.514320: step 25123, loss 7.26431e-08, acc 1
2017-08-08T18:32:50.974444: step 25124, loss 1.41561e-07, acc 1
2017-08-08T18:32:51.353371: step 25125, loss 9.31322e-09, acc 1
2017-08-08T18:32:51.603900: step 25126, loss 1.49012e-08, acc 1
2017-08-08T18:32:51.795703: step 25127, loss 0, acc 1
2017-08-08T18:32:52.184573: step 25128, loss 1.22934e-07, acc 1
2017-08-08T18:32:52.461701: step 25129, loss 2.37401e-05, acc 1
2017-08-08T18:32:52.763409: step 25130, loss 9.68574e-08, acc 1
2017-08-08T18:32:53.041443: step 25131, loss 2.20848e-05, acc 1
2017-08-08T18:32:53.389827: step 25132, loss 5.25258e-07, acc 1
2017-08-08T18:32:53.773355: step 25133, loss 1.86264e-08, acc 1
2017-08-08T18:32:54.133312: step 25134, loss 0, acc 1
2017-08-08T18:32:54.383786: step 25135, loss 0.000176992, acc 1
2017-08-08T18:32:54.638218: step 25136, loss 3.53902e-08, acc 1
2017-08-08T18:32:55.078255: step 25137, loss 5.40166e-08, acc 1
2017-08-08T18:32:55.318973: step 25138, loss 0, acc 1
2017-08-08T18:32:55.558564: step 25139, loss 2.8312e-07, acc 1
2017-08-08T18:32:55.815213: step 25140, loss 0, acc 1
2017-08-08T18:32:56.353693: step 25141, loss 2.19791e-07, acc 1
2017-08-08T18:32:56.737698: step 25142, loss 0, acc 1
2017-08-08T18:32:57.087032: step 25143, loss 0.000392982, acc 1
2017-08-08T18:32:57.331562: step 25144, loss 0, acc 1
2017-08-08T18:32:57.673004: step 25145, loss 1.84506e-05, acc 1
2017-08-08T18:32:57.978294: step 25146, loss 3.72529e-09, acc 1
2017-08-08T18:32:58.207588: step 25147, loss 7.823e-07, acc 1
2017-08-08T18:32:58.461413: step 25148, loss 1.86265e-09, acc 1
2017-08-08T18:32:58.705285: step 25149, loss 2.82816e-05, acc 1
2017-08-08T18:32:59.163651: step 25150, loss 0, acc 1
2017-08-08T18:32:59.553404: step 25151, loss 5.58793e-09, acc 1
2017-08-08T18:32:59.791116: step 25152, loss 0, acc 1
2017-08-08T18:33:00.044710: step 25153, loss 1.93714e-07, acc 1
2017-08-08T18:33:00.306226: step 25154, loss 1.18832e-06, acc 1
2017-08-08T18:33:00.757678: step 25155, loss 2.88709e-07, acc 1
2017-08-08T18:33:01.069569: step 25156, loss 2.73601e-06, acc 1
2017-08-08T18:33:01.426097: step 25157, loss 5.79277e-07, acc 1
2017-08-08T18:33:01.733986: step 25158, loss 2.24805e-06, acc 1
2017-08-08T18:33:02.123699: step 25159, loss 1.11759e-08, acc 1
2017-08-08T18:33:02.574911: step 25160, loss 1.65775e-07, acc 1
2017-08-08T18:33:02.862512: step 25161, loss 1.86265e-09, acc 1
2017-08-08T18:33:03.104642: step 25162, loss 3.12921e-07, acc 1
2017-08-08T18:33:03.413469: step 25163, loss 8.07515e-06, acc 1
2017-08-08T18:33:03.809468: step 25164, loss 5.04769e-07, acc 1
2017-08-08T18:33:04.131314: step 25165, loss 5.97085e-06, acc 1
2017-08-08T18:33:04.435162: step 25166, loss 8.29402e-06, acc 1
2017-08-08T18:33:04.749353: step 25167, loss 0.000196386, acc 1
2017-08-08T18:33:05.178864: step 25168, loss 1.76755e-06, acc 1
2017-08-08T18:33:05.507498: step 25169, loss 7.13383e-07, acc 1
2017-08-08T18:33:05.862905: step 25170, loss 1.6018e-06, acc 1
2017-08-08T18:33:06.110475: step 25171, loss 0, acc 1
2017-08-08T18:33:06.395719: step 25172, loss 1.11759e-08, acc 1
2017-08-08T18:33:06.770284: step 25173, loss 0, acc 1
2017-08-08T18:33:07.008957: step 25174, loss 0, acc 1
2017-08-08T18:33:07.335064: step 25175, loss 0, acc 1
2017-08-08T18:33:07.749418: step 25176, loss 1.28901e-05, acc 1
2017-08-08T18:33:08.089779: step 25177, loss 2.8312e-07, acc 1
2017-08-08T18:33:08.490790: step 25178, loss 3.66937e-07, acc 1
2017-08-08T18:33:08.746178: step 25179, loss 1.30385e-08, acc 1
2017-08-08T18:33:08.970381: step 25180, loss 0, acc 1
2017-08-08T18:33:09.286201: step 25181, loss 8.28521e-05, acc 1
2017-08-08T18:33:09.504896: step 25182, loss 3.83121e-06, acc 1
2017-08-08T18:33:09.714531: step 25183, loss 6.96614e-07, acc 1
2017-08-08T18:33:09.890919: step 25184, loss 1.84401e-07, acc 1
2017-08-08T18:33:10.201052: step 25185, loss 7.00339e-07, acc 1
2017-08-08T18:33:10.489359: step 25186, loss 1.86265e-09, acc 1
2017-08-08T18:33:10.774526: step 25187, loss 1.07098e-06, acc 1
2017-08-08T18:33:10.974116: step 25188, loss 0.00264945, acc 1
2017-08-08T18:33:11.229401: step 25189, loss 1.18646e-06, acc 1
2017-08-08T18:33:11.506843: step 25190, loss 3.53902e-08, acc 1
2017-08-08T18:33:11.696607: step 25191, loss 1.11758e-07, acc 1
2017-08-08T18:33:11.917872: step 25192, loss 8.56815e-08, acc 1
2017-08-08T18:33:12.121674: step 25193, loss 0, acc 1
2017-08-08T18:33:12.572674: step 25194, loss 2.98023e-08, acc 1
2017-08-08T18:33:12.848867: step 25195, loss 2.54604e-06, acc 1
2017-08-08T18:33:13.077437: step 25196, loss 4.54479e-07, acc 1
2017-08-08T18:33:13.295067: step 25197, loss 1.1206e-05, acc 1
2017-08-08T18:33:13.577489: step 25198, loss 0, acc 1
2017-08-08T18:33:13.821467: step 25199, loss 1.86265e-09, acc 1
2017-08-08T18:33:14.053348: step 25200, loss 0.000213028, acc 1

Evaluation:
2017-08-08T18:33:14.551283: step 25200, loss 8.3295, acc 0.727017

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-25200

2017-08-08T18:33:15.115525: step 25201, loss 9.25167e-05, acc 1
2017-08-08T18:33:15.393842: step 25202, loss 2.6077e-08, acc 1
2017-08-08T18:33:15.579900: step 25203, loss 7.29614e-06, acc 1
2017-08-08T18:33:15.920966: step 25204, loss 7.45058e-09, acc 1
2017-08-08T18:33:16.319088: step 25205, loss 1.82346e-06, acc 1
2017-08-08T18:33:16.522255: step 25206, loss 9.63283e-05, acc 1
2017-08-08T18:33:16.832656: step 25207, loss 3.72529e-09, acc 1
2017-08-08T18:33:17.058897: step 25208, loss 0, acc 1
2017-08-08T18:33:17.232236: step 25209, loss 1.86265e-09, acc 1
2017-08-08T18:33:17.551156: step 25210, loss 6.69681e-06, acc 1
2017-08-08T18:33:17.711512: step 25211, loss 0, acc 1
2017-08-08T18:33:17.945767: step 25212, loss 0, acc 1
2017-08-08T18:33:18.227408: step 25213, loss 4.67517e-07, acc 1
2017-08-08T18:33:18.426587: step 25214, loss 2.53124e-06, acc 1
2017-08-08T18:33:18.653466: step 25215, loss 0, acc 1
2017-08-08T18:33:18.859367: step 25216, loss 1.13621e-07, acc 1
2017-08-08T18:33:19.166254: step 25217, loss 2.30036e-05, acc 1
2017-08-08T18:33:19.527684: step 25218, loss 6.34288e-06, acc 1
2017-08-08T18:33:19.751526: step 25219, loss 9.62958e-07, acc 1
2017-08-08T18:33:20.131301: step 25220, loss 5.58793e-09, acc 1
2017-08-08T18:33:20.339279: step 25221, loss 0, acc 1
2017-08-08T18:33:20.632358: step 25222, loss 5.49999e-05, acc 1
2017-08-08T18:33:20.809321: step 25223, loss 0, acc 1
2017-08-08T18:33:21.014507: step 25224, loss 0, acc 1
2017-08-08T18:33:21.339723: step 25225, loss 0.00058005, acc 1
2017-08-08T18:33:21.548722: step 25226, loss 5.02913e-08, acc 1
2017-08-08T18:33:21.877953: step 25227, loss 9.59526e-06, acc 1
2017-08-08T18:33:22.092052: step 25228, loss 5.38443e-06, acc 1
2017-08-08T18:33:22.381347: step 25229, loss 6.30042e-05, acc 1
2017-08-08T18:33:22.648708: step 25230, loss 1.67638e-08, acc 1
2017-08-08T18:33:22.865790: step 25231, loss 2.21653e-07, acc 1
2017-08-08T18:33:23.170163: step 25232, loss 2.42144e-08, acc 1
2017-08-08T18:33:23.500472: step 25233, loss 1.86265e-09, acc 1
2017-08-08T18:33:23.894759: step 25234, loss 4.06052e-07, acc 1
2017-08-08T18:33:24.150230: step 25235, loss 1.86265e-09, acc 1
2017-08-08T18:33:24.554916: step 25236, loss 9.68573e-08, acc 1
2017-08-08T18:33:24.790338: step 25237, loss 0, acc 1
2017-08-08T18:33:25.125526: step 25238, loss 2.16066e-07, acc 1
2017-08-08T18:33:25.336678: step 25239, loss 7.78568e-07, acc 1
2017-08-08T18:33:25.545347: step 25240, loss 9.4992e-07, acc 1
2017-08-08T18:33:25.803397: step 25241, loss 0, acc 1
2017-08-08T18:33:26.102454: step 25242, loss 0, acc 1
2017-08-08T18:33:26.308755: step 25243, loss 6.83027e-05, acc 1
2017-08-08T18:33:26.525900: step 25244, loss 2.14191e-05, acc 1
2017-08-08T18:33:26.756898: step 25245, loss 7.2643e-08, acc 1
2017-08-08T18:33:26.989151: step 25246, loss 4.78804e-05, acc 1
2017-08-08T18:33:27.224776: step 25247, loss 1.30193e-06, acc 1
2017-08-08T18:33:27.435480: step 25248, loss 1.86265e-09, acc 1
2017-08-08T18:33:27.633227: step 25249, loss 9.3132e-08, acc 1
2017-08-08T18:33:27.927878: step 25250, loss 0.00176218, acc 1
2017-08-08T18:33:28.199715: step 25251, loss 2.04891e-08, acc 1
2017-08-08T18:33:28.420370: step 25252, loss 0, acc 1
2017-08-08T18:33:28.689948: step 25253, loss 9.12693e-08, acc 1
2017-08-08T18:33:28.942662: step 25254, loss 8.92586e-05, acc 1
2017-08-08T18:33:29.287738: step 25255, loss 5.58794e-09, acc 1
2017-08-08T18:33:29.688040: step 25256, loss 1.11759e-08, acc 1
2017-08-08T18:33:30.032128: step 25257, loss 2.71944e-07, acc 1
2017-08-08T18:33:30.239698: step 25258, loss 4.61979e-05, acc 1
2017-08-08T18:33:30.504710: step 25259, loss 9.31322e-09, acc 1
2017-08-08T18:33:30.667734: step 25260, loss 0.000367466, acc 1
2017-08-08T18:33:30.860807: step 25261, loss 9.57382e-07, acc 1
2017-08-08T18:33:31.093316: step 25262, loss 2.42144e-08, acc 1
2017-08-08T18:33:31.407453: step 25263, loss 0, acc 1
2017-08-08T18:33:31.643594: step 25264, loss 0, acc 1
2017-08-08T18:33:31.902509: step 25265, loss 1.86265e-09, acc 1
2017-08-08T18:33:32.087181: step 25266, loss 0.000144916, acc 1
2017-08-08T18:33:32.381366: step 25267, loss 1.30385e-08, acc 1
2017-08-08T18:33:32.566827: step 25268, loss 5.39329e-06, acc 1
2017-08-08T18:33:32.784193: step 25269, loss 1.98451e-05, acc 1
2017-08-08T18:33:32.981414: step 25270, loss 3.72529e-09, acc 1
2017-08-08T18:33:33.309452: step 25271, loss 0, acc 1
2017-08-08T18:33:33.537341: step 25272, loss 2.6077e-08, acc 1
2017-08-08T18:33:33.826563: step 25273, loss 0, acc 1
2017-08-08T18:33:34.049053: step 25274, loss 4.26493e-06, acc 1
2017-08-08T18:33:34.407803: step 25275, loss 2.6077e-08, acc 1
2017-08-08T18:33:34.607097: step 25276, loss 1.695e-07, acc 1
2017-08-08T18:33:34.829276: step 25277, loss 7.71768e-06, acc 1
2017-08-08T18:33:35.061115: step 25278, loss 3.16646e-07, acc 1
2017-08-08T18:33:35.381421: step 25279, loss 0.00176363, acc 1
2017-08-08T18:33:35.684306: step 25280, loss 1.55337e-06, acc 1
2017-08-08T18:33:36.005358: step 25281, loss 1.12984e-05, acc 1
2017-08-08T18:33:36.278740: step 25282, loss 1.18135e-05, acc 1
2017-08-08T18:33:36.488990: step 25283, loss 1.86265e-09, acc 1
2017-08-08T18:33:36.833366: step 25284, loss 0, acc 1
2017-08-08T18:33:37.068835: step 25285, loss 5.58794e-09, acc 1
2017-08-08T18:33:37.321392: step 25286, loss 1.04118e-06, acc 1
2017-08-08T18:33:37.576371: step 25287, loss 1.67638e-08, acc 1
2017-08-08T18:33:37.849399: step 25288, loss 0, acc 1
2017-08-08T18:33:38.281970: step 25289, loss 4.30212e-06, acc 1
2017-08-08T18:33:38.680851: step 25290, loss 8.38188e-08, acc 1
2017-08-08T18:33:38.985645: step 25291, loss 1.35969e-06, acc 1
2017-08-08T18:33:39.198183: step 25292, loss 4.42317e-06, acc 1
2017-08-08T18:33:39.565388: step 25293, loss 0.0365155, acc 0.984375
2017-08-08T18:33:39.862937: step 25294, loss 7.45058e-09, acc 1
2017-08-08T18:33:40.136997: step 25295, loss 1.30385e-08, acc 1
2017-08-08T18:33:40.396191: step 25296, loss 1.35781e-06, acc 1
2017-08-08T18:33:40.749556: step 25297, loss 0, acc 1
2017-08-08T18:33:41.137885: step 25298, loss 7.78568e-07, acc 1
2017-08-08T18:33:41.414066: step 25299, loss 8.29228e-06, acc 1
2017-08-08T18:33:41.639826: step 25300, loss 6.03485e-07, acc 1

Evaluation:
2017-08-08T18:33:42.339286: step 25300, loss 8.33601, acc 0.732645

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-25300

2017-08-08T18:33:42.715227: step 25301, loss 2.00478e-05, acc 1
2017-08-08T18:33:42.994205: step 25302, loss 1.03746e-06, acc 1
2017-08-08T18:33:43.273503: step 25303, loss 0, acc 1
2017-08-08T18:33:43.577642: step 25304, loss 1.86265e-09, acc 1
2017-08-08T18:33:43.871810: step 25305, loss 1.43423e-07, acc 1
2017-08-08T18:33:44.115448: step 25306, loss 0, acc 1
2017-08-08T18:33:44.437124: step 25307, loss 0, acc 1
2017-08-08T18:33:44.839625: step 25308, loss 0, acc 1
2017-08-08T18:33:45.144430: step 25309, loss 1.30385e-07, acc 1
2017-08-08T18:33:45.361415: step 25310, loss 8.38581e-05, acc 1
2017-08-08T18:33:45.694339: step 25311, loss 0, acc 1
2017-08-08T18:33:46.074057: step 25312, loss 7.86017e-07, acc 1
2017-08-08T18:33:46.380355: step 25313, loss 5.151e-05, acc 1
2017-08-08T18:33:46.663231: step 25314, loss 1.84401e-07, acc 1
2017-08-08T18:33:46.920885: step 25315, loss 1.19391e-06, acc 1
2017-08-08T18:33:47.392383: step 25316, loss 4.22772e-06, acc 1
2017-08-08T18:33:47.636171: step 25317, loss 0.00152605, acc 1
2017-08-08T18:33:47.894241: step 25318, loss 5.58793e-08, acc 1
2017-08-08T18:33:48.104284: step 25319, loss 7.89749e-07, acc 1
2017-08-08T18:33:48.478243: step 25320, loss 0, acc 1
2017-08-08T18:33:48.761561: step 25321, loss 0.00295688, acc 1
2017-08-08T18:33:48.966244: step 25322, loss 0.0364613, acc 0.984375
2017-08-08T18:33:49.130931: step 25323, loss 1.73225e-07, acc 1
2017-08-08T18:33:49.483920: step 25324, loss 0.000100563, acc 1
2017-08-08T18:33:49.788249: step 25325, loss 0, acc 1
2017-08-08T18:33:50.056175: step 25326, loss 1.39692e-06, acc 1
2017-08-08T18:33:50.323970: step 25327, loss 5.40166e-08, acc 1
2017-08-08T18:33:50.713387: step 25328, loss 2.42144e-08, acc 1
2017-08-08T18:33:51.096487: step 25329, loss 3.16649e-08, acc 1
2017-08-08T18:33:51.455851: step 25330, loss 1.37835e-07, acc 1
2017-08-08T18:33:51.780601: step 25331, loss 8.19562e-08, acc 1
2017-08-08T18:33:52.043510: step 25332, loss 1.30385e-08, acc 1
2017-08-08T18:33:52.444056: step 25333, loss 1.47148e-07, acc 1
2017-08-08T18:33:52.654695: step 25334, loss 3.10344e-05, acc 1
2017-08-08T18:33:52.970985: step 25335, loss 3.53902e-08, acc 1
2017-08-08T18:33:53.258308: step 25336, loss 0, acc 1
2017-08-08T18:33:53.701982: step 25337, loss 3.72529e-09, acc 1
2017-08-08T18:33:53.972869: step 25338, loss 0.000132423, acc 1
2017-08-08T18:33:54.278836: step 25339, loss 3.72529e-09, acc 1
2017-08-08T18:33:54.567112: step 25340, loss 4.35178e-05, acc 1
2017-08-08T18:33:54.813494: step 25341, loss 0, acc 1
2017-08-08T18:33:55.237554: step 25342, loss 1.86265e-09, acc 1
2017-08-08T18:33:55.524124: step 25343, loss 2.42144e-08, acc 1
2017-08-08T18:33:55.729189: step 25344, loss 3.96738e-07, acc 1
2017-08-08T18:33:56.030935: step 25345, loss 3.72529e-09, acc 1
2017-08-08T18:33:56.381627: step 25346, loss 3.91299e-06, acc 1
2017-08-08T18:33:56.639482: step 25347, loss 5.58793e-08, acc 1
2017-08-08T18:33:56.871463: step 25348, loss 1.0654e-06, acc 1
2017-08-08T18:33:57.082537: step 25349, loss 2.49594e-07, acc 1
2017-08-08T18:33:57.449743: step 25350, loss 5.74184e-07, acc 1
2017-08-08T18:33:57.843097: step 25351, loss 2.64662e-06, acc 1
2017-08-08T18:33:58.122535: step 25352, loss 7.63683e-08, acc 1
2017-08-08T18:33:58.365303: step 25353, loss 2.19033e-06, acc 1
2017-08-08T18:33:58.575463: step 25354, loss 4.97448e-06, acc 1
2017-08-08T18:33:58.923516: step 25355, loss 0.0168319, acc 0.984375
2017-08-08T18:33:59.279686: step 25356, loss 9.90899e-07, acc 1
2017-08-08T18:33:59.541823: step 25357, loss 1.86264e-08, acc 1
2017-08-08T18:33:59.865941: step 25358, loss 1.30385e-08, acc 1
2017-08-08T18:34:00.097390: step 25359, loss 4.80555e-07, acc 1
2017-08-08T18:34:00.473150: step 25360, loss 8.40043e-07, acc 1
2017-08-08T18:34:00.715878: step 25361, loss 0, acc 1
2017-08-08T18:34:01.004501: step 25362, loss 0, acc 1
2017-08-08T18:34:01.313562: step 25363, loss 7.8231e-08, acc 1
2017-08-08T18:34:01.666651: step 25364, loss 1.02256e-06, acc 1
2017-08-08T18:34:02.114530: step 25365, loss 0, acc 1
2017-08-08T18:34:02.535388: step 25366, loss 7.63683e-08, acc 1
2017-08-08T18:34:02.830610: step 25367, loss 4.33993e-07, acc 1
2017-08-08T18:34:03.152362: step 25368, loss 1.86265e-09, acc 1
2017-08-08T18:34:03.676249: step 25369, loss 7.45058e-09, acc 1
2017-08-08T18:34:03.997732: step 25370, loss 5.45746e-07, acc 1
2017-08-08T18:34:04.301639: step 25371, loss 0, acc 1
2017-08-08T18:34:04.679611: step 25372, loss 0, acc 1
2017-08-08T18:34:05.055714: step 25373, loss 1.01512e-06, acc 1
2017-08-08T18:34:05.443990: step 25374, loss 4.84287e-08, acc 1
2017-08-08T18:34:05.754830: step 25375, loss 4.65661e-08, acc 1
2017-08-08T18:34:06.014287: step 25376, loss 0, acc 1
2017-08-08T18:34:06.443237: step 25377, loss 0, acc 1
2017-08-08T18:34:06.698713: step 25378, loss 1.15484e-07, acc 1
2017-08-08T18:34:06.982889: step 25379, loss 0.000511661, acc 1
2017-08-08T18:34:07.362261: step 25380, loss 1.29076e-06, acc 1
2017-08-08T18:34:07.813388: step 25381, loss 8.75441e-08, acc 1
2017-08-08T18:34:08.276887: step 25382, loss 1.49012e-08, acc 1
2017-08-08T18:34:08.562582: step 25383, loss 9.31322e-09, acc 1
2017-08-08T18:34:08.845854: step 25384, loss 5.3457e-07, acc 1
2017-08-08T18:34:09.317127: step 25385, loss 8.30733e-05, acc 1
2017-08-08T18:34:09.563961: step 25386, loss 0.0226309, acc 0.984375
2017-08-08T18:34:09.849405: step 25387, loss 0, acc 1
2017-08-08T18:34:10.136789: step 25388, loss 0, acc 1
2017-08-08T18:34:10.453832: step 25389, loss 0.141881, acc 0.984375
2017-08-08T18:34:10.818425: step 25390, loss 0.000501006, acc 1
2017-08-08T18:34:11.259944: step 25391, loss 7.07804e-08, acc 1
2017-08-08T18:34:11.642515: step 25392, loss 0, acc 1
2017-08-08T18:34:11.898862: step 25393, loss 1.04119e-06, acc 1
2017-08-08T18:34:12.235551: step 25394, loss 4.5262e-07, acc 1
2017-08-08T18:34:12.466736: step 25395, loss 9.31322e-09, acc 1
2017-08-08T18:34:12.695569: step 25396, loss 0, acc 1
2017-08-08T18:34:12.934932: step 25397, loss 4.1909e-07, acc 1
2017-08-08T18:34:13.261341: step 25398, loss 7.44136e-06, acc 1
2017-08-08T18:34:13.618491: step 25399, loss 7.45058e-09, acc 1
2017-08-08T18:34:13.904043: step 25400, loss 2.04507e-06, acc 1

Evaluation:
2017-08-08T18:34:14.415157: step 25400, loss 8.73994, acc 0.708255

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-25400

2017-08-08T18:34:14.973649: step 25401, loss 0, acc 1
2017-08-08T18:34:15.202608: step 25402, loss 2.22991e-05, acc 1
2017-08-08T18:34:15.439432: step 25403, loss 2.04891e-08, acc 1
2017-08-08T18:34:15.758033: step 25404, loss 5.01165e-06, acc 1
2017-08-08T18:34:16.047664: step 25405, loss 8.21414e-07, acc 1
2017-08-08T18:34:16.334570: step 25406, loss 9.87199e-08, acc 1
2017-08-08T18:34:16.518788: step 25407, loss 8.31267e-05, acc 1
2017-08-08T18:34:16.735070: step 25408, loss 5.86722e-07, acc 1
2017-08-08T18:34:17.143001: step 25409, loss 0, acc 1
2017-08-08T18:34:17.387147: step 25410, loss 1.11759e-08, acc 1
2017-08-08T18:34:17.669455: step 25411, loss 1.86265e-09, acc 1
2017-08-08T18:34:17.943685: step 25412, loss 0.145732, acc 0.984375
2017-08-08T18:34:18.279005: step 25413, loss 3.21597e-05, acc 1
2017-08-08T18:34:18.687007: step 25414, loss 0, acc 1
2017-08-08T18:34:19.057577: step 25415, loss 0, acc 1
2017-08-08T18:34:19.392640: step 25416, loss 1.86264e-08, acc 1
2017-08-08T18:34:19.660586: step 25417, loss 0.000178162, acc 1
2017-08-08T18:34:19.898450: step 25418, loss 5.58793e-09, acc 1
2017-08-08T18:34:20.234204: step 25419, loss 1.09896e-07, acc 1
2017-08-08T18:34:20.471462: step 25420, loss 2.71944e-07, acc 1
2017-08-08T18:34:20.681675: step 25421, loss 0, acc 1
2017-08-08T18:34:20.899589: step 25422, loss 4.39578e-07, acc 1
2017-08-08T18:34:21.197060: step 25423, loss 0.000119941, acc 1
2017-08-08T18:34:21.422755: step 25424, loss 0, acc 1
2017-08-08T18:34:21.664362: step 25425, loss 3.91155e-08, acc 1
2017-08-08T18:34:21.895657: step 25426, loss 2.57024e-06, acc 1
2017-08-08T18:34:22.124601: step 25427, loss 1.67638e-08, acc 1
2017-08-08T18:34:22.321379: step 25428, loss 1.86264e-08, acc 1
2017-08-08T18:34:22.625358: step 25429, loss 5.58793e-09, acc 1
2017-08-08T18:34:22.850787: step 25430, loss 5.77419e-08, acc 1
2017-08-08T18:34:23.061563: step 25431, loss 0, acc 1
2017-08-08T18:34:23.417383: step 25432, loss 0.000229181, acc 1
2017-08-08T18:34:23.763575: step 25433, loss 0, acc 1
2017-08-08T18:34:24.078420: step 25434, loss 4.71242e-07, acc 1
2017-08-08T18:34:24.292938: step 25435, loss 3.31764e-05, acc 1
2017-08-08T18:34:24.555694: step 25436, loss 1.88866e-06, acc 1
2017-08-08T18:34:24.805458: step 25437, loss 3.91155e-08, acc 1
2017-08-08T18:34:25.019011: step 25438, loss 3.77699e-06, acc 1
2017-08-08T18:34:25.217014: step 25439, loss 0, acc 1
2017-08-08T18:34:25.429174: step 25440, loss 3.50175e-07, acc 1
2017-08-08T18:34:25.749330: step 25441, loss 3.04327e-06, acc 1
2017-08-08T18:34:26.033400: step 25442, loss 1.23598e-05, acc 1
2017-08-08T18:34:26.355971: step 25443, loss 8.56815e-08, acc 1
2017-08-08T18:34:26.562261: step 25444, loss 1.86265e-09, acc 1
2017-08-08T18:34:26.921669: step 25445, loss 1.67638e-08, acc 1
2017-08-08T18:34:27.129984: step 25446, loss 3.68109e-05, acc 1
2017-08-08T18:34:27.343827: step 25447, loss 1.07098e-06, acc 1
2017-08-08T18:34:27.556743: step 25448, loss 0, acc 1
2017-08-08T18:34:27.851113: step 25449, loss 1.72844e-06, acc 1
2017-08-08T18:34:28.218558: step 25450, loss 3.45869e-06, acc 1
2017-08-08T18:34:28.592617: step 25451, loss 4.09914e-06, acc 1
2017-08-08T18:34:28.871736: step 25452, loss 7.37595e-07, acc 1
2017-08-08T18:34:29.306353: step 25453, loss 0, acc 1
2017-08-08T18:34:29.533171: step 25454, loss 1.80676e-07, acc 1
2017-08-08T18:34:29.794458: step 25455, loss 1.6165e-05, acc 1
2017-08-08T18:34:29.981093: step 25456, loss 1.32062e-05, acc 1
2017-08-08T18:34:30.197351: step 25457, loss 0, acc 1
2017-08-08T18:34:30.668894: step 25458, loss 0.00120371, acc 1
2017-08-08T18:34:30.926278: step 25459, loss 2.86845e-07, acc 1
2017-08-08T18:34:31.141252: step 25460, loss 2.5572e-06, acc 1
2017-08-08T18:34:31.519368: step 25461, loss 1.86265e-09, acc 1
2017-08-08T18:34:31.721780: step 25462, loss 2.04891e-08, acc 1
2017-08-08T18:34:31.939451: step 25463, loss 1.86265e-09, acc 1
2017-08-08T18:34:32.146341: step 25464, loss 6.14672e-08, acc 1
2017-08-08T18:34:32.321367: step 25465, loss 1.11759e-08, acc 1
2017-08-08T18:34:32.632978: step 25466, loss 1.49012e-08, acc 1
2017-08-08T18:34:32.902396: step 25467, loss 2.93036e-05, acc 1
2017-08-08T18:34:33.129131: step 25468, loss 6.61233e-07, acc 1
2017-08-08T18:34:33.295924: step 25469, loss 1.74395e-05, acc 1
2017-08-08T18:34:33.604261: step 25470, loss 9.80747e-06, acc 1
2017-08-08T18:34:33.886390: step 25471, loss 9.31322e-09, acc 1
2017-08-08T18:34:34.137698: step 25472, loss 0, acc 1
2017-08-08T18:34:34.423338: step 25473, loss 2.47731e-07, acc 1
2017-08-08T18:34:34.772717: step 25474, loss 1.16763e-05, acc 1
2017-08-08T18:34:35.145353: step 25475, loss 0, acc 1
2017-08-08T18:34:35.375311: step 25476, loss 0, acc 1
2017-08-08T18:34:35.669586: step 25477, loss 9.37747e-06, acc 1
2017-08-08T18:34:35.898094: step 25478, loss 1.86265e-09, acc 1
2017-08-08T18:34:36.205396: step 25479, loss 2.88574e-05, acc 1
2017-08-08T18:34:36.444898: step 25480, loss 2.42143e-07, acc 1
2017-08-08T18:34:36.643957: step 25481, loss 1.30449e-05, acc 1
2017-08-08T18:34:36.865351: step 25482, loss 3.91155e-08, acc 1
2017-08-08T18:34:37.135501: step 25483, loss 7.35727e-07, acc 1
2017-08-08T18:34:37.522657: step 25484, loss 7.54939e-06, acc 1
2017-08-08T18:34:37.799168: step 25485, loss 3.72529e-08, acc 1
2017-08-08T18:34:38.097146: step 25486, loss 2.42144e-08, acc 1
2017-08-08T18:34:38.325238: step 25487, loss 9.12694e-08, acc 1
2017-08-08T18:34:38.650487: step 25488, loss 1.28749e-05, acc 1
2017-08-08T18:34:38.928206: step 25489, loss 1.86265e-09, acc 1
2017-08-08T18:34:39.198544: step 25490, loss 0.000158031, acc 1
2017-08-08T18:34:39.498087: step 25491, loss 0, acc 1
2017-08-08T18:34:39.868036: step 25492, loss 1.92772e-06, acc 1
2017-08-08T18:34:40.146802: step 25493, loss 4.20952e-07, acc 1
2017-08-08T18:34:40.457153: step 25494, loss 0.0287387, acc 0.984375
2017-08-08T18:34:40.732475: step 25495, loss 5.1479e-06, acc 1
2017-08-08T18:34:40.947766: step 25496, loss 2.6077e-08, acc 1
2017-08-08T18:34:41.305316: step 25497, loss 9.31322e-09, acc 1
2017-08-08T18:34:41.599946: step 25498, loss 0, acc 1
2017-08-08T18:34:41.828162: step 25499, loss 1.72844e-06, acc 1
2017-08-08T18:34:42.137318: step 25500, loss 1.08874e-06, acc 1

Evaluation:
2017-08-08T18:34:42.872080: step 25500, loss 8.43872, acc 0.722326

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-25500

2017-08-08T18:34:43.242391: step 25501, loss 2.7567e-07, acc 1
2017-08-08T18:34:43.585370: step 25502, loss 1.67638e-08, acc 1
2017-08-08T18:34:43.974982: step 25503, loss 0.000245041, acc 1
2017-08-08T18:34:44.281930: step 25504, loss 7.45058e-09, acc 1
2017-08-08T18:34:44.631881: step 25505, loss 8.0278e-07, acc 1
2017-08-08T18:34:45.004100: step 25506, loss 2.29104e-07, acc 1
2017-08-08T18:34:45.299894: step 25507, loss 5.14195e-06, acc 1
2017-08-08T18:34:45.582881: step 25508, loss 1.24234e-06, acc 1
2017-08-08T18:34:45.785588: step 25509, loss 1.86265e-09, acc 1
2017-08-08T18:34:46.153378: step 25510, loss 4.43303e-07, acc 1
2017-08-08T18:34:46.550848: step 25511, loss 3.72529e-09, acc 1
2017-08-08T18:34:46.917181: step 25512, loss 1.9127e-05, acc 1
2017-08-08T18:34:47.208278: step 25513, loss 9.31322e-09, acc 1
2017-08-08T18:34:47.617364: step 25514, loss 3.72529e-09, acc 1
2017-08-08T18:34:47.993374: step 25515, loss 8.00936e-08, acc 1
2017-08-08T18:34:48.455127: step 25516, loss 0, acc 1
2017-08-08T18:34:48.667035: step 25517, loss 1.30385e-08, acc 1
2017-08-08T18:34:48.997375: step 25518, loss 1.34663e-06, acc 1
2017-08-08T18:34:49.313724: step 25519, loss 1.86264e-08, acc 1
2017-08-08T18:34:49.552988: step 25520, loss 1.76948e-06, acc 1
2017-08-08T18:34:49.816310: step 25521, loss 1.32247e-07, acc 1
2017-08-08T18:34:50.032897: step 25522, loss 1.27216e-06, acc 1
2017-08-08T18:34:50.450752: step 25523, loss 2.79744e-06, acc 1
2017-08-08T18:34:50.816663: step 25524, loss 3.82566e-05, acc 1
2017-08-08T18:34:51.108701: step 25525, loss 5.08792e-06, acc 1
2017-08-08T18:34:51.374609: step 25526, loss 4.36666e-05, acc 1
2017-08-08T18:34:51.649137: step 25527, loss 0, acc 1
2017-08-08T18:34:52.094271: step 25528, loss 5.95284e-05, acc 1
2017-08-08T18:34:52.368756: step 25529, loss 0.00403961, acc 1
2017-08-08T18:34:52.719974: step 25530, loss 1.86265e-09, acc 1
2017-08-08T18:34:52.967681: step 25531, loss 1.86265e-09, acc 1
2017-08-08T18:34:53.353232: step 25532, loss 4.90369e-05, acc 1
2017-08-08T18:34:53.692906: step 25533, loss 1.11759e-08, acc 1
2017-08-08T18:34:54.043417: step 25534, loss 1.18273e-06, acc 1
2017-08-08T18:34:54.396771: step 25535, loss 0, acc 1
2017-08-08T18:34:54.655832: step 25536, loss 7.1338e-07, acc 1
2017-08-08T18:34:55.062061: step 25537, loss 0, acc 1
2017-08-08T18:34:55.287195: step 25538, loss 1.16225e-06, acc 1
2017-08-08T18:34:55.474168: step 25539, loss 0.000434754, acc 1
2017-08-08T18:34:55.707215: step 25540, loss 1.66139e-06, acc 1
2017-08-08T18:34:56.053013: step 25541, loss 1.67638e-08, acc 1
2017-08-08T18:34:56.347616: step 25542, loss 3.44191e-06, acc 1
2017-08-08T18:34:56.645251: step 25543, loss 3.35276e-08, acc 1
2017-08-08T18:34:56.869487: step 25544, loss 0, acc 1
2017-08-08T18:34:57.110127: step 25545, loss 2.17731e-06, acc 1
2017-08-08T18:34:57.379174: step 25546, loss 0, acc 1
2017-08-08T18:34:57.626873: step 25547, loss 7.45058e-09, acc 1
2017-08-08T18:34:57.839628: step 25548, loss 5.64375e-07, acc 1
2017-08-08T18:34:58.031364: step 25549, loss 0.000184526, acc 1
2017-08-08T18:34:58.434419: step 25550, loss 2.51455e-07, acc 1
2017-08-08T18:34:58.781013: step 25551, loss 7.11844e-05, acc 1
2017-08-08T18:34:58.975771: step 25552, loss 1.99474e-05, acc 1
2017-08-08T18:34:59.173037: step 25553, loss 2.77137e-06, acc 1
2017-08-08T18:34:59.373177: step 25554, loss 2.78259e-06, acc 1
2017-08-08T18:34:59.702682: step 25555, loss 4.84287e-08, acc 1
2017-08-08T18:34:59.946683: step 25556, loss 4.13101e-06, acc 1
2017-08-08T18:35:00.234927: step 25557, loss 1.56462e-07, acc 1
2017-08-08T18:35:00.534708: step 25558, loss 1.11759e-08, acc 1
2017-08-08T18:35:00.913055: step 25559, loss 1.93517e-06, acc 1
2017-08-08T18:35:01.230032: step 25560, loss 3.91155e-08, acc 1
2017-08-08T18:35:01.547882: step 25561, loss 1.30385e-08, acc 1
2017-08-08T18:35:01.779253: step 25562, loss 0, acc 1
2017-08-08T18:35:01.998800: step 25563, loss 3.07335e-07, acc 1
2017-08-08T18:35:02.366646: step 25564, loss 6.33298e-08, acc 1
2017-08-08T18:35:02.779870: step 25565, loss 0, acc 1
2017-08-08T18:35:03.095109: step 25566, loss 3.72529e-09, acc 1
2017-08-08T18:35:03.368349: step 25567, loss 0.000145062, acc 1
2017-08-08T18:35:03.713434: step 25568, loss 2.50878e-06, acc 1
2017-08-08T18:35:04.070830: step 25569, loss 2.4773e-07, acc 1
2017-08-08T18:35:04.427048: step 25570, loss 4.00444e-06, acc 1
2017-08-08T18:35:04.717171: step 25571, loss 1.11759e-08, acc 1
2017-08-08T18:35:04.953893: step 25572, loss 4.28408e-08, acc 1
2017-08-08T18:35:05.281383: step 25573, loss 1.29635e-06, acc 1
2017-08-08T18:35:05.580471: step 25574, loss 5.73267e-06, acc 1
2017-08-08T18:35:05.824537: step 25575, loss 1.04308e-07, acc 1
2017-08-08T18:35:06.079986: step 25576, loss 3.91155e-08, acc 1
2017-08-08T18:35:06.353644: step 25577, loss 2.39726e-05, acc 1
2017-08-08T18:35:06.761785: step 25578, loss 1.39764e-05, acc 1
2017-08-08T18:35:07.177400: step 25579, loss 0.000390101, acc 1
2017-08-08T18:35:07.518422: step 25580, loss 7.47949e-05, acc 1
2017-08-08T18:35:07.777932: step 25581, loss 4.34273e-05, acc 1
2017-08-08T18:35:08.161605: step 25582, loss 1.28706e-06, acc 1
2017-08-08T18:35:08.441220: step 25583, loss 3.51631e-06, acc 1
2017-08-08T18:35:08.707551: step 25584, loss 1.86265e-09, acc 1
2017-08-08T18:35:08.960105: step 25585, loss 1.50874e-07, acc 1
2017-08-08T18:35:09.200387: step 25586, loss 3.34512e-06, acc 1
2017-08-08T18:35:09.574509: step 25587, loss 0, acc 1
2017-08-08T18:35:09.985369: step 25588, loss 6.35149e-07, acc 1
2017-08-08T18:35:10.345848: step 25589, loss 3.09746e-06, acc 1
2017-08-08T18:35:10.587389: step 25590, loss 2.43615e-06, acc 1
2017-08-08T18:35:10.953353: step 25591, loss 5.47608e-07, acc 1
2017-08-08T18:35:11.300660: step 25592, loss 1.86265e-09, acc 1
2017-08-08T18:35:11.594774: step 25593, loss 6.3598e-06, acc 1
2017-08-08T18:35:11.904975: step 25594, loss 0, acc 1
2017-08-08T18:35:12.237303: step 25595, loss 0, acc 1
2017-08-08T18:35:12.676701: step 25596, loss 0, acc 1
2017-08-08T18:35:13.049369: step 25597, loss 3.56305e-05, acc 1
2017-08-08T18:35:13.451592: step 25598, loss 1.82529e-06, acc 1
2017-08-08T18:35:13.697530: step 25599, loss 7.88445e-06, acc 1
2017-08-08T18:35:13.923767: step 25600, loss 1.76383e-06, acc 1

Evaluation:
2017-08-08T18:35:14.547551: step 25600, loss 8.44185, acc 0.726079

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-25600

2017-08-08T18:35:15.056036: step 25601, loss 6.94751e-07, acc 1
2017-08-08T18:35:15.512360: step 25602, loss 5.58794e-09, acc 1
2017-08-08T18:35:15.865985: step 25603, loss 0, acc 1
2017-08-08T18:35:16.244135: step 25604, loss 1.11759e-08, acc 1
2017-08-08T18:35:16.508654: step 25605, loss 6.09078e-07, acc 1
2017-08-08T18:35:16.841678: step 25606, loss 5.2154e-08, acc 1
2017-08-08T18:35:17.186238: step 25607, loss 1.11759e-08, acc 1
2017-08-08T18:35:17.461008: step 25608, loss 8.75424e-07, acc 1
2017-08-08T18:35:17.741968: step 25609, loss 0, acc 1
2017-08-08T18:35:18.009380: step 25610, loss 3.72529e-08, acc 1
2017-08-08T18:35:18.374131: step 25611, loss 4.70614e-05, acc 1
2017-08-08T18:35:18.835948: step 25612, loss 0, acc 1
2017-08-08T18:35:19.210121: step 25613, loss 0, acc 1
2017-08-08T18:35:19.460047: step 25614, loss 3.72529e-09, acc 1
2017-08-08T18:35:19.833321: step 25615, loss 6.14661e-07, acc 1
2017-08-08T18:35:20.228182: step 25616, loss 3.85112e-05, acc 1
2017-08-08T18:35:20.453904: step 25617, loss 1.67638e-08, acc 1
2017-08-08T18:35:20.723607: step 25618, loss 0, acc 1
2017-08-08T18:35:21.082900: step 25619, loss 9.81587e-07, acc 1
2017-08-08T18:35:21.411474: step 25620, loss 0, acc 1
2017-08-08T18:35:21.716366: step 25621, loss 7.36317e-06, acc 1
2017-08-08T18:35:22.072249: step 25622, loss 0, acc 1
2017-08-08T18:35:22.350634: step 25623, loss 9.4531e-05, acc 1
2017-08-08T18:35:22.731290: step 25624, loss 7.45058e-09, acc 1
2017-08-08T18:35:22.990958: step 25625, loss 2.0489e-07, acc 1
2017-08-08T18:35:23.261262: step 25626, loss 6.91026e-07, acc 1
2017-08-08T18:35:23.491708: step 25627, loss 1.95566e-06, acc 1
2017-08-08T18:35:23.774477: step 25628, loss 6.89177e-08, acc 1
2017-08-08T18:35:24.162975: step 25629, loss 9.19504e-06, acc 1
2017-08-08T18:35:24.597429: step 25630, loss 0, acc 1
2017-08-08T18:35:24.807919: step 25631, loss 2.01164e-07, acc 1
2017-08-08T18:35:25.016934: step 25632, loss 0, acc 1
2017-08-08T18:35:25.380922: step 25633, loss 0, acc 1
2017-08-08T18:35:25.631144: step 25634, loss 0, acc 1
2017-08-08T18:35:25.829386: step 25635, loss 5.77419e-08, acc 1
2017-08-08T18:35:26.032431: step 25636, loss 1.3411e-07, acc 1
2017-08-08T18:35:26.445256: step 25637, loss 1.07103e-05, acc 1
2017-08-08T18:35:26.849383: step 25638, loss 8.41249e-06, acc 1
2017-08-08T18:35:27.200469: step 25639, loss 1.86265e-09, acc 1
2017-08-08T18:35:27.388082: step 25640, loss 1.49012e-08, acc 1
2017-08-08T18:35:27.573948: step 25641, loss 1.17346e-07, acc 1
2017-08-08T18:35:27.986391: step 25642, loss 1.86265e-09, acc 1
2017-08-08T18:35:28.260376: step 25643, loss 7.45058e-09, acc 1
2017-08-08T18:35:28.560880: step 25644, loss 0, acc 1
2017-08-08T18:35:28.848405: step 25645, loss 2.66357e-07, acc 1
2017-08-08T18:35:29.148997: step 25646, loss 0.00878058, acc 1
2017-08-08T18:35:29.497399: step 25647, loss 0.0167105, acc 0.984375
2017-08-08T18:35:29.754495: step 25648, loss 0.000949931, acc 1
2017-08-08T18:35:29.937976: step 25649, loss 0.000793771, acc 1
2017-08-08T18:35:30.161313: step 25650, loss 7.9074e-07, acc 1
2017-08-08T18:35:30.530622: step 25651, loss 2.79397e-08, acc 1
2017-08-08T18:35:30.754780: step 25652, loss 0, acc 1
2017-08-08T18:35:30.983908: step 25653, loss 0, acc 1
2017-08-08T18:35:31.165565: step 25654, loss 3.72529e-09, acc 1
2017-08-08T18:35:31.465366: step 25655, loss 9.3132e-08, acc 1
2017-08-08T18:35:31.821319: step 25656, loss 6.79855e-07, acc 1
2017-08-08T18:35:32.156145: step 25657, loss 4.84287e-08, acc 1
2017-08-08T18:35:32.403015: step 25658, loss 0.000117982, acc 1
2017-08-08T18:35:32.624072: step 25659, loss 5.10468e-06, acc 1
2017-08-08T18:35:33.118183: step 25660, loss 6.37012e-07, acc 1
2017-08-08T18:35:33.389772: step 25661, loss 2.38407e-06, acc 1
2017-08-08T18:35:33.656325: step 25662, loss 9.31322e-09, acc 1
2017-08-08T18:35:34.044108: step 25663, loss 1.46559e-05, acc 1
2017-08-08T18:35:34.452146: step 25664, loss 4.30959e-06, acc 1
2017-08-08T18:35:34.772100: step 25665, loss 8.75442e-08, acc 1
2017-08-08T18:35:35.059529: step 25666, loss 0, acc 1
2017-08-08T18:35:35.290511: step 25667, loss 6.51925e-08, acc 1
2017-08-08T18:35:35.657201: step 25668, loss 0, acc 1
2017-08-08T18:35:35.843823: step 25669, loss 1.11759e-08, acc 1
2017-08-08T18:35:36.064520: step 25670, loss 9.79655e-05, acc 1
2017-08-08T18:35:36.344029: step 25671, loss 0, acc 1
2017-08-08T18:35:36.618764: step 25672, loss 2.23517e-08, acc 1
2017-08-08T18:35:36.972425: step 25673, loss 0, acc 1
2017-08-08T18:35:37.394232: step 25674, loss 1.93714e-07, acc 1
2017-08-08T18:35:37.779284: step 25675, loss 1.86265e-09, acc 1
2017-08-08T18:35:38.065386: step 25676, loss 2.06752e-07, acc 1
2017-08-08T18:35:38.303470: step 25677, loss 1.69679e-06, acc 1
2017-08-08T18:35:38.778596: step 25678, loss 2.44005e-07, acc 1
2017-08-08T18:35:39.050180: step 25679, loss 0.00726156, acc 1
2017-08-08T18:35:39.333623: step 25680, loss 9.59239e-07, acc 1
2017-08-08T18:35:39.597334: step 25681, loss 0.000187388, acc 1
2017-08-08T18:35:40.072816: step 25682, loss 9.12694e-08, acc 1
2017-08-08T18:35:40.468686: step 25683, loss 3.72529e-08, acc 1
2017-08-08T18:35:40.843956: step 25684, loss 1.06842e-05, acc 1
2017-08-08T18:35:41.171038: step 25685, loss 5.58793e-09, acc 1
2017-08-08T18:35:41.403778: step 25686, loss 8.36307e-07, acc 1
2017-08-08T18:35:41.779083: step 25687, loss 8.88687e-05, acc 1
2017-08-08T18:35:41.994298: step 25688, loss 2.06753e-07, acc 1
2017-08-08T18:35:42.189383: step 25689, loss 0.000838533, acc 1
2017-08-08T18:35:42.448720: step 25690, loss 1.34128e-05, acc 1
2017-08-08T18:35:42.766949: step 25691, loss 7.24096e-06, acc 1
2017-08-08T18:35:43.137585: step 25692, loss 0, acc 1
2017-08-08T18:35:43.424238: step 25693, loss 0, acc 1
2017-08-08T18:35:43.656192: step 25694, loss 0.0103107, acc 1
2017-08-08T18:35:44.012531: step 25695, loss 1.47148e-07, acc 1
2017-08-08T18:35:44.271173: step 25696, loss 0.000276894, acc 1
2017-08-08T18:35:44.578207: step 25697, loss 1.09896e-07, acc 1
2017-08-08T18:35:44.940125: step 25698, loss 4.54047e-06, acc 1
2017-08-08T18:35:45.453440: step 25699, loss 0.00272263, acc 1
2017-08-08T18:35:45.845371: step 25700, loss 9.07145e-06, acc 1

Evaluation:
2017-08-08T18:35:46.488530: step 25700, loss 8.49243, acc 0.718574

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-25700

2017-08-08T18:35:47.119676: step 25701, loss 0, acc 1
2017-08-08T18:35:47.371910: step 25702, loss 6.55886e-06, acc 1
2017-08-08T18:35:47.673365: step 25703, loss 1.19209e-07, acc 1
2017-08-08T18:35:48.041354: step 25704, loss 5.58793e-09, acc 1
2017-08-08T18:35:48.352179: step 25705, loss 7.45058e-09, acc 1
2017-08-08T18:35:48.613245: step 25706, loss 3.72529e-09, acc 1
2017-08-08T18:35:48.800058: step 25707, loss 2.79397e-08, acc 1
2017-08-08T18:35:49.204980: step 25708, loss 2.6077e-08, acc 1
2017-08-08T18:35:49.524401: step 25709, loss 5.02913e-08, acc 1
2017-08-08T18:35:49.826512: step 25710, loss 4.19092e-07, acc 1
2017-08-08T18:35:50.081415: step 25711, loss 1.47149e-07, acc 1
2017-08-08T18:35:50.469942: step 25712, loss 0.000123955, acc 1
2017-08-08T18:35:50.879752: step 25713, loss 4.24679e-07, acc 1
2017-08-08T18:35:51.177932: step 25714, loss 0, acc 1
2017-08-08T18:35:51.399467: step 25715, loss 6.74267e-07, acc 1
2017-08-08T18:35:51.673783: step 25716, loss 2.6077e-08, acc 1
2017-08-08T18:35:52.117497: step 25717, loss 1.08913e-05, acc 1
2017-08-08T18:35:52.356572: step 25718, loss 3.14571e-06, acc 1
2017-08-08T18:35:52.589871: step 25719, loss 1.46584e-06, acc 1
2017-08-08T18:35:52.834743: step 25720, loss 2.6077e-08, acc 1
2017-08-08T18:35:53.239534: step 25721, loss 3.72529e-09, acc 1
2017-08-08T18:35:53.556747: step 25722, loss 0, acc 1
2017-08-08T18:35:53.889726: step 25723, loss 1.07658e-06, acc 1
2017-08-08T18:35:54.173359: step 25724, loss 2.79397e-08, acc 1
2017-08-08T18:35:54.436289: step 25725, loss 0.218194, acc 0.96875
2017-08-08T18:35:54.864254: step 25726, loss 1.86265e-09, acc 1
2017-08-08T18:35:55.121051: step 25727, loss 1.86264e-08, acc 1
2017-08-08T18:35:55.341390: step 25728, loss 6.54955e-06, acc 1
2017-08-08T18:35:55.566227: step 25729, loss 0.00279429, acc 1
2017-08-08T18:35:55.949594: step 25730, loss 4.24677e-07, acc 1
2017-08-08T18:35:56.372713: step 25731, loss 0.000451865, acc 1
2017-08-08T18:35:56.724967: step 25732, loss 8.85082e-06, acc 1
2017-08-08T18:35:56.964040: step 25733, loss 6.91031e-07, acc 1
2017-08-08T18:35:57.240185: step 25734, loss 1.98732e-06, acc 1
2017-08-08T18:35:57.515950: step 25735, loss 5.96038e-07, acc 1
2017-08-08T18:35:57.730093: step 25736, loss 4.00231e-06, acc 1
2017-08-08T18:35:57.934233: step 25737, loss 0.00078521, acc 1
2017-08-08T18:35:58.277039: step 25738, loss 2.26667e-06, acc 1
2017-08-08T18:35:58.600813: step 25739, loss 0, acc 1
2017-08-08T18:35:58.906593: step 25740, loss 5.58793e-09, acc 1
2017-08-08T18:35:59.129574: step 25741, loss 8.56815e-08, acc 1
2017-08-08T18:35:59.350813: step 25742, loss 1.16785e-06, acc 1
2017-08-08T18:35:59.637470: step 25743, loss 1.86265e-09, acc 1
2017-08-08T18:35:59.881515: step 25744, loss 7.63683e-08, acc 1
2017-08-08T18:36:00.105470: step 25745, loss 1.25808e-05, acc 1
2017-08-08T18:36:00.330867: step 25746, loss 4.07241e-05, acc 1
2017-08-08T18:36:00.703991: step 25747, loss 5.58793e-09, acc 1
2017-08-08T18:36:00.994805: step 25748, loss 1.0617e-07, acc 1
2017-08-08T18:36:01.384242: step 25749, loss 1.53661e-06, acc 1
2017-08-08T18:36:01.595004: step 25750, loss 0, acc 1
2017-08-08T18:36:01.885302: step 25751, loss 2.10692e-05, acc 1
2017-08-08T18:36:02.293410: step 25752, loss 0, acc 1
2017-08-08T18:36:02.629386: step 25753, loss 0, acc 1
2017-08-08T18:36:02.946098: step 25754, loss 6.98476e-07, acc 1
2017-08-08T18:36:03.272421: step 25755, loss 8.09882e-06, acc 1
2017-08-08T18:36:03.736333: step 25756, loss 3.79975e-07, acc 1
2017-08-08T18:36:04.178428: step 25757, loss 0, acc 1
2017-08-08T18:36:04.502525: step 25758, loss 2.25742e-06, acc 1
2017-08-08T18:36:04.734533: step 25759, loss 1.86265e-09, acc 1
2017-08-08T18:36:05.013671: step 25760, loss 2.04891e-08, acc 1
2017-08-08T18:36:05.461829: step 25761, loss 3.53902e-08, acc 1
2017-08-08T18:36:05.749660: step 25762, loss 3.03609e-07, acc 1
2017-08-08T18:36:06.063727: step 25763, loss 0.155351, acc 0.984375
2017-08-08T18:36:06.363591: step 25764, loss 0, acc 1
2017-08-08T18:36:06.779008: step 25765, loss 6.18387e-07, acc 1
2017-08-08T18:36:07.213229: step 25766, loss 2.45867e-07, acc 1
2017-08-08T18:36:07.574154: step 25767, loss 1.39698e-07, acc 1
2017-08-08T18:36:07.806133: step 25768, loss 5.58793e-09, acc 1
2017-08-08T18:36:08.072654: step 25769, loss 7.45058e-09, acc 1
2017-08-08T18:36:08.495811: step 25770, loss 1.13621e-07, acc 1
2017-08-08T18:36:08.801537: step 25771, loss 0, acc 1
2017-08-08T18:36:09.105966: step 25772, loss 3.72529e-09, acc 1
2017-08-08T18:36:09.473993: step 25773, loss 0, acc 1
2017-08-08T18:36:09.825613: step 25774, loss 1.20639e-05, acc 1
2017-08-08T18:36:10.152178: step 25775, loss 4.13453e-06, acc 1
2017-08-08T18:36:10.399566: step 25776, loss 7.45058e-09, acc 1
2017-08-08T18:36:10.607958: step 25777, loss 1.11759e-08, acc 1
2017-08-08T18:36:11.005538: step 25778, loss 4.6938e-07, acc 1
2017-08-08T18:36:11.290835: step 25779, loss 1.27521e-05, acc 1
2017-08-08T18:36:11.557282: step 25780, loss 6.51925e-08, acc 1
2017-08-08T18:36:11.843333: step 25781, loss 1.33732e-06, acc 1
2017-08-08T18:36:12.216234: step 25782, loss 3.72529e-09, acc 1
2017-08-08T18:36:12.610543: step 25783, loss 0, acc 1
2017-08-08T18:36:12.946189: step 25784, loss 1.83222e-05, acc 1
2017-08-08T18:36:13.332948: step 25785, loss 6.51925e-08, acc 1
2017-08-08T18:36:13.640775: step 25786, loss 5.42867e-06, acc 1
2017-08-08T18:36:14.041266: step 25787, loss 0.0109303, acc 1
2017-08-08T18:36:14.355421: step 25788, loss 5.6806e-05, acc 1
2017-08-08T18:36:14.619436: step 25789, loss 8.67969e-07, acc 1
2017-08-08T18:36:14.905206: step 25790, loss 3.20372e-07, acc 1
2017-08-08T18:36:15.314087: step 25791, loss 0.000578021, acc 1
2017-08-08T18:36:15.626167: step 25792, loss 5.45846e-06, acc 1
2017-08-08T18:36:16.051335: step 25793, loss 3.72529e-08, acc 1
2017-08-08T18:36:16.360459: step 25794, loss 5.77419e-08, acc 1
2017-08-08T18:36:16.608729: step 25795, loss 4.84287e-08, acc 1
2017-08-08T18:36:17.036467: step 25796, loss 0, acc 1
2017-08-08T18:36:17.384298: step 25797, loss 0, acc 1
2017-08-08T18:36:17.638742: step 25798, loss 6.33298e-08, acc 1
2017-08-08T18:36:17.933450: step 25799, loss 4.67519e-07, acc 1
2017-08-08T18:36:18.250000: step 25800, loss 1.98682e-09, acc 1

Evaluation:
2017-08-08T18:36:19.168522: step 25800, loss 8.56951, acc 0.721388

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-25800

2017-08-08T18:36:19.726780: step 25801, loss 0, acc 1
2017-08-08T18:36:20.015557: step 25802, loss 5.40158e-07, acc 1
2017-08-08T18:36:20.430390: step 25803, loss 2.21758e-05, acc 1
2017-08-08T18:36:20.767539: step 25804, loss 5.691e-05, acc 1
2017-08-08T18:36:20.997364: step 25805, loss 0, acc 1
2017-08-08T18:36:21.205535: step 25806, loss 1.67638e-08, acc 1
2017-08-08T18:36:21.469708: step 25807, loss 3.72529e-09, acc 1
2017-08-08T18:36:21.746249: step 25808, loss 1.86265e-09, acc 1
2017-08-08T18:36:22.036112: step 25809, loss 0, acc 1
2017-08-08T18:36:22.295772: step 25810, loss 7.19012e-06, acc 1
2017-08-08T18:36:22.510948: step 25811, loss 6.14672e-08, acc 1
2017-08-08T18:36:22.787474: step 25812, loss 9.68572e-08, acc 1
2017-08-08T18:36:23.215977: step 25813, loss 4.02329e-07, acc 1
2017-08-08T18:36:23.492796: step 25814, loss 4.02094e-06, acc 1
2017-08-08T18:36:23.745369: step 25815, loss 7.45058e-09, acc 1
2017-08-08T18:36:23.989344: step 25816, loss 1.95439e-05, acc 1
2017-08-08T18:36:24.362268: step 25817, loss 0.0227778, acc 0.984375
2017-08-08T18:36:24.684965: step 25818, loss 0, acc 1
2017-08-08T18:36:24.967412: step 25819, loss 2.64495e-07, acc 1
2017-08-08T18:36:25.200551: step 25820, loss 5.58794e-09, acc 1
2017-08-08T18:36:25.534746: step 25821, loss 0, acc 1
2017-08-08T18:36:25.818498: step 25822, loss 0, acc 1
2017-08-08T18:36:26.079058: step 25823, loss 1.46957e-06, acc 1
2017-08-08T18:36:26.336230: step 25824, loss 0, acc 1
2017-08-08T18:36:26.837368: step 25825, loss 0.00039872, acc 1
2017-08-08T18:36:27.207145: step 25826, loss 1.11759e-08, acc 1
2017-08-08T18:36:27.547522: step 25827, loss 0.00541213, acc 1
2017-08-08T18:36:27.834551: step 25828, loss 1.86265e-09, acc 1
2017-08-08T18:36:28.113865: step 25829, loss 9.14543e-07, acc 1
2017-08-08T18:36:28.503070: step 25830, loss 1.67638e-08, acc 1
2017-08-08T18:36:28.706459: step 25831, loss 0.00133098, acc 1
2017-08-08T18:36:28.939878: step 25832, loss 2.50878e-06, acc 1
2017-08-08T18:36:29.200986: step 25833, loss 7.45058e-09, acc 1
2017-08-08T18:36:29.569893: step 25834, loss 1.65022e-06, acc 1
2017-08-08T18:36:29.880403: step 25835, loss 8.31294e-06, acc 1
2017-08-08T18:36:30.180680: step 25836, loss 3.72529e-09, acc 1
2017-08-08T18:36:30.362847: step 25837, loss 5.71822e-07, acc 1
2017-08-08T18:36:30.630354: step 25838, loss 0, acc 1
2017-08-08T18:36:30.903671: step 25839, loss 2.06931e-06, acc 1
2017-08-08T18:36:31.130950: step 25840, loss 3.40273e-06, acc 1
2017-08-08T18:36:31.399937: step 25841, loss 1.22185e-06, acc 1
2017-08-08T18:36:31.674853: step 25842, loss 4.78694e-07, acc 1
2017-08-08T18:36:32.006890: step 25843, loss 0, acc 1
2017-08-08T18:36:32.324983: step 25844, loss 5.77409e-07, acc 1
2017-08-08T18:36:32.542800: step 25845, loss 2.4734e-06, acc 1
2017-08-08T18:36:32.753144: step 25846, loss 3.90239e-05, acc 1
2017-08-08T18:36:33.001368: step 25847, loss 2.04891e-08, acc 1
2017-08-08T18:36:33.435944: step 25848, loss 9.48059e-07, acc 1
2017-08-08T18:36:33.650579: step 25849, loss 3.72529e-09, acc 1
2017-08-08T18:36:33.923894: step 25850, loss 0, acc 1
2017-08-08T18:36:34.222647: step 25851, loss 1.86264e-08, acc 1
2017-08-08T18:36:34.628454: step 25852, loss 4.46598e-06, acc 1
2017-08-08T18:36:35.071688: step 25853, loss 2.04887e-06, acc 1
2017-08-08T18:36:35.441324: step 25854, loss 2.62631e-07, acc 1
2017-08-08T18:36:35.673306: step 25855, loss 0, acc 1
2017-08-08T18:36:35.888968: step 25856, loss 0.029812, acc 0.984375
2017-08-08T18:36:36.245963: step 25857, loss 3.25959e-07, acc 1
2017-08-08T18:36:36.484175: step 25858, loss 5.15948e-07, acc 1
2017-08-08T18:36:36.667778: step 25859, loss 1.3411e-07, acc 1
2017-08-08T18:36:36.934671: step 25860, loss 9.12695e-08, acc 1
2017-08-08T18:36:37.377400: step 25861, loss 2.36601e-05, acc 1
2017-08-08T18:36:37.727152: step 25862, loss 1.78749e-05, acc 1
2017-08-08T18:36:38.076557: step 25863, loss 0, acc 1
2017-08-08T18:36:38.348355: step 25864, loss 0, acc 1
2017-08-08T18:36:38.788489: step 25865, loss 5.77419e-08, acc 1
2017-08-08T18:36:39.139075: step 25866, loss 1.00583e-07, acc 1
2017-08-08T18:36:39.395132: step 25867, loss 9.36892e-07, acc 1
2017-08-08T18:36:39.687660: step 25868, loss 7.45058e-09, acc 1
2017-08-08T18:36:40.021317: step 25869, loss 1.67638e-08, acc 1
2017-08-08T18:36:40.427920: step 25870, loss 1.56082e-06, acc 1
2017-08-08T18:36:40.696486: step 25871, loss 1.86265e-09, acc 1
2017-08-08T18:36:41.041873: step 25872, loss 8.39827e-06, acc 1
2017-08-08T18:36:41.265472: step 25873, loss 1.11759e-08, acc 1
2017-08-08T18:36:41.488537: step 25874, loss 0.059417, acc 0.984375
2017-08-08T18:36:41.875739: step 25875, loss 8.52459e-05, acc 1
2017-08-08T18:36:42.140633: step 25876, loss 2.58906e-07, acc 1
2017-08-08T18:36:42.395479: step 25877, loss 9.05598e-05, acc 1
2017-08-08T18:36:42.622205: step 25878, loss 2.23517e-08, acc 1
2017-08-08T18:36:42.851040: step 25879, loss 1.21071e-07, acc 1
2017-08-08T18:36:43.242079: step 25880, loss 3.1198e-05, acc 1
2017-08-08T18:36:43.523398: step 25881, loss 7.86017e-07, acc 1
2017-08-08T18:36:43.819129: step 25882, loss 5.58793e-09, acc 1
2017-08-08T18:36:44.030601: step 25883, loss 6.98476e-07, acc 1
2017-08-08T18:36:44.349445: step 25884, loss 1.86265e-09, acc 1
2017-08-08T18:36:44.596496: step 25885, loss 1.15484e-07, acc 1
2017-08-08T18:36:44.886501: step 25886, loss 1.33364e-05, acc 1
2017-08-08T18:36:45.112264: step 25887, loss 3.72529e-09, acc 1
2017-08-08T18:36:45.456403: step 25888, loss 1.95577e-07, acc 1
2017-08-08T18:36:45.890647: step 25889, loss 8.56024e-06, acc 1
2017-08-08T18:36:46.254554: step 25890, loss 0.00012041, acc 1
2017-08-08T18:36:46.553449: step 25891, loss 3.0222e-05, acc 1
2017-08-08T18:36:46.909700: step 25892, loss 2.6077e-08, acc 1
2017-08-08T18:36:47.294484: step 25893, loss 0, acc 1
2017-08-08T18:36:47.582307: step 25894, loss 6.06732e-06, acc 1
2017-08-08T18:36:47.830534: step 25895, loss 1.01139e-06, acc 1
2017-08-08T18:36:48.217343: step 25896, loss 1.11759e-08, acc 1
2017-08-08T18:36:48.588371: step 25897, loss 4.31515e-06, acc 1
2017-08-08T18:36:48.985398: step 25898, loss 9.00594e-05, acc 1
2017-08-08T18:36:49.316422: step 25899, loss 2.30967e-07, acc 1
2017-08-08T18:36:49.553535: step 25900, loss 1.44605e-05, acc 1

Evaluation:
2017-08-08T18:36:50.157611: step 25900, loss 8.60124, acc 0.721388

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-25900

2017-08-08T18:36:50.584268: step 25901, loss 0, acc 1
2017-08-08T18:36:50.979162: step 25902, loss 6.51925e-08, acc 1
2017-08-08T18:36:51.303256: step 25903, loss 0.00254844, acc 1
2017-08-08T18:36:51.661696: step 25904, loss 0, acc 1
2017-08-08T18:36:52.030771: step 25905, loss 5.40167e-08, acc 1
2017-08-08T18:36:52.266859: step 25906, loss 0, acc 1
2017-08-08T18:36:52.543063: step 25907, loss 5.58793e-09, acc 1
2017-08-08T18:36:52.921379: step 25908, loss 1.08033e-07, acc 1
2017-08-08T18:36:53.160992: step 25909, loss 0, acc 1
2017-08-08T18:36:53.405309: step 25910, loss 0, acc 1
2017-08-08T18:36:53.720663: step 25911, loss 0.00040911, acc 1
2017-08-08T18:36:54.126670: step 25912, loss 1.57055e-05, acc 1
2017-08-08T18:36:54.531289: step 25913, loss 0, acc 1
2017-08-08T18:36:54.846441: step 25914, loss 0, acc 1
2017-08-08T18:36:55.119417: step 25915, loss 4.89868e-07, acc 1
2017-08-08T18:36:55.380683: step 25916, loss 5.38295e-07, acc 1
2017-08-08T18:36:55.797116: step 25917, loss 0.000274037, acc 1
2017-08-08T18:36:56.019672: step 25918, loss 6.37018e-07, acc 1
2017-08-08T18:36:56.310252: step 25919, loss 0.000287944, acc 1
2017-08-08T18:36:56.581648: step 25920, loss 1.68561e-06, acc 1
2017-08-08T18:36:56.987522: step 25921, loss 2.96333e-06, acc 1
2017-08-08T18:36:57.361377: step 25922, loss 1.01314e-05, acc 1
2017-08-08T18:36:57.753282: step 25923, loss 6.93812e-05, acc 1
2017-08-08T18:36:58.056522: step 25924, loss 3.16649e-08, acc 1
2017-08-08T18:36:58.325536: step 25925, loss 1.86265e-09, acc 1
2017-08-08T18:36:58.644223: step 25926, loss 5.58793e-09, acc 1
2017-08-08T18:36:58.836478: step 25927, loss 4.01732e-06, acc 1
2017-08-08T18:36:59.110239: step 25928, loss 0, acc 1
2017-08-08T18:36:59.409381: step 25929, loss 3.72529e-09, acc 1
2017-08-08T18:36:59.820337: step 25930, loss 0.000102431, acc 1
2017-08-08T18:37:00.207883: step 25931, loss 4.84287e-08, acc 1
2017-08-08T18:37:00.559266: step 25932, loss 4.60035e-06, acc 1
2017-08-08T18:37:00.815663: step 25933, loss 2.7708e-05, acc 1
2017-08-08T18:37:01.259845: step 25934, loss 3.37135e-07, acc 1
2017-08-08T18:37:01.617063: step 25935, loss 2.98023e-08, acc 1
2017-08-08T18:37:01.929177: step 25936, loss 2.84982e-07, acc 1
2017-08-08T18:37:02.217660: step 25937, loss 0, acc 1
2017-08-08T18:37:02.542185: step 25938, loss 7.45058e-09, acc 1
2017-08-08T18:37:03.035821: step 25939, loss 0, acc 1
2017-08-08T18:37:03.460773: step 25940, loss 0.000612029, acc 1
2017-08-08T18:37:03.851781: step 25941, loss 1.02256e-06, acc 1
2017-08-08T18:37:04.104365: step 25942, loss 0, acc 1
2017-08-08T18:37:04.527948: step 25943, loss 1.49012e-08, acc 1
2017-08-08T18:37:04.885421: step 25944, loss 1.76951e-07, acc 1
2017-08-08T18:37:05.185583: step 25945, loss 9.83035e-05, acc 1
2017-08-08T18:37:05.486473: step 25946, loss 0, acc 1
2017-08-08T18:37:05.883529: step 25947, loss 0, acc 1
2017-08-08T18:37:06.278167: step 25948, loss 5.19672e-07, acc 1
2017-08-08T18:37:06.677702: step 25949, loss 3.05471e-07, acc 1
2017-08-08T18:37:06.936702: step 25950, loss 1.19209e-08, acc 1
2017-08-08T18:37:07.328082: step 25951, loss 1.86265e-09, acc 1
2017-08-08T18:37:07.619087: step 25952, loss 0.00283602, acc 1
2017-08-08T18:37:07.850437: step 25953, loss 0.000148173, acc 1
2017-08-08T18:37:08.127386: step 25954, loss 0.0980778, acc 0.984375
2017-08-08T18:37:08.579323: step 25955, loss 2.21653e-07, acc 1
2017-08-08T18:37:08.890543: step 25956, loss 3.16649e-08, acc 1
2017-08-08T18:37:09.088020: step 25957, loss 4.84287e-08, acc 1
2017-08-08T18:37:09.331373: step 25958, loss 0, acc 1
2017-08-08T18:37:09.653098: step 25959, loss 0, acc 1
2017-08-08T18:37:09.869149: step 25960, loss 9.90898e-07, acc 1
2017-08-08T18:37:10.075728: step 25961, loss 2.80129e-06, acc 1
2017-08-08T18:37:10.358406: step 25962, loss 2.98181e-06, acc 1
2017-08-08T18:37:10.743689: step 25963, loss 2.30765e-06, acc 1
2017-08-08T18:37:11.151833: step 25964, loss 0, acc 1
2017-08-08T18:37:11.499503: step 25965, loss 1.79978e-05, acc 1
2017-08-08T18:37:11.761313: step 25966, loss 6.13136e-05, acc 1
2017-08-08T18:37:12.017055: step 25967, loss 0.00248951, acc 1
2017-08-08T18:37:12.348549: step 25968, loss 0.000538447, acc 1
2017-08-08T18:37:12.671618: step 25969, loss 3.31155e-06, acc 1
2017-08-08T18:37:12.943702: step 25970, loss 0.00183455, acc 1
2017-08-08T18:37:13.219408: step 25971, loss 0.000292423, acc 1
2017-08-08T18:37:13.491975: step 25972, loss 3.35276e-08, acc 1
2017-08-08T18:37:13.817392: step 25973, loss 1.86265e-09, acc 1
2017-08-08T18:37:14.299058: step 25974, loss 5.02914e-08, acc 1
2017-08-08T18:37:14.659901: step 25975, loss 0.162332, acc 0.984375
2017-08-08T18:37:14.953390: step 25976, loss 4.21493e-06, acc 1
2017-08-08T18:37:15.212029: step 25977, loss 0.000525123, acc 1
2017-08-08T18:37:15.699966: step 25978, loss 2.04448e-05, acc 1
2017-08-08T18:37:15.897905: step 25979, loss 0.139745, acc 0.984375
2017-08-08T18:37:16.171640: step 25980, loss 3.19414e-06, acc 1
2017-08-08T18:37:16.405715: step 25981, loss 2.30966e-07, acc 1
2017-08-08T18:37:16.736525: step 25982, loss 1.60554e-06, acc 1
2017-08-08T18:37:17.109600: step 25983, loss 1.3411e-07, acc 1
2017-08-08T18:37:17.405444: step 25984, loss 3.35276e-08, acc 1
2017-08-08T18:37:17.644829: step 25985, loss 7.45058e-09, acc 1
2017-08-08T18:37:18.059467: step 25986, loss 2.63696e-05, acc 1
2017-08-08T18:37:18.361910: step 25987, loss 0, acc 1
2017-08-08T18:37:18.635294: step 25988, loss 0, acc 1
2017-08-08T18:37:18.919716: step 25989, loss 3.91155e-08, acc 1
2017-08-08T18:37:19.114917: step 25990, loss 1.73225e-07, acc 1
2017-08-08T18:37:19.420553: step 25991, loss 1.11759e-08, acc 1
2017-08-08T18:37:19.855164: step 25992, loss 2.9185e-06, acc 1
2017-08-08T18:37:20.232652: step 25993, loss 1.86265e-09, acc 1
2017-08-08T18:37:20.517212: step 25994, loss 2.89262e-05, acc 1
2017-08-08T18:37:20.971211: step 25995, loss 0, acc 1
2017-08-08T18:37:21.231528: step 25996, loss 1.64463e-06, acc 1
2017-08-08T18:37:21.500417: step 25997, loss 9.31322e-09, acc 1
2017-08-08T18:37:21.772336: step 25998, loss 1.86265e-09, acc 1
2017-08-08T18:37:22.223555: step 25999, loss 0.0416366, acc 0.984375
2017-08-08T18:37:22.553395: step 26000, loss 0, acc 1

Evaluation:
2017-08-08T18:37:23.187027: step 26000, loss 8.54306, acc 0.733584

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-26000

2017-08-08T18:37:23.668274: step 26001, loss 3.72529e-09, acc 1
2017-08-08T18:37:23.977863: step 26002, loss 4.36946e-06, acc 1
2017-08-08T18:37:24.242088: step 26003, loss 2.57044e-07, acc 1
2017-08-08T18:37:24.506041: step 26004, loss 1.02207e-05, acc 1
2017-08-08T18:37:24.831923: step 26005, loss 0, acc 1
2017-08-08T18:37:25.227940: step 26006, loss 1.9744e-07, acc 1
2017-08-08T18:37:25.601856: step 26007, loss 2.01164e-07, acc 1
2017-08-08T18:37:25.944752: step 26008, loss 0.0235333, acc 0.984375
2017-08-08T18:37:26.288519: step 26009, loss 3.87569e-06, acc 1
2017-08-08T18:37:26.639132: step 26010, loss 0, acc 1
2017-08-08T18:37:26.954393: step 26011, loss 0.000993557, acc 1
2017-08-08T18:37:27.187497: step 26012, loss 2.42144e-08, acc 1
2017-08-08T18:37:27.550871: step 26013, loss 5.58794e-09, acc 1
2017-08-08T18:37:27.887152: step 26014, loss 1.86265e-09, acc 1
2017-08-08T18:37:28.156855: step 26015, loss 3.72529e-09, acc 1
2017-08-08T18:37:28.329677: step 26016, loss 1.08775e-06, acc 1
2017-08-08T18:37:28.605335: step 26017, loss 0, acc 1
2017-08-08T18:37:28.884820: step 26018, loss 8.19562e-08, acc 1
2017-08-08T18:37:29.126594: step 26019, loss 1.43423e-07, acc 1
2017-08-08T18:37:29.394459: step 26020, loss 8.34446e-07, acc 1
2017-08-08T18:37:29.717677: step 26021, loss 3.72529e-09, acc 1
2017-08-08T18:37:30.058138: step 26022, loss 2.95015e-06, acc 1
2017-08-08T18:37:30.392844: step 26023, loss 3.72529e-09, acc 1
2017-08-08T18:37:30.649019: step 26024, loss 2.42144e-08, acc 1
2017-08-08T18:37:30.889328: step 26025, loss 0.000162178, acc 1
2017-08-08T18:37:31.231464: step 26026, loss 4.08577e-05, acc 1
2017-08-08T18:37:31.440949: step 26027, loss 1.1976e-05, acc 1
2017-08-08T18:37:31.714616: step 26028, loss 1.86265e-09, acc 1
2017-08-08T18:37:31.912952: step 26029, loss 7.56237e-06, acc 1
2017-08-08T18:37:32.351218: step 26030, loss 7.6443e-06, acc 1
2017-08-08T18:37:32.645374: step 26031, loss 7.58099e-06, acc 1
2017-08-08T18:37:33.006585: step 26032, loss 0.000448523, acc 1
2017-08-08T18:37:33.264895: step 26033, loss 0, acc 1
2017-08-08T18:37:33.581568: step 26034, loss 1.86265e-09, acc 1
2017-08-08T18:37:33.851326: step 26035, loss 8.73446e-06, acc 1
2017-08-08T18:37:34.087642: step 26036, loss 7.45058e-09, acc 1
2017-08-08T18:37:34.308868: step 26037, loss 1.14922e-06, acc 1
2017-08-08T18:37:34.619774: step 26038, loss 1.7695e-07, acc 1
2017-08-08T18:37:34.933405: step 26039, loss 0.0720612, acc 0.984375
2017-08-08T18:37:35.309983: step 26040, loss 7.96917e-06, acc 1
2017-08-08T18:37:35.587261: step 26041, loss 2.98023e-08, acc 1
2017-08-08T18:37:35.833370: step 26042, loss 0, acc 1
2017-08-08T18:37:36.149395: step 26043, loss 0.000362612, acc 1
2017-08-08T18:37:36.369929: step 26044, loss 0.0151877, acc 0.984375
2017-08-08T18:37:36.578040: step 26045, loss 0, acc 1
2017-08-08T18:37:36.764593: step 26046, loss 1.18087e-06, acc 1
2017-08-08T18:37:36.960963: step 26047, loss 0, acc 1
2017-08-08T18:37:37.265367: step 26048, loss 9.1082e-07, acc 1
2017-08-08T18:37:37.585360: step 26049, loss 0.00293045, acc 1
2017-08-08T18:37:37.905389: step 26050, loss 3.98601e-07, acc 1
2017-08-08T18:37:38.149586: step 26051, loss 1.58324e-07, acc 1
2017-08-08T18:37:38.533781: step 26052, loss 3.91155e-08, acc 1
2017-08-08T18:37:38.843055: step 26053, loss 0.0392942, acc 0.984375
2017-08-08T18:37:39.082231: step 26054, loss 0.0021753, acc 1
2017-08-08T18:37:39.301876: step 26055, loss 0, acc 1
2017-08-08T18:37:39.693337: step 26056, loss 0.000142708, acc 1
2017-08-08T18:37:40.113379: step 26057, loss 0, acc 1
2017-08-08T18:37:40.369425: step 26058, loss 1.05989e-05, acc 1
2017-08-08T18:37:40.601593: step 26059, loss 0, acc 1
2017-08-08T18:37:40.840548: step 26060, loss 0.000662679, acc 1
2017-08-08T18:37:41.106816: step 26061, loss 1.86265e-09, acc 1
2017-08-08T18:37:41.348478: step 26062, loss 7.29986e-06, acc 1
2017-08-08T18:37:41.572102: step 26063, loss 1.02445e-07, acc 1
2017-08-08T18:37:41.794046: step 26064, loss 0.000352529, acc 1
2017-08-08T18:37:41.973597: step 26065, loss 0, acc 1
2017-08-08T18:37:42.295684: step 26066, loss 1.49012e-08, acc 1
2017-08-08T18:37:42.580415: step 26067, loss 0.00018441, acc 1
2017-08-08T18:37:42.797710: step 26068, loss 0.000428507, acc 1
2017-08-08T18:37:43.053921: step 26069, loss 3.3945e-05, acc 1
2017-08-08T18:37:43.276799: step 26070, loss 1.86863e-05, acc 1
2017-08-08T18:37:43.638890: step 26071, loss 1.86265e-09, acc 1
2017-08-08T18:37:43.846483: step 26072, loss 1.59621e-06, acc 1
2017-08-08T18:37:44.074221: step 26073, loss 1.00583e-07, acc 1
2017-08-08T18:37:44.306131: step 26074, loss 1.11759e-08, acc 1
2017-08-08T18:37:44.539440: step 26075, loss 7.89376e-06, acc 1
2017-08-08T18:37:44.893453: step 26076, loss 0, acc 1
2017-08-08T18:37:45.185621: step 26077, loss 0, acc 1
2017-08-08T18:37:45.405443: step 26078, loss 3.89288e-07, acc 1
2017-08-08T18:37:45.633589: step 26079, loss 1.81225e-05, acc 1
2017-08-08T18:37:45.999451: step 26080, loss 4.22019e-06, acc 1
2017-08-08T18:37:46.213668: step 26081, loss 6.4397e-06, acc 1
2017-08-08T18:37:46.449448: step 26082, loss 1.86265e-09, acc 1
2017-08-08T18:37:46.676262: step 26083, loss 6.51187e-05, acc 1
2017-08-08T18:37:46.917291: step 26084, loss 0.000458188, acc 1
2017-08-08T18:37:47.192457: step 26085, loss 6.33299e-08, acc 1
2017-08-08T18:37:47.451965: step 26086, loss 4.73419e-05, acc 1
2017-08-08T18:37:47.635967: step 26087, loss 3.06561e-06, acc 1
2017-08-08T18:37:47.968959: step 26088, loss 3.12763e-05, acc 1
2017-08-08T18:37:48.323327: step 26089, loss 1.80676e-07, acc 1
2017-08-08T18:37:48.616080: step 26090, loss 4.22818e-07, acc 1
2017-08-08T18:37:48.907913: step 26091, loss 1.42113e-06, acc 1
2017-08-08T18:37:49.219376: step 26092, loss 0.00400579, acc 1
2017-08-08T18:37:49.619509: step 26093, loss 0.0140619, acc 0.984375
2017-08-08T18:37:49.941866: step 26094, loss 1.1438e-05, acc 1
2017-08-08T18:37:50.149615: step 26095, loss 1.15294e-06, acc 1
2017-08-08T18:37:50.386366: step 26096, loss 2.23517e-08, acc 1
2017-08-08T18:37:50.814914: step 26097, loss 2.51455e-07, acc 1
2017-08-08T18:37:51.109325: step 26098, loss 1.14157e-05, acc 1
2017-08-08T18:37:51.485524: step 26099, loss 0, acc 1
2017-08-08T18:37:51.781471: step 26100, loss 1.98682e-09, acc 1

Evaluation:
2017-08-08T18:37:52.681371: step 26100, loss 8.76954, acc 0.721388

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-26100

2017-08-08T18:37:53.371020: step 26101, loss 0, acc 1
2017-08-08T18:37:53.650659: step 26102, loss 4.09356e-06, acc 1
2017-08-08T18:37:54.043983: step 26103, loss 5.2154e-08, acc 1
2017-08-08T18:37:54.346833: step 26104, loss 5.40158e-07, acc 1
2017-08-08T18:37:54.606826: step 26105, loss 0, acc 1
2017-08-08T18:37:55.077544: step 26106, loss 3.72529e-09, acc 1
2017-08-08T18:37:55.337668: step 26107, loss 0, acc 1
2017-08-08T18:37:55.744420: step 26108, loss 0.000830724, acc 1
2017-08-08T18:37:56.015241: step 26109, loss 0.000704088, acc 1
2017-08-08T18:37:56.239153: step 26110, loss 7.07804e-08, acc 1
2017-08-08T18:37:56.549415: step 26111, loss 0, acc 1
2017-08-08T18:37:56.745698: step 26112, loss 3.47159e-06, acc 1
2017-08-08T18:37:56.986328: step 26113, loss 4.70169e-05, acc 1
2017-08-08T18:37:57.248306: step 26114, loss 3.94876e-07, acc 1
2017-08-08T18:37:57.533404: step 26115, loss 2.16067e-05, acc 1
2017-08-08T18:37:57.838730: step 26116, loss 7.45058e-09, acc 1
2017-08-08T18:37:58.129684: step 26117, loss 2.42144e-08, acc 1
2017-08-08T18:37:58.464611: step 26118, loss 3.72529e-08, acc 1
2017-08-08T18:37:58.661997: step 26119, loss 9.87484e-05, acc 1
2017-08-08T18:37:59.012941: step 26120, loss 1.86265e-09, acc 1
2017-08-08T18:37:59.283116: step 26121, loss 3.19045e-06, acc 1
2017-08-08T18:37:59.517417: step 26122, loss 0.0101073, acc 1
2017-08-08T18:38:00.008307: step 26123, loss 3.72529e-09, acc 1
2017-08-08T18:38:00.297139: step 26124, loss 3.72529e-09, acc 1
2017-08-08T18:38:00.719484: step 26125, loss 1.86265e-09, acc 1
2017-08-08T18:38:00.962809: step 26126, loss 7.45058e-09, acc 1
2017-08-08T18:38:01.320172: step 26127, loss 7.45056e-08, acc 1
2017-08-08T18:38:01.650441: step 26128, loss 0.00254601, acc 1
2017-08-08T18:38:01.920971: step 26129, loss 3.52036e-07, acc 1
2017-08-08T18:38:02.531639: step 26130, loss 5.69966e-07, acc 1
2017-08-08T18:38:02.848782: step 26131, loss 6.61227e-07, acc 1
2017-08-08T18:38:03.155453: step 26132, loss 7.6927e-06, acc 1
2017-08-08T18:38:03.424855: step 26133, loss 0.000351758, acc 1
2017-08-08T18:38:03.809518: step 26134, loss 2.6077e-08, acc 1
2017-08-08T18:38:04.142991: step 26135, loss 4.84287e-08, acc 1
2017-08-08T18:38:04.517830: step 26136, loss 1.32247e-07, acc 1
2017-08-08T18:38:04.764050: step 26137, loss 1.86265e-09, acc 1
2017-08-08T18:38:04.966964: step 26138, loss 4.09782e-08, acc 1
2017-08-08T18:38:05.354627: step 26139, loss 1.21815e-06, acc 1
2017-08-08T18:38:05.619408: step 26140, loss 1.55713e-06, acc 1
2017-08-08T18:38:05.910886: step 26141, loss 7.82309e-08, acc 1
2017-08-08T18:38:06.156787: step 26142, loss 0, acc 1
2017-08-08T18:38:06.637131: step 26143, loss 3.48311e-07, acc 1
2017-08-08T18:38:07.028462: step 26144, loss 3.78113e-07, acc 1
2017-08-08T18:38:07.306689: step 26145, loss 1.86265e-09, acc 1
2017-08-08T18:38:07.497436: step 26146, loss 0, acc 1
2017-08-08T18:38:07.846150: step 26147, loss 0.0782131, acc 0.984375
2017-08-08T18:38:08.133333: step 26148, loss 0.00080669, acc 1
2017-08-08T18:38:08.374977: step 26149, loss 9.12693e-08, acc 1
2017-08-08T18:38:08.592407: step 26150, loss 0, acc 1
2017-08-08T18:38:08.987380: step 26151, loss 3.52036e-07, acc 1
2017-08-08T18:38:09.313363: step 26152, loss 6.69971e-05, acc 1
2017-08-08T18:38:09.613379: step 26153, loss 2.03027e-07, acc 1
2017-08-08T18:38:09.854361: step 26154, loss 7.97192e-07, acc 1
2017-08-08T18:38:10.121478: step 26155, loss 6.33298e-08, acc 1
2017-08-08T18:38:10.432715: step 26156, loss 2.13072e-06, acc 1
2017-08-08T18:38:10.615007: step 26157, loss 3.3341e-07, acc 1
2017-08-08T18:38:10.948370: step 26158, loss 9.31322e-09, acc 1
2017-08-08T18:38:11.325649: step 26159, loss 0, acc 1
2017-08-08T18:38:11.568680: step 26160, loss 0, acc 1
2017-08-08T18:38:11.837212: step 26161, loss 1.76941e-06, acc 1
2017-08-08T18:38:12.040914: step 26162, loss 1.15484e-07, acc 1
2017-08-08T18:38:12.349348: step 26163, loss 0, acc 1
2017-08-08T18:38:12.579682: step 26164, loss 3.72529e-09, acc 1
2017-08-08T18:38:12.809329: step 26165, loss 1.95577e-07, acc 1
2017-08-08T18:38:13.003515: step 26166, loss 6.70551e-08, acc 1
2017-08-08T18:38:13.243894: step 26167, loss 1.89223e-05, acc 1
2017-08-08T18:38:13.579124: step 26168, loss 1.11759e-08, acc 1
2017-08-08T18:38:13.930525: step 26169, loss 0, acc 1
2017-08-08T18:38:14.263041: step 26170, loss 4.28408e-08, acc 1
2017-08-08T18:38:14.463602: step 26171, loss 0, acc 1
2017-08-08T18:38:14.754702: step 26172, loss 1.11759e-08, acc 1
2017-08-08T18:38:15.053534: step 26173, loss 2.59445e-06, acc 1
2017-08-08T18:38:15.260304: step 26174, loss 0, acc 1
2017-08-08T18:38:15.477422: step 26175, loss 5.43883e-07, acc 1
2017-08-08T18:38:15.761378: step 26176, loss 2.9102e-05, acc 1
2017-08-08T18:38:16.128190: step 26177, loss 7.63683e-08, acc 1
2017-08-08T18:38:16.422924: step 26178, loss 3.20158e-05, acc 1
2017-08-08T18:38:16.655360: step 26179, loss 6.16523e-07, acc 1
2017-08-08T18:38:16.881523: step 26180, loss 1.86265e-09, acc 1
2017-08-08T18:38:17.282400: step 26181, loss 5.58793e-09, acc 1
2017-08-08T18:38:17.583949: step 26182, loss 0.0976125, acc 0.984375
2017-08-08T18:38:17.856000: step 26183, loss 0, acc 1
2017-08-08T18:38:18.172634: step 26184, loss 1.77318e-06, acc 1
2017-08-08T18:38:18.597409: step 26185, loss 0.000916949, acc 1
2017-08-08T18:38:19.052652: step 26186, loss 8.7073e-06, acc 1
2017-08-08T18:38:19.395922: step 26187, loss 4.20953e-07, acc 1
2017-08-08T18:38:19.677487: step 26188, loss 4.47035e-08, acc 1
2017-08-08T18:38:20.063471: step 26189, loss 0.0265804, acc 0.984375
2017-08-08T18:38:20.335688: step 26190, loss 3.76958e-05, acc 1
2017-08-08T18:38:20.657173: step 26191, loss 2.79397e-08, acc 1
2017-08-08T18:38:20.929379: step 26192, loss 9.87389e-06, acc 1
2017-08-08T18:38:21.151370: step 26193, loss 2.04891e-08, acc 1
2017-08-08T18:38:21.457398: step 26194, loss 0, acc 1
2017-08-08T18:38:21.702235: step 26195, loss 6.90889e-06, acc 1
2017-08-08T18:38:21.946095: step 26196, loss 0.270368, acc 0.984375
2017-08-08T18:38:22.198642: step 26197, loss 1.30385e-08, acc 1
2017-08-08T18:38:22.562411: step 26198, loss 0.000397595, acc 1
2017-08-08T18:38:22.868463: step 26199, loss 0, acc 1
2017-08-08T18:38:23.113569: step 26200, loss 0.000125013, acc 1

Evaluation:
2017-08-08T18:38:23.583911: step 26200, loss 8.63472, acc 0.725141

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-26200

2017-08-08T18:38:24.156329: step 26201, loss 7.45058e-09, acc 1
2017-08-08T18:38:24.428175: step 26202, loss 2.79397e-08, acc 1
2017-08-08T18:38:24.662808: step 26203, loss 0, acc 1
2017-08-08T18:38:24.991434: step 26204, loss 0, acc 1
2017-08-08T18:38:25.307680: step 26205, loss 4.70061e-06, acc 1
2017-08-08T18:38:25.582172: step 26206, loss 2.81421e-06, acc 1
2017-08-08T18:38:25.792265: step 26207, loss 2.65784e-06, acc 1
2017-08-08T18:38:26.125374: step 26208, loss 0.000378311, acc 1
2017-08-08T18:38:26.397003: step 26209, loss 0, acc 1
2017-08-08T18:38:26.653608: step 26210, loss 2.54044e-06, acc 1
2017-08-08T18:38:26.989128: step 26211, loss 0, acc 1
2017-08-08T18:38:27.279140: step 26212, loss 4.09743e-06, acc 1
2017-08-08T18:38:27.491886: step 26213, loss 0.000713992, acc 1
2017-08-08T18:38:27.726599: step 26214, loss 8.21407e-07, acc 1
2017-08-08T18:38:28.006006: step 26215, loss 1.60925e-06, acc 1
2017-08-08T18:38:28.279106: step 26216, loss 0, acc 1
2017-08-08T18:38:28.478012: step 26217, loss 0, acc 1
2017-08-08T18:38:28.689724: step 26218, loss 0.000490435, acc 1
2017-08-08T18:38:28.950062: step 26219, loss 0.000612079, acc 1
2017-08-08T18:38:29.265479: step 26220, loss 0.000150369, acc 1
2017-08-08T18:38:29.506048: step 26221, loss 1.11759e-08, acc 1
2017-08-08T18:38:29.681759: step 26222, loss 0, acc 1
2017-08-08T18:38:29.863307: step 26223, loss 4.833e-06, acc 1
2017-08-08T18:38:30.146442: step 26224, loss 0, acc 1
2017-08-08T18:38:30.343677: step 26225, loss 0, acc 1
2017-08-08T18:38:30.548246: step 26226, loss 0, acc 1
2017-08-08T18:38:30.769386: step 26227, loss 0.0996375, acc 0.984375
2017-08-08T18:38:31.128200: step 26228, loss 2.86449e-06, acc 1
2017-08-08T18:38:31.501121: step 26229, loss 4.09782e-08, acc 1
2017-08-08T18:38:31.870391: step 26230, loss 1.86265e-09, acc 1
2017-08-08T18:38:32.145532: step 26231, loss 1.67638e-08, acc 1
2017-08-08T18:38:32.360189: step 26232, loss 2.42142e-07, acc 1
2017-08-08T18:38:32.772166: step 26233, loss 1.86734e-05, acc 1
2017-08-08T18:38:33.037913: step 26234, loss 8.75441e-08, acc 1
2017-08-08T18:38:33.307149: step 26235, loss 1.86263e-07, acc 1
2017-08-08T18:38:33.513839: step 26236, loss 1.43937e-05, acc 1
2017-08-08T18:38:33.843412: step 26237, loss 0.000322338, acc 1
2017-08-08T18:38:34.042577: step 26238, loss 0, acc 1
2017-08-08T18:38:34.308273: step 26239, loss 0.00170312, acc 1
2017-08-08T18:38:34.625269: step 26240, loss 0.0343582, acc 0.984375
2017-08-08T18:38:34.860818: step 26241, loss 8.94068e-08, acc 1
2017-08-08T18:38:35.176326: step 26242, loss 0, acc 1
2017-08-08T18:38:35.419466: step 26243, loss 9.31322e-09, acc 1
2017-08-08T18:38:35.615876: step 26244, loss 5.73685e-07, acc 1
2017-08-08T18:38:35.914225: step 26245, loss 0.12832, acc 0.984375
2017-08-08T18:38:36.254664: step 26246, loss 2.42144e-08, acc 1
2017-08-08T18:38:36.624283: step 26247, loss 3.72529e-09, acc 1
2017-08-08T18:38:36.981202: step 26248, loss 1.43417e-06, acc 1
2017-08-08T18:38:37.324237: step 26249, loss 1.72099e-06, acc 1
2017-08-08T18:38:37.539380: step 26250, loss 2.56887e-06, acc 1
2017-08-08T18:38:37.755570: step 26251, loss 4.63467e-05, acc 1
2017-08-08T18:38:38.089432: step 26252, loss 2.86845e-07, acc 1
2017-08-08T18:38:38.401709: step 26253, loss 0.000125475, acc 1
2017-08-08T18:38:38.687717: step 26254, loss 1.45982e-05, acc 1
2017-08-08T18:38:38.895985: step 26255, loss 3.00193e-05, acc 1
2017-08-08T18:38:39.176738: step 26256, loss 3.35252e-05, acc 1
2017-08-08T18:38:39.485903: step 26257, loss 0.000103747, acc 1
2017-08-08T18:38:39.758657: step 26258, loss 4.86965e-05, acc 1
2017-08-08T18:38:39.989019: step 26259, loss 1.67071e-06, acc 1
2017-08-08T18:38:40.199573: step 26260, loss 1.86265e-09, acc 1
2017-08-08T18:38:40.511519: step 26261, loss 1.86265e-09, acc 1
2017-08-08T18:38:40.730121: step 26262, loss 0, acc 1
2017-08-08T18:38:40.990757: step 26263, loss 0.00340105, acc 1
2017-08-08T18:38:41.289752: step 26264, loss 0.00380119, acc 1
2017-08-08T18:38:41.605685: step 26265, loss 8.0941e-05, acc 1
2017-08-08T18:38:41.984613: step 26266, loss 1.52663e-05, acc 1
2017-08-08T18:38:42.261615: step 26267, loss 0.0740338, acc 0.984375
2017-08-08T18:38:42.452879: step 26268, loss 4.1909e-07, acc 1
2017-08-08T18:38:42.806158: step 26269, loss 5.1672e-05, acc 1
2017-08-08T18:38:43.034450: step 26270, loss 9.50408e-06, acc 1
2017-08-08T18:38:43.264227: step 26271, loss 1.49012e-08, acc 1
2017-08-08T18:38:43.487477: step 26272, loss 0.000396912, acc 1
2017-08-08T18:38:43.866752: step 26273, loss 0, acc 1
2017-08-08T18:38:44.206769: step 26274, loss 1.30385e-08, acc 1
2017-08-08T18:38:44.574269: step 26275, loss 1.58689e-06, acc 1
2017-08-08T18:38:44.891965: step 26276, loss 0.000145912, acc 1
2017-08-08T18:38:45.135315: step 26277, loss 1.29177e-05, acc 1
2017-08-08T18:38:45.489266: step 26278, loss 0.000164137, acc 1
2017-08-08T18:38:45.688937: step 26279, loss 3.46427e-05, acc 1
2017-08-08T18:38:45.908343: step 26280, loss 6.77536e-06, acc 1
2017-08-08T18:38:46.225391: step 26281, loss 0, acc 1
2017-08-08T18:38:46.562240: step 26282, loss 6.57748e-05, acc 1
2017-08-08T18:38:46.881949: step 26283, loss 6.38874e-07, acc 1
2017-08-08T18:38:47.114287: step 26284, loss 5.58793e-08, acc 1
2017-08-08T18:38:47.387576: step 26285, loss 8.97782e-07, acc 1
2017-08-08T18:38:47.773802: step 26286, loss 3.02811e-05, acc 1
2017-08-08T18:38:48.034439: step 26287, loss 1.73225e-07, acc 1
2017-08-08T18:38:48.330606: step 26288, loss 0, acc 1
2017-08-08T18:38:48.621095: step 26289, loss 2.10465e-06, acc 1
2017-08-08T18:38:49.108526: step 26290, loss 1.86265e-09, acc 1
2017-08-08T18:38:49.457803: step 26291, loss 1.86265e-09, acc 1
2017-08-08T18:38:49.684289: step 26292, loss 6.64957e-07, acc 1
2017-08-08T18:38:49.876119: step 26293, loss 2.23517e-08, acc 1
2017-08-08T18:38:50.075570: step 26294, loss 2.65218e-06, acc 1
2017-08-08T18:38:50.394714: step 26295, loss 0, acc 1
2017-08-08T18:38:50.597765: step 26296, loss 7.45058e-09, acc 1
2017-08-08T18:38:50.795678: step 26297, loss 0, acc 1
2017-08-08T18:38:51.102034: step 26298, loss 0.0134101, acc 0.984375
2017-08-08T18:38:51.425762: step 26299, loss 8.13955e-07, acc 1
2017-08-08T18:38:51.691515: step 26300, loss 0.000384181, acc 1

Evaluation:
2017-08-08T18:38:52.259013: step 26300, loss 8.61558, acc 0.729831

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-26300

2017-08-08T18:38:52.654605: step 26301, loss 9.31322e-09, acc 1
2017-08-08T18:38:52.903499: step 26302, loss 9.31322e-09, acc 1
2017-08-08T18:38:53.344343: step 26303, loss 0.000394629, acc 1
2017-08-08T18:38:53.777338: step 26304, loss 1.86264e-08, acc 1
2017-08-08T18:38:54.147896: step 26305, loss 1.00207e-06, acc 1
2017-08-08T18:38:54.398860: step 26306, loss 5.10357e-07, acc 1
2017-08-08T18:38:54.651302: step 26307, loss 0, acc 1
2017-08-08T18:38:55.102160: step 26308, loss 0, acc 1
2017-08-08T18:38:55.398467: step 26309, loss 0, acc 1
2017-08-08T18:38:55.688489: step 26310, loss 4.28408e-08, acc 1
2017-08-08T18:38:55.972769: step 26311, loss 3.70662e-07, acc 1
2017-08-08T18:38:56.445873: step 26312, loss 2.48457e-06, acc 1
2017-08-08T18:38:56.834577: step 26313, loss 1.02445e-07, acc 1
2017-08-08T18:38:57.166256: step 26314, loss 0, acc 1
2017-08-08T18:38:57.378153: step 26315, loss 1.08033e-07, acc 1
2017-08-08T18:38:57.792886: step 26316, loss 7.45058e-09, acc 1
2017-08-08T18:38:58.039564: step 26317, loss 0.0789853, acc 0.984375
2017-08-08T18:38:58.259650: step 26318, loss 0, acc 1
2017-08-08T18:38:58.477325: step 26319, loss 1.86264e-08, acc 1
2017-08-08T18:38:58.844401: step 26320, loss 1.86265e-09, acc 1
2017-08-08T18:38:59.207107: step 26321, loss 3.29686e-07, acc 1
2017-08-08T18:38:59.595302: step 26322, loss 3.74387e-07, acc 1
2017-08-08T18:38:59.828926: step 26323, loss 1.95938e-06, acc 1
2017-08-08T18:39:00.006157: step 26324, loss 6.76126e-07, acc 1
2017-08-08T18:39:00.317577: step 26325, loss 7.45058e-09, acc 1
2017-08-08T18:39:00.661230: step 26326, loss 6.05349e-07, acc 1
2017-08-08T18:39:00.899108: step 26327, loss 0.0043907, acc 1
2017-08-08T18:39:01.117460: step 26328, loss 0, acc 1
2017-08-08T18:39:01.389552: step 26329, loss 8.02511e-06, acc 1
2017-08-08T18:39:02.293833: step 26330, loss 0, acc 1
2017-08-08T18:39:02.820290: step 26331, loss 2.4773e-07, acc 1
2017-08-08T18:39:03.237412: step 26332, loss 0.000259106, acc 1
2017-08-08T18:39:03.509657: step 26333, loss 1.11759e-08, acc 1
2017-08-08T18:39:03.812393: step 26334, loss 0, acc 1
2017-08-08T18:39:04.261751: step 26335, loss 3.72529e-09, acc 1
2017-08-08T18:39:04.564306: step 26336, loss 5.58794e-09, acc 1
2017-08-08T18:39:04.878797: step 26337, loss 0, acc 1
2017-08-08T18:39:05.161994: step 26338, loss 1.15666e-06, acc 1
2017-08-08T18:39:05.513711: step 26339, loss 0, acc 1
2017-08-08T18:39:05.925419: step 26340, loss 2.38028e-06, acc 1
2017-08-08T18:39:06.268016: step 26341, loss 5.96045e-08, acc 1
2017-08-08T18:39:06.533182: step 26342, loss 0, acc 1
2017-08-08T18:39:06.792402: step 26343, loss 3.61349e-07, acc 1
2017-08-08T18:39:07.205391: step 26344, loss 1.67638e-08, acc 1
2017-08-08T18:39:07.402394: step 26345, loss 0, acc 1
2017-08-08T18:39:07.669973: step 26346, loss 7.2643e-08, acc 1
2017-08-08T18:39:07.872763: step 26347, loss 0, acc 1
2017-08-08T18:39:08.228959: step 26348, loss 6.07212e-07, acc 1
2017-08-08T18:39:08.519351: step 26349, loss 3.27891e-05, acc 1
2017-08-08T18:39:08.815284: step 26350, loss 0, acc 1
2017-08-08T18:39:09.023962: step 26351, loss 2.57043e-07, acc 1
2017-08-08T18:39:09.204156: step 26352, loss 6.66595e-05, acc 1
2017-08-08T18:39:09.565372: step 26353, loss 2.06367e-06, acc 1
2017-08-08T18:39:09.842582: step 26354, loss 2.53282e-05, acc 1
2017-08-08T18:39:10.109206: step 26355, loss 0, acc 1
2017-08-08T18:39:10.345400: step 26356, loss 1.05609e-06, acc 1
2017-08-08T18:39:10.650336: step 26357, loss 4.73107e-07, acc 1
2017-08-08T18:39:10.918548: step 26358, loss 8.38188e-08, acc 1
2017-08-08T18:39:11.267174: step 26359, loss 0, acc 1
2017-08-08T18:39:11.608977: step 26360, loss 0.00167835, acc 1
2017-08-08T18:39:11.850371: step 26361, loss 0, acc 1
2017-08-08T18:39:12.061326: step 26362, loss 2.79397e-08, acc 1
2017-08-08T18:39:12.362368: step 26363, loss 0, acc 1
2017-08-08T18:39:12.621935: step 26364, loss 0, acc 1
2017-08-08T18:39:12.883285: step 26365, loss 5.58793e-09, acc 1
2017-08-08T18:39:13.219585: step 26366, loss 2.38176e-05, acc 1
2017-08-08T18:39:13.556512: step 26367, loss 3.35276e-08, acc 1
2017-08-08T18:39:13.938058: step 26368, loss 4.92592e-06, acc 1
2017-08-08T18:39:14.266025: step 26369, loss 5.58794e-09, acc 1
2017-08-08T18:39:14.468260: step 26370, loss 0, acc 1
2017-08-08T18:39:14.652850: step 26371, loss 1.86265e-09, acc 1
2017-08-08T18:39:14.931532: step 26372, loss 4.57559e-05, acc 1
2017-08-08T18:39:15.136506: step 26373, loss 2.0489e-07, acc 1
2017-08-08T18:39:15.339970: step 26374, loss 1.22253e-05, acc 1
2017-08-08T18:39:15.542059: step 26375, loss 7.56216e-07, acc 1
2017-08-08T18:39:15.890479: step 26376, loss 0.000101591, acc 1
2017-08-08T18:39:16.217379: step 26377, loss 0.00307428, acc 1
2017-08-08T18:39:16.597420: step 26378, loss 0.000298987, acc 1
2017-08-08T18:39:16.940059: step 26379, loss 1.49012e-08, acc 1
2017-08-08T18:39:17.239396: step 26380, loss 5.58793e-09, acc 1
2017-08-08T18:39:17.558660: step 26381, loss 0, acc 1
2017-08-08T18:39:17.968011: step 26382, loss 6.92889e-07, acc 1
2017-08-08T18:39:18.234386: step 26383, loss 2.22948e-05, acc 1
2017-08-08T18:39:18.501467: step 26384, loss 0, acc 1
2017-08-08T18:39:18.955906: step 26385, loss 2.79397e-08, acc 1
2017-08-08T18:39:19.305473: step 26386, loss 7.2643e-08, acc 1
2017-08-08T18:39:19.617552: step 26387, loss 0, acc 1
2017-08-08T18:39:19.948027: step 26388, loss 9.0895e-07, acc 1
2017-08-08T18:39:20.226231: step 26389, loss 3.72529e-09, acc 1
2017-08-08T18:39:20.667222: step 26390, loss 0.000459109, acc 1
2017-08-08T18:39:20.950022: step 26391, loss 0, acc 1
2017-08-08T18:39:21.204631: step 26392, loss 0, acc 1
2017-08-08T18:39:21.478496: step 26393, loss 2.4622e-05, acc 1
2017-08-08T18:39:21.907378: step 26394, loss 7.85466e-06, acc 1
2017-08-08T18:39:22.321047: step 26395, loss 1.86265e-09, acc 1
2017-08-08T18:39:22.653295: step 26396, loss 7.86016e-07, acc 1
2017-08-08T18:39:22.855603: step 26397, loss 1.67638e-08, acc 1
2017-08-08T18:39:23.085401: step 26398, loss 0, acc 1
2017-08-08T18:39:23.425125: step 26399, loss 0, acc 1
2017-08-08T18:39:23.601343: step 26400, loss 5.96046e-09, acc 1

Evaluation:
2017-08-08T18:39:24.110934: step 26400, loss 8.61155, acc 0.718574

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-26400

2017-08-08T18:39:24.657410: step 26401, loss 3.72529e-09, acc 1
2017-08-08T18:39:24.861833: step 26402, loss 5.08606e-06, acc 1
2017-08-08T18:39:25.073174: step 26403, loss 0, acc 1
2017-08-08T18:39:25.525795: step 26404, loss 2.74685e-05, acc 1
2017-08-08T18:39:25.773767: step 26405, loss 5.58793e-09, acc 1
2017-08-08T18:39:26.091291: step 26406, loss 9.872e-08, acc 1
2017-08-08T18:39:26.371735: step 26407, loss 1.68934e-06, acc 1
2017-08-08T18:39:26.709415: step 26408, loss 0, acc 1
2017-08-08T18:39:27.056567: step 26409, loss 5.06635e-07, acc 1
2017-08-08T18:39:27.285025: step 26410, loss 0, acc 1
2017-08-08T18:39:27.496771: step 26411, loss 1.86265e-09, acc 1
2017-08-08T18:39:27.830795: step 26412, loss 3.72529e-09, acc 1
2017-08-08T18:39:28.100597: step 26413, loss 3.53902e-08, acc 1
2017-08-08T18:39:28.357392: step 26414, loss 0, acc 1
2017-08-08T18:39:28.613165: step 26415, loss 4.84287e-08, acc 1
2017-08-08T18:39:29.006251: step 26416, loss 0, acc 1
2017-08-08T18:39:29.350408: step 26417, loss 1.60187e-07, acc 1
2017-08-08T18:39:29.650373: step 26418, loss 2.98023e-08, acc 1
2017-08-08T18:39:29.915874: step 26419, loss 1.93204e-05, acc 1
2017-08-08T18:39:30.111122: step 26420, loss 0, acc 1
2017-08-08T18:39:30.465505: step 26421, loss 1.21997e-05, acc 1
2017-08-08T18:39:30.785062: step 26422, loss 0, acc 1
2017-08-08T18:39:31.058631: step 26423, loss 9.12669e-07, acc 1
2017-08-08T18:39:31.349850: step 26424, loss 4.19092e-07, acc 1
2017-08-08T18:39:31.684094: step 26425, loss 0, acc 1
2017-08-08T18:39:31.992630: step 26426, loss 3.5498e-06, acc 1
2017-08-08T18:39:32.253171: step 26427, loss 0, acc 1
2017-08-08T18:39:32.501991: step 26428, loss 0, acc 1
2017-08-08T18:39:32.771645: step 26429, loss 5.77419e-08, acc 1
2017-08-08T18:39:33.125582: step 26430, loss 1.30385e-08, acc 1
2017-08-08T18:39:33.307520: step 26431, loss 2.94295e-07, acc 1
2017-08-08T18:39:33.509331: step 26432, loss 1.86265e-09, acc 1
2017-08-08T18:39:33.859621: step 26433, loss 9.52639e-06, acc 1
2017-08-08T18:39:34.240467: step 26434, loss 0, acc 1
2017-08-08T18:39:34.589417: step 26435, loss 9.67513e-05, acc 1
2017-08-08T18:39:34.893278: step 26436, loss 9.04984e-06, acc 1
2017-08-08T18:39:35.116756: step 26437, loss 3.48481e-06, acc 1
2017-08-08T18:39:35.458643: step 26438, loss 9.51343e-06, acc 1
2017-08-08T18:39:35.756037: step 26439, loss 1.86265e-09, acc 1
2017-08-08T18:39:35.984488: step 26440, loss 2.79397e-08, acc 1
2017-08-08T18:39:36.213936: step 26441, loss 0.0403579, acc 0.984375
2017-08-08T18:39:36.580800: step 26442, loss 1.08216e-06, acc 1
2017-08-08T18:39:36.863929: step 26443, loss 5.02913e-08, acc 1
2017-08-08T18:39:37.172703: step 26444, loss 8.38188e-08, acc 1
2017-08-08T18:39:37.408458: step 26445, loss 0, acc 1
2017-08-08T18:39:37.762119: step 26446, loss 3.72529e-09, acc 1
2017-08-08T18:39:38.102251: step 26447, loss 0, acc 1
2017-08-08T18:39:38.379859: step 26448, loss 0.018041, acc 0.984375
2017-08-08T18:39:38.614007: step 26449, loss 1.22557e-06, acc 1
2017-08-08T18:39:38.861609: step 26450, loss 9.66697e-07, acc 1
2017-08-08T18:39:39.267394: step 26451, loss 1.626e-06, acc 1
2017-08-08T18:39:39.549584: step 26452, loss 2.41008e-06, acc 1
2017-08-08T18:39:39.939666: step 26453, loss 1.00583e-07, acc 1
2017-08-08T18:39:40.212859: step 26454, loss 1.86265e-09, acc 1
2017-08-08T18:39:40.614462: step 26455, loss 0, acc 1
2017-08-08T18:39:40.829883: step 26456, loss 0, acc 1
2017-08-08T18:39:41.090673: step 26457, loss 7.45058e-09, acc 1
2017-08-08T18:39:41.304641: step 26458, loss 5.69953e-05, acc 1
2017-08-08T18:39:41.690001: step 26459, loss 0, acc 1
2017-08-08T18:39:42.085097: step 26460, loss 1.75088e-07, acc 1
2017-08-08T18:39:42.341961: step 26461, loss 1.17342e-06, acc 1
2017-08-08T18:39:42.549417: step 26462, loss 4.56121e-05, acc 1
2017-08-08T18:39:42.887070: step 26463, loss 1.45178e-05, acc 1
2017-08-08T18:39:43.125531: step 26464, loss 0, acc 1
2017-08-08T18:39:43.387153: step 26465, loss 0, acc 1
2017-08-08T18:39:43.673120: step 26466, loss 2.04891e-08, acc 1
2017-08-08T18:39:44.067468: step 26467, loss 1.67637e-07, acc 1
2017-08-08T18:39:44.471551: step 26468, loss 2.23315e-06, acc 1
2017-08-08T18:39:44.767413: step 26469, loss 1.1963e-05, acc 1
2017-08-08T18:39:44.952046: step 26470, loss 2.04891e-08, acc 1
2017-08-08T18:39:45.143936: step 26471, loss 3.53902e-08, acc 1
2017-08-08T18:39:45.451663: step 26472, loss 0.00290136, acc 1
2017-08-08T18:39:45.619135: step 26473, loss 0.0036269, acc 1
2017-08-08T18:39:45.798094: step 26474, loss 1.89989e-07, acc 1
2017-08-08T18:39:45.989429: step 26475, loss 0, acc 1
2017-08-08T18:39:46.287875: step 26476, loss 9.49947e-08, acc 1
2017-08-08T18:39:46.590985: step 26477, loss 3.72529e-09, acc 1
2017-08-08T18:39:46.868507: step 26478, loss 0, acc 1
2017-08-08T18:39:47.156485: step 26479, loss 9.8689e-06, acc 1
2017-08-08T18:39:47.371533: step 26480, loss 0, acc 1
2017-08-08T18:39:47.788512: step 26481, loss 0, acc 1
2017-08-08T18:39:48.031854: step 26482, loss 0, acc 1
2017-08-08T18:39:48.221256: step 26483, loss 0.00013061, acc 1
2017-08-08T18:39:48.540152: step 26484, loss 0, acc 1
2017-08-08T18:39:48.847195: step 26485, loss 4.78692e-07, acc 1
2017-08-08T18:39:49.249269: step 26486, loss 5.7587e-06, acc 1
2017-08-08T18:39:49.499598: step 26487, loss 1.49012e-08, acc 1
2017-08-08T18:39:49.753870: step 26488, loss 0, acc 1
2017-08-08T18:39:50.093670: step 26489, loss 3.72493e-06, acc 1
2017-08-08T18:39:50.345504: step 26490, loss 1.6819e-06, acc 1
2017-08-08T18:39:50.511114: step 26491, loss 6.89178e-08, acc 1
2017-08-08T18:39:50.725962: step 26492, loss 1.86265e-09, acc 1
2017-08-08T18:39:51.020852: step 26493, loss 2.13817e-06, acc 1
2017-08-08T18:39:51.298353: step 26494, loss 1.19209e-07, acc 1
2017-08-08T18:39:51.580963: step 26495, loss 3.30585e-06, acc 1
2017-08-08T18:39:51.793042: step 26496, loss 8.12095e-07, acc 1
2017-08-08T18:39:52.101671: step 26497, loss 1.86265e-09, acc 1
2017-08-08T18:39:52.356365: step 26498, loss 8.56815e-08, acc 1
2017-08-08T18:39:52.560613: step 26499, loss 2.36555e-07, acc 1
2017-08-08T18:39:52.846580: step 26500, loss 5.02913e-08, acc 1

Evaluation:
2017-08-08T18:39:53.739902: step 26500, loss 8.60535, acc 0.728893

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-26500

2017-08-08T18:39:54.430661: step 26501, loss 0.0177859, acc 0.984375
2017-08-08T18:39:54.744073: step 26502, loss 2.42144e-08, acc 1
2017-08-08T18:39:54.945492: step 26503, loss 0.00242169, acc 1
2017-08-08T18:39:55.249358: step 26504, loss 3.72529e-09, acc 1
2017-08-08T18:39:55.426065: step 26505, loss 1.35114e-05, acc 1
2017-08-08T18:39:55.655930: step 26506, loss 4.59849e-06, acc 1
2017-08-08T18:39:55.966830: step 26507, loss 7.63683e-08, acc 1
2017-08-08T18:39:56.322578: step 26508, loss 0, acc 1
2017-08-08T18:39:56.561417: step 26509, loss 0, acc 1
2017-08-08T18:39:56.722589: step 26510, loss 3.77699e-06, acc 1
2017-08-08T18:39:56.977458: step 26511, loss 6.68684e-07, acc 1
2017-08-08T18:39:57.269996: step 26512, loss 1.91851e-07, acc 1
2017-08-08T18:39:57.467466: step 26513, loss 0.00912096, acc 1
2017-08-08T18:39:57.711178: step 26514, loss 9.87199e-08, acc 1
2017-08-08T18:39:57.945977: step 26515, loss 5.38295e-07, acc 1
2017-08-08T18:39:58.262020: step 26516, loss 9.12693e-08, acc 1
2017-08-08T18:39:58.597464: step 26517, loss 0, acc 1
2017-08-08T18:39:58.880100: step 26518, loss 1.32247e-07, acc 1
2017-08-08T18:39:59.088949: step 26519, loss 0.130831, acc 0.984375
2017-08-08T18:39:59.425393: step 26520, loss 3.16649e-08, acc 1
2017-08-08T18:39:59.770334: step 26521, loss 5.02914e-08, acc 1
2017-08-08T18:40:00.017424: step 26522, loss 0, acc 1
2017-08-08T18:40:00.304037: step 26523, loss 2.89708e-05, acc 1
2017-08-08T18:40:00.577211: step 26524, loss 9.35409e-05, acc 1
2017-08-08T18:40:01.048959: step 26525, loss 0.000346742, acc 1
2017-08-08T18:40:01.483129: step 26526, loss 0.000187308, acc 1
2017-08-08T18:40:01.884136: step 26527, loss 0, acc 1
2017-08-08T18:40:02.208390: step 26528, loss 1.49012e-08, acc 1
2017-08-08T18:40:02.658326: step 26529, loss 0, acc 1
2017-08-08T18:40:03.050885: step 26530, loss 0.000198894, acc 1
2017-08-08T18:40:03.372814: step 26531, loss 3.29685e-07, acc 1
2017-08-08T18:40:03.598566: step 26532, loss 0, acc 1
2017-08-08T18:40:03.991515: step 26533, loss 5.58793e-09, acc 1
2017-08-08T18:40:04.309464: step 26534, loss 9.27571e-07, acc 1
2017-08-08T18:40:04.622166: step 26535, loss 1.86265e-09, acc 1
2017-08-08T18:40:04.821275: step 26536, loss 2.6077e-08, acc 1
2017-08-08T18:40:05.035920: step 26537, loss 1.05658e-05, acc 1
2017-08-08T18:40:05.363153: step 26538, loss 1.30385e-08, acc 1
2017-08-08T18:40:05.714168: step 26539, loss 0.000149313, acc 1
2017-08-08T18:40:06.004062: step 26540, loss 4.18744e-05, acc 1
2017-08-08T18:40:06.279192: step 26541, loss 1.10249e-05, acc 1
2017-08-08T18:40:06.625002: step 26542, loss 4.84287e-08, acc 1
2017-08-08T18:40:07.105206: step 26543, loss 0.00476937, acc 1
2017-08-08T18:40:07.533575: step 26544, loss 3.72529e-09, acc 1
2017-08-08T18:40:07.806579: step 26545, loss 1.53102e-06, acc 1
2017-08-08T18:40:08.035129: step 26546, loss 1.11759e-08, acc 1
2017-08-08T18:40:08.317386: step 26547, loss 1.73219e-06, acc 1
2017-08-08T18:40:08.693696: step 26548, loss 7.45058e-09, acc 1
2017-08-08T18:40:08.946919: step 26549, loss 9.53645e-07, acc 1
2017-08-08T18:40:09.229601: step 26550, loss 5.96046e-09, acc 1
2017-08-08T18:40:09.545232: step 26551, loss 5.96046e-08, acc 1
2017-08-08T18:40:09.945736: step 26552, loss 4.44367e-06, acc 1
2017-08-08T18:40:10.416679: step 26553, loss 2.94296e-07, acc 1
2017-08-08T18:40:10.783132: step 26554, loss 8.51827e-05, acc 1
2017-08-08T18:40:11.090563: step 26555, loss 1.21071e-07, acc 1
2017-08-08T18:40:11.342212: step 26556, loss 2.04891e-08, acc 1
2017-08-08T18:40:11.771277: step 26557, loss 6.89178e-08, acc 1
2017-08-08T18:40:12.095011: step 26558, loss 1.47515e-06, acc 1
2017-08-08T18:40:12.347934: step 26559, loss 1.86265e-09, acc 1
2017-08-08T18:40:12.618529: step 26560, loss 8.75441e-08, acc 1
2017-08-08T18:40:12.970138: step 26561, loss 1.86265e-09, acc 1
2017-08-08T18:40:13.289638: step 26562, loss 0, acc 1
2017-08-08T18:40:13.733478: step 26563, loss 1.86265e-09, acc 1
2017-08-08T18:40:14.026240: step 26564, loss 2.14748e-06, acc 1
2017-08-08T18:40:14.305861: step 26565, loss 0.00039871, acc 1
2017-08-08T18:40:14.693393: step 26566, loss 1.2889e-06, acc 1
2017-08-08T18:40:14.996591: step 26567, loss 3.72529e-09, acc 1
2017-08-08T18:40:15.300328: step 26568, loss 1.3411e-07, acc 1
2017-08-08T18:40:15.562775: step 26569, loss 0, acc 1
2017-08-08T18:40:15.922719: step 26570, loss 0, acc 1
2017-08-08T18:40:16.282915: step 26571, loss 2.21014e-05, acc 1
2017-08-08T18:40:16.679236: step 26572, loss 6.1839e-07, acc 1
2017-08-08T18:40:16.986635: step 26573, loss 8.7222e-06, acc 1
2017-08-08T18:40:17.242617: step 26574, loss 0, acc 1
2017-08-08T18:40:17.640760: step 26575, loss 1.86265e-09, acc 1
2017-08-08T18:40:17.967902: step 26576, loss 3.74387e-07, acc 1
2017-08-08T18:40:18.177533: step 26577, loss 0, acc 1
2017-08-08T18:40:18.485710: step 26578, loss 2.42311e-06, acc 1
2017-08-08T18:40:18.845050: step 26579, loss 0, acc 1
2017-08-08T18:40:19.135906: step 26580, loss 0, acc 1
2017-08-08T18:40:19.469683: step 26581, loss 5.60646e-07, acc 1
2017-08-08T18:40:19.662487: step 26582, loss 0.000100244, acc 1
2017-08-08T18:40:19.890114: step 26583, loss 1.86265e-09, acc 1
2017-08-08T18:40:20.260908: step 26584, loss 7.45058e-09, acc 1
2017-08-08T18:40:20.536865: step 26585, loss 0, acc 1
2017-08-08T18:40:20.864070: step 26586, loss 8.0691e-05, acc 1
2017-08-08T18:40:21.176688: step 26587, loss 7.71116e-07, acc 1
2017-08-08T18:40:21.670507: step 26588, loss 1.67638e-08, acc 1
2017-08-08T18:40:22.029253: step 26589, loss 8.94067e-08, acc 1
2017-08-08T18:40:22.346859: step 26590, loss 5.40167e-08, acc 1
2017-08-08T18:40:22.597725: step 26591, loss 0.000824643, acc 1
2017-08-08T18:40:22.887225: step 26592, loss 0, acc 1
2017-08-08T18:40:23.260723: step 26593, loss 3.72529e-09, acc 1
2017-08-08T18:40:23.500620: step 26594, loss 1.86265e-09, acc 1
2017-08-08T18:40:23.774395: step 26595, loss 0, acc 1
2017-08-08T18:40:24.095977: step 26596, loss 0.00184269, acc 1
2017-08-08T18:40:24.457350: step 26597, loss 0.000727851, acc 1
2017-08-08T18:40:24.859264: step 26598, loss 0, acc 1
2017-08-08T18:40:25.155022: step 26599, loss 1.56877e-05, acc 1
2017-08-08T18:40:25.387206: step 26600, loss 7.07804e-08, acc 1

Evaluation:
2017-08-08T18:40:26.217615: step 26600, loss 8.62679, acc 0.727017

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-26600

2017-08-08T18:40:26.629371: step 26601, loss 0, acc 1
2017-08-08T18:40:26.889384: step 26602, loss 5.40166e-08, acc 1
2017-08-08T18:40:27.365663: step 26603, loss 1.30385e-08, acc 1
2017-08-08T18:40:27.743652: step 26604, loss 1.25084e-05, acc 1
2017-08-08T18:40:28.071706: step 26605, loss 0, acc 1
2017-08-08T18:40:28.294081: step 26606, loss 0, acc 1
2017-08-08T18:40:28.514906: step 26607, loss 1.11759e-08, acc 1
2017-08-08T18:40:28.896633: step 26608, loss 1.22934e-07, acc 1
2017-08-08T18:40:29.166271: step 26609, loss 0, acc 1
2017-08-08T18:40:29.389722: step 26610, loss 0, acc 1
2017-08-08T18:40:29.601051: step 26611, loss 2.89392e-05, acc 1
2017-08-08T18:40:29.959534: step 26612, loss 0, acc 1
2017-08-08T18:40:30.309438: step 26613, loss 0, acc 1
2017-08-08T18:40:30.601613: step 26614, loss 4.8428e-07, acc 1
2017-08-08T18:40:30.861850: step 26615, loss 0, acc 1
2017-08-08T18:40:31.100707: step 26616, loss 4.07914e-07, acc 1
2017-08-08T18:40:31.397323: step 26617, loss 7.87514e-06, acc 1
2017-08-08T18:40:31.765318: step 26618, loss 1.56461e-07, acc 1
2017-08-08T18:40:32.023718: step 26619, loss 1.67638e-08, acc 1
2017-08-08T18:40:32.288572: step 26620, loss 2.98023e-08, acc 1
2017-08-08T18:40:32.509174: step 26621, loss 0, acc 1
2017-08-08T18:40:32.852747: step 26622, loss 0, acc 1
2017-08-08T18:40:33.186172: step 26623, loss 2.45867e-07, acc 1
2017-08-08T18:40:33.457033: step 26624, loss 0, acc 1
2017-08-08T18:40:33.688318: step 26625, loss 1.95086e-05, acc 1
2017-08-08T18:40:33.931211: step 26626, loss 5.51919e-05, acc 1
2017-08-08T18:40:34.412254: step 26627, loss 5.58793e-09, acc 1
2017-08-08T18:40:34.696896: step 26628, loss 1.51769e-05, acc 1
2017-08-08T18:40:34.981455: step 26629, loss 0.000891837, acc 1
2017-08-08T18:40:35.330573: step 26630, loss 5.12702e-06, acc 1
2017-08-08T18:40:35.797383: step 26631, loss 0, acc 1
2017-08-08T18:40:36.077837: step 26632, loss 0, acc 1
2017-08-08T18:40:36.279402: step 26633, loss 0, acc 1
2017-08-08T18:40:36.457839: step 26634, loss 1.86265e-09, acc 1
2017-08-08T18:40:36.739142: step 26635, loss 1.22458e-05, acc 1
2017-08-08T18:40:37.032932: step 26636, loss 0.000801002, acc 1
2017-08-08T18:40:37.231781: step 26637, loss 0, acc 1
2017-08-08T18:40:37.418566: step 26638, loss 1.88116e-06, acc 1
2017-08-08T18:40:37.653800: step 26639, loss 8.75441e-08, acc 1
2017-08-08T18:40:38.043555: step 26640, loss 0, acc 1
2017-08-08T18:40:38.317965: step 26641, loss 1.11759e-08, acc 1
2017-08-08T18:40:38.582190: step 26642, loss 3.16664e-05, acc 1
2017-08-08T18:40:38.804626: step 26643, loss 2.71115e-05, acc 1
2017-08-08T18:40:39.193027: step 26644, loss 1.72889e-05, acc 1
2017-08-08T18:40:39.564338: step 26645, loss 0, acc 1
2017-08-08T18:40:39.762115: step 26646, loss 2.04891e-08, acc 1
2017-08-08T18:40:39.986169: step 26647, loss 3.12921e-07, acc 1
2017-08-08T18:40:40.241409: step 26648, loss 2.79397e-08, acc 1
2017-08-08T18:40:40.567472: step 26649, loss 4.82727e-05, acc 1
2017-08-08T18:40:40.849321: step 26650, loss 3.26125e-05, acc 1
2017-08-08T18:40:41.098867: step 26651, loss 4.09782e-08, acc 1
2017-08-08T18:40:41.293344: step 26652, loss 0, acc 1
2017-08-08T18:40:41.561343: step 26653, loss 0, acc 1
2017-08-08T18:40:41.831880: step 26654, loss 4.60596e-06, acc 1
2017-08-08T18:40:42.047124: step 26655, loss 3.53136e-06, acc 1
2017-08-08T18:40:42.288197: step 26656, loss 7.45058e-09, acc 1
2017-08-08T18:40:42.551386: step 26657, loss 1.86265e-09, acc 1
2017-08-08T18:40:42.830893: step 26658, loss 1.86265e-09, acc 1
2017-08-08T18:40:43.110971: step 26659, loss 4.28408e-08, acc 1
2017-08-08T18:40:43.342318: step 26660, loss 1.34872e-05, acc 1
2017-08-08T18:40:43.548000: step 26661, loss 0, acc 1
2017-08-08T18:40:43.720872: step 26662, loss 4.20952e-07, acc 1
2017-08-08T18:40:44.116080: step 26663, loss 5.50168e-05, acc 1
2017-08-08T18:40:44.457333: step 26664, loss 6.33298e-08, acc 1
2017-08-08T18:40:44.812290: step 26665, loss 1.45548e-05, acc 1
2017-08-08T18:40:45.210931: step 26666, loss 6.57375e-06, acc 1
2017-08-08T18:40:45.481544: step 26667, loss 0, acc 1
2017-08-08T18:40:45.796764: step 26668, loss 1.68933e-06, acc 1
2017-08-08T18:40:46.036395: step 26669, loss 3.16649e-08, acc 1
2017-08-08T18:40:46.276556: step 26670, loss 1.11759e-08, acc 1
2017-08-08T18:40:46.584695: step 26671, loss 0.0476135, acc 0.984375
2017-08-08T18:40:46.782528: step 26672, loss 2.04891e-08, acc 1
2017-08-08T18:40:46.981364: step 26673, loss 1.55584e-05, acc 1
2017-08-08T18:40:47.209222: step 26674, loss 1.39698e-07, acc 1
2017-08-08T18:40:47.473390: step 26675, loss 1.08033e-07, acc 1
2017-08-08T18:40:47.805715: step 26676, loss 0, acc 1
2017-08-08T18:40:48.111391: step 26677, loss 7.82293e-07, acc 1
2017-08-08T18:40:48.354368: step 26678, loss 8.19562e-08, acc 1
2017-08-08T18:40:48.536213: step 26679, loss 5.46066e-05, acc 1
2017-08-08T18:40:48.944555: step 26680, loss 2.67768e-05, acc 1
2017-08-08T18:40:49.146295: step 26681, loss 9.09782e-05, acc 1
2017-08-08T18:40:49.338428: step 26682, loss 0, acc 1
2017-08-08T18:40:49.533557: step 26683, loss 7.32003e-07, acc 1
2017-08-08T18:40:49.815351: step 26684, loss 4.93076e-05, acc 1
2017-08-08T18:40:50.149605: step 26685, loss 0.00811623, acc 1
2017-08-08T18:40:50.444549: step 26686, loss 0.000155795, acc 1
2017-08-08T18:40:50.640728: step 26687, loss 6.03486e-07, acc 1
2017-08-08T18:40:50.872092: step 26688, loss 0, acc 1
2017-08-08T18:40:51.163353: step 26689, loss 0, acc 1
2017-08-08T18:40:51.341940: step 26690, loss 1.80676e-07, acc 1
2017-08-08T18:40:51.554678: step 26691, loss 7.65919e-06, acc 1
2017-08-08T18:40:51.759281: step 26692, loss 9.35032e-07, acc 1
2017-08-08T18:40:52.053623: step 26693, loss 0, acc 1
2017-08-08T18:40:52.309313: step 26694, loss 3.44586e-07, acc 1
2017-08-08T18:40:52.604970: step 26695, loss 4.73107e-07, acc 1
2017-08-08T18:40:52.801455: step 26696, loss 0, acc 1
2017-08-08T18:40:53.020046: step 26697, loss 0.0254991, acc 0.984375
2017-08-08T18:40:53.313518: step 26698, loss 6.61103e-06, acc 1
2017-08-08T18:40:53.543243: step 26699, loss 7.54424e-05, acc 1
2017-08-08T18:40:53.756503: step 26700, loss 1.81388e-06, acc 1

Evaluation:
2017-08-08T18:40:54.290079: step 26700, loss 8.72522, acc 0.73546

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-26700

2017-08-08T18:40:54.841454: step 26701, loss 1.11717e-05, acc 1
2017-08-08T18:40:55.203261: step 26702, loss 3.72529e-09, acc 1
2017-08-08T18:40:55.515376: step 26703, loss 1.67638e-08, acc 1
2017-08-08T18:40:55.819592: step 26704, loss 3.96738e-07, acc 1
2017-08-08T18:40:56.242737: step 26705, loss 9.30632e-05, acc 1
2017-08-08T18:40:56.535055: step 26706, loss 5.58794e-09, acc 1
2017-08-08T18:40:56.855570: step 26707, loss 3.72529e-09, acc 1
2017-08-08T18:40:57.196846: step 26708, loss 2.21653e-07, acc 1
2017-08-08T18:40:57.593833: step 26709, loss 5.55059e-07, acc 1
2017-08-08T18:40:57.873349: step 26710, loss 2.53318e-07, acc 1
2017-08-08T18:40:58.073328: step 26711, loss 1.86265e-09, acc 1
2017-08-08T18:40:58.274853: step 26712, loss 2.42498e-06, acc 1
2017-08-08T18:40:58.581905: step 26713, loss 0, acc 1
2017-08-08T18:40:58.822267: step 26714, loss 5.58793e-09, acc 1
2017-08-08T18:40:59.118905: step 26715, loss 4.56468e-06, acc 1
2017-08-08T18:40:59.357744: step 26716, loss 0, acc 1
2017-08-08T18:40:59.657293: step 26717, loss 1.3187e-06, acc 1
2017-08-08T18:40:59.979217: step 26718, loss 0.0160246, acc 0.984375
2017-08-08T18:41:00.285212: step 26719, loss 3.88159e-05, acc 1
2017-08-08T18:41:00.575214: step 26720, loss 1.66698e-06, acc 1
2017-08-08T18:41:00.815381: step 26721, loss 0, acc 1
2017-08-08T18:41:01.174435: step 26722, loss 7.2643e-08, acc 1
2017-08-08T18:41:01.487906: step 26723, loss 9.31322e-09, acc 1
2017-08-08T18:41:01.783459: step 26724, loss 2.21498e-05, acc 1
2017-08-08T18:41:02.062665: step 26725, loss 7.53653e-05, acc 1
2017-08-08T18:41:02.509510: step 26726, loss 3.12921e-07, acc 1
2017-08-08T18:41:02.961763: step 26727, loss 3.01746e-07, acc 1
2017-08-08T18:41:03.411048: step 26728, loss 2.1234e-07, acc 1
2017-08-08T18:41:03.713555: step 26729, loss 0, acc 1
2017-08-08T18:41:04.133297: step 26730, loss 2.42144e-08, acc 1
2017-08-08T18:41:04.402087: step 26731, loss 0, acc 1
2017-08-08T18:41:04.677424: step 26732, loss 5.1397e-05, acc 1
2017-08-08T18:41:04.968868: step 26733, loss 9.31322e-09, acc 1
2017-08-08T18:41:05.381195: step 26734, loss 0.000257356, acc 1
2017-08-08T18:41:05.822502: step 26735, loss 1.99303e-07, acc 1
2017-08-08T18:41:06.128036: step 26736, loss 0, acc 1
2017-08-08T18:41:06.433644: step 26737, loss 5.71494e-05, acc 1
2017-08-08T18:41:06.847466: step 26738, loss 0.000772156, acc 1
2017-08-08T18:41:07.112613: step 26739, loss 0, acc 1
2017-08-08T18:41:07.324686: step 26740, loss 2.6077e-08, acc 1
2017-08-08T18:41:07.538868: step 26741, loss 4.91861e-06, acc 1
2017-08-08T18:41:07.888313: step 26742, loss 1.11759e-08, acc 1
2017-08-08T18:41:08.249480: step 26743, loss 0, acc 1
2017-08-08T18:41:08.466065: step 26744, loss 8.56023e-06, acc 1
2017-08-08T18:41:08.697298: step 26745, loss 5.40166e-08, acc 1
2017-08-08T18:41:09.171921: step 26746, loss 5.2313e-06, acc 1
2017-08-08T18:41:09.450373: step 26747, loss 1.86264e-08, acc 1
2017-08-08T18:41:09.730264: step 26748, loss 2.03027e-07, acc 1
2017-08-08T18:41:09.989632: step 26749, loss 4.21113e-06, acc 1
2017-08-08T18:41:10.321447: step 26750, loss 0, acc 1
2017-08-08T18:41:10.618539: step 26751, loss 2.23517e-08, acc 1
2017-08-08T18:41:10.959845: step 26752, loss 0, acc 1
2017-08-08T18:41:11.194879: step 26753, loss 4.93593e-07, acc 1
2017-08-08T18:41:11.500194: step 26754, loss 2.23517e-08, acc 1
2017-08-08T18:41:11.839211: step 26755, loss 0, acc 1
2017-08-08T18:41:12.044558: step 26756, loss 9.31322e-09, acc 1
2017-08-08T18:41:12.257067: step 26757, loss 0, acc 1
2017-08-08T18:41:12.570335: step 26758, loss 0.000117014, acc 1
2017-08-08T18:41:12.880593: step 26759, loss 5.5672e-05, acc 1
2017-08-08T18:41:13.187727: step 26760, loss 6.33298e-08, acc 1
2017-08-08T18:41:13.381900: step 26761, loss 0, acc 1
2017-08-08T18:41:13.559455: step 26762, loss 0, acc 1
2017-08-08T18:41:13.849425: step 26763, loss 5.58793e-09, acc 1
2017-08-08T18:41:14.177254: step 26764, loss 0, acc 1
2017-08-08T18:41:14.456170: step 26765, loss 2.17928e-07, acc 1
2017-08-08T18:41:14.681329: step 26766, loss 1.46949e-05, acc 1
2017-08-08T18:41:14.848791: step 26767, loss 1.59549e-05, acc 1
2017-08-08T18:41:15.117327: step 26768, loss 7.45058e-09, acc 1
2017-08-08T18:41:15.351880: step 26769, loss 3.39e-07, acc 1
2017-08-08T18:41:15.621337: step 26770, loss 0, acc 1
2017-08-08T18:41:15.796051: step 26771, loss 1.97439e-07, acc 1
2017-08-08T18:41:16.035184: step 26772, loss 8.47481e-07, acc 1
2017-08-08T18:41:16.384248: step 26773, loss 0.000382994, acc 1
2017-08-08T18:41:16.619825: step 26774, loss 2.69144e-05, acc 1
2017-08-08T18:41:16.899396: step 26775, loss 1.86265e-09, acc 1
2017-08-08T18:41:17.136323: step 26776, loss 0, acc 1
2017-08-08T18:41:17.470864: step 26777, loss 1.86265e-09, acc 1
2017-08-08T18:41:17.735696: step 26778, loss 1.66636e-05, acc 1
2017-08-08T18:41:18.041407: step 26779, loss 4.93593e-07, acc 1
2017-08-08T18:41:18.219893: step 26780, loss 0, acc 1
2017-08-08T18:41:18.543743: step 26781, loss 1.16411e-06, acc 1
2017-08-08T18:41:18.962842: step 26782, loss 3.35273e-07, acc 1
2017-08-08T18:41:19.240214: step 26783, loss 0, acc 1
2017-08-08T18:41:19.532398: step 26784, loss 0, acc 1
2017-08-08T18:41:19.957636: step 26785, loss 0, acc 1
2017-08-08T18:41:20.304349: step 26786, loss 3.72529e-09, acc 1
2017-08-08T18:41:20.675226: step 26787, loss 7.82291e-07, acc 1
2017-08-08T18:41:20.915149: step 26788, loss 5.22734e-05, acc 1
2017-08-08T18:41:21.288631: step 26789, loss 9.87199e-08, acc 1
2017-08-08T18:41:21.654818: step 26790, loss 0, acc 1
2017-08-08T18:41:21.934280: step 26791, loss 2.58906e-07, acc 1
2017-08-08T18:41:22.243365: step 26792, loss 0, acc 1
2017-08-08T18:41:22.625816: step 26793, loss 1.92401e-06, acc 1
2017-08-08T18:41:22.975140: step 26794, loss 0, acc 1
2017-08-08T18:41:23.311993: step 26795, loss 5.15945e-07, acc 1
2017-08-08T18:41:23.631041: step 26796, loss 6.53093e-06, acc 1
2017-08-08T18:41:23.875129: step 26797, loss 1.19209e-07, acc 1
2017-08-08T18:41:24.227467: step 26798, loss 0, acc 1
2017-08-08T18:41:24.636357: step 26799, loss 1.55881e-05, acc 1
2017-08-08T18:41:25.033041: step 26800, loss 0.000300601, acc 1

Evaluation:
2017-08-08T18:41:25.949424: step 26800, loss 8.74678, acc 0.731707

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-26800

2017-08-08T18:41:26.618470: step 26801, loss 1.49012e-08, acc 1
2017-08-08T18:41:26.869344: step 26802, loss 0, acc 1
2017-08-08T18:41:27.144136: step 26803, loss 3.01746e-07, acc 1
2017-08-08T18:41:27.570772: step 26804, loss 8.53068e-07, acc 1
2017-08-08T18:41:27.870307: step 26805, loss 2.25378e-07, acc 1
2017-08-08T18:41:28.158092: step 26806, loss 1.86265e-09, acc 1
2017-08-08T18:41:28.491103: step 26807, loss 3.20372e-07, acc 1
2017-08-08T18:41:28.897137: step 26808, loss 3.72529e-09, acc 1
2017-08-08T18:41:29.300296: step 26809, loss 0, acc 1
2017-08-08T18:41:29.593882: step 26810, loss 4.47034e-08, acc 1
2017-08-08T18:41:29.956078: step 26811, loss 1.86265e-09, acc 1
2017-08-08T18:41:30.304404: step 26812, loss 0, acc 1
2017-08-08T18:41:30.580628: step 26813, loss 9.08944e-07, acc 1
2017-08-08T18:41:30.853973: step 26814, loss 4.35262e-06, acc 1
2017-08-08T18:41:31.372510: step 26815, loss 0, acc 1
2017-08-08T18:41:31.884679: step 26816, loss 7.01129e-06, acc 1
2017-08-08T18:41:32.219288: step 26817, loss 0, acc 1
2017-08-08T18:41:32.507399: step 26818, loss 2.14204e-07, acc 1
2017-08-08T18:41:32.951148: step 26819, loss 0, acc 1
2017-08-08T18:41:33.211438: step 26820, loss 0, acc 1
2017-08-08T18:41:33.461311: step 26821, loss 5.8486e-07, acc 1
2017-08-08T18:41:33.659827: step 26822, loss 2.38417e-07, acc 1
2017-08-08T18:41:33.974775: step 26823, loss 0, acc 1
2017-08-08T18:41:34.338164: step 26824, loss 2.83361e-05, acc 1
2017-08-08T18:41:34.733455: step 26825, loss 0, acc 1
2017-08-08T18:41:34.942651: step 26826, loss 2.04891e-08, acc 1
2017-08-08T18:41:35.185258: step 26827, loss 0, acc 1
2017-08-08T18:41:35.644536: step 26828, loss 5.58793e-09, acc 1
2017-08-08T18:41:35.887455: step 26829, loss 7.45058e-09, acc 1
2017-08-08T18:41:36.092113: step 26830, loss 0.000299325, acc 1
2017-08-08T18:41:36.307872: step 26831, loss 0, acc 1
2017-08-08T18:41:36.552087: step 26832, loss 0, acc 1
2017-08-08T18:41:36.821047: step 26833, loss 0, acc 1
2017-08-08T18:41:37.189351: step 26834, loss 0, acc 1
2017-08-08T18:41:37.411203: step 26835, loss 0.000204887, acc 1
2017-08-08T18:41:37.802767: step 26836, loss 1.95577e-07, acc 1
2017-08-08T18:41:38.098401: step 26837, loss 1.67638e-08, acc 1
2017-08-08T18:41:38.413138: step 26838, loss 4.91289e-06, acc 1
2017-08-08T18:41:38.692123: step 26839, loss 0, acc 1
2017-08-08T18:41:39.076816: step 26840, loss 0.000289387, acc 1
2017-08-08T18:41:39.364394: step 26841, loss 0, acc 1
2017-08-08T18:41:39.613905: step 26842, loss 0, acc 1
2017-08-08T18:41:39.836586: step 26843, loss 1.35973e-07, acc 1
2017-08-08T18:41:40.134705: step 26844, loss 0, acc 1
2017-08-08T18:41:40.534957: step 26845, loss 0.000183201, acc 1
2017-08-08T18:41:40.797184: step 26846, loss 0, acc 1
2017-08-08T18:41:41.033291: step 26847, loss 2.84982e-07, acc 1
2017-08-08T18:41:41.241417: step 26848, loss 3.72529e-09, acc 1
2017-08-08T18:41:41.548953: step 26849, loss 0.000237077, acc 1
2017-08-08T18:41:41.853320: step 26850, loss 1.98682e-09, acc 1
2017-08-08T18:41:42.193676: step 26851, loss 0, acc 1
2017-08-08T18:41:42.393036: step 26852, loss 2.77324e-06, acc 1
2017-08-08T18:41:42.611871: step 26853, loss 2.30142e-05, acc 1
2017-08-08T18:41:43.014052: step 26854, loss 5.58794e-09, acc 1
2017-08-08T18:41:43.234029: step 26855, loss 3.72529e-09, acc 1
2017-08-08T18:41:43.436683: step 26856, loss 1.03436e-05, acc 1
2017-08-08T18:41:43.711326: step 26857, loss 3.05975e-05, acc 1
2017-08-08T18:41:44.109416: step 26858, loss 1.24797e-07, acc 1
2017-08-08T18:41:44.426180: step 26859, loss 5.97902e-07, acc 1
2017-08-08T18:41:44.767709: step 26860, loss 1.86265e-09, acc 1
2017-08-08T18:41:45.012577: step 26861, loss 7.07804e-08, acc 1
2017-08-08T18:41:45.404906: step 26862, loss 0.00251494, acc 1
2017-08-08T18:41:45.648136: step 26863, loss 0, acc 1
2017-08-08T18:41:45.869236: step 26864, loss 2.82165e-06, acc 1
2017-08-08T18:41:46.069620: step 26865, loss 4.66895e-06, acc 1
2017-08-08T18:41:46.333786: step 26866, loss 0, acc 1
2017-08-08T18:41:46.689700: step 26867, loss 2.06752e-07, acc 1
2017-08-08T18:41:46.953293: step 26868, loss 1.11759e-08, acc 1
2017-08-08T18:41:47.223303: step 26869, loss 1.24797e-07, acc 1
2017-08-08T18:41:47.380524: step 26870, loss 3.43993e-06, acc 1
2017-08-08T18:41:47.616301: step 26871, loss 0, acc 1
2017-08-08T18:41:47.875736: step 26872, loss 1.73225e-07, acc 1
2017-08-08T18:41:48.108683: step 26873, loss 3.52685e-05, acc 1
2017-08-08T18:41:48.318503: step 26874, loss 1.86265e-09, acc 1
2017-08-08T18:41:48.504548: step 26875, loss 5.69959e-07, acc 1
2017-08-08T18:41:48.821330: step 26876, loss 0, acc 1
2017-08-08T18:41:49.109886: step 26877, loss 0.000252381, acc 1
2017-08-08T18:41:49.374644: step 26878, loss 1.27586e-06, acc 1
2017-08-08T18:41:49.654834: step 26879, loss 1.0617e-07, acc 1
2017-08-08T18:41:49.895976: step 26880, loss 2.14568e-06, acc 1
2017-08-08T18:41:50.195320: step 26881, loss 0, acc 1
2017-08-08T18:41:50.444091: step 26882, loss 1.86265e-09, acc 1
2017-08-08T18:41:50.653145: step 26883, loss 4.34495e-06, acc 1
2017-08-08T18:41:50.846042: step 26884, loss 2.05064e-06, acc 1
2017-08-08T18:41:51.142621: step 26885, loss 5.58793e-09, acc 1
2017-08-08T18:41:51.419722: step 26886, loss 2.04891e-08, acc 1
2017-08-08T18:41:51.730915: step 26887, loss 0, acc 1
2017-08-08T18:41:51.992996: step 26888, loss 8.60518e-07, acc 1
2017-08-08T18:41:52.168232: step 26889, loss 5.58793e-09, acc 1
2017-08-08T18:41:52.422658: step 26890, loss 1.28519e-06, acc 1
2017-08-08T18:41:52.711525: step 26891, loss 1.22744e-06, acc 1
2017-08-08T18:41:52.914540: step 26892, loss 2.8589e-06, acc 1
2017-08-08T18:41:53.116118: step 26893, loss 9.44344e-07, acc 1
2017-08-08T18:41:53.343972: step 26894, loss 0, acc 1
2017-08-08T18:41:53.694558: step 26895, loss 0, acc 1
2017-08-08T18:41:54.011447: step 26896, loss 1.71362e-07, acc 1
2017-08-08T18:41:54.306939: step 26897, loss 1.00583e-07, acc 1
2017-08-08T18:41:54.496222: step 26898, loss 0, acc 1
2017-08-08T18:41:54.668949: step 26899, loss 8.66117e-07, acc 1
2017-08-08T18:41:54.961653: step 26900, loss 5.58794e-09, acc 1

Evaluation:
2017-08-08T18:41:55.493378: step 26900, loss 8.74354, acc 0.730769

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-26900

2017-08-08T18:41:56.084561: step 26901, loss 0, acc 1
2017-08-08T18:41:56.425414: step 26902, loss 2.6077e-08, acc 1
2017-08-08T18:41:56.725383: step 26903, loss 6.6282e-06, acc 1
2017-08-08T18:41:56.990865: step 26904, loss 4.47034e-08, acc 1
2017-08-08T18:41:57.228093: step 26905, loss 0, acc 1
2017-08-08T18:41:57.650314: step 26906, loss 0, acc 1
2017-08-08T18:41:57.919484: step 26907, loss 3.42296e-05, acc 1
2017-08-08T18:41:58.202121: step 26908, loss 4.93523e-06, acc 1
2017-08-08T18:41:58.514941: step 26909, loss 8.19563e-08, acc 1
2017-08-08T18:41:58.873632: step 26910, loss 0.000129466, acc 1
2017-08-08T18:41:59.271141: step 26911, loss 1.30385e-08, acc 1
2017-08-08T18:41:59.649784: step 26912, loss 3.18107e-06, acc 1
2017-08-08T18:41:59.850745: step 26913, loss 1.11759e-08, acc 1
2017-08-08T18:42:00.094717: step 26914, loss 3.53902e-08, acc 1
2017-08-08T18:42:00.434362: step 26915, loss 3.72529e-09, acc 1
2017-08-08T18:42:00.659974: step 26916, loss 1.86264e-08, acc 1
2017-08-08T18:42:00.900747: step 26917, loss 1.67257e-06, acc 1
2017-08-08T18:42:01.156781: step 26918, loss 0, acc 1
2017-08-08T18:42:01.635373: step 26919, loss 1.86265e-09, acc 1
2017-08-08T18:42:01.977413: step 26920, loss 2.58907e-07, acc 1
2017-08-08T18:42:02.427356: step 26921, loss 0, acc 1
2017-08-08T18:42:02.746040: step 26922, loss 9.31321e-08, acc 1
2017-08-08T18:42:03.034947: step 26923, loss 4.28408e-08, acc 1
2017-08-08T18:42:03.567506: step 26924, loss 1.6205e-07, acc 1
2017-08-08T18:42:03.851014: step 26925, loss 1.04308e-07, acc 1
2017-08-08T18:42:04.191076: step 26926, loss 3.72529e-09, acc 1
2017-08-08T18:42:04.528668: step 26927, loss 1.86265e-09, acc 1
2017-08-08T18:42:04.933631: step 26928, loss 0, acc 1
2017-08-08T18:42:05.253427: step 26929, loss 0, acc 1
2017-08-08T18:42:05.694043: step 26930, loss 2.04891e-08, acc 1
2017-08-08T18:42:05.955739: step 26931, loss 1.49011e-07, acc 1
2017-08-08T18:42:06.201607: step 26932, loss 3.24098e-07, acc 1
2017-08-08T18:42:06.485315: step 26933, loss 0, acc 1
2017-08-08T18:42:06.721989: step 26934, loss 3.35276e-08, acc 1
2017-08-08T18:42:06.943120: step 26935, loss 1.86265e-09, acc 1
2017-08-08T18:42:07.165371: step 26936, loss 3.91155e-08, acc 1
2017-08-08T18:42:07.362626: step 26937, loss 5.2154e-08, acc 1
2017-08-08T18:42:07.705470: step 26938, loss 1.52736e-07, acc 1
2017-08-08T18:42:08.005440: step 26939, loss 7.54353e-07, acc 1
2017-08-08T18:42:08.333208: step 26940, loss 1.86265e-09, acc 1
2017-08-08T18:42:08.522836: step 26941, loss 2.02643e-06, acc 1
2017-08-08T18:42:08.685379: step 26942, loss 0, acc 1
2017-08-08T18:42:09.061395: step 26943, loss 8.19779e-05, acc 1
2017-08-08T18:42:09.267463: step 26944, loss 1.52422e-05, acc 1
2017-08-08T18:42:09.477749: step 26945, loss 2.6077e-08, acc 1
2017-08-08T18:42:09.685352: step 26946, loss 1.11759e-08, acc 1
2017-08-08T18:42:09.984262: step 26947, loss 0.00136676, acc 1
2017-08-08T18:42:10.342899: step 26948, loss 0, acc 1
2017-08-08T18:42:10.761998: step 26949, loss 0, acc 1
2017-08-08T18:42:11.030312: step 26950, loss 4.47034e-08, acc 1
2017-08-08T18:42:11.511858: step 26951, loss 3.92224e-06, acc 1
2017-08-08T18:42:11.764450: step 26952, loss 9.31322e-09, acc 1
2017-08-08T18:42:12.054972: step 26953, loss 9.12694e-08, acc 1
2017-08-08T18:42:12.321165: step 26954, loss 8.87348e-05, acc 1
2017-08-08T18:42:12.633357: step 26955, loss 0, acc 1
2017-08-08T18:42:13.105003: step 26956, loss 0.00311244, acc 1
2017-08-08T18:42:13.444392: step 26957, loss 9.31322e-09, acc 1
2017-08-08T18:42:13.800103: step 26958, loss 1.49011e-07, acc 1
2017-08-08T18:42:14.044050: step 26959, loss 7.45058e-09, acc 1
2017-08-08T18:42:14.500572: step 26960, loss 0, acc 1
2017-08-08T18:42:14.743654: step 26961, loss 0, acc 1
2017-08-08T18:42:14.948324: step 26962, loss 6.51925e-08, acc 1
2017-08-08T18:42:15.194332: step 26963, loss 5.21266e-05, acc 1
2017-08-08T18:42:15.458773: step 26964, loss 4.71896e-05, acc 1
2017-08-08T18:42:15.725472: step 26965, loss 0, acc 1
2017-08-08T18:42:15.987057: step 26966, loss 0.000121672, acc 1
2017-08-08T18:42:16.156000: step 26967, loss 1.86265e-09, acc 1
2017-08-08T18:42:16.357527: step 26968, loss 1.30385e-08, acc 1
2017-08-08T18:42:16.606865: step 26969, loss 1.86265e-09, acc 1
2017-08-08T18:42:16.771746: step 26970, loss 3.53902e-08, acc 1
2017-08-08T18:42:16.976401: step 26971, loss 0, acc 1
2017-08-08T18:42:17.204546: step 26972, loss 1.21072e-07, acc 1
2017-08-08T18:42:17.461563: step 26973, loss 0, acc 1
2017-08-08T18:42:17.673486: step 26974, loss 0, acc 1
2017-08-08T18:42:17.904014: step 26975, loss 1.86265e-09, acc 1
2017-08-08T18:42:18.090812: step 26976, loss 3.72529e-09, acc 1
2017-08-08T18:42:18.369402: step 26977, loss 0, acc 1
2017-08-08T18:42:18.590970: step 26978, loss 6.48177e-05, acc 1
2017-08-08T18:42:18.767793: step 26979, loss 0.15514, acc 0.984375
2017-08-08T18:42:19.071385: step 26980, loss 0.0136442, acc 0.984375
2017-08-08T18:42:19.255494: step 26981, loss 0, acc 1
2017-08-08T18:42:19.433429: step 26982, loss 0, acc 1
2017-08-08T18:42:19.725933: step 26983, loss 1.86265e-09, acc 1
2017-08-08T18:42:19.936127: step 26984, loss 4.4283e-05, acc 1
2017-08-08T18:42:20.282586: step 26985, loss 1.54964e-06, acc 1
2017-08-08T18:42:20.638609: step 26986, loss 0.000615832, acc 1
2017-08-08T18:42:20.905458: step 26987, loss 1.02629e-06, acc 1
2017-08-08T18:42:21.237397: step 26988, loss 1.86265e-09, acc 1
2017-08-08T18:42:21.466310: step 26989, loss 2.98023e-08, acc 1
2017-08-08T18:42:21.829375: step 26990, loss 1.80294e-06, acc 1
2017-08-08T18:42:22.163907: step 26991, loss 3.55762e-07, acc 1
2017-08-08T18:42:22.429342: step 26992, loss 0, acc 1
2017-08-08T18:42:22.694511: step 26993, loss 2.77414e-05, acc 1
2017-08-08T18:42:22.967137: step 26994, loss 4.09414e-05, acc 1
2017-08-08T18:42:23.232686: step 26995, loss 5.58793e-09, acc 1
2017-08-08T18:42:23.475346: step 26996, loss 0, acc 1
2017-08-08T18:42:23.863325: step 26997, loss 3.53902e-08, acc 1
2017-08-08T18:42:24.141460: step 26998, loss 1.67638e-08, acc 1
2017-08-08T18:42:24.453455: step 26999, loss 4.89724e-05, acc 1
2017-08-08T18:42:24.746450: step 27000, loss 6.15914e-08, acc 1

Evaluation:
2017-08-08T18:42:25.493306: step 27000, loss 9.61246, acc 0.698874

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-27000

2017-08-08T18:42:25.985416: step 27001, loss 2.90954e-05, acc 1
2017-08-08T18:42:26.271649: step 27002, loss 9.3132e-08, acc 1
2017-08-08T18:42:26.456175: step 27003, loss 3.28544e-06, acc 1
2017-08-08T18:42:26.750932: step 27004, loss 5.2154e-08, acc 1
2017-08-08T18:42:27.119368: step 27005, loss 1.30385e-08, acc 1
2017-08-08T18:42:27.498126: step 27006, loss 1.3485e-06, acc 1
2017-08-08T18:42:27.833863: step 27007, loss 1.28712e-05, acc 1
2017-08-08T18:42:28.101340: step 27008, loss 1.86626e-06, acc 1
2017-08-08T18:42:28.323661: step 27009, loss 1.21072e-07, acc 1
2017-08-08T18:42:28.641019: step 27010, loss 0, acc 1
2017-08-08T18:42:28.927566: step 27011, loss 4.45484e-06, acc 1
2017-08-08T18:42:29.369476: step 27012, loss 3.35276e-08, acc 1
2017-08-08T18:42:29.692374: step 27013, loss 7.45057e-08, acc 1
2017-08-08T18:42:29.952195: step 27014, loss 9.64513e-06, acc 1
2017-08-08T18:42:30.229190: step 27015, loss 1.64278e-06, acc 1
2017-08-08T18:42:30.506542: step 27016, loss 0, acc 1
2017-08-08T18:42:31.009518: step 27017, loss 0.288709, acc 0.984375
2017-08-08T18:42:31.402557: step 27018, loss 3.72529e-09, acc 1
2017-08-08T18:42:31.753541: step 27019, loss 0.000589924, acc 1
2017-08-08T18:42:31.999089: step 27020, loss 6.46018e-06, acc 1
2017-08-08T18:42:32.247890: step 27021, loss 8.54536e-06, acc 1
2017-08-08T18:42:32.657752: step 27022, loss 5.7742e-08, acc 1
2017-08-08T18:42:33.009179: step 27023, loss 0.0051101, acc 1
2017-08-08T18:42:33.300776: step 27024, loss 2.84214e-06, acc 1
2017-08-08T18:42:33.605342: step 27025, loss 0.129088, acc 0.984375
2017-08-08T18:42:33.919931: step 27026, loss 2.5386e-06, acc 1
2017-08-08T18:42:34.244527: step 27027, loss 1.67638e-08, acc 1
2017-08-08T18:42:34.568241: step 27028, loss 1.11839e-05, acc 1
2017-08-08T18:42:34.779568: step 27029, loss 0.0561173, acc 0.984375
2017-08-08T18:42:35.147660: step 27030, loss 0.000103973, acc 1
2017-08-08T18:42:35.354029: step 27031, loss 0.00795953, acc 1
2017-08-08T18:42:35.634078: step 27032, loss 1.86265e-09, acc 1
2017-08-08T18:42:35.893139: step 27033, loss 6.33298e-08, acc 1
2017-08-08T18:42:36.273404: step 27034, loss 0.000303263, acc 1
2017-08-08T18:42:36.676230: step 27035, loss 3.72529e-09, acc 1
2017-08-08T18:42:37.009465: step 27036, loss 0.00107462, acc 1
2017-08-08T18:42:37.264149: step 27037, loss 0, acc 1
2017-08-08T18:42:37.618841: step 27038, loss 4.99109e-06, acc 1
2017-08-08T18:42:38.060266: step 27039, loss 7.2643e-08, acc 1
2017-08-08T18:42:38.340672: step 27040, loss 1.52916e-06, acc 1
2017-08-08T18:42:38.621911: step 27041, loss 1.67638e-08, acc 1
2017-08-08T18:42:38.865011: step 27042, loss 3.29284e-06, acc 1
2017-08-08T18:42:39.261395: step 27043, loss 0, acc 1
2017-08-08T18:42:39.670393: step 27044, loss 0.00469294, acc 1
2017-08-08T18:42:40.012878: step 27045, loss 0, acc 1
2017-08-08T18:42:40.239039: step 27046, loss 4.43303e-07, acc 1
2017-08-08T18:42:40.449396: step 27047, loss 0.000512946, acc 1
2017-08-08T18:42:40.873333: step 27048, loss 0.000264419, acc 1
2017-08-08T18:42:41.141022: step 27049, loss 2.6077e-08, acc 1
2017-08-08T18:42:41.404807: step 27050, loss 1.0654e-06, acc 1
2017-08-08T18:42:41.662780: step 27051, loss 0.000814897, acc 1
2017-08-08T18:42:42.045800: step 27052, loss 1.86265e-09, acc 1
2017-08-08T18:42:42.503380: step 27053, loss 3.72529e-09, acc 1
2017-08-08T18:42:42.820964: step 27054, loss 2.77532e-07, acc 1
2017-08-08T18:42:43.077903: step 27055, loss 0.0192329, acc 0.984375
2017-08-08T18:42:43.394471: step 27056, loss 1.67638e-08, acc 1
2017-08-08T18:42:43.733484: step 27057, loss 0.120512, acc 0.984375
2017-08-08T18:42:43.973964: step 27058, loss 1.67638e-08, acc 1
2017-08-08T18:42:44.223662: step 27059, loss 0, acc 1
2017-08-08T18:42:44.487181: step 27060, loss 7.45058e-09, acc 1
2017-08-08T18:42:44.889064: step 27061, loss 0, acc 1
2017-08-08T18:42:45.177392: step 27062, loss 2.34303e-06, acc 1
2017-08-08T18:42:45.520737: step 27063, loss 1.04308e-07, acc 1
2017-08-08T18:42:45.728191: step 27064, loss 8.13605e-06, acc 1
2017-08-08T18:42:45.979480: step 27065, loss 3.53902e-08, acc 1
2017-08-08T18:42:46.261115: step 27066, loss 5.58793e-08, acc 1
2017-08-08T18:42:46.483866: step 27067, loss 0, acc 1
2017-08-08T18:42:46.703483: step 27068, loss 1.86265e-09, acc 1
2017-08-08T18:42:46.881497: step 27069, loss 0, acc 1
2017-08-08T18:42:47.196259: step 27070, loss 7.11514e-07, acc 1
2017-08-08T18:42:47.515353: step 27071, loss 1.34779e-05, acc 1
2017-08-08T18:42:47.813799: step 27072, loss 4.20952e-07, acc 1
2017-08-08T18:42:48.005590: step 27073, loss 3.16649e-08, acc 1
2017-08-08T18:42:48.167304: step 27074, loss 1.57572e-06, acc 1
2017-08-08T18:42:48.461919: step 27075, loss 3.53902e-08, acc 1
2017-08-08T18:42:48.704838: step 27076, loss 1.30385e-08, acc 1
2017-08-08T18:42:48.977435: step 27077, loss 1.53594e-05, acc 1
2017-08-08T18:42:49.265944: step 27078, loss 2.98022e-07, acc 1
2017-08-08T18:42:49.519296: step 27079, loss 7.45057e-08, acc 1
2017-08-08T18:42:49.863184: step 27080, loss 7.45058e-09, acc 1
2017-08-08T18:42:50.279763: step 27081, loss 0, acc 1
2017-08-08T18:42:50.627440: step 27082, loss 0, acc 1
2017-08-08T18:42:50.833430: step 27083, loss 1.86265e-09, acc 1
2017-08-08T18:42:51.022984: step 27084, loss 1.66512e-06, acc 1
2017-08-08T18:42:51.361538: step 27085, loss 2.37097e-06, acc 1
2017-08-08T18:42:51.615299: step 27086, loss 4.47034e-08, acc 1
2017-08-08T18:42:51.893463: step 27087, loss 2.79397e-08, acc 1
2017-08-08T18:42:52.169323: step 27088, loss 5.58793e-09, acc 1
2017-08-08T18:42:52.555744: step 27089, loss 2.21653e-07, acc 1
2017-08-08T18:42:52.768358: step 27090, loss 1.86265e-09, acc 1
2017-08-08T18:42:52.961219: step 27091, loss 0, acc 1
2017-08-08T18:42:53.310226: step 27092, loss 1.86265e-09, acc 1
2017-08-08T18:42:53.532709: step 27093, loss 1.86264e-08, acc 1
2017-08-08T18:42:53.778258: step 27094, loss 6.48187e-07, acc 1
2017-08-08T18:42:54.011185: step 27095, loss 1.7767e-05, acc 1
2017-08-08T18:42:54.354882: step 27096, loss 5.88586e-07, acc 1
2017-08-08T18:42:54.771345: step 27097, loss 2.77456e-05, acc 1
2017-08-08T18:42:55.134076: step 27098, loss 2.29841e-05, acc 1
2017-08-08T18:42:55.415198: step 27099, loss 1.13621e-07, acc 1
2017-08-08T18:42:55.850638: step 27100, loss 3.19843e-05, acc 1

Evaluation:
2017-08-08T18:42:56.466882: step 27100, loss 8.76091, acc 0.724203

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-27100

2017-08-08T18:42:57.066236: step 27101, loss 0, acc 1
2017-08-08T18:42:57.460728: step 27102, loss 4.65661e-08, acc 1
2017-08-08T18:42:57.717342: step 27103, loss 0, acc 1
2017-08-08T18:42:57.925346: step 27104, loss 1.86265e-09, acc 1
2017-08-08T18:42:58.295936: step 27105, loss 0.0700099, acc 0.984375
2017-08-08T18:42:58.569740: step 27106, loss 2.75669e-07, acc 1
2017-08-08T18:42:58.804891: step 27107, loss 0, acc 1
2017-08-08T18:42:59.011786: step 27108, loss 2.8312e-07, acc 1
2017-08-08T18:42:59.384792: step 27109, loss 0, acc 1
2017-08-08T18:42:59.725332: step 27110, loss 3.49766e-06, acc 1
2017-08-08T18:43:00.014504: step 27111, loss 9.31322e-09, acc 1
2017-08-08T18:43:00.217178: step 27112, loss 0, acc 1
2017-08-08T18:43:00.570086: step 27113, loss 0, acc 1
2017-08-08T18:43:00.898986: step 27114, loss 2.1136e-05, acc 1
2017-08-08T18:43:01.140937: step 27115, loss 2.13445e-06, acc 1
2017-08-08T18:43:01.385459: step 27116, loss 3.60969e-05, acc 1
2017-08-08T18:43:01.713379: step 27117, loss 7.45058e-09, acc 1
2017-08-08T18:43:02.163236: step 27118, loss 5.58794e-09, acc 1
2017-08-08T18:43:02.572001: step 27119, loss 1.4286e-06, acc 1
2017-08-08T18:43:02.986783: step 27120, loss 5.9644e-05, acc 1
2017-08-08T18:43:03.281633: step 27121, loss 2.31063e-05, acc 1
2017-08-08T18:43:03.772720: step 27122, loss 1.11759e-07, acc 1
2017-08-08T18:43:04.114649: step 27123, loss 0, acc 1
2017-08-08T18:43:04.365752: step 27124, loss 0.00301922, acc 1
2017-08-08T18:43:04.614784: step 27125, loss 0, acc 1
2017-08-08T18:43:04.991401: step 27126, loss 0.00247837, acc 1
2017-08-08T18:43:05.387956: step 27127, loss 1.45655e-06, acc 1
2017-08-08T18:43:05.594256: step 27128, loss 0.00228463, acc 1
2017-08-08T18:43:05.888153: step 27129, loss 0, acc 1
2017-08-08T18:43:06.137575: step 27130, loss 3.34285e-05, acc 1
2017-08-08T18:43:06.376016: step 27131, loss 5.38299e-07, acc 1
2017-08-08T18:43:06.754935: step 27132, loss 5.58793e-09, acc 1
2017-08-08T18:43:07.039642: step 27133, loss 1.86265e-09, acc 1
2017-08-08T18:43:07.326102: step 27134, loss 3.23136e-06, acc 1
2017-08-08T18:43:07.609411: step 27135, loss 0.0145965, acc 0.984375
2017-08-08T18:43:08.000198: step 27136, loss 3.63212e-07, acc 1
2017-08-08T18:43:08.425362: step 27137, loss 0, acc 1
2017-08-08T18:43:08.758109: step 27138, loss 2.11023e-06, acc 1
2017-08-08T18:43:08.994202: step 27139, loss 2.04891e-08, acc 1
2017-08-08T18:43:09.225037: step 27140, loss 2.57044e-07, acc 1
2017-08-08T18:43:09.592941: step 27141, loss 0.000381987, acc 1
2017-08-08T18:43:09.845180: step 27142, loss 0.000126109, acc 1
2017-08-08T18:43:10.056159: step 27143, loss 0, acc 1
2017-08-08T18:43:10.392909: step 27144, loss 3.29686e-07, acc 1
2017-08-08T18:43:10.764817: step 27145, loss 4.84281e-07, acc 1
2017-08-08T18:43:11.132541: step 27146, loss 1.24864e-05, acc 1
2017-08-08T18:43:11.347351: step 27147, loss 1.02215e-05, acc 1
2017-08-08T18:43:11.545507: step 27148, loss 9.31321e-08, acc 1
2017-08-08T18:43:11.922724: step 27149, loss 1.57572e-06, acc 1
2017-08-08T18:43:12.128021: step 27150, loss 1.3709e-07, acc 1
2017-08-08T18:43:12.398513: step 27151, loss 0.00986387, acc 1
2017-08-08T18:43:12.684419: step 27152, loss 5.58793e-09, acc 1
2017-08-08T18:43:13.075517: step 27153, loss 9.70409e-07, acc 1
2017-08-08T18:43:13.444242: step 27154, loss 5.58793e-09, acc 1
2017-08-08T18:43:13.740490: step 27155, loss 2.5518e-07, acc 1
2017-08-08T18:43:13.951526: step 27156, loss 1.30385e-08, acc 1
2017-08-08T18:43:14.177497: step 27157, loss 1.22185e-06, acc 1
2017-08-08T18:43:14.538439: step 27158, loss 0, acc 1
2017-08-08T18:43:14.794141: step 27159, loss 0, acc 1
2017-08-08T18:43:15.048847: step 27160, loss 1.86263e-07, acc 1
2017-08-08T18:43:15.381903: step 27161, loss 3.72529e-09, acc 1
2017-08-08T18:43:15.802901: step 27162, loss 1.8644e-06, acc 1
2017-08-08T18:43:16.249103: step 27163, loss 5.30847e-07, acc 1
2017-08-08T18:43:16.632895: step 27164, loss 0.0701687, acc 0.984375
2017-08-08T18:43:16.878581: step 27165, loss 4.21662e-06, acc 1
2017-08-08T18:43:17.154145: step 27166, loss 1.45614e-05, acc 1
2017-08-08T18:43:17.493360: step 27167, loss 1.24797e-07, acc 1
2017-08-08T18:43:17.722735: step 27168, loss 3.72529e-09, acc 1
2017-08-08T18:43:18.013925: step 27169, loss 7.43026e-06, acc 1
2017-08-08T18:43:18.318821: step 27170, loss 6.91028e-07, acc 1
2017-08-08T18:43:18.707731: step 27171, loss 1.86265e-09, acc 1
2017-08-08T18:43:18.912497: step 27172, loss 2.30333e-05, acc 1
2017-08-08T18:43:19.161823: step 27173, loss 1.86265e-09, acc 1
2017-08-08T18:43:19.337407: step 27174, loss 9.68089e-06, acc 1
2017-08-08T18:43:19.550703: step 27175, loss 4.09777e-07, acc 1
2017-08-08T18:43:19.881391: step 27176, loss 2.29841e-06, acc 1
2017-08-08T18:43:20.154142: step 27177, loss 3.72529e-08, acc 1
2017-08-08T18:43:20.361477: step 27178, loss 1.33215e-05, acc 1
2017-08-08T18:43:20.565618: step 27179, loss 0, acc 1
2017-08-08T18:43:20.845656: step 27180, loss 6.23117e-06, acc 1
2017-08-08T18:43:21.125572: step 27181, loss 1.49012e-08, acc 1
2017-08-08T18:43:21.347558: step 27182, loss 1.49012e-08, acc 1
2017-08-08T18:43:21.521376: step 27183, loss 0, acc 1
2017-08-08T18:43:21.825527: step 27184, loss 6.44462e-07, acc 1
2017-08-08T18:43:22.095581: step 27185, loss 3.72529e-09, acc 1
2017-08-08T18:43:22.309557: step 27186, loss 0.000200619, acc 1
2017-08-08T18:43:22.505344: step 27187, loss 1.86265e-09, acc 1
2017-08-08T18:43:22.811418: step 27188, loss 5.51624e-05, acc 1
2017-08-08T18:43:23.015185: step 27189, loss 0.00946126, acc 1
2017-08-08T18:43:23.242484: step 27190, loss 1.30385e-08, acc 1
2017-08-08T18:43:23.438275: step 27191, loss 0, acc 1
2017-08-08T18:43:23.642627: step 27192, loss 0, acc 1
2017-08-08T18:43:23.941393: step 27193, loss 1.49012e-08, acc 1
2017-08-08T18:43:24.156517: step 27194, loss 4.50754e-07, acc 1
2017-08-08T18:43:24.380640: step 27195, loss 2.4902e-06, acc 1
2017-08-08T18:43:24.633295: step 27196, loss 1.86265e-09, acc 1
2017-08-08T18:43:24.911133: step 27197, loss 7.45058e-09, acc 1
2017-08-08T18:43:25.237600: step 27198, loss 2.58906e-07, acc 1
2017-08-08T18:43:25.550614: step 27199, loss 3.53902e-08, acc 1
2017-08-08T18:43:25.732014: step 27200, loss 3.09913e-06, acc 1

Evaluation:
2017-08-08T18:43:26.242920: step 27200, loss 8.79535, acc 0.717636

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-27200

2017-08-08T18:43:26.575815: step 27201, loss 0.00012215, acc 1
2017-08-08T18:43:26.751694: step 27202, loss 3.72529e-09, acc 1
2017-08-08T18:43:26.938098: step 27203, loss 7.18965e-07, acc 1
2017-08-08T18:43:27.253349: step 27204, loss 0, acc 1
2017-08-08T18:43:27.580436: step 27205, loss 1.97614e-06, acc 1
2017-08-08T18:43:27.840686: step 27206, loss 5.12219e-07, acc 1
2017-08-08T18:43:28.075311: step 27207, loss 1.86265e-09, acc 1
2017-08-08T18:43:28.365387: step 27208, loss 1.0714e-05, acc 1
2017-08-08T18:43:28.714142: step 27209, loss 1.92725e-05, acc 1
2017-08-08T18:43:29.024514: step 27210, loss 1.93714e-07, acc 1
2017-08-08T18:43:29.305524: step 27211, loss 5.58793e-09, acc 1
2017-08-08T18:43:29.585721: step 27212, loss 4.84281e-07, acc 1
2017-08-08T18:43:29.820931: step 27213, loss 0, acc 1
2017-08-08T18:43:30.148462: step 27214, loss 2.31635e-05, acc 1
2017-08-08T18:43:30.443202: step 27215, loss 0, acc 1
2017-08-08T18:43:30.709464: step 27216, loss 1.49012e-08, acc 1
2017-08-08T18:43:30.994612: step 27217, loss 1.22371e-06, acc 1
2017-08-08T18:43:31.245325: step 27218, loss 0, acc 1
2017-08-08T18:43:31.551312: step 27219, loss 9.31322e-09, acc 1
2017-08-08T18:43:31.829059: step 27220, loss 1.50874e-07, acc 1
2017-08-08T18:43:32.337385: step 27221, loss 0.00029999, acc 1
2017-08-08T18:43:32.712422: step 27222, loss 2.29719e-05, acc 1
2017-08-08T18:43:33.037587: step 27223, loss 1.33919e-06, acc 1
2017-08-08T18:43:33.341040: step 27224, loss 0, acc 1
2017-08-08T18:43:33.613100: step 27225, loss 4.48891e-07, acc 1
2017-08-08T18:43:34.095718: step 27226, loss 3.72529e-09, acc 1
2017-08-08T18:43:34.334285: step 27227, loss 3.72529e-09, acc 1
2017-08-08T18:43:34.624524: step 27228, loss 1.72367e-05, acc 1
2017-08-08T18:43:34.902159: step 27229, loss 0, acc 1
2017-08-08T18:43:35.226130: step 27230, loss 0, acc 1
2017-08-08T18:43:35.617733: step 27231, loss 6.21444e-06, acc 1
2017-08-08T18:43:35.898854: step 27232, loss 1.86265e-09, acc 1
2017-08-08T18:43:36.129307: step 27233, loss 4.34101e-05, acc 1
2017-08-08T18:43:36.414519: step 27234, loss 3.72529e-09, acc 1
2017-08-08T18:43:36.749413: step 27235, loss 6.53783e-07, acc 1
2017-08-08T18:43:37.007280: step 27236, loss 0.00108921, acc 1
2017-08-08T18:43:37.208523: step 27237, loss 0, acc 1
2017-08-08T18:43:37.408875: step 27238, loss 0, acc 1
2017-08-08T18:43:37.649105: step 27239, loss 1.56641e-06, acc 1
2017-08-08T18:43:37.923303: step 27240, loss 5.58793e-09, acc 1
2017-08-08T18:43:38.213855: step 27241, loss 7.63683e-08, acc 1
2017-08-08T18:43:38.400915: step 27242, loss 1.35972e-07, acc 1
2017-08-08T18:43:38.567406: step 27243, loss 1.86265e-09, acc 1
2017-08-08T18:43:38.795846: step 27244, loss 4.19974e-06, acc 1
2017-08-08T18:43:39.001804: step 27245, loss 0, acc 1
2017-08-08T18:43:39.229411: step 27246, loss 0, acc 1
2017-08-08T18:43:39.525921: step 27247, loss 0, acc 1
2017-08-08T18:43:39.807844: step 27248, loss 1.11759e-08, acc 1
2017-08-08T18:43:40.092562: step 27249, loss 8.36309e-07, acc 1
2017-08-08T18:43:40.360439: step 27250, loss 7.71116e-07, acc 1
2017-08-08T18:43:40.533281: step 27251, loss 0, acc 1
2017-08-08T18:43:40.734469: step 27252, loss 0, acc 1
2017-08-08T18:43:41.077868: step 27253, loss 3.72529e-09, acc 1
2017-08-08T18:43:41.299786: step 27254, loss 0, acc 1
2017-08-08T18:43:41.545926: step 27255, loss 1.80666e-06, acc 1
2017-08-08T18:43:41.823510: step 27256, loss 3.49842e-05, acc 1
2017-08-08T18:43:42.205396: step 27257, loss 0.00014867, acc 1
2017-08-08T18:43:42.489375: step 27258, loss 9.90241e-06, acc 1
2017-08-08T18:43:42.796551: step 27259, loss 3.08797e-06, acc 1
2017-08-08T18:43:43.013792: step 27260, loss 9.62553e-06, acc 1
2017-08-08T18:43:43.368544: step 27261, loss 0, acc 1
2017-08-08T18:43:43.704174: step 27262, loss 1.0617e-07, acc 1
2017-08-08T18:43:43.871242: step 27263, loss 0, acc 1
2017-08-08T18:43:44.154693: step 27264, loss 0, acc 1
2017-08-08T18:43:44.341247: step 27265, loss 6.33298e-08, acc 1
2017-08-08T18:43:44.737157: step 27266, loss 9.31322e-09, acc 1
2017-08-08T18:43:45.018926: step 27267, loss 0.000366989, acc 1
2017-08-08T18:43:45.280246: step 27268, loss 1.70795e-06, acc 1
2017-08-08T18:43:45.490599: step 27269, loss 1.57202e-06, acc 1
2017-08-08T18:43:45.812205: step 27270, loss 0.000146957, acc 1
2017-08-08T18:43:46.010454: step 27271, loss 0.105809, acc 0.984375
2017-08-08T18:43:46.194468: step 27272, loss 8.06505e-07, acc 1
2017-08-08T18:43:46.421366: step 27273, loss 1.86265e-09, acc 1
2017-08-08T18:43:46.705314: step 27274, loss 3.72529e-09, acc 1
2017-08-08T18:43:46.949028: step 27275, loss 1.4156e-07, acc 1
2017-08-08T18:43:47.257774: step 27276, loss 5.91841e-06, acc 1
2017-08-08T18:43:47.482910: step 27277, loss 1.89989e-07, acc 1
2017-08-08T18:43:47.687007: step 27278, loss 1.67638e-08, acc 1
2017-08-08T18:43:48.132207: step 27279, loss 3.18509e-07, acc 1
2017-08-08T18:43:48.407210: step 27280, loss 8.71699e-07, acc 1
2017-08-08T18:43:48.707252: step 27281, loss 0, acc 1
2017-08-08T18:43:48.910791: step 27282, loss 2.45868e-07, acc 1
2017-08-08T18:43:49.261888: step 27283, loss 0.00227987, acc 1
2017-08-08T18:43:49.590985: step 27284, loss 0, acc 1
2017-08-08T18:43:49.882757: step 27285, loss 0.0118318, acc 0.984375
2017-08-08T18:43:50.086008: step 27286, loss 0.000797224, acc 1
2017-08-08T18:43:50.509366: step 27287, loss 2.10467e-05, acc 1
2017-08-08T18:43:50.851209: step 27288, loss 0, acc 1
2017-08-08T18:43:51.136115: step 27289, loss 2.91849e-06, acc 1
2017-08-08T18:43:51.421710: step 27290, loss 1.39506e-06, acc 1
2017-08-08T18:43:51.759254: step 27291, loss 0.00179005, acc 1
2017-08-08T18:43:52.134163: step 27292, loss 6.91026e-07, acc 1
2017-08-08T18:43:52.430045: step 27293, loss 6.87305e-07, acc 1
2017-08-08T18:43:52.674686: step 27294, loss 0, acc 1
2017-08-08T18:43:52.865706: step 27295, loss 3.72529e-09, acc 1
2017-08-08T18:43:53.151940: step 27296, loss 1.86265e-09, acc 1
2017-08-08T18:43:53.381513: step 27297, loss 7.2643e-08, acc 1
2017-08-08T18:43:53.602802: step 27298, loss 2.99883e-07, acc 1
2017-08-08T18:43:53.831003: step 27299, loss 0, acc 1
2017-08-08T18:43:54.135587: step 27300, loss 0.000499793, acc 1

Evaluation:
2017-08-08T18:43:54.771935: step 27300, loss 8.8753, acc 0.72045

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-27300

2017-08-08T18:43:55.142124: step 27301, loss 3.11059e-07, acc 1
2017-08-08T18:43:55.503768: step 27302, loss 0.000100048, acc 1
2017-08-08T18:43:55.727643: step 27303, loss 0, acc 1
2017-08-08T18:43:55.978900: step 27304, loss 1.67638e-08, acc 1
2017-08-08T18:43:56.335627: step 27305, loss 4.27507e-05, acc 1
2017-08-08T18:43:56.715935: step 27306, loss 1.95577e-07, acc 1
2017-08-08T18:43:57.068509: step 27307, loss 0, acc 1
2017-08-08T18:43:57.280461: step 27308, loss 3.35276e-08, acc 1
2017-08-08T18:43:57.504674: step 27309, loss 7.8454e-06, acc 1
2017-08-08T18:43:57.765372: step 27310, loss 7.45058e-09, acc 1
2017-08-08T18:43:58.205608: step 27311, loss 1.86265e-09, acc 1
2017-08-08T18:43:58.474830: step 27312, loss 3.72529e-09, acc 1
2017-08-08T18:43:58.704077: step 27313, loss 0, acc 1
2017-08-08T18:43:58.947348: step 27314, loss 3.26316e-06, acc 1
2017-08-08T18:43:59.387789: step 27315, loss 1.67638e-08, acc 1
2017-08-08T18:43:59.810183: step 27316, loss 0, acc 1
2017-08-08T18:44:00.118015: step 27317, loss 7.23747e-05, acc 1
2017-08-08T18:44:00.457556: step 27318, loss 9.12693e-08, acc 1
2017-08-08T18:44:00.681705: step 27319, loss 5.64371e-07, acc 1
2017-08-08T18:44:01.050376: step 27320, loss 1.67638e-08, acc 1
2017-08-08T18:44:01.333297: step 27321, loss 0, acc 1
2017-08-08T18:44:01.603811: step 27322, loss 2.98023e-08, acc 1
2017-08-08T18:44:01.959650: step 27323, loss 0, acc 1
2017-08-08T18:44:02.434607: step 27324, loss 5.58794e-09, acc 1
2017-08-08T18:44:02.768541: step 27325, loss 0, acc 1
2017-08-08T18:44:03.096445: step 27326, loss 2.75669e-07, acc 1
2017-08-08T18:44:03.332692: step 27327, loss 7.45058e-09, acc 1
2017-08-08T18:44:03.687629: step 27328, loss 3.72529e-09, acc 1
2017-08-08T18:44:04.049725: step 27329, loss 0, acc 1
2017-08-08T18:44:04.302955: step 27330, loss 0, acc 1
2017-08-08T18:44:04.621203: step 27331, loss 1.17346e-07, acc 1
2017-08-08T18:44:04.913378: step 27332, loss 1.86265e-09, acc 1
2017-08-08T18:44:05.354600: step 27333, loss 3.76509e-05, acc 1
2017-08-08T18:44:05.669221: step 27334, loss 1.97242e-06, acc 1
2017-08-08T18:44:05.990602: step 27335, loss 1.86265e-09, acc 1
2017-08-08T18:44:06.226596: step 27336, loss 2.27241e-07, acc 1
2017-08-08T18:44:06.469390: step 27337, loss 1.86265e-09, acc 1
2017-08-08T18:44:06.767866: step 27338, loss 3.37891e-05, acc 1
2017-08-08T18:44:07.026680: step 27339, loss 0, acc 1
2017-08-08T18:44:07.272036: step 27340, loss 1.86265e-09, acc 1
2017-08-08T18:44:07.621379: step 27341, loss 1.86265e-09, acc 1
2017-08-08T18:44:08.018552: step 27342, loss 7.45058e-09, acc 1
2017-08-08T18:44:08.383033: step 27343, loss 3.35276e-08, acc 1
2017-08-08T18:44:08.607992: step 27344, loss 1.49012e-08, acc 1
2017-08-08T18:44:08.795784: step 27345, loss 1.22934e-07, acc 1
2017-08-08T18:44:09.192915: step 27346, loss 3.72529e-09, acc 1
2017-08-08T18:44:09.467961: step 27347, loss 8.15071e-05, acc 1
2017-08-08T18:44:09.734710: step 27348, loss 1.30385e-08, acc 1
2017-08-08T18:44:10.045633: step 27349, loss 0.000299902, acc 1
2017-08-08T18:44:10.476143: step 27350, loss 4.4809e-06, acc 1
2017-08-08T18:44:10.861195: step 27351, loss 5.40166e-08, acc 1
2017-08-08T18:44:11.102723: step 27352, loss 2.4028e-07, acc 1
2017-08-08T18:44:11.453405: step 27353, loss 0, acc 1
2017-08-08T18:44:11.759057: step 27354, loss 1.22934e-07, acc 1
2017-08-08T18:44:12.054587: step 27355, loss 2.81574e-05, acc 1
2017-08-08T18:44:12.327202: step 27356, loss 3.44289e-05, acc 1
2017-08-08T18:44:12.538653: step 27357, loss 6.37014e-07, acc 1
2017-08-08T18:44:12.957151: step 27358, loss 3.72529e-09, acc 1
2017-08-08T18:44:13.336475: step 27359, loss 3.837e-07, acc 1
2017-08-08T18:44:13.713616: step 27360, loss 0.000141409, acc 1
2017-08-08T18:44:13.957564: step 27361, loss 7.07273e-06, acc 1
2017-08-08T18:44:14.159415: step 27362, loss 9.31322e-09, acc 1
2017-08-08T18:44:14.540485: step 27363, loss 0.108884, acc 0.984375
2017-08-08T18:44:14.913997: step 27364, loss 1.86265e-09, acc 1
2017-08-08T18:44:15.183755: step 27365, loss 0, acc 1
2017-08-08T18:44:15.386090: step 27366, loss 9.61202e-06, acc 1
2017-08-08T18:44:15.690229: step 27367, loss 0, acc 1
2017-08-08T18:44:16.029401: step 27368, loss 1.98435e-05, acc 1
2017-08-08T18:44:16.252014: step 27369, loss 2.23517e-08, acc 1
2017-08-08T18:44:16.450711: step 27370, loss 4.73105e-07, acc 1
2017-08-08T18:44:16.688548: step 27371, loss 1.60187e-07, acc 1
2017-08-08T18:44:16.873328: step 27372, loss 1.0258e-05, acc 1
2017-08-08T18:44:17.334558: step 27373, loss 2.04891e-08, acc 1
2017-08-08T18:44:17.575734: step 27374, loss 2.00553e-05, acc 1
2017-08-08T18:44:17.819887: step 27375, loss 1.65775e-07, acc 1
2017-08-08T18:44:18.066571: step 27376, loss 8.13955e-07, acc 1
2017-08-08T18:44:18.312175: step 27377, loss 0, acc 1
2017-08-08T18:44:18.704440: step 27378, loss 0.00268214, acc 1
2017-08-08T18:44:19.097308: step 27379, loss 6.33298e-08, acc 1
2017-08-08T18:44:19.524251: step 27380, loss 1.67638e-08, acc 1
2017-08-08T18:44:19.788327: step 27381, loss 0.00121227, acc 1
2017-08-08T18:44:20.022964: step 27382, loss 3.03768e-06, acc 1
2017-08-08T18:44:20.423010: step 27383, loss 2.09906e-06, acc 1
2017-08-08T18:44:20.719596: step 27384, loss 1.11759e-08, acc 1
2017-08-08T18:44:20.940463: step 27385, loss 0.235749, acc 0.984375
2017-08-08T18:44:21.172145: step 27386, loss 1.62787e-06, acc 1
2017-08-08T18:44:21.483463: step 27387, loss 3.72529e-08, acc 1
2017-08-08T18:44:21.828412: step 27388, loss 8.67979e-07, acc 1
2017-08-08T18:44:22.115757: step 27389, loss 2.44005e-07, acc 1
2017-08-08T18:44:22.462500: step 27390, loss 2.98023e-08, acc 1
2017-08-08T18:44:22.772163: step 27391, loss 8.08367e-07, acc 1
2017-08-08T18:44:23.224825: step 27392, loss 4.65661e-08, acc 1
2017-08-08T18:44:23.537578: step 27393, loss 0, acc 1
2017-08-08T18:44:23.781371: step 27394, loss 0, acc 1
2017-08-08T18:44:24.109386: step 27395, loss 0, acc 1
2017-08-08T18:44:24.358318: step 27396, loss 8.78921e-06, acc 1
2017-08-08T18:44:24.613408: step 27397, loss 1.67638e-08, acc 1
2017-08-08T18:44:24.891097: step 27398, loss 2.58906e-07, acc 1
2017-08-08T18:44:25.102564: step 27399, loss 0, acc 1
2017-08-08T18:44:25.324936: step 27400, loss 3.44924e-06, acc 1

Evaluation:
2017-08-08T18:44:26.010983: step 27400, loss 8.87505, acc 0.719512

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-27400

2017-08-08T18:44:26.410971: step 27401, loss 3.72529e-09, acc 1
2017-08-08T18:44:26.642292: step 27402, loss 0, acc 1
2017-08-08T18:44:27.038180: step 27403, loss 0, acc 1
2017-08-08T18:44:27.284103: step 27404, loss 8.19563e-08, acc 1
2017-08-08T18:44:27.561504: step 27405, loss 5.06006e-06, acc 1
2017-08-08T18:44:27.825821: step 27406, loss 0, acc 1
2017-08-08T18:44:28.158759: step 27407, loss 0.000748732, acc 1
2017-08-08T18:44:28.460129: step 27408, loss 0, acc 1
2017-08-08T18:44:28.744869: step 27409, loss 1.14436e-05, acc 1
2017-08-08T18:44:28.951258: step 27410, loss 1.2666e-07, acc 1
2017-08-08T18:44:29.144600: step 27411, loss 0.000142905, acc 1
2017-08-08T18:44:29.561440: step 27412, loss 4.37667e-06, acc 1
2017-08-08T18:44:29.851020: step 27413, loss 0, acc 1
2017-08-08T18:44:30.146600: step 27414, loss 0, acc 1
2017-08-08T18:44:30.432221: step 27415, loss 1.86265e-09, acc 1
2017-08-08T18:44:30.809431: step 27416, loss 1.78813e-07, acc 1
2017-08-08T18:44:31.164426: step 27417, loss 2.10477e-07, acc 1
2017-08-08T18:44:31.513735: step 27418, loss 1.67638e-08, acc 1
2017-08-08T18:44:31.746583: step 27419, loss 7.48316e-05, acc 1
2017-08-08T18:44:32.025379: step 27420, loss 7.2643e-08, acc 1
2017-08-08T18:44:32.519138: step 27421, loss 1.61054e-05, acc 1
2017-08-08T18:44:32.796599: step 27422, loss 0.000354909, acc 1
2017-08-08T18:44:33.087515: step 27423, loss 4.74344e-06, acc 1
2017-08-08T18:44:33.297595: step 27424, loss 2.8794e-06, acc 1
2017-08-08T18:44:33.630876: step 27425, loss 5.58794e-09, acc 1
2017-08-08T18:44:33.912609: step 27426, loss 0, acc 1
2017-08-08T18:44:34.197674: step 27427, loss 2.45868e-07, acc 1
2017-08-08T18:44:34.476629: step 27428, loss 9.31322e-09, acc 1
2017-08-08T18:44:34.751490: step 27429, loss 1.58131e-06, acc 1
2017-08-08T18:44:35.148980: step 27430, loss 4.30212e-05, acc 1
2017-08-08T18:44:35.418772: step 27431, loss 1.37835e-07, acc 1
2017-08-08T18:44:35.669876: step 27432, loss 1.86264e-08, acc 1
2017-08-08T18:44:35.885563: step 27433, loss 0, acc 1
2017-08-08T18:44:36.255523: step 27434, loss 2.23517e-08, acc 1
2017-08-08T18:44:36.592421: step 27435, loss 0, acc 1
2017-08-08T18:44:36.852376: step 27436, loss 0.000422993, acc 1
2017-08-08T18:44:37.045297: step 27437, loss 0, acc 1
2017-08-08T18:44:37.225337: step 27438, loss 5.58344e-06, acc 1
2017-08-08T18:44:37.590160: step 27439, loss 1.86265e-09, acc 1
2017-08-08T18:44:37.824236: step 27440, loss 0.111129, acc 0.984375
2017-08-08T18:44:38.060632: step 27441, loss 1.68747e-06, acc 1
2017-08-08T18:44:38.280259: step 27442, loss 0, acc 1
2017-08-08T18:44:38.583969: step 27443, loss 0.000301669, acc 1
2017-08-08T18:44:38.892014: step 27444, loss 4.83468e-06, acc 1
2017-08-08T18:44:39.237251: step 27445, loss 1.23489e-06, acc 1
2017-08-08T18:44:39.434986: step 27446, loss 2.14376e-06, acc 1
2017-08-08T18:44:39.655064: step 27447, loss 1.86265e-09, acc 1
2017-08-08T18:44:39.955639: step 27448, loss 1.30385e-08, acc 1
2017-08-08T18:44:40.238834: step 27449, loss 3.82737e-05, acc 1
2017-08-08T18:44:40.469151: step 27450, loss 0, acc 1
2017-08-08T18:44:40.685963: step 27451, loss 0, acc 1
2017-08-08T18:44:40.969462: step 27452, loss 7.45057e-08, acc 1
2017-08-08T18:44:41.299211: step 27453, loss 0.000653181, acc 1
2017-08-08T18:44:41.565365: step 27454, loss 0.107673, acc 0.984375
2017-08-08T18:44:41.783493: step 27455, loss 0, acc 1
2017-08-08T18:44:41.973391: step 27456, loss 0, acc 1
2017-08-08T18:44:42.280779: step 27457, loss 3.31549e-07, acc 1
2017-08-08T18:44:42.510655: step 27458, loss 2.4883e-06, acc 1
2017-08-08T18:44:42.691179: step 27459, loss 4.2576e-06, acc 1
2017-08-08T18:44:42.932105: step 27460, loss 3.16649e-08, acc 1
2017-08-08T18:44:43.262638: step 27461, loss 2.42144e-08, acc 1
2017-08-08T18:44:43.646502: step 27462, loss 2.6077e-08, acc 1
2017-08-08T18:44:43.956071: step 27463, loss 1.86265e-09, acc 1
2017-08-08T18:44:44.222022: step 27464, loss 3.98601e-07, acc 1
2017-08-08T18:44:44.430990: step 27465, loss 3.61884e-05, acc 1
2017-08-08T18:44:44.841880: step 27466, loss 2.04891e-08, acc 1
2017-08-08T18:44:45.107422: step 27467, loss 0, acc 1
2017-08-08T18:44:45.371593: step 27468, loss 7.45058e-09, acc 1
2017-08-08T18:44:45.589286: step 27469, loss 7.2083e-07, acc 1
2017-08-08T18:44:45.914006: step 27470, loss 3.12921e-07, acc 1
2017-08-08T18:44:46.360646: step 27471, loss 0.000132216, acc 1
2017-08-08T18:44:46.732740: step 27472, loss 1.57199e-06, acc 1
2017-08-08T18:44:47.008645: step 27473, loss 0, acc 1
2017-08-08T18:44:47.224081: step 27474, loss 1.67638e-08, acc 1
2017-08-08T18:44:47.473438: step 27475, loss 0, acc 1
2017-08-08T18:44:47.742734: step 27476, loss 0.0182231, acc 0.984375
2017-08-08T18:44:48.014447: step 27477, loss 9.38745e-07, acc 1
2017-08-08T18:44:48.301367: step 27478, loss 0, acc 1
2017-08-08T18:44:48.692253: step 27479, loss 1.11759e-08, acc 1
2017-08-08T18:44:49.147201: step 27480, loss 2.6077e-08, acc 1
2017-08-08T18:44:49.463842: step 27481, loss 0, acc 1
2017-08-08T18:44:49.664919: step 27482, loss 2.98023e-08, acc 1
2017-08-08T18:44:50.067789: step 27483, loss 3.74387e-07, acc 1
2017-08-08T18:44:50.427668: step 27484, loss 7.45058e-09, acc 1
2017-08-08T18:44:50.651214: step 27485, loss 1.86265e-09, acc 1
2017-08-08T18:44:50.842062: step 27486, loss 2.59259e-06, acc 1
2017-08-08T18:44:51.121383: step 27487, loss 1.80676e-07, acc 1
2017-08-08T18:44:51.404868: step 27488, loss 0.000215327, acc 1
2017-08-08T18:44:51.719063: step 27489, loss 0, acc 1
2017-08-08T18:44:51.948665: step 27490, loss 2.81342e-05, acc 1
2017-08-08T18:44:52.129363: step 27491, loss 3.72529e-09, acc 1
2017-08-08T18:44:52.484934: step 27492, loss 8.41893e-07, acc 1
2017-08-08T18:44:52.707472: step 27493, loss 0.013963, acc 0.984375
2017-08-08T18:44:52.944606: step 27494, loss 8.56814e-08, acc 1
2017-08-08T18:44:53.192444: step 27495, loss 3.53913e-05, acc 1
2017-08-08T18:44:53.532953: step 27496, loss 1.01883e-06, acc 1
2017-08-08T18:44:53.985974: step 27497, loss 0, acc 1
2017-08-08T18:44:54.333700: step 27498, loss 1.23861e-06, acc 1
2017-08-08T18:44:54.581577: step 27499, loss 1.30385e-08, acc 1
2017-08-08T18:44:54.905724: step 27500, loss 4.28408e-08, acc 1

Evaluation:
2017-08-08T18:44:55.456892: step 27500, loss 8.9466, acc 0.712008

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-27500

2017-08-08T18:44:56.160219: step 27501, loss 2.03027e-07, acc 1
2017-08-08T18:44:56.494225: step 27502, loss 0, acc 1
2017-08-08T18:44:56.748947: step 27503, loss 1.86265e-09, acc 1
2017-08-08T18:44:56.944461: step 27504, loss 2.63915e-06, acc 1
2017-08-08T18:44:57.281318: step 27505, loss 5.58793e-09, acc 1
2017-08-08T18:44:57.545358: step 27506, loss 9.3132e-08, acc 1
2017-08-08T18:44:57.769130: step 27507, loss 9.9836e-07, acc 1
2017-08-08T18:44:57.951256: step 27508, loss 3.09197e-07, acc 1
2017-08-08T18:44:58.153211: step 27509, loss 0, acc 1
2017-08-08T18:44:58.391288: step 27510, loss 3.35276e-08, acc 1
2017-08-08T18:44:58.591672: step 27511, loss 1.86265e-09, acc 1
2017-08-08T18:44:58.892119: step 27512, loss 3.24097e-07, acc 1
2017-08-08T18:44:59.080405: step 27513, loss 1.91851e-07, acc 1
2017-08-08T18:44:59.273591: step 27514, loss 1.11759e-08, acc 1
2017-08-08T18:44:59.584016: step 27515, loss 4.84287e-08, acc 1
2017-08-08T18:44:59.779186: step 27516, loss 1.55151e-06, acc 1
2017-08-08T18:44:59.979209: step 27517, loss 1.06749e-05, acc 1
2017-08-08T18:45:00.301028: step 27518, loss 5.02647e-06, acc 1
2017-08-08T18:45:00.614825: step 27519, loss 1.11759e-08, acc 1
2017-08-08T18:45:00.957341: step 27520, loss 8.55279e-06, acc 1
2017-08-08T18:45:01.204888: step 27521, loss 0, acc 1
2017-08-08T18:45:01.457192: step 27522, loss 0, acc 1
2017-08-08T18:45:01.877737: step 27523, loss 5.9165e-06, acc 1
2017-08-08T18:45:02.316123: step 27524, loss 0.00271721, acc 1
2017-08-08T18:45:02.702423: step 27525, loss 5.02913e-08, acc 1
2017-08-08T18:45:03.016020: step 27526, loss 3.53902e-08, acc 1
2017-08-08T18:45:03.379588: step 27527, loss 0.0010541, acc 1
2017-08-08T18:45:03.840067: step 27528, loss 9.4373e-05, acc 1
2017-08-08T18:45:04.281660: step 27529, loss 2.06926e-06, acc 1
2017-08-08T18:45:04.705272: step 27530, loss 3.03584e-06, acc 1
2017-08-08T18:45:05.017378: step 27531, loss 4.91731e-07, acc 1
2017-08-08T18:45:05.282649: step 27532, loss 3.16649e-08, acc 1
2017-08-08T18:45:05.777813: step 27533, loss 3.72529e-09, acc 1
2017-08-08T18:45:06.021511: step 27534, loss 0.0503152, acc 0.984375
2017-08-08T18:45:06.249375: step 27535, loss 3.72529e-08, acc 1
2017-08-08T18:45:06.563990: step 27536, loss 3.43062e-06, acc 1
2017-08-08T18:45:06.999208: step 27537, loss 6.72141e-05, acc 1
2017-08-08T18:45:07.382384: step 27538, loss 1.86265e-09, acc 1
2017-08-08T18:45:07.803687: step 27539, loss 0, acc 1
2017-08-08T18:45:08.089887: step 27540, loss 5.40167e-08, acc 1
2017-08-08T18:45:08.434890: step 27541, loss 0, acc 1
2017-08-08T18:45:08.739280: step 27542, loss 0, acc 1
2017-08-08T18:45:09.006691: step 27543, loss 1.1213e-05, acc 1
2017-08-08T18:45:09.298455: step 27544, loss 0, acc 1
2017-08-08T18:45:09.567182: step 27545, loss 1.13621e-07, acc 1
2017-08-08T18:45:10.065620: step 27546, loss 1.52449e-05, acc 1
2017-08-08T18:45:10.461319: step 27547, loss 4.62799e-06, acc 1
2017-08-08T18:45:10.802896: step 27548, loss 8.94067e-08, acc 1
2017-08-08T18:45:11.045026: step 27549, loss 4.17177e-06, acc 1
2017-08-08T18:45:11.400264: step 27550, loss 2.53394e-05, acc 1
2017-08-08T18:45:11.771893: step 27551, loss 0.000196404, acc 1
2017-08-08T18:45:12.088031: step 27552, loss 0, acc 1
2017-08-08T18:45:12.378200: step 27553, loss 7.45058e-09, acc 1
2017-08-08T18:45:12.705385: step 27554, loss 1.86265e-09, acc 1
2017-08-08T18:45:13.114187: step 27555, loss 1.62049e-07, acc 1
2017-08-08T18:45:13.430030: step 27556, loss 2.80303e-06, acc 1
2017-08-08T18:45:13.714947: step 27557, loss 3.88687e-06, acc 1
2017-08-08T18:45:13.943040: step 27558, loss 8.26423e-06, acc 1
2017-08-08T18:45:14.208518: step 27559, loss 6.03815e-06, acc 1
2017-08-08T18:45:14.589212: step 27560, loss 0, acc 1
2017-08-08T18:45:14.852562: step 27561, loss 0, acc 1
2017-08-08T18:45:15.134866: step 27562, loss 7.86016e-07, acc 1
2017-08-08T18:45:15.369049: step 27563, loss 2.42144e-08, acc 1
2017-08-08T18:45:15.720744: step 27564, loss 0.000322123, acc 1
2017-08-08T18:45:16.079661: step 27565, loss 0, acc 1
2017-08-08T18:45:16.420530: step 27566, loss 0, acc 1
2017-08-08T18:45:16.761629: step 27567, loss 0, acc 1
2017-08-08T18:45:16.987545: step 27568, loss 0.000349693, acc 1
2017-08-08T18:45:17.384619: step 27569, loss 5.58794e-09, acc 1
2017-08-08T18:45:17.743077: step 27570, loss 0, acc 1
2017-08-08T18:45:18.006581: step 27571, loss 1.36938e-05, acc 1
2017-08-08T18:45:18.284590: step 27572, loss 5.23773e-05, acc 1
2017-08-08T18:45:18.687534: step 27573, loss 1.48592e-05, acc 1
2017-08-08T18:45:19.101124: step 27574, loss 6.35149e-07, acc 1
2017-08-08T18:45:19.469904: step 27575, loss 0, acc 1
2017-08-08T18:45:19.731358: step 27576, loss 3.11059e-07, acc 1
2017-08-08T18:45:19.999850: step 27577, loss 6.2025e-07, acc 1
2017-08-08T18:45:20.318059: step 27578, loss 1.32056e-06, acc 1
2017-08-08T18:45:20.657173: step 27579, loss 9.45462e-05, acc 1
2017-08-08T18:45:20.912508: step 27580, loss 6.51925e-08, acc 1
2017-08-08T18:45:21.193732: step 27581, loss 7.99243e-06, acc 1
2017-08-08T18:45:21.595512: step 27582, loss 1.56462e-07, acc 1
2017-08-08T18:45:22.037337: step 27583, loss 1.62049e-07, acc 1
2017-08-08T18:45:22.433944: step 27584, loss 0, acc 1
2017-08-08T18:45:22.669264: step 27585, loss 5.2154e-08, acc 1
2017-08-08T18:45:22.941395: step 27586, loss 1.775e-06, acc 1
2017-08-08T18:45:23.341006: step 27587, loss 1.65767e-06, acc 1
2017-08-08T18:45:23.671924: step 27588, loss 2.08615e-07, acc 1
2017-08-08T18:45:23.925643: step 27589, loss 1.04664e-05, acc 1
2017-08-08T18:45:24.208223: step 27590, loss 2.69999e-05, acc 1
2017-08-08T18:45:24.601373: step 27591, loss 9.49946e-08, acc 1
2017-08-08T18:45:24.954832: step 27592, loss 6.77433e-05, acc 1
2017-08-08T18:45:25.201397: step 27593, loss 0, acc 1
2017-08-08T18:45:25.455858: step 27594, loss 0.000170637, acc 1
2017-08-08T18:45:25.709013: step 27595, loss 0, acc 1
2017-08-08T18:45:26.120813: step 27596, loss 2.9204e-06, acc 1
2017-08-08T18:45:26.358778: step 27597, loss 1.86265e-09, acc 1
2017-08-08T18:45:26.606728: step 27598, loss 1.04308e-07, acc 1
2017-08-08T18:45:26.864549: step 27599, loss 2.56573e-05, acc 1
2017-08-08T18:45:27.250227: step 27600, loss 0, acc 1

Evaluation:
2017-08-08T18:45:28.179175: step 27600, loss 9.17442, acc 0.706379

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-27600

2017-08-08T18:45:28.564312: step 27601, loss 8.02791e-07, acc 1
2017-08-08T18:45:28.829645: step 27602, loss 6.35157e-07, acc 1
2017-08-08T18:45:29.120579: step 27603, loss 1.24797e-07, acc 1
2017-08-08T18:45:29.365706: step 27604, loss 5.02913e-08, acc 1
2017-08-08T18:45:29.615253: step 27605, loss 8.75442e-08, acc 1
2017-08-08T18:45:29.836730: step 27606, loss 2.98023e-08, acc 1
2017-08-08T18:45:30.205044: step 27607, loss 0, acc 1
2017-08-08T18:45:30.621458: step 27608, loss 0.0141879, acc 0.984375
2017-08-08T18:45:30.950264: step 27609, loss 1.24989e-05, acc 1
2017-08-08T18:45:31.144878: step 27610, loss 8.09955e-06, acc 1
2017-08-08T18:45:31.344203: step 27611, loss 0, acc 1
2017-08-08T18:45:31.672260: step 27612, loss 1.05428e-05, acc 1
2017-08-08T18:45:31.933342: step 27613, loss 1.86265e-09, acc 1
2017-08-08T18:45:32.254191: step 27614, loss 3.72529e-09, acc 1
2017-08-08T18:45:32.555743: step 27615, loss 9.31322e-09, acc 1
2017-08-08T18:45:32.834006: step 27616, loss 1.17346e-07, acc 1
2017-08-08T18:45:33.218532: step 27617, loss 4.84287e-08, acc 1
2017-08-08T18:45:33.519417: step 27618, loss 0, acc 1
2017-08-08T18:45:33.760722: step 27619, loss 1.30385e-08, acc 1
2017-08-08T18:45:34.103463: step 27620, loss 1.86264e-08, acc 1
2017-08-08T18:45:34.541617: step 27621, loss 9.31322e-09, acc 1
2017-08-08T18:45:34.835220: step 27622, loss 3.72529e-09, acc 1
2017-08-08T18:45:35.118037: step 27623, loss 3.2576e-06, acc 1
2017-08-08T18:45:35.376605: step 27624, loss 1.86265e-09, acc 1
2017-08-08T18:45:35.817407: step 27625, loss 3.16649e-08, acc 1
2017-08-08T18:45:36.179526: step 27626, loss 1.93714e-07, acc 1
2017-08-08T18:45:36.513328: step 27627, loss 4.09781e-08, acc 1
2017-08-08T18:45:36.729900: step 27628, loss 0, acc 1
2017-08-08T18:45:36.985125: step 27629, loss 0.000159578, acc 1
2017-08-08T18:45:37.384398: step 27630, loss 3.46738e-05, acc 1
2017-08-08T18:45:37.650889: step 27631, loss 5.58793e-08, acc 1
2017-08-08T18:45:37.899783: step 27632, loss 0, acc 1
2017-08-08T18:45:38.212951: step 27633, loss 0, acc 1
2017-08-08T18:45:38.572744: step 27634, loss 3.4847e-06, acc 1
2017-08-08T18:45:38.927521: step 27635, loss 9.31322e-09, acc 1
2017-08-08T18:45:39.254966: step 27636, loss 3.72529e-09, acc 1
2017-08-08T18:45:39.525716: step 27637, loss 1.19209e-07, acc 1
2017-08-08T18:45:39.758436: step 27638, loss 3.3724e-05, acc 1
2017-08-08T18:45:40.149342: step 27639, loss 3.25184e-06, acc 1
2017-08-08T18:45:40.464338: step 27640, loss 3.65953e-05, acc 1
2017-08-08T18:45:40.696897: step 27641, loss 2.52262e-05, acc 1
2017-08-08T18:45:40.940392: step 27642, loss 1.72425e-05, acc 1
2017-08-08T18:45:41.259115: step 27643, loss 3.56347e-05, acc 1
2017-08-08T18:45:41.531251: step 27644, loss 4.78692e-07, acc 1
2017-08-08T18:45:41.824868: step 27645, loss 3.16649e-08, acc 1
2017-08-08T18:45:42.084912: step 27646, loss 0, acc 1
2017-08-08T18:45:42.255760: step 27647, loss 1.24792e-06, acc 1
2017-08-08T18:45:42.611281: step 27648, loss 1.30385e-08, acc 1
2017-08-08T18:45:42.815162: step 27649, loss 1.86265e-09, acc 1
2017-08-08T18:45:43.083298: step 27650, loss 3.688e-07, acc 1
2017-08-08T18:45:43.365950: step 27651, loss 3.16618e-06, acc 1
2017-08-08T18:45:43.771754: step 27652, loss 0, acc 1
2017-08-08T18:45:44.101554: step 27653, loss 1.2177e-05, acc 1
2017-08-08T18:45:44.428020: step 27654, loss 1.86265e-09, acc 1
2017-08-08T18:45:44.674743: step 27655, loss 0, acc 1
2017-08-08T18:45:44.945373: step 27656, loss 0, acc 1
2017-08-08T18:45:45.343244: step 27657, loss 1.86265e-09, acc 1
2017-08-08T18:45:45.510134: step 27658, loss 1.63697e-05, acc 1
2017-08-08T18:45:45.710151: step 27659, loss 7.56423e-06, acc 1
2017-08-08T18:45:46.012083: step 27660, loss 1.52736e-07, acc 1
2017-08-08T18:45:46.289952: step 27661, loss 0, acc 1
2017-08-08T18:45:46.551172: step 27662, loss 0.0115854, acc 0.984375
2017-08-08T18:45:46.722704: step 27663, loss 0, acc 1
2017-08-08T18:45:47.042252: step 27664, loss 3.35426e-06, acc 1
2017-08-08T18:45:47.350981: step 27665, loss 1.11759e-08, acc 1
2017-08-08T18:45:47.627970: step 27666, loss 2.23517e-08, acc 1
2017-08-08T18:45:47.933005: step 27667, loss 7.12165e-05, acc 1
2017-08-08T18:45:48.270104: step 27668, loss 0, acc 1
2017-08-08T18:45:48.669244: step 27669, loss 0, acc 1
2017-08-08T18:45:48.995263: step 27670, loss 0, acc 1
2017-08-08T18:45:49.207945: step 27671, loss 9.55617e-06, acc 1
2017-08-08T18:45:49.483892: step 27672, loss 0, acc 1
2017-08-08T18:45:49.771375: step 27673, loss 0, acc 1
2017-08-08T18:45:49.993314: step 27674, loss 1.45286e-07, acc 1
2017-08-08T18:45:50.280616: step 27675, loss 1.86265e-09, acc 1
2017-08-08T18:45:50.597184: step 27676, loss 8.19563e-08, acc 1
2017-08-08T18:45:50.876644: step 27677, loss 0.001776, acc 1
2017-08-08T18:45:51.194268: step 27678, loss 0, acc 1
2017-08-08T18:45:51.393251: step 27679, loss 9.31322e-09, acc 1
2017-08-08T18:45:51.576432: step 27680, loss 9.31322e-09, acc 1
2017-08-08T18:45:51.962723: step 27681, loss 7.48091e-06, acc 1
2017-08-08T18:45:52.205899: step 27682, loss 1.95193e-06, acc 1
2017-08-08T18:45:52.447757: step 27683, loss 5.58793e-09, acc 1
2017-08-08T18:45:52.667827: step 27684, loss 4.65661e-08, acc 1
2017-08-08T18:45:52.948447: step 27685, loss 8.38188e-08, acc 1
2017-08-08T18:45:53.292140: step 27686, loss 2.14715e-05, acc 1
2017-08-08T18:45:53.551362: step 27687, loss 0, acc 1
2017-08-08T18:45:53.797787: step 27688, loss 4.84287e-08, acc 1
2017-08-08T18:45:54.054419: step 27689, loss 0.00358777, acc 1
2017-08-08T18:45:54.476879: step 27690, loss 4.09781e-08, acc 1
2017-08-08T18:45:54.678231: step 27691, loss 7.07804e-08, acc 1
2017-08-08T18:45:54.982714: step 27692, loss 0, acc 1
2017-08-08T18:45:55.180947: step 27693, loss 2.68219e-07, acc 1
2017-08-08T18:45:55.589222: step 27694, loss 2.98038e-05, acc 1
2017-08-08T18:45:56.012858: step 27695, loss 2.42144e-08, acc 1
2017-08-08T18:45:56.431302: step 27696, loss 1.62049e-07, acc 1
2017-08-08T18:45:56.697670: step 27697, loss 2.28026e-05, acc 1
2017-08-08T18:45:56.952805: step 27698, loss 7.77461e-06, acc 1
2017-08-08T18:45:57.331644: step 27699, loss 0, acc 1
2017-08-08T18:45:57.569723: step 27700, loss 4.67517e-07, acc 1

Evaluation:
2017-08-08T18:45:58.102056: step 27700, loss 9.09091, acc 0.712008

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-27700

2017-08-08T18:45:58.702354: step 27701, loss 1.695e-07, acc 1
2017-08-08T18:45:58.970814: step 27702, loss 0, acc 1
2017-08-08T18:45:59.186253: step 27703, loss 1.49012e-08, acc 1
2017-08-08T18:45:59.366694: step 27704, loss 3.22202e-05, acc 1
2017-08-08T18:45:59.647973: step 27705, loss 2.36555e-07, acc 1
2017-08-08T18:45:59.824901: step 27706, loss 0, acc 1
2017-08-08T18:46:00.005236: step 27707, loss 1.86265e-09, acc 1
2017-08-08T18:46:00.261605: step 27708, loss 1.21071e-07, acc 1
2017-08-08T18:46:00.646582: step 27709, loss 1.91851e-07, acc 1
2017-08-08T18:46:00.959275: step 27710, loss 1.11759e-08, acc 1
2017-08-08T18:46:01.340270: step 27711, loss 3.73602e-06, acc 1
2017-08-08T18:46:01.611654: step 27712, loss 0.000453684, acc 1
2017-08-08T18:46:01.861133: step 27713, loss 0.00024956, acc 1
2017-08-08T18:46:02.233447: step 27714, loss 5.58794e-09, acc 1
2017-08-08T18:46:02.588669: step 27715, loss 2.79397e-08, acc 1
2017-08-08T18:46:02.899101: step 27716, loss 0, acc 1
2017-08-08T18:46:03.219675: step 27717, loss 5.86722e-07, acc 1
2017-08-08T18:46:03.488903: step 27718, loss 4.73599e-06, acc 1
2017-08-08T18:46:03.869373: step 27719, loss 0.0293395, acc 0.984375
2017-08-08T18:46:04.229405: step 27720, loss 8.00935e-08, acc 1
2017-08-08T18:46:04.705298: step 27721, loss 9.31322e-09, acc 1
2017-08-08T18:46:05.061251: step 27722, loss 0, acc 1
2017-08-08T18:46:05.313137: step 27723, loss 3.91155e-08, acc 1
2017-08-08T18:46:05.587437: step 27724, loss 1.86265e-09, acc 1
2017-08-08T18:46:06.011622: step 27725, loss 0, acc 1
2017-08-08T18:46:06.310126: step 27726, loss 7.82292e-07, acc 1
2017-08-08T18:46:06.569554: step 27727, loss 9.70469e-05, acc 1
2017-08-08T18:46:06.948185: step 27728, loss 4.65661e-08, acc 1
2017-08-08T18:46:07.277387: step 27729, loss 1.20695e-06, acc 1
2017-08-08T18:46:07.659707: step 27730, loss 1.56462e-07, acc 1
2017-08-08T18:46:07.909025: step 27731, loss 5.14082e-07, acc 1
2017-08-08T18:46:08.164013: step 27732, loss 1.30385e-07, acc 1
2017-08-08T18:46:08.610966: step 27733, loss 2.60935e-06, acc 1
2017-08-08T18:46:08.931725: step 27734, loss 2.19341e-05, acc 1
2017-08-08T18:46:09.194259: step 27735, loss 0.0439595, acc 0.984375
2017-08-08T18:46:09.483232: step 27736, loss 0.000906745, acc 1
2017-08-08T18:46:09.813447: step 27737, loss 7.45058e-09, acc 1
2017-08-08T18:46:10.223569: step 27738, loss 6.14672e-08, acc 1
2017-08-08T18:46:10.469384: step 27739, loss 2.04891e-08, acc 1
2017-08-08T18:46:10.743785: step 27740, loss 7.87879e-07, acc 1
2017-08-08T18:46:10.984188: step 27741, loss 8.56815e-08, acc 1
2017-08-08T18:46:11.368271: step 27742, loss 0, acc 1
2017-08-08T18:46:11.739425: step 27743, loss 9.83447e-07, acc 1
2017-08-08T18:46:12.018598: step 27744, loss 4.29438e-05, acc 1
2017-08-08T18:46:12.308583: step 27745, loss 3.59486e-07, acc 1
2017-08-08T18:46:12.532031: step 27746, loss 3.25514e-05, acc 1
2017-08-08T18:46:12.861306: step 27747, loss 0, acc 1
2017-08-08T18:46:13.181348: step 27748, loss 0, acc 1
2017-08-08T18:46:13.438652: step 27749, loss 0, acc 1
2017-08-08T18:46:13.665131: step 27750, loss 5.80046e-05, acc 1
2017-08-08T18:46:13.855321: step 27751, loss 4.24677e-07, acc 1
2017-08-08T18:46:14.214422: step 27752, loss 0, acc 1
2017-08-08T18:46:14.401191: step 27753, loss 2.32729e-05, acc 1
2017-08-08T18:46:14.583989: step 27754, loss 1.86264e-08, acc 1
2017-08-08T18:46:14.821513: step 27755, loss 5.60646e-07, acc 1
2017-08-08T18:46:15.141316: step 27756, loss 3.00008e-05, acc 1
2017-08-08T18:46:15.437498: step 27757, loss 3.72529e-09, acc 1
2017-08-08T18:46:15.702502: step 27758, loss 6.20249e-07, acc 1
2017-08-08T18:46:15.887173: step 27759, loss 1.11759e-08, acc 1
2017-08-08T18:46:16.195380: step 27760, loss 1.86265e-09, acc 1
2017-08-08T18:46:16.361105: step 27761, loss 2.53318e-07, acc 1
2017-08-08T18:46:16.547388: step 27762, loss 1.67638e-08, acc 1
2017-08-08T18:46:16.749336: step 27763, loss 0, acc 1
2017-08-08T18:46:17.038617: step 27764, loss 0, acc 1
2017-08-08T18:46:17.393393: step 27765, loss 0, acc 1
2017-08-08T18:46:17.615466: step 27766, loss 1.49012e-08, acc 1
2017-08-08T18:46:17.809764: step 27767, loss 9.06158e-06, acc 1
2017-08-08T18:46:18.094983: step 27768, loss 0, acc 1
2017-08-08T18:46:18.384491: step 27769, loss 0, acc 1
2017-08-08T18:46:18.582922: step 27770, loss 5.06557e-06, acc 1
2017-08-08T18:46:18.750440: step 27771, loss 0, acc 1
2017-08-08T18:46:18.989201: step 27772, loss 2.20522e-06, acc 1
2017-08-08T18:46:19.248912: step 27773, loss 2.23517e-08, acc 1
2017-08-08T18:46:19.488728: step 27774, loss 1.26659e-07, acc 1
2017-08-08T18:46:19.690939: step 27775, loss 0, acc 1
2017-08-08T18:46:19.953457: step 27776, loss 4.1755e-06, acc 1
2017-08-08T18:46:20.317617: step 27777, loss 0, acc 1
2017-08-08T18:46:20.516361: step 27778, loss 0, acc 1
2017-08-08T18:46:20.788342: step 27779, loss 2.01003e-05, acc 1
2017-08-08T18:46:21.073033: step 27780, loss 7.45058e-09, acc 1
2017-08-08T18:46:21.405347: step 27781, loss 9.31322e-09, acc 1
2017-08-08T18:46:21.737569: step 27782, loss 1.86265e-09, acc 1
2017-08-08T18:46:21.981194: step 27783, loss 0, acc 1
2017-08-08T18:46:22.172315: step 27784, loss 2.98023e-08, acc 1
2017-08-08T18:46:22.397385: step 27785, loss 4.84287e-08, acc 1
2017-08-08T18:46:22.749423: step 27786, loss 3.4567e-06, acc 1
2017-08-08T18:46:23.030289: step 27787, loss 2.23517e-08, acc 1
2017-08-08T18:46:23.237615: step 27788, loss 0, acc 1
2017-08-08T18:46:23.449262: step 27789, loss 4.78692e-07, acc 1
2017-08-08T18:46:23.752870: step 27790, loss 0, acc 1
2017-08-08T18:46:24.024479: step 27791, loss 7.76707e-07, acc 1
2017-08-08T18:46:24.437359: step 27792, loss 7.00942e-06, acc 1
2017-08-08T18:46:24.705710: step 27793, loss 0, acc 1
2017-08-08T18:46:24.887516: step 27794, loss 1.86265e-09, acc 1
2017-08-08T18:46:25.205569: step 27795, loss 1.35973e-07, acc 1
2017-08-08T18:46:25.524757: step 27796, loss 2.67453e-06, acc 1
2017-08-08T18:46:25.808374: step 27797, loss 0, acc 1
2017-08-08T18:46:26.088920: step 27798, loss 0, acc 1
2017-08-08T18:46:26.336443: step 27799, loss 6.70551e-08, acc 1
2017-08-08T18:46:26.771249: step 27800, loss 7.87879e-07, acc 1

Evaluation:
2017-08-08T18:46:27.668060: step 27800, loss 9.10583, acc 0.714822

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-27800

2017-08-08T18:46:28.119756: step 27801, loss 3.72529e-09, acc 1
2017-08-08T18:46:28.521311: step 27802, loss 7.71833e-05, acc 1
2017-08-08T18:46:28.813738: step 27803, loss 0, acc 1
2017-08-08T18:46:29.136190: step 27804, loss 4.61886e-05, acc 1
2017-08-08T18:46:29.416262: step 27805, loss 2.36735e-06, acc 1
2017-08-08T18:46:29.760750: step 27806, loss 0, acc 1
2017-08-08T18:46:30.025942: step 27807, loss 0, acc 1
2017-08-08T18:46:30.290438: step 27808, loss 2.99884e-07, acc 1
2017-08-08T18:46:30.512027: step 27809, loss 9.49946e-08, acc 1
2017-08-08T18:46:30.883897: step 27810, loss 2.6077e-08, acc 1
2017-08-08T18:46:31.252748: step 27811, loss 0, acc 1
2017-08-08T18:46:31.544395: step 27812, loss 0, acc 1
2017-08-08T18:46:31.809780: step 27813, loss 1.50122e-06, acc 1
2017-08-08T18:46:32.083717: step 27814, loss 2.5518e-07, acc 1
2017-08-08T18:46:32.454203: step 27815, loss 1.49012e-08, acc 1
2017-08-08T18:46:32.807427: step 27816, loss 1.30385e-08, acc 1
2017-08-08T18:46:33.102796: step 27817, loss 1.30385e-08, acc 1
2017-08-08T18:46:33.312181: step 27818, loss 3.9674e-07, acc 1
2017-08-08T18:46:33.640179: step 27819, loss 0, acc 1
2017-08-08T18:46:33.883709: step 27820, loss 8.25965e-05, acc 1
2017-08-08T18:46:34.143814: step 27821, loss 0, acc 1
2017-08-08T18:46:34.385922: step 27822, loss 0, acc 1
2017-08-08T18:46:34.692142: step 27823, loss 1.86265e-09, acc 1
2017-08-08T18:46:35.166058: step 27824, loss 1.75639e-06, acc 1
2017-08-08T18:46:35.501783: step 27825, loss 0, acc 1
2017-08-08T18:46:35.865673: step 27826, loss 3.48311e-07, acc 1
2017-08-08T18:46:36.135724: step 27827, loss 4.84287e-08, acc 1
2017-08-08T18:46:36.553880: step 27828, loss 1.04308e-07, acc 1
2017-08-08T18:46:36.816766: step 27829, loss 6.85975e-05, acc 1
2017-08-08T18:46:37.115577: step 27830, loss 9.10807e-07, acc 1
2017-08-08T18:46:37.387840: step 27831, loss 1.86265e-09, acc 1
2017-08-08T18:46:37.848233: step 27832, loss 2.42144e-08, acc 1
2017-08-08T18:46:38.281679: step 27833, loss 0, acc 1
2017-08-08T18:46:38.703602: step 27834, loss 3.08561e-05, acc 1
2017-08-08T18:46:38.904184: step 27835, loss 1.73225e-07, acc 1
2017-08-08T18:46:39.111675: step 27836, loss 0, acc 1
2017-08-08T18:46:39.404100: step 27837, loss 3.1665e-08, acc 1
2017-08-08T18:46:39.761271: step 27838, loss 2.30587e-06, acc 1
2017-08-08T18:46:40.034692: step 27839, loss 2.36554e-07, acc 1
2017-08-08T18:46:40.305988: step 27840, loss 1.86264e-08, acc 1
2017-08-08T18:46:40.765384: step 27841, loss 3.55185e-06, acc 1
2017-08-08T18:46:41.177401: step 27842, loss 0, acc 1
2017-08-08T18:46:41.561452: step 27843, loss 6.14672e-08, acc 1
2017-08-08T18:46:41.849611: step 27844, loss 4.34443e-05, acc 1
2017-08-08T18:46:42.126524: step 27845, loss 0, acc 1
2017-08-08T18:46:42.572007: step 27846, loss 6.81021e-06, acc 1
2017-08-08T18:46:42.830686: step 27847, loss 9.31322e-09, acc 1
2017-08-08T18:46:43.132368: step 27848, loss 4.76951e-06, acc 1
2017-08-08T18:46:43.397497: step 27849, loss 4.53677e-06, acc 1
2017-08-08T18:46:43.900797: step 27850, loss 3.05471e-07, acc 1
2017-08-08T18:46:44.324324: step 27851, loss 2.08615e-07, acc 1
2017-08-08T18:46:44.612999: step 27852, loss 0, acc 1
2017-08-08T18:46:44.866209: step 27853, loss 5.60994e-05, acc 1
2017-08-08T18:46:45.102547: step 27854, loss 0, acc 1
2017-08-08T18:46:45.417824: step 27855, loss 1.08779e-05, acc 1
2017-08-08T18:46:45.736276: step 27856, loss 1.30385e-08, acc 1
2017-08-08T18:46:45.960350: step 27857, loss 5.58793e-08, acc 1
2017-08-08T18:46:46.215670: step 27858, loss 1.86265e-09, acc 1
2017-08-08T18:46:46.453345: step 27859, loss 0, acc 1
2017-08-08T18:46:46.870452: step 27860, loss 4.13654e-06, acc 1
2017-08-08T18:46:47.256303: step 27861, loss 1.60187e-07, acc 1
2017-08-08T18:46:47.640825: step 27862, loss 1.95566e-06, acc 1
2017-08-08T18:46:47.916400: step 27863, loss 3.72529e-08, acc 1
2017-08-08T18:46:48.414904: step 27864, loss 3.98603e-07, acc 1
2017-08-08T18:46:48.715437: step 27865, loss 1.57572e-06, acc 1
2017-08-08T18:46:48.919519: step 27866, loss 8.36306e-07, acc 1
2017-08-08T18:46:49.135488: step 27867, loss 2.6077e-08, acc 1
2017-08-08T18:46:49.528561: step 27868, loss 1.37835e-07, acc 1
2017-08-08T18:46:49.874218: step 27869, loss 3.72529e-09, acc 1
2017-08-08T18:46:50.146638: step 27870, loss 0.000619763, acc 1
2017-08-08T18:46:50.321285: step 27871, loss 1.86265e-09, acc 1
2017-08-08T18:46:50.491941: step 27872, loss 3.488e-05, acc 1
2017-08-08T18:46:50.877008: step 27873, loss 1.21071e-07, acc 1
2017-08-08T18:46:51.097997: step 27874, loss 6.12803e-07, acc 1
2017-08-08T18:46:51.423454: step 27875, loss 7.98126e-06, acc 1
2017-08-08T18:46:51.668136: step 27876, loss 5.58793e-09, acc 1
2017-08-08T18:46:51.981418: step 27877, loss 0, acc 1
2017-08-08T18:46:52.325059: step 27878, loss 0, acc 1
2017-08-08T18:46:52.583237: step 27879, loss 0.000125504, acc 1
2017-08-08T18:46:52.969583: step 27880, loss 0, acc 1
2017-08-08T18:46:53.166016: step 27881, loss 1.86265e-09, acc 1
2017-08-08T18:46:53.382027: step 27882, loss 5.76382e-06, acc 1
2017-08-08T18:46:53.688223: step 27883, loss 3.91155e-08, acc 1
2017-08-08T18:46:53.910751: step 27884, loss 3.72529e-09, acc 1
2017-08-08T18:46:54.115596: step 27885, loss 1.34105e-06, acc 1
2017-08-08T18:46:54.417550: step 27886, loss 4.84287e-08, acc 1
2017-08-08T18:46:54.658106: step 27887, loss 6.25836e-07, acc 1
2017-08-08T18:46:55.046447: step 27888, loss 0, acc 1
2017-08-08T18:46:55.355970: step 27889, loss 5.2015e-06, acc 1
2017-08-08T18:46:55.693358: step 27890, loss 0, acc 1
2017-08-08T18:46:55.963477: step 27891, loss 4.47034e-08, acc 1
2017-08-08T18:46:56.191707: step 27892, loss 7.39464e-07, acc 1
2017-08-08T18:46:56.511959: step 27893, loss 1.86265e-09, acc 1
2017-08-08T18:46:56.797165: step 27894, loss 8.94067e-08, acc 1
2017-08-08T18:46:57.002233: step 27895, loss 1.86265e-09, acc 1
2017-08-08T18:46:57.339819: step 27896, loss 1.11759e-08, acc 1
2017-08-08T18:46:57.563499: step 27897, loss 1.49012e-08, acc 1
2017-08-08T18:46:58.019713: step 27898, loss 7.15239e-07, acc 1
2017-08-08T18:46:58.270215: step 27899, loss 0, acc 1
2017-08-08T18:46:58.609797: step 27900, loss 2.16284e-05, acc 1

Evaluation:
2017-08-08T18:46:59.109326: step 27900, loss 9.08632, acc 0.716698

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-27900

2017-08-08T18:46:59.456477: step 27901, loss 0.00448375, acc 1
2017-08-08T18:46:59.824253: step 27902, loss 1.86265e-09, acc 1
2017-08-08T18:47:00.052577: step 27903, loss 0, acc 1
2017-08-08T18:47:00.325311: step 27904, loss 0, acc 1
2017-08-08T18:47:00.575947: step 27905, loss 1.03659e-05, acc 1
2017-08-08T18:47:00.826187: step 27906, loss 2.81676e-05, acc 1
2017-08-08T18:47:01.105413: step 27907, loss 7.63683e-08, acc 1
2017-08-08T18:47:01.564143: step 27908, loss 0, acc 1
2017-08-08T18:47:01.877489: step 27909, loss 2.35421e-06, acc 1
2017-08-08T18:47:02.203490: step 27910, loss 9.31322e-09, acc 1
2017-08-08T18:47:02.535739: step 27911, loss 1.91851e-07, acc 1
2017-08-08T18:47:03.131857: step 27912, loss 1.30567e-06, acc 1
2017-08-08T18:47:03.617619: step 27913, loss 1.18308e-05, acc 1
2017-08-08T18:47:04.018925: step 27914, loss 0, acc 1
2017-08-08T18:47:04.282242: step 27915, loss 1.07098e-06, acc 1
2017-08-08T18:47:04.508613: step 27916, loss 2.44369e-06, acc 1
2017-08-08T18:47:04.935439: step 27917, loss 4.44366e-06, acc 1
2017-08-08T18:47:05.223889: step 27918, loss 0.00597833, acc 1
2017-08-08T18:47:05.486245: step 27919, loss 3.72529e-09, acc 1
2017-08-08T18:47:05.783404: step 27920, loss 3.69572e-05, acc 1
2017-08-08T18:47:06.127985: step 27921, loss 1.86265e-09, acc 1
2017-08-08T18:47:06.594271: step 27922, loss 0, acc 1
2017-08-08T18:47:06.964543: step 27923, loss 1.86265e-09, acc 1
2017-08-08T18:47:07.265679: step 27924, loss 0, acc 1
2017-08-08T18:47:07.513011: step 27925, loss 0, acc 1
2017-08-08T18:47:07.849450: step 27926, loss 2.9802e-07, acc 1
2017-08-08T18:47:08.264609: step 27927, loss 0, acc 1
2017-08-08T18:47:08.510890: step 27928, loss 7.45058e-09, acc 1
2017-08-08T18:47:08.712381: step 27929, loss 0, acc 1
2017-08-08T18:47:08.941226: step 27930, loss 4.56341e-07, acc 1
2017-08-08T18:47:09.213383: step 27931, loss 6.94751e-07, acc 1
2017-08-08T18:47:09.589499: step 27932, loss 0, acc 1
2017-08-08T18:47:09.973398: step 27933, loss 1.84391e-06, acc 1
2017-08-08T18:47:10.272536: step 27934, loss 0, acc 1
2017-08-08T18:47:10.531891: step 27935, loss 0, acc 1
2017-08-08T18:47:10.957003: step 27936, loss 8.00936e-08, acc 1
2017-08-08T18:47:11.238228: step 27937, loss 5.23959e-05, acc 1
2017-08-08T18:47:11.457229: step 27938, loss 9.68574e-08, acc 1
2017-08-08T18:47:11.755443: step 27939, loss 0, acc 1
2017-08-08T18:47:12.027517: step 27940, loss 5.27979e-06, acc 1
2017-08-08T18:47:12.271174: step 27941, loss 0.000192127, acc 1
2017-08-08T18:47:12.533981: step 27942, loss 2.75669e-07, acc 1
2017-08-08T18:47:12.760868: step 27943, loss 9.31322e-09, acc 1
2017-08-08T18:47:13.139559: step 27944, loss 0, acc 1
2017-08-08T18:47:13.396924: step 27945, loss 2.94021e-05, acc 1
2017-08-08T18:47:13.672299: step 27946, loss 1.86265e-09, acc 1
2017-08-08T18:47:13.973896: step 27947, loss 1.20695e-06, acc 1
2017-08-08T18:47:14.415524: step 27948, loss 1.86264e-08, acc 1
2017-08-08T18:47:14.827796: step 27949, loss 1.47899e-05, acc 1
2017-08-08T18:47:15.105343: step 27950, loss 1.58637e-05, acc 1
2017-08-08T18:47:15.337928: step 27951, loss 0, acc 1
2017-08-08T18:47:15.513749: step 27952, loss 2.62631e-07, acc 1
2017-08-08T18:47:15.878524: step 27953, loss 2.47244e-05, acc 1
2017-08-08T18:47:16.086742: step 27954, loss 1.39066e-05, acc 1
2017-08-08T18:47:16.318503: step 27955, loss 0, acc 1
2017-08-08T18:47:16.599362: step 27956, loss 1.86265e-09, acc 1
2017-08-08T18:47:16.899823: step 27957, loss 1.19209e-07, acc 1
2017-08-08T18:47:17.173389: step 27958, loss 1.86264e-08, acc 1
2017-08-08T18:47:17.413231: step 27959, loss 7.67397e-07, acc 1
2017-08-08T18:47:17.678590: step 27960, loss 0.00059126, acc 1
2017-08-08T18:47:18.098049: step 27961, loss 1.8424e-05, acc 1
2017-08-08T18:47:18.365854: step 27962, loss 0, acc 1
2017-08-08T18:47:18.633176: step 27963, loss 3.16649e-08, acc 1
2017-08-08T18:47:18.932920: step 27964, loss 1.19209e-07, acc 1
2017-08-08T18:47:19.341594: step 27965, loss 1.61111e-06, acc 1
2017-08-08T18:47:19.792271: step 27966, loss 9.89041e-07, acc 1
2017-08-08T18:47:20.098917: step 27967, loss 6.38012e-06, acc 1
2017-08-08T18:47:20.452405: step 27968, loss 0.0935288, acc 0.984375
2017-08-08T18:47:20.650210: step 27969, loss 2.42144e-08, acc 1
2017-08-08T18:47:20.935854: step 27970, loss 4.63794e-07, acc 1
2017-08-08T18:47:21.351459: step 27971, loss 0.0174234, acc 0.984375
2017-08-08T18:47:21.533330: step 27972, loss 0, acc 1
2017-08-08T18:47:21.738312: step 27973, loss 0, acc 1
2017-08-08T18:47:21.967726: step 27974, loss 1.52736e-07, acc 1
2017-08-08T18:47:22.346346: step 27975, loss 0, acc 1
2017-08-08T18:47:22.821774: step 27976, loss 6.44719e-06, acc 1
2017-08-08T18:47:23.182876: step 27977, loss 1.40251e-06, acc 1
2017-08-08T18:47:23.459056: step 27978, loss 8.70293e-06, acc 1
2017-08-08T18:47:23.688877: step 27979, loss 1.35596e-06, acc 1
2017-08-08T18:47:24.087802: step 27980, loss 0.0109268, acc 1
2017-08-08T18:47:24.331763: step 27981, loss 0.00016048, acc 1
2017-08-08T18:47:24.593405: step 27982, loss 3.16649e-08, acc 1
2017-08-08T18:47:24.871607: step 27983, loss 0.000638096, acc 1
2017-08-08T18:47:25.204505: step 27984, loss 3.41386e-06, acc 1
2017-08-08T18:47:25.591724: step 27985, loss 0.00832204, acc 1
2017-08-08T18:47:25.907518: step 27986, loss 1.86265e-09, acc 1
2017-08-08T18:47:26.150887: step 27987, loss 2.92434e-07, acc 1
2017-08-08T18:47:26.545048: step 27988, loss 0.000202998, acc 1
2017-08-08T18:47:26.744548: step 27989, loss 5.01044e-07, acc 1
2017-08-08T18:47:26.981473: step 27990, loss 2.70081e-07, acc 1
2017-08-08T18:47:27.183104: step 27991, loss 2.04891e-08, acc 1
2017-08-08T18:47:27.529579: step 27992, loss 1.86265e-09, acc 1
2017-08-08T18:47:27.799174: step 27993, loss 0.000194452, acc 1
2017-08-08T18:47:28.057382: step 27994, loss 5.53196e-07, acc 1
2017-08-08T18:47:28.344571: step 27995, loss 1.23116e-06, acc 1
2017-08-08T18:47:28.552800: step 27996, loss 2.04891e-08, acc 1
2017-08-08T18:47:28.801093: step 27997, loss 9.76156e-05, acc 1
2017-08-08T18:47:29.025315: step 27998, loss 0.0106426, acc 1
2017-08-08T18:47:29.202170: step 27999, loss 1.13621e-07, acc 1
2017-08-08T18:47:29.364405: step 28000, loss 1.67638e-08, acc 1

Evaluation:
2017-08-08T18:47:29.865771: step 28000, loss 9.03935, acc 0.719512

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-28000

2017-08-08T18:47:30.413346: step 28001, loss 1.61017e-05, acc 1
2017-08-08T18:47:30.638946: step 28002, loss 3.72529e-09, acc 1
2017-08-08T18:47:30.897502: step 28003, loss 3.72529e-09, acc 1
2017-08-08T18:47:31.375085: step 28004, loss 0, acc 1
2017-08-08T18:47:31.628472: step 28005, loss 0, acc 1
2017-08-08T18:47:31.885856: step 28006, loss 1.49012e-08, acc 1
2017-08-08T18:47:32.189378: step 28007, loss 6.28977e-05, acc 1
2017-08-08T18:47:32.610240: step 28008, loss 0, acc 1
2017-08-08T18:47:32.984058: step 28009, loss 7.45058e-09, acc 1
2017-08-08T18:47:33.325646: step 28010, loss 0, acc 1
2017-08-08T18:47:33.598047: step 28011, loss 1.02815e-06, acc 1
2017-08-08T18:47:34.023893: step 28012, loss 1.30385e-08, acc 1
2017-08-08T18:47:34.340397: step 28013, loss 2.77532e-07, acc 1
2017-08-08T18:47:34.598219: step 28014, loss 0, acc 1
2017-08-08T18:47:34.891166: step 28015, loss 1.97707e-05, acc 1
2017-08-08T18:47:35.302151: step 28016, loss 2.60956e-05, acc 1
2017-08-08T18:47:35.638596: step 28017, loss 1.11759e-08, acc 1
2017-08-08T18:47:35.992102: step 28018, loss 1.31001e-05, acc 1
2017-08-08T18:47:36.339354: step 28019, loss 0.000127969, acc 1
2017-08-08T18:47:36.566764: step 28020, loss 1.75088e-07, acc 1
2017-08-08T18:47:36.988821: step 28021, loss 9.24436e-05, acc 1
2017-08-08T18:47:37.302667: step 28022, loss 2.05809e-06, acc 1
2017-08-08T18:47:37.581800: step 28023, loss 2.50641e-05, acc 1
2017-08-08T18:47:37.866623: step 28024, loss 0.0424042, acc 0.984375
2017-08-08T18:47:38.288176: step 28025, loss 3.05082e-06, acc 1
2017-08-08T18:47:38.649395: step 28026, loss 0, acc 1
2017-08-08T18:47:38.926096: step 28027, loss 3.20372e-07, acc 1
2017-08-08T18:47:39.140178: step 28028, loss 1.49012e-08, acc 1
2017-08-08T18:47:39.443790: step 28029, loss 0, acc 1
2017-08-08T18:47:39.834670: step 28030, loss 3.98173e-05, acc 1
2017-08-08T18:47:40.074033: step 28031, loss 6.14672e-08, acc 1
2017-08-08T18:47:40.361324: step 28032, loss 5.58793e-09, acc 1
2017-08-08T18:47:40.629441: step 28033, loss 2.26198e-05, acc 1
2017-08-08T18:47:41.004963: step 28034, loss 0.00140652, acc 1
2017-08-08T18:47:41.424347: step 28035, loss 1.86265e-09, acc 1
2017-08-08T18:47:41.730058: step 28036, loss 3.72529e-09, acc 1
2017-08-08T18:47:41.926722: step 28037, loss 9.296e-05, acc 1
2017-08-08T18:47:42.301304: step 28038, loss 0, acc 1
2017-08-08T18:47:42.584499: step 28039, loss 1.24606e-06, acc 1
2017-08-08T18:47:42.875705: step 28040, loss 1.60367e-06, acc 1
2017-08-08T18:47:43.197397: step 28041, loss 0.0132373, acc 0.984375
2017-08-08T18:47:43.573558: step 28042, loss 0, acc 1
2017-08-08T18:47:43.837352: step 28043, loss 1.11759e-08, acc 1
2017-08-08T18:47:44.146886: step 28044, loss 0, acc 1
2017-08-08T18:47:44.376366: step 28045, loss 1.86265e-09, acc 1
2017-08-08T18:47:44.711340: step 28046, loss 0, acc 1
2017-08-08T18:47:45.086446: step 28047, loss 0.00151963, acc 1
2017-08-08T18:47:45.348022: step 28048, loss 0, acc 1
2017-08-08T18:47:45.603978: step 28049, loss 3.72529e-09, acc 1
2017-08-08T18:47:45.922900: step 28050, loss 7.94728e-09, acc 1
2017-08-08T18:47:46.267630: step 28051, loss 3.72529e-09, acc 1
2017-08-08T18:47:46.604706: step 28052, loss 0.00355574, acc 1
2017-08-08T18:47:46.908662: step 28053, loss 3.10478e-06, acc 1
2017-08-08T18:47:47.160021: step 28054, loss 4.09779e-07, acc 1
2017-08-08T18:47:47.577905: step 28055, loss 0, acc 1
2017-08-08T18:47:47.885207: step 28056, loss 9.68573e-08, acc 1
2017-08-08T18:47:48.161787: step 28057, loss 2.70082e-07, acc 1
2017-08-08T18:47:48.395365: step 28058, loss 0, acc 1
2017-08-08T18:47:48.697396: step 28059, loss 0, acc 1
2017-08-08T18:47:49.070191: step 28060, loss 0, acc 1
2017-08-08T18:47:49.491647: step 28061, loss 2.4773e-07, acc 1
2017-08-08T18:47:49.783277: step 28062, loss 6.33298e-08, acc 1
2017-08-08T18:47:50.021171: step 28063, loss 2.71944e-07, acc 1
2017-08-08T18:47:50.513208: step 28064, loss 1.86265e-09, acc 1
2017-08-08T18:47:50.763866: step 28065, loss 5.58793e-09, acc 1
2017-08-08T18:47:50.968755: step 28066, loss 8.56814e-08, acc 1
2017-08-08T18:47:51.231908: step 28067, loss 2.42144e-08, acc 1
2017-08-08T18:47:51.578348: step 28068, loss 1.88126e-07, acc 1
2017-08-08T18:47:51.935820: step 28069, loss 4.09781e-08, acc 1
2017-08-08T18:47:52.231560: step 28070, loss 0, acc 1
2017-08-08T18:47:52.491918: step 28071, loss 2.06752e-07, acc 1
2017-08-08T18:47:52.715710: step 28072, loss 3.72529e-08, acc 1
2017-08-08T18:47:53.024136: step 28073, loss 3.70625e-06, acc 1
2017-08-08T18:47:53.334023: step 28074, loss 7.45058e-09, acc 1
2017-08-08T18:47:53.607452: step 28075, loss 8.95907e-07, acc 1
2017-08-08T18:47:53.824614: step 28076, loss 0.0941647, acc 0.984375
2017-08-08T18:47:54.176408: step 28077, loss 2.04891e-08, acc 1
2017-08-08T18:47:54.563711: step 28078, loss 0, acc 1
2017-08-08T18:47:54.869911: step 28079, loss 2.94295e-07, acc 1
2017-08-08T18:47:55.102181: step 28080, loss 0, acc 1
2017-08-08T18:47:55.317712: step 28081, loss 2.21653e-07, acc 1
2017-08-08T18:47:55.643058: step 28082, loss 0, acc 1
2017-08-08T18:47:55.978633: step 28083, loss 4.16445e-06, acc 1
2017-08-08T18:47:56.168474: step 28084, loss 1.17346e-07, acc 1
2017-08-08T18:47:56.380108: step 28085, loss 1.68936e-06, acc 1
2017-08-08T18:47:56.602720: step 28086, loss 3.72529e-09, acc 1
2017-08-08T18:47:56.957326: step 28087, loss 0.0133649, acc 1
2017-08-08T18:47:57.241243: step 28088, loss 0, acc 1
2017-08-08T18:47:57.501624: step 28089, loss 1.71363e-07, acc 1
2017-08-08T18:47:57.736495: step 28090, loss 1.98395e-05, acc 1
2017-08-08T18:47:57.958719: step 28091, loss 0, acc 1
2017-08-08T18:47:58.323413: step 28092, loss 1.67638e-08, acc 1
2017-08-08T18:47:58.540245: step 28093, loss 0.000270047, acc 1
2017-08-08T18:47:58.793762: step 28094, loss 5.49471e-07, acc 1
2017-08-08T18:47:59.029043: step 28095, loss 3.72529e-09, acc 1
2017-08-08T18:47:59.460165: step 28096, loss 2.71564e-06, acc 1
2017-08-08T18:47:59.884273: step 28097, loss 0, acc 1
2017-08-08T18:48:00.270592: step 28098, loss 2.79397e-08, acc 1
2017-08-08T18:48:00.508823: step 28099, loss 0, acc 1
2017-08-08T18:48:00.732255: step 28100, loss 1.86265e-09, acc 1

Evaluation:
2017-08-08T18:48:01.622937: step 28100, loss 9.27961, acc 0.713884

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-28100

2017-08-08T18:48:02.197212: step 28101, loss 5.58793e-09, acc 1
2017-08-08T18:48:02.660088: step 28102, loss 1.96915e-05, acc 1
2017-08-08T18:48:02.977678: step 28103, loss 1.39133e-06, acc 1
2017-08-08T18:48:03.457337: step 28104, loss 1.48818e-06, acc 1
2017-08-08T18:48:03.737476: step 28105, loss 0.000113829, acc 1
2017-08-08T18:48:04.021782: step 28106, loss 3.14861e-05, acc 1
2017-08-08T18:48:04.485721: step 28107, loss 2.5518e-07, acc 1
2017-08-08T18:48:04.741014: step 28108, loss 1.37888e-05, acc 1
2017-08-08T18:48:05.043320: step 28109, loss 0.000110704, acc 1
2017-08-08T18:48:05.440774: step 28110, loss 1.86265e-09, acc 1
2017-08-08T18:48:05.905853: step 28111, loss 0, acc 1
2017-08-08T18:48:06.270507: step 28112, loss 9.10812e-07, acc 1
2017-08-08T18:48:06.645752: step 28113, loss 0.00205705, acc 1
2017-08-08T18:48:06.837722: step 28114, loss 2.42144e-08, acc 1
2017-08-08T18:48:07.206370: step 28115, loss 9.31322e-09, acc 1
2017-08-08T18:48:07.555923: step 28116, loss 9.31322e-09, acc 1
2017-08-08T18:48:07.818389: step 28117, loss 1.61056e-05, acc 1
2017-08-08T18:48:08.115945: step 28118, loss 1.86265e-09, acc 1
2017-08-08T18:48:08.493410: step 28119, loss 3.02837e-06, acc 1
2017-08-08T18:48:08.828607: step 28120, loss 1.86265e-09, acc 1
2017-08-08T18:48:09.136945: step 28121, loss 3.72529e-08, acc 1
2017-08-08T18:48:09.397255: step 28122, loss 1.11759e-08, acc 1
2017-08-08T18:48:09.592335: step 28123, loss 1.39698e-07, acc 1
2017-08-08T18:48:09.972034: step 28124, loss 1.11759e-08, acc 1
2017-08-08T18:48:10.297145: step 28125, loss 3.61349e-07, acc 1
2017-08-08T18:48:10.533689: step 28126, loss 2.98023e-08, acc 1
2017-08-08T18:48:10.887753: step 28127, loss 0, acc 1
2017-08-08T18:48:11.356971: step 28128, loss 1.86265e-09, acc 1
2017-08-08T18:48:11.770555: step 28129, loss 2.79397e-08, acc 1
2017-08-08T18:48:12.055869: step 28130, loss 3.42725e-07, acc 1
2017-08-08T18:48:12.281708: step 28131, loss 2.38417e-07, acc 1
2017-08-08T18:48:12.525098: step 28132, loss 5.63162e-06, acc 1
2017-08-08T18:48:12.914696: step 28133, loss 0, acc 1
2017-08-08T18:48:13.212435: step 28134, loss 0, acc 1
2017-08-08T18:48:13.510083: step 28135, loss 0.000376735, acc 1
2017-08-08T18:48:13.752898: step 28136, loss 0, acc 1
2017-08-08T18:48:14.146686: step 28137, loss 2.60768e-07, acc 1
2017-08-08T18:48:14.501374: step 28138, loss 0, acc 1
2017-08-08T18:48:14.931438: step 28139, loss 3.72529e-09, acc 1
2017-08-08T18:48:15.270035: step 28140, loss 4.84287e-08, acc 1
2017-08-08T18:48:15.499668: step 28141, loss 0, acc 1
2017-08-08T18:48:15.948093: step 28142, loss 3.7625e-07, acc 1
2017-08-08T18:48:16.266182: step 28143, loss 2.42144e-08, acc 1
2017-08-08T18:48:16.465345: step 28144, loss 0, acc 1
2017-08-08T18:48:16.668365: step 28145, loss 1.17715e-06, acc 1
2017-08-08T18:48:16.887311: step 28146, loss 0, acc 1
2017-08-08T18:48:17.312999: step 28147, loss 2.38418e-07, acc 1
2017-08-08T18:48:17.671249: step 28148, loss 1.00583e-07, acc 1
2017-08-08T18:48:18.025343: step 28149, loss 3.72529e-08, acc 1
2017-08-08T18:48:18.267649: step 28150, loss 2.27241e-07, acc 1
2017-08-08T18:48:18.631246: step 28151, loss 0, acc 1
2017-08-08T18:48:18.898349: step 28152, loss 0.00496142, acc 1
2017-08-08T18:48:19.186905: step 28153, loss 6.51924e-08, acc 1
2017-08-08T18:48:19.568766: step 28154, loss 1.86265e-09, acc 1
2017-08-08T18:48:20.036802: step 28155, loss 1.626e-06, acc 1
2017-08-08T18:48:20.413313: step 28156, loss 1.10671e-05, acc 1
2017-08-08T18:48:20.797432: step 28157, loss 6.61225e-07, acc 1
2017-08-08T18:48:21.073918: step 28158, loss 1.86265e-09, acc 1
2017-08-08T18:48:21.258791: step 28159, loss 0, acc 1
2017-08-08T18:48:21.565767: step 28160, loss 1.86265e-09, acc 1
2017-08-08T18:48:21.742454: step 28161, loss 1.93144e-06, acc 1
2017-08-08T18:48:21.945293: step 28162, loss 1.7695e-07, acc 1
2017-08-08T18:48:22.181557: step 28163, loss 9.31322e-09, acc 1
2017-08-08T18:48:22.542307: step 28164, loss 0, acc 1
2017-08-08T18:48:22.833403: step 28165, loss 1.47149e-07, acc 1
2017-08-08T18:48:23.172499: step 28166, loss 2.42144e-08, acc 1
2017-08-08T18:48:23.420631: step 28167, loss 2.44095e-05, acc 1
2017-08-08T18:48:23.760871: step 28168, loss 1.30385e-08, acc 1
2017-08-08T18:48:24.016326: step 28169, loss 3.37289e-06, acc 1
2017-08-08T18:48:24.298263: step 28170, loss 0, acc 1
2017-08-08T18:48:24.601371: step 28171, loss 2.68219e-07, acc 1
2017-08-08T18:48:24.948416: step 28172, loss 1.84401e-07, acc 1
2017-08-08T18:48:25.287977: step 28173, loss 0, acc 1
2017-08-08T18:48:25.629332: step 28174, loss 4.4885e-05, acc 1
2017-08-08T18:48:25.897039: step 28175, loss 1.88833e-05, acc 1
2017-08-08T18:48:26.094751: step 28176, loss 1.30385e-08, acc 1
2017-08-08T18:48:26.410074: step 28177, loss 5.58794e-09, acc 1
2017-08-08T18:48:26.661326: step 28178, loss 1.0031e-05, acc 1
2017-08-08T18:48:26.952566: step 28179, loss 5.77419e-08, acc 1
2017-08-08T18:48:27.229724: step 28180, loss 6.33298e-08, acc 1
2017-08-08T18:48:27.478865: step 28181, loss 6.20707e-05, acc 1
2017-08-08T18:48:27.923917: step 28182, loss 2.6077e-08, acc 1
2017-08-08T18:48:28.321355: step 28183, loss 0, acc 1
2017-08-08T18:48:28.665312: step 28184, loss 0, acc 1
2017-08-08T18:48:28.935857: step 28185, loss 0, acc 1
2017-08-08T18:48:29.189575: step 28186, loss 1.11759e-08, acc 1
2017-08-08T18:48:29.604016: step 28187, loss 5.9463e-06, acc 1
2017-08-08T18:48:29.911580: step 28188, loss 5.06634e-07, acc 1
2017-08-08T18:48:30.260240: step 28189, loss 4.26542e-07, acc 1
2017-08-08T18:48:30.690901: step 28190, loss 1.68607e-05, acc 1
2017-08-08T18:48:31.045345: step 28191, loss 8.54931e-07, acc 1
2017-08-08T18:48:31.371799: step 28192, loss 5.58794e-09, acc 1
2017-08-08T18:48:31.649905: step 28193, loss 2.75819e-05, acc 1
2017-08-08T18:48:31.951315: step 28194, loss 1.99468e-05, acc 1
2017-08-08T18:48:32.371089: step 28195, loss 9.61095e-07, acc 1
2017-08-08T18:48:32.648578: step 28196, loss 7.45058e-09, acc 1
2017-08-08T18:48:32.994405: step 28197, loss 8.19562e-08, acc 1
2017-08-08T18:48:33.396923: step 28198, loss 0.000267959, acc 1
2017-08-08T18:48:33.789386: step 28199, loss 5.25257e-07, acc 1
2017-08-08T18:48:34.172692: step 28200, loss 4.56969e-08, acc 1

Evaluation:
2017-08-08T18:48:34.955898: step 28200, loss 9.20328, acc 0.718574

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-28200

2017-08-08T18:48:35.552953: step 28201, loss 1.43423e-07, acc 1
2017-08-08T18:48:35.792271: step 28202, loss 2.0299e-05, acc 1
2017-08-08T18:48:36.058397: step 28203, loss 8.94044e-07, acc 1
2017-08-08T18:48:36.421410: step 28204, loss 3.72529e-09, acc 1
2017-08-08T18:48:36.854422: step 28205, loss 2.04891e-08, acc 1
2017-08-08T18:48:37.307908: step 28206, loss 2.04891e-08, acc 1
2017-08-08T18:48:37.672261: step 28207, loss 8.75441e-08, acc 1
2017-08-08T18:48:37.925460: step 28208, loss 1.30385e-08, acc 1
2017-08-08T18:48:38.174818: step 28209, loss 3.35276e-08, acc 1
2017-08-08T18:48:38.638317: step 28210, loss 5.12219e-07, acc 1
2017-08-08T18:48:38.897860: step 28211, loss 3.59487e-07, acc 1
2017-08-08T18:48:39.110670: step 28212, loss 3.63213e-07, acc 1
2017-08-08T18:48:39.370652: step 28213, loss 0, acc 1
2017-08-08T18:48:39.697284: step 28214, loss 2.89242e-06, acc 1
2017-08-08T18:48:40.033246: step 28215, loss 0, acc 1
2017-08-08T18:48:40.305440: step 28216, loss 5.4705e-05, acc 1
2017-08-08T18:48:40.531772: step 28217, loss 7.45058e-09, acc 1
2017-08-08T18:48:40.764547: step 28218, loss 9.18288e-05, acc 1
2017-08-08T18:48:40.993372: step 28219, loss 2.23517e-08, acc 1
2017-08-08T18:48:41.467029: step 28220, loss 0, acc 1
2017-08-08T18:48:41.760901: step 28221, loss 9.61098e-07, acc 1
2017-08-08T18:48:42.066446: step 28222, loss 0.000206574, acc 1
2017-08-08T18:48:42.343150: step 28223, loss 1.29159e-05, acc 1
2017-08-08T18:48:42.707278: step 28224, loss 1.24797e-07, acc 1
2017-08-08T18:48:43.066594: step 28225, loss 9.31322e-09, acc 1
2017-08-08T18:48:43.404339: step 28226, loss 2.42144e-08, acc 1
2017-08-08T18:48:43.671800: step 28227, loss 3.82359e-06, acc 1
2017-08-08T18:48:43.937591: step 28228, loss 7.45058e-09, acc 1
2017-08-08T18:48:44.393300: step 28229, loss 2.23517e-08, acc 1
2017-08-08T18:48:44.659719: step 28230, loss 1.8808e-05, acc 1
2017-08-08T18:48:44.958747: step 28231, loss 5.80159e-06, acc 1
2017-08-08T18:48:45.187234: step 28232, loss 4.09781e-08, acc 1
2017-08-08T18:48:45.631409: step 28233, loss 1.47515e-06, acc 1
2017-08-08T18:48:46.007861: step 28234, loss 0, acc 1
2017-08-08T18:48:46.315229: step 28235, loss 3.91155e-08, acc 1
2017-08-08T18:48:46.533788: step 28236, loss 0, acc 1
2017-08-08T18:48:46.915218: step 28237, loss 0.000178576, acc 1
2017-08-08T18:48:47.200333: step 28238, loss 0.000174933, acc 1
2017-08-08T18:48:47.450251: step 28239, loss 5.82997e-07, acc 1
2017-08-08T18:48:47.707351: step 28240, loss 0, acc 1
2017-08-08T18:48:48.071758: step 28241, loss 4.47034e-08, acc 1
2017-08-08T18:48:48.464613: step 28242, loss 0.000170727, acc 1
2017-08-08T18:48:48.888637: step 28243, loss 4.00464e-07, acc 1
2017-08-08T18:48:49.176902: step 28244, loss 0, acc 1
2017-08-08T18:48:49.520847: step 28245, loss 0, acc 1
2017-08-08T18:48:49.943038: step 28246, loss 1.695e-07, acc 1
2017-08-08T18:48:50.245241: step 28247, loss 0, acc 1
2017-08-08T18:48:50.500061: step 28248, loss 0.000188831, acc 1
2017-08-08T18:48:50.873171: step 28249, loss 1.37835e-07, acc 1
2017-08-08T18:48:51.281566: step 28250, loss 1.11759e-08, acc 1
2017-08-08T18:48:51.709425: step 28251, loss 8.94067e-08, acc 1
2017-08-08T18:48:52.019464: step 28252, loss 3.72529e-09, acc 1
2017-08-08T18:48:52.294597: step 28253, loss 1.37271e-06, acc 1
2017-08-08T18:48:52.729745: step 28254, loss 7.45058e-09, acc 1
2017-08-08T18:48:52.937598: step 28255, loss 1.40624e-06, acc 1
2017-08-08T18:48:53.181104: step 28256, loss 3.35273e-07, acc 1
2017-08-08T18:48:53.429913: step 28257, loss 9.31322e-09, acc 1
2017-08-08T18:48:53.838297: step 28258, loss 1.56329e-05, acc 1
2017-08-08T18:48:54.277420: step 28259, loss 8.75441e-08, acc 1
2017-08-08T18:48:54.637739: step 28260, loss 0, acc 1
2017-08-08T18:48:54.908301: step 28261, loss 9.4631e-06, acc 1
2017-08-08T18:48:55.132746: step 28262, loss 0, acc 1
2017-08-08T18:48:55.439356: step 28263, loss 7.05231e-06, acc 1
2017-08-08T18:48:55.854957: step 28264, loss 3.69695e-06, acc 1
2017-08-08T18:48:56.118508: step 28265, loss 0, acc 1
2017-08-08T18:48:56.411753: step 28266, loss 0, acc 1
2017-08-08T18:48:56.669923: step 28267, loss 1.13621e-07, acc 1
2017-08-08T18:48:57.089370: step 28268, loss 3.96739e-07, acc 1
2017-08-08T18:48:57.428581: step 28269, loss 0, acc 1
2017-08-08T18:48:57.798997: step 28270, loss 2.42144e-08, acc 1
2017-08-08T18:48:58.079245: step 28271, loss 2.04891e-08, acc 1
2017-08-08T18:48:58.357067: step 28272, loss 0, acc 1
2017-08-08T18:48:58.738515: step 28273, loss 4.62889e-05, acc 1
2017-08-08T18:48:59.101763: step 28274, loss 5.34574e-07, acc 1
2017-08-08T18:48:59.456192: step 28275, loss 0, acc 1
2017-08-08T18:48:59.778901: step 28276, loss 4.28403e-07, acc 1
2017-08-08T18:49:00.129370: step 28277, loss 3.53902e-08, acc 1
2017-08-08T18:49:00.484243: step 28278, loss 0, acc 1
2017-08-08T18:49:00.808334: step 28279, loss 2.01165e-07, acc 1
2017-08-08T18:49:01.077507: step 28280, loss 0, acc 1
2017-08-08T18:49:01.339124: step 28281, loss 2.73599e-06, acc 1
2017-08-08T18:49:01.803701: step 28282, loss 7.99524e-06, acc 1
2017-08-08T18:49:02.164823: step 28283, loss 1.67638e-08, acc 1
2017-08-08T18:49:02.453524: step 28284, loss 1.54132e-05, acc 1
2017-08-08T18:49:02.766997: step 28285, loss 0, acc 1
2017-08-08T18:49:03.207242: step 28286, loss 2.12083e-05, acc 1
2017-08-08T18:49:03.593311: step 28287, loss 3.24097e-07, acc 1
2017-08-08T18:49:04.084353: step 28288, loss 1.30385e-08, acc 1
2017-08-08T18:49:04.440378: step 28289, loss 0, acc 1
2017-08-08T18:49:04.873292: step 28290, loss 0.000174874, acc 1
2017-08-08T18:49:05.323400: step 28291, loss 7.45058e-09, acc 1
2017-08-08T18:49:05.689534: step 28292, loss 1.695e-07, acc 1
2017-08-08T18:49:05.992195: step 28293, loss 1.15108e-06, acc 1
2017-08-08T18:49:06.313948: step 28294, loss 3.7625e-07, acc 1
2017-08-08T18:49:06.662903: step 28295, loss 7.45058e-09, acc 1
2017-08-08T18:49:07.028351: step 28296, loss 0, acc 1
2017-08-08T18:49:07.410900: step 28297, loss 5.56921e-07, acc 1
2017-08-08T18:49:07.704661: step 28298, loss 2.21653e-07, acc 1
2017-08-08T18:49:07.939239: step 28299, loss 0, acc 1
2017-08-08T18:49:08.400323: step 28300, loss 0, acc 1

Evaluation:
2017-08-08T18:49:09.270261: step 28300, loss 9.22465, acc 0.721388

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-28300

2017-08-08T18:49:09.853170: step 28301, loss 0.0127611, acc 0.984375
2017-08-08T18:49:10.146579: step 28302, loss 5.63894e-05, acc 1
2017-08-08T18:49:10.523083: step 28303, loss 1.04308e-07, acc 1
2017-08-08T18:49:10.779798: step 28304, loss 1.52737e-07, acc 1
2017-08-08T18:49:11.045176: step 28305, loss 0, acc 1
2017-08-08T18:49:11.464287: step 28306, loss 4.26746e-05, acc 1
2017-08-08T18:49:11.746229: step 28307, loss 3.16649e-08, acc 1
2017-08-08T18:49:12.049294: step 28308, loss 6.70551e-08, acc 1
2017-08-08T18:49:12.432295: step 28309, loss 3.42723e-07, acc 1
2017-08-08T18:49:12.787251: step 28310, loss 0, acc 1
2017-08-08T18:49:13.107222: step 28311, loss 4.84389e-05, acc 1
2017-08-08T18:49:13.463076: step 28312, loss 2.56375e-05, acc 1
2017-08-08T18:49:13.800058: step 28313, loss 1.03791e-05, acc 1
2017-08-08T18:49:14.131432: step 28314, loss 3.72529e-09, acc 1
2017-08-08T18:49:14.571703: step 28315, loss 1.49012e-08, acc 1
2017-08-08T18:49:14.924149: step 28316, loss 5.87267e-05, acc 1
2017-08-08T18:49:15.215285: step 28317, loss 1.39698e-07, acc 1
2017-08-08T18:49:15.488403: step 28318, loss 1.30385e-08, acc 1
2017-08-08T18:49:15.917430: step 28319, loss 9.12693e-08, acc 1
2017-08-08T18:49:16.257753: step 28320, loss 5.76756e-06, acc 1
2017-08-08T18:49:16.663622: step 28321, loss 5.58793e-09, acc 1
2017-08-08T18:49:16.887634: step 28322, loss 9.31322e-09, acc 1
2017-08-08T18:49:17.080227: step 28323, loss 7.45058e-09, acc 1
2017-08-08T18:49:17.545764: step 28324, loss 5.96045e-08, acc 1
2017-08-08T18:49:17.876637: step 28325, loss 0, acc 1
2017-08-08T18:49:18.155602: step 28326, loss 6.97033e-06, acc 1
2017-08-08T18:49:18.403129: step 28327, loss 4.06051e-07, acc 1
2017-08-08T18:49:18.749014: step 28328, loss 0, acc 1
2017-08-08T18:49:19.059680: step 28329, loss 1.67638e-08, acc 1
2017-08-08T18:49:19.382796: step 28330, loss 0, acc 1
2017-08-08T18:49:19.759627: step 28331, loss 2.42144e-08, acc 1
2017-08-08T18:49:20.089607: step 28332, loss 1.86264e-08, acc 1
2017-08-08T18:49:20.416535: step 28333, loss 5.58794e-09, acc 1
2017-08-08T18:49:20.682530: step 28334, loss 9.31322e-09, acc 1
2017-08-08T18:49:20.941184: step 28335, loss 0.0147558, acc 0.984375
2017-08-08T18:49:21.236583: step 28336, loss 3.35276e-08, acc 1
2017-08-08T18:49:21.542332: step 28337, loss 0, acc 1
2017-08-08T18:49:21.946924: step 28338, loss 1.11759e-08, acc 1
2017-08-08T18:49:22.313372: step 28339, loss 4.00615e-06, acc 1
2017-08-08T18:49:22.527452: step 28340, loss 0, acc 1
2017-08-08T18:49:22.718242: step 28341, loss 1.86265e-09, acc 1
2017-08-08T18:49:23.049053: step 28342, loss 1.51986e-06, acc 1
2017-08-08T18:49:23.299092: step 28343, loss 1.11759e-08, acc 1
2017-08-08T18:49:23.554941: step 28344, loss 8.38188e-08, acc 1
2017-08-08T18:49:23.822408: step 28345, loss 1.76755e-06, acc 1
2017-08-08T18:49:24.147116: step 28346, loss 4.84287e-08, acc 1
2017-08-08T18:49:24.490339: step 28347, loss 4.3399e-07, acc 1
2017-08-08T18:49:24.852683: step 28348, loss 1.32247e-07, acc 1
2017-08-08T18:49:25.098959: step 28349, loss 8.10233e-07, acc 1
2017-08-08T18:49:25.280849: step 28350, loss 1.10066e-06, acc 1
2017-08-08T18:49:25.556827: step 28351, loss 9.40607e-07, acc 1
2017-08-08T18:49:25.809676: step 28352, loss 6.70545e-07, acc 1
2017-08-08T18:49:26.027914: step 28353, loss 2.98023e-08, acc 1
2017-08-08T18:49:26.234396: step 28354, loss 1.58324e-07, acc 1
2017-08-08T18:49:26.533320: step 28355, loss 3.72529e-09, acc 1
2017-08-08T18:49:26.851630: step 28356, loss 1.86265e-09, acc 1
2017-08-08T18:49:27.154278: step 28357, loss 7.45058e-09, acc 1
2017-08-08T18:49:27.361214: step 28358, loss 1.58077e-05, acc 1
2017-08-08T18:49:27.599016: step 28359, loss 5.43883e-07, acc 1
2017-08-08T18:49:27.974090: step 28360, loss 0, acc 1
2017-08-08T18:49:28.203704: step 28361, loss 1.58689e-06, acc 1
2017-08-08T18:49:28.472281: step 28362, loss 0, acc 1
2017-08-08T18:49:28.957399: step 28363, loss 2.06752e-07, acc 1
2017-08-08T18:49:29.313370: step 28364, loss 0, acc 1
2017-08-08T18:49:29.572051: step 28365, loss 6.53775e-07, acc 1
2017-08-08T18:49:29.768868: step 28366, loss 1.67344e-05, acc 1
2017-08-08T18:49:30.007132: step 28367, loss 1.86265e-09, acc 1
2017-08-08T18:49:30.334446: step 28368, loss 5.21532e-07, acc 1
2017-08-08T18:49:30.522408: step 28369, loss 2.79397e-08, acc 1
2017-08-08T18:49:30.745351: step 28370, loss 0, acc 1
2017-08-08T18:49:30.948693: step 28371, loss 0, acc 1
2017-08-08T18:49:31.207439: step 28372, loss 0, acc 1
2017-08-08T18:49:31.556815: step 28373, loss 1.09896e-07, acc 1
2017-08-08T18:49:31.795104: step 28374, loss 1.38761e-06, acc 1
2017-08-08T18:49:32.049512: step 28375, loss 1.48074e-06, acc 1
2017-08-08T18:49:32.225406: step 28376, loss 0, acc 1
2017-08-08T18:49:32.571669: step 28377, loss 3.72529e-09, acc 1
2017-08-08T18:49:32.760391: step 28378, loss 6.35964e-06, acc 1
2017-08-08T18:49:32.971604: step 28379, loss 0, acc 1
2017-08-08T18:49:33.211561: step 28380, loss 3.18988e-05, acc 1
2017-08-08T18:49:33.605428: step 28381, loss 6.72272e-06, acc 1
2017-08-08T18:49:33.877458: step 28382, loss 1.22756e-05, acc 1
2017-08-08T18:49:34.163038: step 28383, loss 0, acc 1
2017-08-08T18:49:34.362961: step 28384, loss 6.89178e-08, acc 1
2017-08-08T18:49:34.626437: step 28385, loss 1.07845e-06, acc 1
2017-08-08T18:49:35.054618: step 28386, loss 5.58793e-09, acc 1
2017-08-08T18:49:35.309199: step 28387, loss 3.16649e-08, acc 1
2017-08-08T18:49:35.562010: step 28388, loss 7.29242e-06, acc 1
2017-08-08T18:49:35.813601: step 28389, loss 5.58794e-09, acc 1
2017-08-08T18:49:36.182263: step 28390, loss 3.35276e-08, acc 1
2017-08-08T18:49:36.491264: step 28391, loss 3.76251e-07, acc 1
2017-08-08T18:49:36.899291: step 28392, loss 3.74553e-05, acc 1
2017-08-08T18:49:37.199131: step 28393, loss 7.45058e-09, acc 1
2017-08-08T18:49:37.443087: step 28394, loss 9.51696e-05, acc 1
2017-08-08T18:49:37.980559: step 28395, loss 2.02271e-06, acc 1
2017-08-08T18:49:38.237053: step 28396, loss 6.64823e-06, acc 1
2017-08-08T18:49:38.517791: step 28397, loss 5.2154e-08, acc 1
2017-08-08T18:49:38.802074: step 28398, loss 0, acc 1
2017-08-08T18:49:39.237394: step 28399, loss 3.72529e-09, acc 1
2017-08-08T18:49:39.636945: step 28400, loss 5.21532e-07, acc 1

Evaluation:
2017-08-08T18:49:40.451180: step 28400, loss 9.32282, acc 0.713884

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-28400

2017-08-08T18:49:40.920460: step 28401, loss 0, acc 1
2017-08-08T18:49:41.335849: step 28402, loss 3.16649e-08, acc 1
2017-08-08T18:49:41.669209: step 28403, loss 0, acc 1
2017-08-08T18:49:41.894273: step 28404, loss 0, acc 1
2017-08-08T18:49:42.161419: step 28405, loss 0, acc 1
2017-08-08T18:49:42.537457: step 28406, loss 1.56737e-05, acc 1
2017-08-08T18:49:42.893387: step 28407, loss 1.86265e-09, acc 1
2017-08-08T18:49:43.204623: step 28408, loss 0, acc 1
2017-08-08T18:49:43.384170: step 28409, loss 3.16649e-08, acc 1
2017-08-08T18:49:43.643318: step 28410, loss 0, acc 1
2017-08-08T18:49:43.859256: step 28411, loss 5.58793e-09, acc 1
2017-08-08T18:49:44.065325: step 28412, loss 0, acc 1
2017-08-08T18:49:44.271630: step 28413, loss 2.50319e-06, acc 1
2017-08-08T18:49:44.585734: step 28414, loss 7.13377e-07, acc 1
2017-08-08T18:49:44.910951: step 28415, loss 0, acc 1
2017-08-08T18:49:45.285557: step 28416, loss 5.58793e-09, acc 1
2017-08-08T18:49:45.521604: step 28417, loss 1.47308e-05, acc 1
2017-08-08T18:49:45.735834: step 28418, loss 2.07671e-06, acc 1
2017-08-08T18:49:46.076721: step 28419, loss 8.94067e-08, acc 1
2017-08-08T18:49:46.298006: step 28420, loss 1.30941e-06, acc 1
2017-08-08T18:49:46.505585: step 28421, loss 1.86265e-09, acc 1
2017-08-08T18:49:46.743146: step 28422, loss 9.3132e-08, acc 1
2017-08-08T18:49:47.100657: step 28423, loss 1.04032e-05, acc 1
2017-08-08T18:49:47.453385: step 28424, loss 9.5115e-06, acc 1
2017-08-08T18:49:47.689342: step 28425, loss 0, acc 1
2017-08-08T18:49:47.923302: step 28426, loss 0.0653769, acc 0.984375
2017-08-08T18:49:48.178720: step 28427, loss 2.53318e-07, acc 1
2017-08-08T18:49:48.564230: step 28428, loss 1.30385e-08, acc 1
2017-08-08T18:49:48.823998: step 28429, loss 5.72291e-06, acc 1
2017-08-08T18:49:49.075461: step 28430, loss 0.000325844, acc 1
2017-08-08T18:49:49.319361: step 28431, loss 2.77532e-07, acc 1
2017-08-08T18:49:49.652923: step 28432, loss 1.19276e-05, acc 1
2017-08-08T18:49:49.973454: step 28433, loss 5.36437e-07, acc 1
2017-08-08T18:49:50.238017: step 28434, loss 0, acc 1
2017-08-08T18:49:50.454295: step 28435, loss 1.86265e-09, acc 1
2017-08-08T18:49:50.709488: step 28436, loss 9.31322e-09, acc 1
2017-08-08T18:49:51.003477: step 28437, loss 0, acc 1
2017-08-08T18:49:51.236655: step 28438, loss 3.7704e-05, acc 1
2017-08-08T18:49:51.497023: step 28439, loss 0, acc 1
2017-08-08T18:49:51.821574: step 28440, loss 0, acc 1
2017-08-08T18:49:52.098486: step 28441, loss 2.1959e-06, acc 1
2017-08-08T18:49:52.405390: step 28442, loss 1.74893e-06, acc 1
2017-08-08T18:49:52.717464: step 28443, loss 1.32899e-05, acc 1
2017-08-08T18:49:52.943153: step 28444, loss 3.3878e-06, acc 1
2017-08-08T18:49:53.367312: step 28445, loss 2.05064e-06, acc 1
2017-08-08T18:49:53.633451: step 28446, loss 0, acc 1
2017-08-08T18:49:53.844771: step 28447, loss 1.49011e-07, acc 1
2017-08-08T18:49:54.120952: step 28448, loss 8.97689e-05, acc 1
2017-08-08T18:49:54.561447: step 28449, loss 2.67641e-06, acc 1
2017-08-08T18:49:54.944815: step 28450, loss 2.6077e-08, acc 1
2017-08-08T18:49:55.283052: step 28451, loss 0, acc 1
2017-08-08T18:49:55.482795: step 28452, loss 1.01314e-05, acc 1
2017-08-08T18:49:55.738749: step 28453, loss 1.53474e-06, acc 1
2017-08-08T18:49:56.118909: step 28454, loss 0, acc 1
2017-08-08T18:49:56.302628: step 28455, loss 3.16649e-08, acc 1
2017-08-08T18:49:56.514986: step 28456, loss 4.74716e-06, acc 1
2017-08-08T18:49:56.937460: step 28457, loss 1.0356e-06, acc 1
2017-08-08T18:49:57.420832: step 28458, loss 5.77419e-08, acc 1
2017-08-08T18:49:57.797015: step 28459, loss 0, acc 1
2017-08-08T18:49:58.055480: step 28460, loss 2.67936e-05, acc 1
2017-08-08T18:49:58.372256: step 28461, loss 1.86265e-09, acc 1
2017-08-08T18:49:58.733395: step 28462, loss 5.7369e-07, acc 1
2017-08-08T18:49:58.979421: step 28463, loss 0.000108161, acc 1
2017-08-08T18:49:59.204046: step 28464, loss 3.53902e-08, acc 1
2017-08-08T18:49:59.641467: step 28465, loss 1.02071e-06, acc 1
2017-08-08T18:50:00.144728: step 28466, loss 2.6764e-06, acc 1
2017-08-08T18:50:00.398132: step 28467, loss 9.53224e-05, acc 1
2017-08-08T18:50:00.595213: step 28468, loss 3.72529e-09, acc 1
2017-08-08T18:50:00.851819: step 28469, loss 7.66663e-06, acc 1
2017-08-08T18:50:01.066048: step 28470, loss 9.53304e-06, acc 1
2017-08-08T18:50:01.279738: step 28471, loss 0, acc 1
2017-08-08T18:50:01.535428: step 28472, loss 2.45368e-05, acc 1
2017-08-08T18:50:01.919074: step 28473, loss 7.71117e-07, acc 1
2017-08-08T18:50:02.378017: step 28474, loss 3.72529e-08, acc 1
2017-08-08T18:50:02.713460: step 28475, loss 3.72529e-09, acc 1
2017-08-08T18:50:03.020595: step 28476, loss 2.42144e-08, acc 1
2017-08-08T18:50:03.244636: step 28477, loss 0, acc 1
2017-08-08T18:50:03.518112: step 28478, loss 3.50173e-07, acc 1
2017-08-08T18:50:03.893310: step 28479, loss 0, acc 1
2017-08-08T18:50:04.121547: step 28480, loss 1.69868e-06, acc 1
2017-08-08T18:50:04.382588: step 28481, loss 0.00703667, acc 1
2017-08-08T18:50:04.697363: step 28482, loss 0.000203899, acc 1
2017-08-08T18:50:05.170166: step 28483, loss 1.4528e-05, acc 1
2017-08-08T18:50:05.525386: step 28484, loss 5.58793e-08, acc 1
2017-08-08T18:50:05.859838: step 28485, loss 6.12924e-05, acc 1
2017-08-08T18:50:06.129585: step 28486, loss 1.86264e-08, acc 1
2017-08-08T18:50:06.605392: step 28487, loss 1.06171e-07, acc 1
2017-08-08T18:50:06.903465: step 28488, loss 1.67638e-08, acc 1
2017-08-08T18:50:07.168109: step 28489, loss 0, acc 1
2017-08-08T18:50:07.459316: step 28490, loss 0, acc 1
2017-08-08T18:50:07.872435: step 28491, loss 1.86265e-09, acc 1
2017-08-08T18:50:08.255977: step 28492, loss 8.00936e-08, acc 1
2017-08-08T18:50:08.637861: step 28493, loss 1.05299e-05, acc 1
2017-08-08T18:50:08.977432: step 28494, loss 0, acc 1
2017-08-08T18:50:09.222851: step 28495, loss 0, acc 1
2017-08-08T18:50:09.689108: step 28496, loss 5.45745e-07, acc 1
2017-08-08T18:50:10.007260: step 28497, loss 2.40077e-06, acc 1
2017-08-08T18:50:10.263952: step 28498, loss 7.45058e-09, acc 1
2017-08-08T18:50:10.593047: step 28499, loss 8.70172e-06, acc 1
2017-08-08T18:50:11.043734: step 28500, loss 1.27367e-05, acc 1

Evaluation:
2017-08-08T18:50:11.977040: step 28500, loss 9.25568, acc 0.718574

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-28500

2017-08-08T18:50:12.596836: step 28501, loss 1.86264e-08, acc 1
2017-08-08T18:50:12.909582: step 28502, loss 1.86265e-09, acc 1
2017-08-08T18:50:13.155918: step 28503, loss 0.00441261, acc 1
2017-08-08T18:50:13.443923: step 28504, loss 8.58906e-06, acc 1
2017-08-08T18:50:13.786186: step 28505, loss 0, acc 1
2017-08-08T18:50:14.226105: step 28506, loss 0, acc 1
2017-08-08T18:50:14.640086: step 28507, loss 0.000645451, acc 1
2017-08-08T18:50:14.985302: step 28508, loss 3.14784e-07, acc 1
2017-08-08T18:50:15.249706: step 28509, loss 2.04891e-08, acc 1
2017-08-08T18:50:15.629932: step 28510, loss 3.02092e-06, acc 1
2017-08-08T18:50:15.976702: step 28511, loss 4.32128e-07, acc 1
2017-08-08T18:50:16.209973: step 28512, loss 0, acc 1
2017-08-08T18:50:16.464792: step 28513, loss 1.86265e-09, acc 1
2017-08-08T18:50:16.912586: step 28514, loss 2.91244e-05, acc 1
2017-08-08T18:50:17.317621: step 28515, loss 2.647e-05, acc 1
2017-08-08T18:50:17.651640: step 28516, loss 0, acc 1
2017-08-08T18:50:17.917474: step 28517, loss 0, acc 1
2017-08-08T18:50:18.144373: step 28518, loss 1.86265e-09, acc 1
2017-08-08T18:50:18.639308: step 28519, loss 9.3132e-08, acc 1
2017-08-08T18:50:18.948739: step 28520, loss 3.52036e-07, acc 1
2017-08-08T18:50:19.223530: step 28521, loss 0, acc 1
2017-08-08T18:50:19.648666: step 28522, loss 1.11759e-08, acc 1
2017-08-08T18:50:20.038506: step 28523, loss 1.00768e-06, acc 1
2017-08-08T18:50:20.309483: step 28524, loss 2.27242e-07, acc 1
2017-08-08T18:50:20.595543: step 28525, loss 9.31322e-09, acc 1
2017-08-08T18:50:20.826592: step 28526, loss 0.000131117, acc 1
2017-08-08T18:50:21.236020: step 28527, loss 0, acc 1
2017-08-08T18:50:21.515630: step 28528, loss 1.86265e-09, acc 1
2017-08-08T18:50:21.789005: step 28529, loss 1.65446e-05, acc 1
2017-08-08T18:50:22.063224: step 28530, loss 1.73225e-07, acc 1
2017-08-08T18:50:22.572838: step 28531, loss 1.7899e-06, acc 1
2017-08-08T18:50:22.963647: step 28532, loss 7.2643e-08, acc 1
2017-08-08T18:50:23.305618: step 28533, loss 9.12672e-07, acc 1
2017-08-08T18:50:23.583374: step 28534, loss 0, acc 1
2017-08-08T18:50:23.807743: step 28535, loss 5.58793e-09, acc 1
2017-08-08T18:50:24.217850: step 28536, loss 7.45058e-09, acc 1
2017-08-08T18:50:24.494796: step 28537, loss 3.48312e-07, acc 1
2017-08-08T18:50:24.698139: step 28538, loss 1.91851e-07, acc 1
2017-08-08T18:50:24.945489: step 28539, loss 1.30385e-07, acc 1
2017-08-08T18:50:25.303749: step 28540, loss 1.86264e-08, acc 1
2017-08-08T18:50:25.728996: step 28541, loss 0, acc 1
2017-08-08T18:50:26.099390: step 28542, loss 9.31321e-08, acc 1
2017-08-08T18:50:26.407924: step 28543, loss 3.05471e-07, acc 1
2017-08-08T18:50:26.670986: step 28544, loss 1.0617e-07, acc 1
2017-08-08T18:50:26.959798: step 28545, loss 1.39698e-07, acc 1
2017-08-08T18:50:27.300729: step 28546, loss 4.41444e-07, acc 1
2017-08-08T18:50:27.539972: step 28547, loss 3.72529e-09, acc 1
2017-08-08T18:50:27.769574: step 28548, loss 4.64102e-06, acc 1
2017-08-08T18:50:28.048563: step 28549, loss 1.32242e-06, acc 1
2017-08-08T18:50:28.417527: step 28550, loss 0, acc 1
2017-08-08T18:50:28.797829: step 28551, loss 6.89177e-08, acc 1
2017-08-08T18:50:29.147233: step 28552, loss 1.75088e-07, acc 1
2017-08-08T18:50:29.447746: step 28553, loss 8.94068e-08, acc 1
2017-08-08T18:50:29.686597: step 28554, loss 3.85706e-06, acc 1
2017-08-08T18:50:29.977454: step 28555, loss 0, acc 1
2017-08-08T18:50:30.188792: step 28556, loss 0, acc 1
2017-08-08T18:50:30.392546: step 28557, loss 0, acc 1
2017-08-08T18:50:30.622540: step 28558, loss 3.17736e-06, acc 1
2017-08-08T18:50:30.937866: step 28559, loss 0, acc 1
2017-08-08T18:50:31.280752: step 28560, loss 0, acc 1
2017-08-08T18:50:31.511136: step 28561, loss 2.6077e-08, acc 1
2017-08-08T18:50:31.788859: step 28562, loss 1.47936e-05, acc 1
2017-08-08T18:50:31.989342: step 28563, loss 0, acc 1
2017-08-08T18:50:32.300338: step 28564, loss 0, acc 1
2017-08-08T18:50:32.523259: step 28565, loss 7.45058e-09, acc 1
2017-08-08T18:50:32.745736: step 28566, loss 5.40166e-08, acc 1
2017-08-08T18:50:33.032087: step 28567, loss 1.86265e-09, acc 1
2017-08-08T18:50:33.485369: step 28568, loss 1.86265e-09, acc 1
2017-08-08T18:50:33.870174: step 28569, loss 0, acc 1
2017-08-08T18:50:34.235250: step 28570, loss 4.84287e-08, acc 1
2017-08-08T18:50:34.466534: step 28571, loss 4.50754e-07, acc 1
2017-08-08T18:50:34.698487: step 28572, loss 0, acc 1
2017-08-08T18:50:35.169092: step 28573, loss 2.79397e-08, acc 1
2017-08-08T18:50:35.454727: step 28574, loss 2.39167e-05, acc 1
2017-08-08T18:50:35.755696: step 28575, loss 3.30977e-05, acc 1
2017-08-08T18:50:36.058082: step 28576, loss 0, acc 1
2017-08-08T18:50:36.468192: step 28577, loss 3.01746e-07, acc 1
2017-08-08T18:50:36.834807: step 28578, loss 4.84287e-08, acc 1
2017-08-08T18:50:37.178614: step 28579, loss 5.58793e-09, acc 1
2017-08-08T18:50:37.494631: step 28580, loss 0, acc 1
2017-08-08T18:50:37.761268: step 28581, loss 4.65661e-08, acc 1
2017-08-08T18:50:38.215776: step 28582, loss 3.16649e-08, acc 1
2017-08-08T18:50:38.518673: step 28583, loss 0, acc 1
2017-08-08T18:50:38.798464: step 28584, loss 0.00340819, acc 1
2017-08-08T18:50:39.089561: step 28585, loss 2.51455e-07, acc 1
2017-08-08T18:50:39.460583: step 28586, loss 1.86265e-09, acc 1
2017-08-08T18:50:39.924121: step 28587, loss 4.28404e-07, acc 1
2017-08-08T18:50:40.344014: step 28588, loss 5.05627e-06, acc 1
2017-08-08T18:50:40.647298: step 28589, loss 5.58793e-08, acc 1
2017-08-08T18:50:40.873419: step 28590, loss 0, acc 1
2017-08-08T18:50:41.256883: step 28591, loss 5.94173e-07, acc 1
2017-08-08T18:50:41.600655: step 28592, loss 3.35276e-08, acc 1
2017-08-08T18:50:41.830812: step 28593, loss 9.31322e-09, acc 1
2017-08-08T18:50:42.041129: step 28594, loss 1.08033e-07, acc 1
2017-08-08T18:50:42.301321: step 28595, loss 5.58793e-08, acc 1
2017-08-08T18:50:42.597314: step 28596, loss 0, acc 1
2017-08-08T18:50:42.872870: step 28597, loss 1.1108e-05, acc 1
2017-08-08T18:50:43.076259: step 28598, loss 0, acc 1
2017-08-08T18:50:43.266494: step 28599, loss 9.25707e-07, acc 1
2017-08-08T18:50:43.575456: step 28600, loss 7.50628e-07, acc 1

Evaluation:
2017-08-08T18:50:44.130528: step 28600, loss 9.3569, acc 0.714822

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-28600

2017-08-08T18:50:44.689505: step 28601, loss 1.11942e-06, acc 1
2017-08-08T18:50:45.021319: step 28602, loss 9.31322e-09, acc 1
2017-08-08T18:50:45.337468: step 28603, loss 2.6077e-08, acc 1
2017-08-08T18:50:45.619306: step 28604, loss 1.45286e-07, acc 1
2017-08-08T18:50:45.996691: step 28605, loss 8.75441e-08, acc 1
2017-08-08T18:50:46.251195: step 28606, loss 7.95445e-05, acc 1
2017-08-08T18:50:46.492549: step 28607, loss 0, acc 1
2017-08-08T18:50:46.868587: step 28608, loss 4.71098e-05, acc 1
2017-08-08T18:50:47.099125: step 28609, loss 0.0057519, acc 1
2017-08-08T18:50:47.369396: step 28610, loss 5.02913e-08, acc 1
2017-08-08T18:50:47.623224: step 28611, loss 0, acc 1
2017-08-08T18:50:47.861343: step 28612, loss 0, acc 1
2017-08-08T18:50:48.126551: step 28613, loss 0.00162097, acc 1
2017-08-08T18:50:48.355964: step 28614, loss 1.0654e-06, acc 1
2017-08-08T18:50:48.726949: step 28615, loss 0, acc 1
2017-08-08T18:50:49.046644: step 28616, loss 0, acc 1
2017-08-08T18:50:49.260828: step 28617, loss 1.46584e-06, acc 1
2017-08-08T18:50:49.564570: step 28618, loss 0, acc 1
2017-08-08T18:50:49.842734: step 28619, loss 1.86265e-09, acc 1
2017-08-08T18:50:50.045719: step 28620, loss 2.32829e-07, acc 1
2017-08-08T18:50:50.218874: step 28621, loss 4.64055e-05, acc 1
2017-08-08T18:50:50.510267: step 28622, loss 5.98726e-06, acc 1
2017-08-08T18:50:50.910115: step 28623, loss 1.86265e-09, acc 1
2017-08-08T18:50:51.352757: step 28624, loss 3.20528e-06, acc 1
2017-08-08T18:50:51.672665: step 28625, loss 0.000217336, acc 1
2017-08-08T18:50:51.994125: step 28626, loss 0, acc 1
2017-08-08T18:50:52.231174: step 28627, loss 0, acc 1
2017-08-08T18:50:52.410525: step 28628, loss 3.87425e-07, acc 1
2017-08-08T18:50:52.584444: step 28629, loss 1.61241e-05, acc 1
2017-08-08T18:50:52.791835: step 28630, loss 2.94271e-06, acc 1
2017-08-08T18:50:53.124218: step 28631, loss 0.000156733, acc 1
2017-08-08T18:50:53.467742: step 28632, loss 0, acc 1
2017-08-08T18:50:53.643579: step 28633, loss 2.25379e-07, acc 1
2017-08-08T18:50:53.880947: step 28634, loss 0.00131358, acc 1
2017-08-08T18:50:54.274231: step 28635, loss 4.15364e-07, acc 1
2017-08-08T18:50:54.574416: step 28636, loss 5.58794e-09, acc 1
2017-08-08T18:50:54.817777: step 28637, loss 3.72529e-09, acc 1
2017-08-08T18:50:55.035929: step 28638, loss 0, acc 1
2017-08-08T18:50:55.481373: step 28639, loss 9.4587e-05, acc 1
2017-08-08T18:50:55.803124: step 28640, loss 1.86265e-09, acc 1
2017-08-08T18:50:56.014112: step 28641, loss 2.23517e-08, acc 1
2017-08-08T18:50:56.209990: step 28642, loss 5.58794e-09, acc 1
2017-08-08T18:50:56.435648: step 28643, loss 9.31322e-09, acc 1
2017-08-08T18:50:56.720110: step 28644, loss 0, acc 1
2017-08-08T18:50:56.927934: step 28645, loss 0, acc 1
2017-08-08T18:50:57.306148: step 28646, loss 5.58793e-09, acc 1
2017-08-08T18:50:57.658071: step 28647, loss 3.72529e-09, acc 1
2017-08-08T18:50:57.922418: step 28648, loss 7.45057e-08, acc 1
2017-08-08T18:50:58.221408: step 28649, loss 9.38745e-07, acc 1
2017-08-08T18:50:58.429998: step 28650, loss 9.457e-07, acc 1
2017-08-08T18:50:58.710514: step 28651, loss 0, acc 1
2017-08-08T18:50:59.009556: step 28652, loss 3.03608e-07, acc 1
2017-08-08T18:50:59.221174: step 28653, loss 0, acc 1
2017-08-08T18:50:59.420334: step 28654, loss 9.31322e-09, acc 1
2017-08-08T18:50:59.682233: step 28655, loss 1.49012e-08, acc 1
2017-08-08T18:51:00.014076: step 28656, loss 1.88126e-07, acc 1
2017-08-08T18:51:00.268866: step 28657, loss 0, acc 1
2017-08-08T18:51:00.571775: step 28658, loss 3.24097e-07, acc 1
2017-08-08T18:51:00.845371: step 28659, loss 0, acc 1
2017-08-08T18:51:01.261908: step 28660, loss 3.72529e-09, acc 1
2017-08-08T18:51:01.524629: step 28661, loss 1.86265e-09, acc 1
2017-08-08T18:51:01.841106: step 28662, loss 1.99302e-07, acc 1
2017-08-08T18:51:02.227741: step 28663, loss 8.21414e-07, acc 1
2017-08-08T18:51:02.580098: step 28664, loss 8.95907e-07, acc 1
2017-08-08T18:51:03.076587: step 28665, loss 0, acc 1
2017-08-08T18:51:03.479727: step 28666, loss 8.4404e-05, acc 1
2017-08-08T18:51:03.882881: step 28667, loss 0, acc 1
2017-08-08T18:51:04.137621: step 28668, loss 5.05077e-05, acc 1
2017-08-08T18:51:04.385759: step 28669, loss 0, acc 1
2017-08-08T18:51:04.827790: step 28670, loss 4.8757e-06, acc 1
2017-08-08T18:51:05.080741: step 28671, loss 4.59447e-06, acc 1
2017-08-08T18:51:05.331243: step 28672, loss 3.08251e-06, acc 1
2017-08-08T18:51:05.542402: step 28673, loss 2.30393e-06, acc 1
2017-08-08T18:51:05.885374: step 28674, loss 0, acc 1
2017-08-08T18:51:06.275403: step 28675, loss 2.17928e-07, acc 1
2017-08-08T18:51:06.598468: step 28676, loss 2.04891e-08, acc 1
2017-08-08T18:51:06.861115: step 28677, loss 7.45058e-09, acc 1
2017-08-08T18:51:07.099777: step 28678, loss 3.72529e-09, acc 1
2017-08-08T18:51:07.417766: step 28679, loss 6.89178e-08, acc 1
2017-08-08T18:51:07.878425: step 28680, loss 0, acc 1
2017-08-08T18:51:08.172662: step 28681, loss 3.27824e-07, acc 1
2017-08-08T18:51:08.473410: step 28682, loss 1.37085e-06, acc 1
2017-08-08T18:51:08.933381: step 28683, loss 0, acc 1
2017-08-08T18:51:09.291823: step 28684, loss 0, acc 1
2017-08-08T18:51:09.646551: step 28685, loss 0, acc 1
2017-08-08T18:51:09.906832: step 28686, loss 1.19209e-07, acc 1
2017-08-08T18:51:10.159513: step 28687, loss 7.82309e-08, acc 1
2017-08-08T18:51:10.444647: step 28688, loss 3.72529e-09, acc 1
2017-08-08T18:51:10.722456: step 28689, loss 1.16783e-06, acc 1
2017-08-08T18:51:10.994557: step 28690, loss 1.02445e-07, acc 1
2017-08-08T18:51:11.259515: step 28691, loss 4.02372e-05, acc 1
2017-08-08T18:51:11.592811: step 28692, loss 4.15688e-06, acc 1
2017-08-08T18:51:12.016687: step 28693, loss 3.63546e-06, acc 1
2017-08-08T18:51:12.371295: step 28694, loss 7.67391e-07, acc 1
2017-08-08T18:51:12.780824: step 28695, loss 0, acc 1
2017-08-08T18:51:13.007602: step 28696, loss 5.58793e-09, acc 1
2017-08-08T18:51:13.331198: step 28697, loss 1.11759e-08, acc 1
2017-08-08T18:51:13.548916: step 28698, loss 0.000773196, acc 1
2017-08-08T18:51:13.781396: step 28699, loss 0, acc 1
2017-08-08T18:51:13.993392: step 28700, loss 0, acc 1

Evaluation:
2017-08-08T18:51:14.891354: step 28700, loss 9.30865, acc 0.71576

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-28700

2017-08-08T18:51:15.409803: step 28701, loss 0.00639995, acc 1
2017-08-08T18:51:15.555773: step 28702, loss 6.05349e-07, acc 1
2017-08-08T18:51:15.747018: step 28703, loss 3.16649e-08, acc 1
2017-08-08T18:51:16.009353: step 28704, loss 5.36438e-07, acc 1
2017-08-08T18:51:16.224429: step 28705, loss 7.3014e-07, acc 1
2017-08-08T18:51:16.450239: step 28706, loss 8.75441e-08, acc 1
2017-08-08T18:51:16.752025: step 28707, loss 5.27226e-06, acc 1
2017-08-08T18:51:17.057346: step 28708, loss 6.51924e-08, acc 1
2017-08-08T18:51:17.289445: step 28709, loss 0, acc 1
2017-08-08T18:51:17.533215: step 28710, loss 4.87192e-06, acc 1
2017-08-08T18:51:17.721895: step 28711, loss 1.13059e-06, acc 1
2017-08-08T18:51:18.073482: step 28712, loss 0, acc 1
2017-08-08T18:51:18.298520: step 28713, loss 2.3727e-05, acc 1
2017-08-08T18:51:18.506012: step 28714, loss 9.31322e-09, acc 1
2017-08-08T18:51:18.761682: step 28715, loss 3.48312e-07, acc 1
2017-08-08T18:51:19.046899: step 28716, loss 1.11759e-08, acc 1
2017-08-08T18:51:19.325245: step 28717, loss 9.31322e-09, acc 1
2017-08-08T18:51:19.532840: step 28718, loss 2.98023e-08, acc 1
2017-08-08T18:51:19.765654: step 28719, loss 0, acc 1
2017-08-08T18:51:20.022017: step 28720, loss 0, acc 1
2017-08-08T18:51:20.413575: step 28721, loss 0, acc 1
2017-08-08T18:51:20.687725: step 28722, loss 1.26096e-06, acc 1
2017-08-08T18:51:20.993641: step 28723, loss 0, acc 1
2017-08-08T18:51:21.269419: step 28724, loss 5.18525e-05, acc 1
2017-08-08T18:51:21.685380: step 28725, loss 2.68219e-07, acc 1
2017-08-08T18:51:22.046050: step 28726, loss 3.72529e-09, acc 1
2017-08-08T18:51:22.329267: step 28727, loss 4.06051e-07, acc 1
2017-08-08T18:51:22.561387: step 28728, loss 1.11759e-08, acc 1
2017-08-08T18:51:22.753274: step 28729, loss 5.25551e-06, acc 1
2017-08-08T18:51:23.140430: step 28730, loss 9.31322e-09, acc 1
2017-08-08T18:51:23.395343: step 28731, loss 2.9802e-07, acc 1
2017-08-08T18:51:23.649206: step 28732, loss 2.49715e-05, acc 1
2017-08-08T18:51:23.921165: step 28733, loss 0.00287284, acc 1
2017-08-08T18:51:24.178931: step 28734, loss 5.01977e-05, acc 1
2017-08-08T18:51:24.561403: step 28735, loss 1.11759e-08, acc 1
2017-08-08T18:51:24.928706: step 28736, loss 0, acc 1
2017-08-08T18:51:25.282807: step 28737, loss 1.86265e-09, acc 1
2017-08-08T18:51:25.548586: step 28738, loss 1.86265e-09, acc 1
2017-08-08T18:51:25.808745: step 28739, loss 5.58793e-09, acc 1
2017-08-08T18:51:26.261214: step 28740, loss 3.71884e-05, acc 1
2017-08-08T18:51:26.471876: step 28741, loss 0.00104269, acc 1
2017-08-08T18:51:26.700408: step 28742, loss 4.47034e-08, acc 1
2017-08-08T18:51:26.951987: step 28743, loss 2.66356e-07, acc 1
2017-08-08T18:51:27.209209: step 28744, loss 5.58793e-08, acc 1
2017-08-08T18:51:27.637417: step 28745, loss 9.49946e-08, acc 1
2017-08-08T18:51:27.972289: step 28746, loss 0, acc 1
2017-08-08T18:51:28.257407: step 28747, loss 0, acc 1
2017-08-08T18:51:28.537775: step 28748, loss 0, acc 1
2017-08-08T18:51:28.877415: step 28749, loss 0, acc 1
2017-08-08T18:51:29.197188: step 28750, loss 1.89989e-07, acc 1
2017-08-08T18:51:29.431572: step 28751, loss 1.40623e-06, acc 1
2017-08-08T18:51:29.669460: step 28752, loss 0, acc 1
2017-08-08T18:51:29.873842: step 28753, loss 4.68256e-05, acc 1
2017-08-08T18:51:30.188122: step 28754, loss 5.96045e-08, acc 1
2017-08-08T18:51:30.487913: step 28755, loss 0, acc 1
2017-08-08T18:51:30.868229: step 28756, loss 8.65501e-05, acc 1
2017-08-08T18:51:31.201533: step 28757, loss 3.72529e-09, acc 1
2017-08-08T18:51:31.446877: step 28758, loss 0, acc 1
2017-08-08T18:51:31.695718: step 28759, loss 2.75807e-05, acc 1
2017-08-08T18:51:32.127604: step 28760, loss 0, acc 1
2017-08-08T18:51:32.380370: step 28761, loss 3.94645e-06, acc 1
2017-08-08T18:51:32.637829: step 28762, loss 2.51455e-07, acc 1
2017-08-08T18:51:32.905389: step 28763, loss 0, acc 1
2017-08-08T18:51:33.327372: step 28764, loss 0, acc 1
2017-08-08T18:51:33.641398: step 28765, loss 1.86264e-08, acc 1
2017-08-08T18:51:33.936664: step 28766, loss 2.98023e-08, acc 1
2017-08-08T18:51:34.184585: step 28767, loss 8.74455e-06, acc 1
2017-08-08T18:51:34.403402: step 28768, loss 0, acc 1
2017-08-08T18:51:34.745402: step 28769, loss 1.3411e-07, acc 1
2017-08-08T18:51:35.012536: step 28770, loss 5.73688e-07, acc 1
2017-08-08T18:51:35.288703: step 28771, loss 0, acc 1
2017-08-08T18:51:35.607509: step 28772, loss 0, acc 1
2017-08-08T18:51:35.849264: step 28773, loss 0, acc 1
2017-08-08T18:51:36.269201: step 28774, loss 7.43178e-07, acc 1
2017-08-08T18:51:36.549496: step 28775, loss 0, acc 1
2017-08-08T18:51:36.910164: step 28776, loss 0, acc 1
2017-08-08T18:51:37.237702: step 28777, loss 2.58906e-07, acc 1
2017-08-08T18:51:37.494282: step 28778, loss 1.08033e-07, acc 1
2017-08-08T18:51:37.948328: step 28779, loss 0, acc 1
2017-08-08T18:51:38.217234: step 28780, loss 0, acc 1
2017-08-08T18:51:38.581157: step 28781, loss 5.2154e-08, acc 1
2017-08-08T18:51:38.985530: step 28782, loss 0, acc 1
2017-08-08T18:51:39.265361: step 28783, loss 1.86265e-09, acc 1
2017-08-08T18:51:39.609353: step 28784, loss 5.58793e-09, acc 1
2017-08-08T18:51:39.854275: step 28785, loss 8.97778e-07, acc 1
2017-08-08T18:51:40.158262: step 28786, loss 1.86265e-09, acc 1
2017-08-08T18:51:40.520069: step 28787, loss 0, acc 1
2017-08-08T18:51:40.785384: step 28788, loss 4.6752e-07, acc 1
2017-08-08T18:51:41.078155: step 28789, loss 0, acc 1
2017-08-08T18:51:41.298477: step 28790, loss 0, acc 1
2017-08-08T18:51:41.629551: step 28791, loss 1.1995e-06, acc 1
2017-08-08T18:51:41.858306: step 28792, loss 8.38188e-08, acc 1
2017-08-08T18:51:42.135216: step 28793, loss 1.0617e-07, acc 1
2017-08-08T18:51:42.529374: step 28794, loss 1.03746e-06, acc 1
2017-08-08T18:51:42.847340: step 28795, loss 1.85509e-06, acc 1
2017-08-08T18:51:43.221548: step 28796, loss 0, acc 1
2017-08-08T18:51:43.578446: step 28797, loss 1.86265e-09, acc 1
2017-08-08T18:51:43.851491: step 28798, loss 8.1768e-07, acc 1
2017-08-08T18:51:44.249517: step 28799, loss 0.000163922, acc 1
2017-08-08T18:51:44.522149: step 28800, loss 0, acc 1

Evaluation:
2017-08-08T18:51:45.536033: step 28800, loss 9.29661, acc 0.72045

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-28800

2017-08-08T18:51:46.013214: step 28801, loss 3.18509e-07, acc 1
2017-08-08T18:51:46.291154: step 28802, loss 2.23517e-08, acc 1
2017-08-08T18:51:46.527952: step 28803, loss 1.43423e-07, acc 1
2017-08-08T18:51:46.714154: step 28804, loss 1.11759e-08, acc 1
2017-08-08T18:51:47.007988: step 28805, loss 0, acc 1
2017-08-08T18:51:47.234169: step 28806, loss 0.000915644, acc 1
2017-08-08T18:51:47.454990: step 28807, loss 1.11759e-08, acc 1
2017-08-08T18:51:47.733523: step 28808, loss 2.51252e-06, acc 1
2017-08-08T18:51:48.099802: step 28809, loss 0, acc 1
2017-08-08T18:51:48.537045: step 28810, loss 6.89178e-08, acc 1
2017-08-08T18:51:48.916636: step 28811, loss 0, acc 1
2017-08-08T18:51:49.217521: step 28812, loss 1.75088e-07, acc 1
2017-08-08T18:51:49.464330: step 28813, loss 3.72529e-09, acc 1
2017-08-08T18:51:49.719581: step 28814, loss 1.67638e-08, acc 1
2017-08-08T18:51:50.129398: step 28815, loss 4.28408e-08, acc 1
2017-08-08T18:51:50.410146: step 28816, loss 1.86265e-09, acc 1
2017-08-08T18:51:50.679682: step 28817, loss 1.86265e-09, acc 1
2017-08-08T18:51:50.940551: step 28818, loss 8.38189e-08, acc 1
2017-08-08T18:51:51.257387: step 28819, loss 0.000394408, acc 1
2017-08-08T18:51:51.548561: step 28820, loss 1.30385e-08, acc 1
2017-08-08T18:51:51.881878: step 28821, loss 6.63089e-07, acc 1
2017-08-08T18:51:52.102184: step 28822, loss 1.28517e-06, acc 1
2017-08-08T18:51:52.318523: step 28823, loss 8.64244e-07, acc 1
2017-08-08T18:51:52.712837: step 28824, loss 0.000581898, acc 1
2017-08-08T18:51:53.002657: step 28825, loss 7.45058e-09, acc 1
2017-08-08T18:51:53.238873: step 28826, loss 1.86265e-09, acc 1
2017-08-08T18:51:53.482103: step 28827, loss 4.37715e-07, acc 1
2017-08-08T18:51:53.837009: step 28828, loss 0, acc 1
2017-08-08T18:51:54.131809: step 28829, loss 2.08615e-07, acc 1
2017-08-08T18:51:54.448078: step 28830, loss 0, acc 1
2017-08-08T18:51:54.629775: step 28831, loss 1.86264e-07, acc 1
2017-08-08T18:51:54.924823: step 28832, loss 0, acc 1
2017-08-08T18:51:55.187817: step 28833, loss 1.89048e-06, acc 1
2017-08-08T18:51:55.366761: step 28834, loss 4.28408e-08, acc 1
2017-08-08T18:51:55.579094: step 28835, loss 2.80117e-06, acc 1
2017-08-08T18:51:55.783678: step 28836, loss 1.49012e-08, acc 1
2017-08-08T18:51:56.095339: step 28837, loss 1.81614e-05, acc 1
2017-08-08T18:51:56.537745: step 28838, loss 0.000117735, acc 1
2017-08-08T18:51:56.875965: step 28839, loss 0, acc 1
2017-08-08T18:51:57.164049: step 28840, loss 9.4992e-07, acc 1
2017-08-08T18:51:57.425397: step 28841, loss 3.61944e-05, acc 1
2017-08-08T18:51:57.741383: step 28842, loss 0, acc 1
2017-08-08T18:51:57.983672: step 28843, loss 0, acc 1
2017-08-08T18:51:58.207251: step 28844, loss 2.6077e-08, acc 1
2017-08-08T18:51:58.433408: step 28845, loss 2.42144e-08, acc 1
2017-08-08T18:51:58.753408: step 28846, loss 1.695e-07, acc 1
2017-08-08T18:51:59.035614: step 28847, loss 0, acc 1
2017-08-08T18:51:59.370140: step 28848, loss 3.79377e-06, acc 1
2017-08-08T18:51:59.678620: step 28849, loss 1.16093e-05, acc 1
2017-08-08T18:51:59.870780: step 28850, loss 0, acc 1
2017-08-08T18:52:00.205395: step 28851, loss 0, acc 1
2017-08-08T18:52:00.570081: step 28852, loss 0, acc 1
2017-08-08T18:52:00.815807: step 28853, loss 2.51623e-06, acc 1
2017-08-08T18:52:01.088446: step 28854, loss 0, acc 1
2017-08-08T18:52:01.451123: step 28855, loss 0, acc 1
2017-08-08T18:52:01.801437: step 28856, loss 2.00783e-06, acc 1
2017-08-08T18:52:02.237464: step 28857, loss 2.92433e-07, acc 1
2017-08-08T18:52:02.596870: step 28858, loss 0, acc 1
2017-08-08T18:52:02.879872: step 28859, loss 2.915e-05, acc 1
2017-08-08T18:52:03.241346: step 28860, loss 0, acc 1
2017-08-08T18:52:03.636042: step 28861, loss 2.39172e-05, acc 1
2017-08-08T18:52:03.960276: step 28862, loss 0, acc 1
2017-08-08T18:52:04.260447: step 28863, loss 0, acc 1
2017-08-08T18:52:04.563302: step 28864, loss 3.72529e-09, acc 1
2017-08-08T18:52:04.970809: step 28865, loss 7.45058e-09, acc 1
2017-08-08T18:52:05.315299: step 28866, loss 1.46772e-06, acc 1
2017-08-08T18:52:05.682790: step 28867, loss 0, acc 1
2017-08-08T18:52:05.907514: step 28868, loss 1.67638e-08, acc 1
2017-08-08T18:52:06.150603: step 28869, loss 5.97898e-07, acc 1
2017-08-08T18:52:06.587747: step 28870, loss 7.00341e-07, acc 1
2017-08-08T18:52:06.890459: step 28871, loss 5.86728e-07, acc 1
2017-08-08T18:52:07.174149: step 28872, loss 8.19562e-08, acc 1
2017-08-08T18:52:07.405159: step 28873, loss 1.11759e-08, acc 1
2017-08-08T18:52:07.798714: step 28874, loss 0, acc 1
2017-08-08T18:52:08.112561: step 28875, loss 3.66715e-05, acc 1
2017-08-08T18:52:08.369450: step 28876, loss 0, acc 1
2017-08-08T18:52:08.559452: step 28877, loss 9.276e-06, acc 1
2017-08-08T18:52:08.789793: step 28878, loss 0, acc 1
2017-08-08T18:52:09.104384: step 28879, loss 1.28522e-07, acc 1
2017-08-08T18:52:09.303435: step 28880, loss 2.0637e-06, acc 1
2017-08-08T18:52:09.568335: step 28881, loss 0.000153769, acc 1
2017-08-08T18:52:09.849592: step 28882, loss 0, acc 1
2017-08-08T18:52:10.169809: step 28883, loss 7.45058e-09, acc 1
2017-08-08T18:52:10.519196: step 28884, loss 9.25713e-07, acc 1
2017-08-08T18:52:10.794284: step 28885, loss 6.73774e-05, acc 1
2017-08-08T18:52:10.979207: step 28886, loss 3.71367e-06, acc 1
2017-08-08T18:52:11.221471: step 28887, loss 3.85171e-06, acc 1
2017-08-08T18:52:11.459272: step 28888, loss 4.75413e-05, acc 1
2017-08-08T18:52:11.736303: step 28889, loss 0, acc 1
2017-08-08T18:52:11.987705: step 28890, loss 1.86265e-09, acc 1
2017-08-08T18:52:12.261420: step 28891, loss 6.83576e-07, acc 1
2017-08-08T18:52:12.541500: step 28892, loss 0, acc 1
2017-08-08T18:52:12.772321: step 28893, loss 1.86264e-08, acc 1
2017-08-08T18:52:12.952702: step 28894, loss 6.20248e-07, acc 1
2017-08-08T18:52:13.244398: step 28895, loss 3.76769e-06, acc 1
2017-08-08T18:52:13.569715: step 28896, loss 2.68219e-07, acc 1
2017-08-08T18:52:13.822291: step 28897, loss 1.86998e-06, acc 1
2017-08-08T18:52:14.079435: step 28898, loss 3.20374e-07, acc 1
2017-08-08T18:52:14.471306: step 28899, loss 5.77419e-08, acc 1
2017-08-08T18:52:14.883219: step 28900, loss 6.27699e-07, acc 1

Evaluation:
2017-08-08T18:52:15.636494: step 28900, loss 9.29357, acc 0.721388

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-28900

2017-08-08T18:52:16.113755: step 28901, loss 1.88491e-06, acc 1
2017-08-08T18:52:16.379584: step 28902, loss 0, acc 1
2017-08-08T18:52:16.606703: step 28903, loss 0, acc 1
2017-08-08T18:52:16.831613: step 28904, loss 3.46449e-07, acc 1
2017-08-08T18:52:17.169532: step 28905, loss 0, acc 1
2017-08-08T18:52:17.498484: step 28906, loss 0, acc 1
2017-08-08T18:52:17.749006: step 28907, loss 0.000148135, acc 1
2017-08-08T18:52:18.000572: step 28908, loss 0, acc 1
2017-08-08T18:52:18.225649: step 28909, loss 2.36294e-05, acc 1
2017-08-08T18:52:18.537370: step 28910, loss 2.17914e-06, acc 1
2017-08-08T18:52:18.721359: step 28911, loss 9.31321e-08, acc 1
2017-08-08T18:52:18.909295: step 28912, loss 0, acc 1
2017-08-08T18:52:19.108988: step 28913, loss 4.00464e-07, acc 1
2017-08-08T18:52:19.473614: step 28914, loss 3.52387e-05, acc 1
2017-08-08T18:52:19.776948: step 28915, loss 0, acc 1
2017-08-08T18:52:20.081994: step 28916, loss 1.69495e-06, acc 1
2017-08-08T18:52:20.396177: step 28917, loss 7.46903e-07, acc 1
2017-08-08T18:52:20.639880: step 28918, loss 1.11759e-08, acc 1
2017-08-08T18:52:21.098366: step 28919, loss 1.86265e-09, acc 1
2017-08-08T18:52:21.393346: step 28920, loss 0.000253514, acc 1
2017-08-08T18:52:21.670280: step 28921, loss 1.39698e-07, acc 1
2017-08-08T18:52:21.862830: step 28922, loss 0, acc 1
2017-08-08T18:52:22.108700: step 28923, loss 9.0528e-06, acc 1
2017-08-08T18:52:22.514162: step 28924, loss 0, acc 1
2017-08-08T18:52:22.869147: step 28925, loss 5.58793e-08, acc 1
2017-08-08T18:52:23.206623: step 28926, loss 3.90734e-06, acc 1
2017-08-08T18:52:23.452444: step 28927, loss 1.24096e-05, acc 1
2017-08-08T18:52:23.855564: step 28928, loss 0, acc 1
2017-08-08T18:52:24.125534: step 28929, loss 2.70082e-07, acc 1
2017-08-08T18:52:24.442714: step 28930, loss 5.58793e-09, acc 1
2017-08-08T18:52:24.704842: step 28931, loss 1.86265e-09, acc 1
2017-08-08T18:52:25.046485: step 28932, loss 0, acc 1
2017-08-08T18:52:25.452362: step 28933, loss 0, acc 1
2017-08-08T18:52:25.793405: step 28934, loss 0, acc 1
2017-08-08T18:52:26.067141: step 28935, loss 3.35274e-07, acc 1
2017-08-08T18:52:26.279902: step 28936, loss 0, acc 1
2017-08-08T18:52:26.651297: step 28937, loss 0, acc 1
2017-08-08T18:52:26.909361: step 28938, loss 1.21308e-05, acc 1
2017-08-08T18:52:27.148777: step 28939, loss 1.11759e-08, acc 1
2017-08-08T18:52:27.404361: step 28940, loss 0, acc 1
2017-08-08T18:52:27.625106: step 28941, loss 0, acc 1
2017-08-08T18:52:28.002667: step 28942, loss 5.58793e-08, acc 1
2017-08-08T18:52:28.300895: step 28943, loss 0.00159023, acc 1
2017-08-08T18:52:28.540936: step 28944, loss 0, acc 1
2017-08-08T18:52:28.734484: step 28945, loss 6.33298e-08, acc 1
2017-08-08T18:52:29.003722: step 28946, loss 3.10286e-06, acc 1
2017-08-08T18:52:29.310192: step 28947, loss 0, acc 1
2017-08-08T18:52:29.499039: step 28948, loss 3.72529e-09, acc 1
2017-08-08T18:52:29.722740: step 28949, loss 0, acc 1
2017-08-08T18:52:30.010377: step 28950, loss 5.16573e-08, acc 1
2017-08-08T18:52:30.313659: step 28951, loss 1.17346e-07, acc 1
2017-08-08T18:52:30.574824: step 28952, loss 0.000220201, acc 1
2017-08-08T18:52:30.841831: step 28953, loss 0, acc 1
2017-08-08T18:52:31.068688: step 28954, loss 5.41191e-06, acc 1
2017-08-08T18:52:31.396372: step 28955, loss 1.32615e-06, acc 1
2017-08-08T18:52:31.685742: step 28956, loss 0, acc 1
2017-08-08T18:52:31.899781: step 28957, loss 0, acc 1
2017-08-08T18:52:32.105998: step 28958, loss 3.6038e-06, acc 1
2017-08-08T18:52:32.409662: step 28959, loss 0, acc 1
2017-08-08T18:52:32.745543: step 28960, loss 2.23517e-08, acc 1
2017-08-08T18:52:33.018012: step 28961, loss 1.86265e-09, acc 1
2017-08-08T18:52:33.262758: step 28962, loss 1.86265e-09, acc 1
2017-08-08T18:52:33.505401: step 28963, loss 6.33298e-08, acc 1
2017-08-08T18:52:33.868726: step 28964, loss 7.2643e-08, acc 1
2017-08-08T18:52:34.138774: step 28965, loss 0, acc 1
2017-08-08T18:52:34.382170: step 28966, loss 0, acc 1
2017-08-08T18:52:34.654654: step 28967, loss 0, acc 1
2017-08-08T18:52:34.963186: step 28968, loss 0, acc 1
2017-08-08T18:52:35.253792: step 28969, loss 2.42144e-08, acc 1
2017-08-08T18:52:35.575938: step 28970, loss 4.09781e-08, acc 1
2017-08-08T18:52:35.845526: step 28971, loss 0.000192337, acc 1
2017-08-08T18:52:36.050405: step 28972, loss 9.49946e-08, acc 1
2017-08-08T18:52:36.440985: step 28973, loss 5.58793e-09, acc 1
2017-08-08T18:52:36.680604: step 28974, loss 0, acc 1
2017-08-08T18:52:36.910866: step 28975, loss 1.86265e-09, acc 1
2017-08-08T18:52:37.249396: step 28976, loss 7.2643e-08, acc 1
2017-08-08T18:52:37.682207: step 28977, loss 0, acc 1
2017-08-08T18:52:38.101378: step 28978, loss 0, acc 1
2017-08-08T18:52:38.365568: step 28979, loss 2.78863e-05, acc 1
2017-08-08T18:52:38.592273: step 28980, loss 1.86265e-09, acc 1
2017-08-08T18:52:38.933346: step 28981, loss 0, acc 1
2017-08-08T18:52:39.160164: step 28982, loss 0, acc 1
2017-08-08T18:52:39.368222: step 28983, loss 3.99859e-06, acc 1
2017-08-08T18:52:39.590083: step 28984, loss 0, acc 1
2017-08-08T18:52:39.862695: step 28985, loss 4.06935e-06, acc 1
2017-08-08T18:52:40.273269: step 28986, loss 5.47608e-07, acc 1
2017-08-08T18:52:40.648638: step 28987, loss 2.23517e-08, acc 1
2017-08-08T18:52:40.925813: step 28988, loss 7.50628e-07, acc 1
2017-08-08T18:52:41.161678: step 28989, loss 1.3411e-07, acc 1
2017-08-08T18:52:41.428665: step 28990, loss 6.18386e-07, acc 1
2017-08-08T18:52:41.867982: step 28991, loss 0, acc 1
2017-08-08T18:52:42.160897: step 28992, loss 1.30385e-08, acc 1
2017-08-08T18:52:42.454207: step 28993, loss 1.02815e-06, acc 1
2017-08-08T18:52:42.749382: step 28994, loss 1.86265e-09, acc 1
2017-08-08T18:52:43.159618: step 28995, loss 0, acc 1
2017-08-08T18:52:43.624370: step 28996, loss 3.72529e-09, acc 1
2017-08-08T18:52:43.883321: step 28997, loss 1.72658e-06, acc 1
2017-08-08T18:52:44.064551: step 28998, loss 1.86265e-09, acc 1
2017-08-08T18:52:44.354775: step 28999, loss 2.77325e-06, acc 1
2017-08-08T18:52:44.764897: step 29000, loss 2.37842e-06, acc 1

Evaluation:
2017-08-08T18:52:45.414563: step 29000, loss 9.31042, acc 0.721388

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-29000

2017-08-08T18:52:45.958313: step 29001, loss 5.96046e-08, acc 1
2017-08-08T18:52:46.290976: step 29002, loss 0, acc 1
2017-08-08T18:52:46.573863: step 29003, loss 0, acc 1
2017-08-08T18:52:46.805002: step 29004, loss 1.86265e-09, acc 1
2017-08-08T18:52:47.073406: step 29005, loss 0, acc 1
2017-08-08T18:52:47.405024: step 29006, loss 2.68198e-06, acc 1
2017-08-08T18:52:47.623928: step 29007, loss 0, acc 1
2017-08-08T18:52:47.827454: step 29008, loss 1.67638e-08, acc 1
2017-08-08T18:52:48.218027: step 29009, loss 2.04891e-08, acc 1
2017-08-08T18:52:48.518585: step 29010, loss 3.27167e-05, acc 1
2017-08-08T18:52:48.812299: step 29011, loss 1.99104e-06, acc 1
2017-08-08T18:52:48.986502: step 29012, loss 3.72529e-09, acc 1
2017-08-08T18:52:49.238615: step 29013, loss 8.79145e-07, acc 1
2017-08-08T18:52:49.529822: step 29014, loss 1.28522e-07, acc 1
2017-08-08T18:52:49.755565: step 29015, loss 1.08934e-05, acc 1
2017-08-08T18:52:49.976397: step 29016, loss 0, acc 1
2017-08-08T18:52:50.307030: step 29017, loss 0, acc 1
2017-08-08T18:52:50.714065: step 29018, loss 2.70248e-06, acc 1
2017-08-08T18:52:51.078224: step 29019, loss 1.5938e-05, acc 1
2017-08-08T18:52:51.367805: step 29020, loss 6.23979e-07, acc 1
2017-08-08T18:52:51.606820: step 29021, loss 0, acc 1
2017-08-08T18:52:52.016965: step 29022, loss 0.00017909, acc 1
2017-08-08T18:52:52.368219: step 29023, loss 1.11759e-08, acc 1
2017-08-08T18:52:52.667267: step 29024, loss 1.67629e-06, acc 1
2017-08-08T18:52:52.949966: step 29025, loss 2.9057e-07, acc 1
2017-08-08T18:52:53.354571: step 29026, loss 0, acc 1
2017-08-08T18:52:53.741379: step 29027, loss 2.9057e-07, acc 1
2017-08-08T18:52:54.069471: step 29028, loss 2.6077e-08, acc 1
2017-08-08T18:52:54.333494: step 29029, loss 9.31322e-09, acc 1
2017-08-08T18:52:54.582140: step 29030, loss 4.00552e-05, acc 1
2017-08-08T18:52:54.911489: step 29031, loss 1.86265e-09, acc 1
2017-08-08T18:52:55.106230: step 29032, loss 5.58793e-09, acc 1
2017-08-08T18:52:55.360539: step 29033, loss 4.09781e-08, acc 1
2017-08-08T18:52:55.597405: step 29034, loss 1.62973e-06, acc 1
2017-08-08T18:52:56.053368: step 29035, loss 0.000416223, acc 1
2017-08-08T18:52:56.288506: step 29036, loss 0, acc 1
2017-08-08T18:52:56.527877: step 29037, loss 1.86265e-09, acc 1
2017-08-08T18:52:56.798664: step 29038, loss 3.20871e-05, acc 1
2017-08-08T18:52:57.031420: step 29039, loss 0, acc 1
2017-08-08T18:52:57.489429: step 29040, loss 9.36899e-07, acc 1
2017-08-08T18:52:57.823712: step 29041, loss 2.98023e-08, acc 1
2017-08-08T18:52:58.041480: step 29042, loss 3.16649e-08, acc 1
2017-08-08T18:52:58.245219: step 29043, loss 2.49761e-06, acc 1
2017-08-08T18:52:58.477520: step 29044, loss 9.96484e-07, acc 1
2017-08-08T18:52:58.762473: step 29045, loss 0, acc 1
2017-08-08T18:52:58.971853: step 29046, loss 0.000204876, acc 1
2017-08-08T18:52:59.197162: step 29047, loss 0, acc 1
2017-08-08T18:52:59.360108: step 29048, loss 0, acc 1
2017-08-08T18:52:59.539896: step 29049, loss 7.45058e-09, acc 1
2017-08-08T18:52:59.897566: step 29050, loss 5.43883e-07, acc 1
2017-08-08T18:53:00.097266: step 29051, loss 3.72529e-09, acc 1
2017-08-08T18:53:00.314985: step 29052, loss 0, acc 1
2017-08-08T18:53:00.552872: step 29053, loss 6.14672e-08, acc 1
2017-08-08T18:53:00.775468: step 29054, loss 7.10268e-05, acc 1
2017-08-08T18:53:01.244564: step 29055, loss 6.2025e-07, acc 1
2017-08-08T18:53:01.629916: step 29056, loss 2.23517e-08, acc 1
2017-08-08T18:53:01.924380: step 29057, loss 3.55761e-07, acc 1
2017-08-08T18:53:02.177848: step 29058, loss 1.30385e-08, acc 1
2017-08-08T18:53:02.405444: step 29059, loss 2.08626e-05, acc 1
2017-08-08T18:53:02.815086: step 29060, loss 0.000152739, acc 1
2017-08-08T18:53:03.121415: step 29061, loss 7.45058e-09, acc 1
2017-08-08T18:53:03.421943: step 29062, loss 5.17807e-07, acc 1
2017-08-08T18:53:03.807470: step 29063, loss 0, acc 1
2017-08-08T18:53:04.238584: step 29064, loss 7.77854e-06, acc 1
2017-08-08T18:53:04.581749: step 29065, loss 0, acc 1
2017-08-08T18:53:04.934206: step 29066, loss 0, acc 1
2017-08-08T18:53:05.179301: step 29067, loss 2.02278e-06, acc 1
2017-08-08T18:53:05.565906: step 29068, loss 3.72529e-09, acc 1
2017-08-08T18:53:05.828014: step 29069, loss 1.52736e-07, acc 1
2017-08-08T18:53:06.061735: step 29070, loss 7.45058e-09, acc 1
2017-08-08T18:53:06.332547: step 29071, loss 2.79397e-08, acc 1
2017-08-08T18:53:06.604122: step 29072, loss 0, acc 1
2017-08-08T18:53:07.002592: step 29073, loss 2.95192e-05, acc 1
2017-08-08T18:53:07.449919: step 29074, loss 3.72529e-09, acc 1
2017-08-08T18:53:07.809342: step 29075, loss 3.44586e-07, acc 1
2017-08-08T18:53:08.028307: step 29076, loss 7.84362e-05, acc 1
2017-08-08T18:53:08.267528: step 29077, loss 1.56461e-07, acc 1
2017-08-08T18:53:08.643057: step 29078, loss 3.7439e-07, acc 1
2017-08-08T18:53:08.895845: step 29079, loss 0, acc 1
2017-08-08T18:53:09.139435: step 29080, loss 5.96045e-08, acc 1
2017-08-08T18:53:09.587936: step 29081, loss 3.72529e-09, acc 1
2017-08-08T18:53:09.898823: step 29082, loss 2.40693e-05, acc 1
2017-08-08T18:53:10.245423: step 29083, loss 1.86265e-09, acc 1
2017-08-08T18:53:10.455073: step 29084, loss 1.30385e-08, acc 1
2017-08-08T18:53:10.845417: step 29085, loss 0, acc 1
2017-08-08T18:53:11.071494: step 29086, loss 5.88588e-07, acc 1
2017-08-08T18:53:11.290640: step 29087, loss 1.86265e-09, acc 1
2017-08-08T18:53:11.525002: step 29088, loss 1.13621e-07, acc 1
2017-08-08T18:53:11.868170: step 29089, loss 2.04891e-08, acc 1
2017-08-08T18:53:12.219165: step 29090, loss 1.01511e-06, acc 1
2017-08-08T18:53:12.569044: step 29091, loss 3.9089e-05, acc 1
2017-08-08T18:53:12.809709: step 29092, loss 9.31322e-09, acc 1
2017-08-08T18:53:13.039193: step 29093, loss 2.79397e-08, acc 1
2017-08-08T18:53:13.418693: step 29094, loss 1.35973e-07, acc 1
2017-08-08T18:53:13.665324: step 29095, loss 1.86265e-09, acc 1
2017-08-08T18:53:13.956397: step 29096, loss 1.62049e-07, acc 1
2017-08-08T18:53:14.360741: step 29097, loss 0, acc 1
2017-08-08T18:53:14.681326: step 29098, loss 1.78358e-05, acc 1
2017-08-08T18:53:14.996697: step 29099, loss 3.16648e-07, acc 1
2017-08-08T18:53:15.184401: step 29100, loss 1.54171e-06, acc 1

Evaluation:
2017-08-08T18:53:15.819297: step 29100, loss 9.30394, acc 0.719512

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-29100

2017-08-08T18:53:16.276979: step 29101, loss 9.49946e-08, acc 1
2017-08-08T18:53:16.536958: step 29102, loss 3.72529e-09, acc 1
2017-08-08T18:53:16.982800: step 29103, loss 1.18833e-06, acc 1
2017-08-08T18:53:17.390320: step 29104, loss 1.38538e-05, acc 1
2017-08-08T18:53:17.703075: step 29105, loss 9.06105e-06, acc 1
2017-08-08T18:53:17.893398: step 29106, loss 0, acc 1
2017-08-08T18:53:18.358206: step 29107, loss 2.45301e-05, acc 1
2017-08-08T18:53:18.642244: step 29108, loss 2.32064e-05, acc 1
2017-08-08T18:53:18.868710: step 29109, loss 0, acc 1
2017-08-08T18:53:19.109888: step 29110, loss 7.45057e-08, acc 1
2017-08-08T18:53:19.371473: step 29111, loss 0, acc 1
2017-08-08T18:53:19.886405: step 29112, loss 1.14753e-05, acc 1
2017-08-08T18:53:20.290514: step 29113, loss 7.45058e-09, acc 1
2017-08-08T18:53:20.616960: step 29114, loss 0, acc 1
2017-08-08T18:53:20.793424: step 29115, loss 5.58793e-08, acc 1
2017-08-08T18:53:21.034487: step 29116, loss 5.40166e-08, acc 1
2017-08-08T18:53:21.332922: step 29117, loss 1.86265e-09, acc 1
2017-08-08T18:53:21.545204: step 29118, loss 2.73684e-05, acc 1
2017-08-08T18:53:21.767921: step 29119, loss 8.58656e-07, acc 1
2017-08-08T18:53:22.068224: step 29120, loss 9.90056e-06, acc 1
2017-08-08T18:53:22.357283: step 29121, loss 5.4026e-06, acc 1
2017-08-08T18:53:22.586765: step 29122, loss 0, acc 1
2017-08-08T18:53:22.830154: step 29123, loss 2.28344e-06, acc 1
2017-08-08T18:53:23.010976: step 29124, loss 0, acc 1
2017-08-08T18:53:23.313368: step 29125, loss 2.57043e-07, acc 1
2017-08-08T18:53:23.585919: step 29126, loss 0, acc 1
2017-08-08T18:53:23.876461: step 29127, loss 0, acc 1
2017-08-08T18:53:24.156442: step 29128, loss 0, acc 1
2017-08-08T18:53:24.537344: step 29129, loss 0, acc 1
2017-08-08T18:53:24.883339: step 29130, loss 6.51925e-08, acc 1
2017-08-08T18:53:25.266919: step 29131, loss 5.2154e-08, acc 1
2017-08-08T18:53:25.549561: step 29132, loss 2.79397e-08, acc 1
2017-08-08T18:53:25.843507: step 29133, loss 0, acc 1
2017-08-08T18:53:26.154801: step 29134, loss 3.51403e-05, acc 1
2017-08-08T18:53:26.397272: step 29135, loss 5.71169e-06, acc 1
2017-08-08T18:53:26.637786: step 29136, loss 0, acc 1
2017-08-08T18:53:26.873374: step 29137, loss 0.000224892, acc 1
2017-08-08T18:53:27.250556: step 29138, loss 3.65408e-06, acc 1
2017-08-08T18:53:27.521374: step 29139, loss 3.79975e-07, acc 1
2017-08-08T18:53:27.761623: step 29140, loss 0, acc 1
2017-08-08T18:53:27.979031: step 29141, loss 1.67638e-08, acc 1
2017-08-08T18:53:28.184549: step 29142, loss 0, acc 1
2017-08-08T18:53:28.524800: step 29143, loss 2.90861e-05, acc 1
2017-08-08T18:53:28.747823: step 29144, loss 3.53902e-08, acc 1
2017-08-08T18:53:29.002287: step 29145, loss 3.35276e-08, acc 1
2017-08-08T18:53:29.203513: step 29146, loss 0, acc 1
2017-08-08T18:53:29.581377: step 29147, loss 0, acc 1
2017-08-08T18:53:29.869408: step 29148, loss 0, acc 1
2017-08-08T18:53:30.155048: step 29149, loss 0, acc 1
2017-08-08T18:53:30.356392: step 29150, loss 5.59997e-06, acc 1
2017-08-08T18:53:30.593360: step 29151, loss 0, acc 1
2017-08-08T18:53:30.914927: step 29152, loss 0, acc 1
2017-08-08T18:53:31.161435: step 29153, loss 1.86265e-09, acc 1
2017-08-08T18:53:31.394628: step 29154, loss 0, acc 1
2017-08-08T18:53:31.648010: step 29155, loss 0, acc 1
2017-08-08T18:53:32.067462: step 29156, loss 0, acc 1
2017-08-08T18:53:32.505378: step 29157, loss 3.05456e-06, acc 1
2017-08-08T18:53:32.876372: step 29158, loss 9.03357e-07, acc 1
2017-08-08T18:53:33.109333: step 29159, loss 1.86264e-08, acc 1
2017-08-08T18:53:33.483663: step 29160, loss 2.04891e-08, acc 1
2017-08-08T18:53:33.823971: step 29161, loss 2.23517e-08, acc 1
2017-08-08T18:53:34.033261: step 29162, loss 8.56814e-08, acc 1
2017-08-08T18:53:34.309046: step 29163, loss 2.62631e-07, acc 1
2017-08-08T18:53:34.532021: step 29164, loss 8.36311e-07, acc 1
2017-08-08T18:53:35.007303: step 29165, loss 3.72529e-08, acc 1
2017-08-08T18:53:35.400143: step 29166, loss 7.38923e-06, acc 1
2017-08-08T18:53:35.682337: step 29167, loss 0.000180332, acc 1
2017-08-08T18:53:35.915421: step 29168, loss 0, acc 1
2017-08-08T18:53:36.294224: step 29169, loss 1.19209e-07, acc 1
2017-08-08T18:53:36.534977: step 29170, loss 2.21653e-07, acc 1
2017-08-08T18:53:36.803643: step 29171, loss 1.98786e-05, acc 1
2017-08-08T18:53:37.055273: step 29172, loss 2.11396e-06, acc 1
2017-08-08T18:53:37.407960: step 29173, loss 0, acc 1
2017-08-08T18:53:37.675836: step 29174, loss 2.04891e-08, acc 1
2017-08-08T18:53:37.911247: step 29175, loss 1.30406e-05, acc 1
2017-08-08T18:53:38.132097: step 29176, loss 0, acc 1
2017-08-08T18:53:38.445368: step 29177, loss 1.48073e-06, acc 1
2017-08-08T18:53:38.789292: step 29178, loss 0, acc 1
2017-08-08T18:53:39.092866: step 29179, loss 5.02913e-08, acc 1
2017-08-08T18:53:39.381361: step 29180, loss 5.62529e-05, acc 1
2017-08-08T18:53:39.740980: step 29181, loss 8.00935e-08, acc 1
2017-08-08T18:53:40.020389: step 29182, loss 3.72529e-09, acc 1
2017-08-08T18:53:40.236467: step 29183, loss 0, acc 1
2017-08-08T18:53:40.435536: step 29184, loss 3.91155e-08, acc 1
2017-08-08T18:53:40.657382: step 29185, loss 2.81257e-07, acc 1
2017-08-08T18:53:41.068223: step 29186, loss 0, acc 1
2017-08-08T18:53:41.339783: step 29187, loss 1.86265e-09, acc 1
2017-08-08T18:53:41.622419: step 29188, loss 5.58793e-09, acc 1
2017-08-08T18:53:41.954394: step 29189, loss 0, acc 1
2017-08-08T18:53:42.347616: step 29190, loss 1.49012e-08, acc 1
2017-08-08T18:53:42.677377: step 29191, loss 0, acc 1
2017-08-08T18:53:42.993067: step 29192, loss 1.49012e-08, acc 1
2017-08-08T18:53:43.200415: step 29193, loss 0, acc 1
2017-08-08T18:53:43.529366: step 29194, loss 2.10279e-06, acc 1
2017-08-08T18:53:43.863910: step 29195, loss 0, acc 1
2017-08-08T18:53:44.113202: step 29196, loss 0, acc 1
2017-08-08T18:53:44.407184: step 29197, loss 1.11759e-08, acc 1
2017-08-08T18:53:44.809353: step 29198, loss 0, acc 1
2017-08-08T18:53:45.278290: step 29199, loss 2.36554e-07, acc 1
2017-08-08T18:53:45.634026: step 29200, loss 2.6077e-08, acc 1

Evaluation:
2017-08-08T18:53:46.413185: step 29200, loss 9.30253, acc 0.719512

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-29200

2017-08-08T18:53:47.023229: step 29201, loss 0, acc 1
2017-08-08T18:53:47.304310: step 29202, loss 1.49012e-08, acc 1
2017-08-08T18:53:47.662920: step 29203, loss 9.31322e-09, acc 1
2017-08-08T18:53:48.077367: step 29204, loss 0, acc 1
2017-08-08T18:53:48.414009: step 29205, loss 2.38215e-06, acc 1
2017-08-08T18:53:48.663994: step 29206, loss 7.35013e-06, acc 1
2017-08-08T18:53:48.913356: step 29207, loss 0, acc 1
2017-08-08T18:53:49.207444: step 29208, loss 0.000109972, acc 1
2017-08-08T18:53:49.592252: step 29209, loss 0, acc 1
2017-08-08T18:53:49.878863: step 29210, loss 1.90724e-06, acc 1
2017-08-08T18:53:50.148728: step 29211, loss 0, acc 1
2017-08-08T18:53:50.521657: step 29212, loss 1.20547e-05, acc 1
2017-08-08T18:53:50.926734: step 29213, loss 7.72983e-07, acc 1
2017-08-08T18:53:51.174922: step 29214, loss 1.63912e-07, acc 1
2017-08-08T18:53:51.438521: step 29215, loss 1.74991e-05, acc 1
2017-08-08T18:53:51.669481: step 29216, loss 1.67638e-08, acc 1
2017-08-08T18:53:52.172955: step 29217, loss 1.86265e-09, acc 1
2017-08-08T18:53:52.407400: step 29218, loss 0, acc 1
2017-08-08T18:53:52.671312: step 29219, loss 7.59774e-06, acc 1
2017-08-08T18:53:52.911137: step 29220, loss 0, acc 1
2017-08-08T18:53:53.266718: step 29221, loss 1.55807e-05, acc 1
2017-08-08T18:53:53.688111: step 29222, loss 2.23517e-08, acc 1
2017-08-08T18:53:53.961867: step 29223, loss 9.40626e-07, acc 1
2017-08-08T18:53:54.191143: step 29224, loss 0, acc 1
2017-08-08T18:53:54.541999: step 29225, loss 2.23517e-08, acc 1
2017-08-08T18:53:54.899078: step 29226, loss 1.7695e-07, acc 1
2017-08-08T18:53:55.194935: step 29227, loss 3.72529e-09, acc 1
2017-08-08T18:53:55.485723: step 29228, loss 5.36433e-07, acc 1
2017-08-08T18:53:55.842623: step 29229, loss 1.32247e-07, acc 1
2017-08-08T18:53:56.228700: step 29230, loss 0.000228037, acc 1
2017-08-08T18:53:56.615297: step 29231, loss 1.17717e-06, acc 1
2017-08-08T18:53:56.998501: step 29232, loss 0, acc 1
2017-08-08T18:53:57.255925: step 29233, loss 6.44462e-07, acc 1
2017-08-08T18:53:57.507133: step 29234, loss 0, acc 1
2017-08-08T18:53:57.830071: step 29235, loss 1.67638e-08, acc 1
2017-08-08T18:53:58.145328: step 29236, loss 0, acc 1
2017-08-08T18:53:58.406658: step 29237, loss 0, acc 1
2017-08-08T18:53:58.616396: step 29238, loss 1.08033e-07, acc 1
2017-08-08T18:53:58.943751: step 29239, loss 3.35276e-08, acc 1
2017-08-08T18:53:59.189197: step 29240, loss 0, acc 1
2017-08-08T18:53:59.495578: step 29241, loss 0.000196611, acc 1
2017-08-08T18:53:59.681616: step 29242, loss 5.97902e-07, acc 1
2017-08-08T18:53:59.872246: step 29243, loss 1.89792e-06, acc 1
2017-08-08T18:54:00.251919: step 29244, loss 0, acc 1
2017-08-08T18:54:00.444139: step 29245, loss 0, acc 1
2017-08-08T18:54:00.727657: step 29246, loss 0, acc 1
2017-08-08T18:54:00.969371: step 29247, loss 1.3411e-07, acc 1
2017-08-08T18:54:01.285348: step 29248, loss 1.05595e-05, acc 1
2017-08-08T18:54:01.501329: step 29249, loss 1.5627e-06, acc 1
2017-08-08T18:54:01.755214: step 29250, loss 0, acc 1
2017-08-08T18:54:02.083295: step 29251, loss 3.3748e-06, acc 1
2017-08-08T18:54:02.392688: step 29252, loss 1.67638e-08, acc 1
2017-08-08T18:54:02.825846: step 29253, loss 4.65661e-08, acc 1
2017-08-08T18:54:03.194399: step 29254, loss 2.04891e-08, acc 1
2017-08-08T18:54:03.427463: step 29255, loss 3.81609e-06, acc 1
2017-08-08T18:54:03.745532: step 29256, loss 0.000286634, acc 1
2017-08-08T18:54:04.099055: step 29257, loss 7.63683e-08, acc 1
2017-08-08T18:54:04.437438: step 29258, loss 3.29663e-06, acc 1
2017-08-08T18:54:04.745025: step 29259, loss 3.29685e-07, acc 1
2017-08-08T18:54:05.012977: step 29260, loss 3.78112e-07, acc 1
2017-08-08T18:54:05.354753: step 29261, loss 7.2643e-08, acc 1
2017-08-08T18:54:05.742564: step 29262, loss 5.58793e-09, acc 1
2017-08-08T18:54:05.993931: step 29263, loss 1.86265e-09, acc 1
2017-08-08T18:54:06.241951: step 29264, loss 0, acc 1
2017-08-08T18:54:06.462489: step 29265, loss 7.82309e-08, acc 1
2017-08-08T18:54:06.809913: step 29266, loss 0, acc 1
2017-08-08T18:54:07.106992: step 29267, loss 3.51795e-05, acc 1
2017-08-08T18:54:07.367283: step 29268, loss 0, acc 1
2017-08-08T18:54:07.567639: step 29269, loss 1.30385e-08, acc 1
2017-08-08T18:54:07.755144: step 29270, loss 0, acc 1
2017-08-08T18:54:08.071033: step 29271, loss 1.86265e-09, acc 1
2017-08-08T18:54:08.297742: step 29272, loss 1.86265e-09, acc 1
2017-08-08T18:54:08.492287: step 29273, loss 4.47034e-08, acc 1
2017-08-08T18:54:08.692669: step 29274, loss 1.86265e-09, acc 1
2017-08-08T18:54:09.043656: step 29275, loss 4.84287e-08, acc 1
2017-08-08T18:54:09.286856: step 29276, loss 5.52637e-05, acc 1
2017-08-08T18:54:09.536473: step 29277, loss 1.9631e-06, acc 1
2017-08-08T18:54:09.781720: step 29278, loss 1.66787e-05, acc 1
2017-08-08T18:54:10.015465: step 29279, loss 0.000121537, acc 1
2017-08-08T18:54:10.344661: step 29280, loss 1.96683e-06, acc 1
2017-08-08T18:54:10.619150: step 29281, loss 2.23517e-08, acc 1
2017-08-08T18:54:10.910049: step 29282, loss 0, acc 1
2017-08-08T18:54:11.281174: step 29283, loss 2.04891e-08, acc 1
2017-08-08T18:54:11.676463: step 29284, loss 7.66868e-06, acc 1
2017-08-08T18:54:12.016365: step 29285, loss 1.21071e-07, acc 1
2017-08-08T18:54:12.324957: step 29286, loss 1.49012e-08, acc 1
2017-08-08T18:54:12.536423: step 29287, loss 2.27242e-07, acc 1
2017-08-08T18:54:13.014541: step 29288, loss 1.86265e-09, acc 1
2017-08-08T18:54:13.230473: step 29289, loss 2.42144e-08, acc 1
2017-08-08T18:54:13.489643: step 29290, loss 1.49012e-08, acc 1
2017-08-08T18:54:13.748771: step 29291, loss 6.51925e-08, acc 1
2017-08-08T18:54:14.174564: step 29292, loss 0, acc 1
2017-08-08T18:54:14.610333: step 29293, loss 1.50874e-07, acc 1
2017-08-08T18:54:14.941216: step 29294, loss 4.47034e-08, acc 1
2017-08-08T18:54:15.186596: step 29295, loss 1.59062e-06, acc 1
2017-08-08T18:54:15.523189: step 29296, loss 3.59486e-07, acc 1
2017-08-08T18:54:15.900623: step 29297, loss 0, acc 1
2017-08-08T18:54:16.092840: step 29298, loss 1.11759e-08, acc 1
2017-08-08T18:54:16.357201: step 29299, loss 2.21653e-07, acc 1
2017-08-08T18:54:16.677390: step 29300, loss 1.86265e-09, acc 1

Evaluation:
2017-08-08T18:54:17.498293: step 29300, loss 9.29445, acc 0.722326

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-29300

2017-08-08T18:54:17.847923: step 29301, loss 0, acc 1
2017-08-08T18:54:18.231402: step 29302, loss 0, acc 1
2017-08-08T18:54:18.504117: step 29303, loss 1.22934e-07, acc 1
2017-08-08T18:54:18.789130: step 29304, loss 2.16066e-07, acc 1
2017-08-08T18:54:19.159695: step 29305, loss 0, acc 1
2017-08-08T18:54:19.559473: step 29306, loss 2.98023e-08, acc 1
2017-08-08T18:54:19.865227: step 29307, loss 9.31322e-09, acc 1
2017-08-08T18:54:20.097683: step 29308, loss 8.67379e-06, acc 1
2017-08-08T18:54:20.309330: step 29309, loss 0, acc 1
2017-08-08T18:54:20.617259: step 29310, loss 0, acc 1
2017-08-08T18:54:20.819306: step 29311, loss 0, acc 1
2017-08-08T18:54:21.003601: step 29312, loss 0, acc 1
2017-08-08T18:54:21.241612: step 29313, loss 5.58793e-09, acc 1
2017-08-08T18:54:21.672998: step 29314, loss 0, acc 1
2017-08-08T18:54:22.006391: step 29315, loss 4.73105e-07, acc 1
2017-08-08T18:54:22.275599: step 29316, loss 8.92182e-07, acc 1
2017-08-08T18:54:22.600325: step 29317, loss 2.5518e-07, acc 1
2017-08-08T18:54:22.843076: step 29318, loss 0, acc 1
2017-08-08T18:54:23.113008: step 29319, loss 0, acc 1
2017-08-08T18:54:23.291617: step 29320, loss 0, acc 1
2017-08-08T18:54:23.488281: step 29321, loss 2.42144e-08, acc 1
2017-08-08T18:54:23.767024: step 29322, loss 1.86265e-09, acc 1
2017-08-08T18:54:24.112484: step 29323, loss 1.67638e-08, acc 1
2017-08-08T18:54:24.390290: step 29324, loss 1.86265e-09, acc 1
2017-08-08T18:54:24.569494: step 29325, loss 5.58793e-09, acc 1
2017-08-08T18:54:24.841311: step 29326, loss 0, acc 1
2017-08-08T18:54:25.171432: step 29327, loss 0.000181601, acc 1
2017-08-08T18:54:25.441747: step 29328, loss 0, acc 1
2017-08-08T18:54:25.707142: step 29329, loss 1.67638e-08, acc 1
2017-08-08T18:54:25.886378: step 29330, loss 2.71945e-07, acc 1
2017-08-08T18:54:26.154774: step 29331, loss 4.3046e-05, acc 1
2017-08-08T18:54:26.476709: step 29332, loss 6.29561e-07, acc 1
2017-08-08T18:54:26.721038: step 29333, loss 1.00583e-07, acc 1
2017-08-08T18:54:26.927788: step 29334, loss 1.55152e-06, acc 1
2017-08-08T18:54:27.130306: step 29335, loss 8.19562e-08, acc 1
2017-08-08T18:54:27.595912: step 29336, loss 4.84287e-08, acc 1
2017-08-08T18:54:27.819867: step 29337, loss 1.59736e-05, acc 1
2017-08-08T18:54:28.058678: step 29338, loss 1.38401e-05, acc 1
2017-08-08T18:54:28.437630: step 29339, loss 1.75088e-07, acc 1
2017-08-08T18:54:28.827149: step 29340, loss 1.86265e-09, acc 1
2017-08-08T18:54:29.156601: step 29341, loss 1.3411e-07, acc 1
2017-08-08T18:54:29.367852: step 29342, loss 4.99062e-05, acc 1
2017-08-08T18:54:29.561825: step 29343, loss 0, acc 1
2017-08-08T18:54:30.089592: step 29344, loss 0, acc 1
2017-08-08T18:54:30.463951: step 29345, loss 2.49593e-07, acc 1
2017-08-08T18:54:30.902194: step 29346, loss 0, acc 1
2017-08-08T18:54:31.500750: step 29347, loss 0, acc 1
2017-08-08T18:54:32.055331: step 29348, loss 0, acc 1
2017-08-08T18:54:32.555395: step 29349, loss 1.04308e-07, acc 1
2017-08-08T18:54:32.929288: step 29350, loss 6.59366e-07, acc 1
2017-08-08T18:54:33.206087: step 29351, loss 3.72529e-09, acc 1
2017-08-08T18:54:33.658384: step 29352, loss 0, acc 1
2017-08-08T18:54:33.943418: step 29353, loss 1.86265e-09, acc 1
2017-08-08T18:54:34.204671: step 29354, loss 1.30385e-08, acc 1
2017-08-08T18:54:34.484787: step 29355, loss 7.43181e-07, acc 1
2017-08-08T18:54:34.737630: step 29356, loss 1.86265e-09, acc 1
2017-08-08T18:54:35.258121: step 29357, loss 1.91851e-07, acc 1
2017-08-08T18:54:35.661413: step 29358, loss 1.49012e-08, acc 1
2017-08-08T18:54:35.980484: step 29359, loss 0, acc 1
2017-08-08T18:54:36.176244: step 29360, loss 2.42144e-08, acc 1
2017-08-08T18:54:36.402770: step 29361, loss 0, acc 1
2017-08-08T18:54:36.863004: step 29362, loss 0.000217139, acc 1
2017-08-08T18:54:37.179524: step 29363, loss 5.58794e-09, acc 1
2017-08-08T18:54:37.436851: step 29364, loss 1.17098e-05, acc 1
2017-08-08T18:54:37.793318: step 29365, loss 0, acc 1
2017-08-08T18:54:38.081336: step 29366, loss 0, acc 1
2017-08-08T18:54:38.363927: step 29367, loss 4.66354e-06, acc 1
2017-08-08T18:54:38.538118: step 29368, loss 0, acc 1
2017-08-08T18:54:38.773409: step 29369, loss 7.45058e-09, acc 1
2017-08-08T18:54:39.127489: step 29370, loss 6.05825e-05, acc 1
2017-08-08T18:54:39.329373: step 29371, loss 0, acc 1
2017-08-08T18:54:39.546749: step 29372, loss 0, acc 1
2017-08-08T18:54:39.748565: step 29373, loss 9.87199e-08, acc 1
2017-08-08T18:54:40.047600: step 29374, loss 0, acc 1
2017-08-08T18:54:40.324538: step 29375, loss 1.36154e-06, acc 1
2017-08-08T18:54:40.599069: step 29376, loss 5.58793e-09, acc 1
2017-08-08T18:54:40.871366: step 29377, loss 0, acc 1
2017-08-08T18:54:41.046949: step 29378, loss 2.35421e-06, acc 1
2017-08-08T18:54:41.341771: step 29379, loss 1.21071e-07, acc 1
2017-08-08T18:54:41.605687: step 29380, loss 0, acc 1
2017-08-08T18:54:41.805027: step 29381, loss 2.77532e-07, acc 1
2017-08-08T18:54:41.992148: step 29382, loss 1.86265e-09, acc 1
2017-08-08T18:54:42.275093: step 29383, loss 1.38371e-05, acc 1
2017-08-08T18:54:42.573354: step 29384, loss 2.06676e-05, acc 1
2017-08-08T18:54:42.877534: step 29385, loss 1.86265e-09, acc 1
2017-08-08T18:54:43.066121: step 29386, loss 0, acc 1
2017-08-08T18:54:43.284508: step 29387, loss 1.21986e-05, acc 1
2017-08-08T18:54:43.526753: step 29388, loss 3.07333e-07, acc 1
2017-08-08T18:54:43.710508: step 29389, loss 1.86265e-09, acc 1
2017-08-08T18:54:43.900512: step 29390, loss 8.23673e-06, acc 1
2017-08-08T18:54:44.105739: step 29391, loss 0.000485947, acc 1
2017-08-08T18:54:44.417666: step 29392, loss 0.0290234, acc 0.984375
2017-08-08T18:54:44.690071: step 29393, loss 0, acc 1
2017-08-08T18:54:45.001907: step 29394, loss 2.19791e-07, acc 1
2017-08-08T18:54:45.271515: step 29395, loss 0, acc 1
2017-08-08T18:54:45.455755: step 29396, loss 0, acc 1
2017-08-08T18:54:45.730532: step 29397, loss 3.95025e-06, acc 1
2017-08-08T18:54:46.008464: step 29398, loss 0, acc 1
2017-08-08T18:54:46.257577: step 29399, loss 1.86265e-09, acc 1
2017-08-08T18:54:46.539221: step 29400, loss 0, acc 1

Evaluation:
2017-08-08T18:54:47.295541: step 29400, loss 9.32868, acc 0.713884

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-29400

2017-08-08T18:54:47.742440: step 29401, loss 4.16991e-06, acc 1
2017-08-08T18:54:47.977159: step 29402, loss 3.01746e-07, acc 1
2017-08-08T18:54:48.264981: step 29403, loss 0, acc 1
2017-08-08T18:54:48.580660: step 29404, loss 5.40166e-08, acc 1
2017-08-08T18:54:48.838709: step 29405, loss 0, acc 1
2017-08-08T18:54:49.112153: step 29406, loss 4.66157e-05, acc 1
2017-08-08T18:54:49.416262: step 29407, loss 0, acc 1
2017-08-08T18:54:49.906998: step 29408, loss 1.978e-06, acc 1
2017-08-08T18:54:50.449388: step 29409, loss 0, acc 1
2017-08-08T18:54:50.951757: step 29410, loss 2.04891e-08, acc 1
2017-08-08T18:54:51.339437: step 29411, loss 1.86265e-09, acc 1
2017-08-08T18:54:51.685244: step 29412, loss 1.86265e-09, acc 1
2017-08-08T18:54:52.001096: step 29413, loss 3.837e-07, acc 1
2017-08-08T18:54:52.479396: step 29414, loss 0, acc 1
2017-08-08T18:54:52.894831: step 29415, loss 5.40166e-08, acc 1
2017-08-08T18:54:53.160527: step 29416, loss 0.000162615, acc 1
2017-08-08T18:54:53.368276: step 29417, loss 5.02913e-08, acc 1
2017-08-08T18:54:53.615774: step 29418, loss 0.000794645, acc 1
2017-08-08T18:54:54.004227: step 29419, loss 1.27772e-06, acc 1
2017-08-08T18:54:54.351059: step 29420, loss 0.0664721, acc 0.984375
2017-08-08T18:54:54.633725: step 29421, loss 7.10638e-05, acc 1
2017-08-08T18:54:54.893282: step 29422, loss 3.53902e-08, acc 1
2017-08-08T18:54:55.327937: step 29423, loss 0.00338536, acc 1
2017-08-08T18:54:55.615699: step 29424, loss 2.54241e-06, acc 1
2017-08-08T18:54:55.908114: step 29425, loss 0, acc 1
2017-08-08T18:54:56.200958: step 29426, loss 3.73369e-05, acc 1
2017-08-08T18:54:56.621341: step 29427, loss 0.158363, acc 0.984375
2017-08-08T18:54:57.031370: step 29428, loss 5.02906e-07, acc 1
2017-08-08T18:54:57.368601: step 29429, loss 0, acc 1
2017-08-08T18:54:57.613713: step 29430, loss 0, acc 1
2017-08-08T18:54:57.992991: step 29431, loss 5.52365e-06, acc 1
2017-08-08T18:54:58.411814: step 29432, loss 0.00107063, acc 1
2017-08-08T18:54:58.677950: step 29433, loss 0, acc 1
2017-08-08T18:54:58.971768: step 29434, loss 2.98023e-08, acc 1
2017-08-08T18:54:59.375519: step 29435, loss 1.34705e-05, acc 1
2017-08-08T18:54:59.784785: step 29436, loss 3.53902e-08, acc 1
2017-08-08T18:55:00.101325: step 29437, loss 4.47034e-08, acc 1
2017-08-08T18:55:00.361467: step 29438, loss 0.000114129, acc 1
2017-08-08T18:55:00.687834: step 29439, loss 7.8231e-08, acc 1
2017-08-08T18:55:01.034814: step 29440, loss 1.86265e-09, acc 1
2017-08-08T18:55:01.265655: step 29441, loss 7.45058e-09, acc 1
2017-08-08T18:55:01.637855: step 29442, loss 1.49012e-08, acc 1
2017-08-08T18:55:02.071534: step 29443, loss 0, acc 1
2017-08-08T18:55:02.483529: step 29444, loss 1.49012e-08, acc 1
2017-08-08T18:55:02.881703: step 29445, loss 0.000181833, acc 1
2017-08-08T18:55:03.176852: step 29446, loss 4.18965e-05, acc 1
2017-08-08T18:55:03.516439: step 29447, loss 3.02468e-06, acc 1
2017-08-08T18:55:03.765928: step 29448, loss 2.17928e-07, acc 1
2017-08-08T18:55:04.100868: step 29449, loss 0, acc 1
2017-08-08T18:55:04.405949: step 29450, loss 0, acc 1
2017-08-08T18:55:04.642294: step 29451, loss 1.86265e-09, acc 1
2017-08-08T18:55:04.922500: step 29452, loss 3.72529e-09, acc 1
2017-08-08T18:55:05.309319: step 29453, loss 6.74263e-07, acc 1
2017-08-08T18:55:05.705438: step 29454, loss 6.01267e-05, acc 1
2017-08-08T18:55:06.028427: step 29455, loss 0, acc 1
2017-08-08T18:55:06.271609: step 29456, loss 7.26431e-08, acc 1
2017-08-08T18:55:06.497628: step 29457, loss 0, acc 1
2017-08-08T18:55:06.839788: step 29458, loss 1.1399e-06, acc 1
2017-08-08T18:55:07.085134: step 29459, loss 2.78255e-06, acc 1
2017-08-08T18:55:07.328061: step 29460, loss 4.74967e-07, acc 1
2017-08-08T18:55:07.548024: step 29461, loss 3.91155e-08, acc 1
2017-08-08T18:55:07.795977: step 29462, loss 7.52723e-06, acc 1
2017-08-08T18:55:08.208377: step 29463, loss 1.11759e-08, acc 1
2017-08-08T18:55:08.660220: step 29464, loss 1.62049e-07, acc 1
2017-08-08T18:55:08.937929: step 29465, loss 1.86265e-09, acc 1
2017-08-08T18:55:09.245390: step 29466, loss 0.000681085, acc 1
2017-08-08T18:55:09.643222: step 29467, loss 1.14176e-06, acc 1
2017-08-08T18:55:09.926274: step 29468, loss 1.86265e-09, acc 1
2017-08-08T18:55:10.191817: step 29469, loss 1.86265e-09, acc 1
2017-08-08T18:55:10.463981: step 29470, loss 5.58793e-09, acc 1
2017-08-08T18:55:10.923272: step 29471, loss 0, acc 1
2017-08-08T18:55:11.353706: step 29472, loss 1.70981e-06, acc 1
2017-08-08T18:55:11.668085: step 29473, loss 7.823e-07, acc 1
2017-08-08T18:55:11.939669: step 29474, loss 1.11759e-08, acc 1
2017-08-08T18:55:12.304933: step 29475, loss 2.39576e-05, acc 1
2017-08-08T18:55:12.654196: step 29476, loss 0, acc 1
2017-08-08T18:55:12.917952: step 29477, loss 3.20372e-07, acc 1
2017-08-08T18:55:13.207815: step 29478, loss 0, acc 1
2017-08-08T18:55:13.563949: step 29479, loss 2.97172e-05, acc 1
2017-08-08T18:55:13.942115: step 29480, loss 1.86265e-09, acc 1
2017-08-08T18:55:14.297690: step 29481, loss 1.88126e-07, acc 1
2017-08-08T18:55:14.660620: step 29482, loss 0.00296163, acc 1
2017-08-08T18:55:14.896967: step 29483, loss 0, acc 1
2017-08-08T18:55:15.345496: step 29484, loss 0, acc 1
2017-08-08T18:55:15.739413: step 29485, loss 4.5481e-05, acc 1
2017-08-08T18:55:16.013888: step 29486, loss 0, acc 1
2017-08-08T18:55:16.292349: step 29487, loss 1.54599e-07, acc 1
2017-08-08T18:55:16.801396: step 29488, loss 0, acc 1
2017-08-08T18:55:17.221510: step 29489, loss 5.9976e-07, acc 1
2017-08-08T18:55:17.573998: step 29490, loss 9.64821e-07, acc 1
2017-08-08T18:55:17.869391: step 29491, loss 0.00016409, acc 1
2017-08-08T18:55:18.291586: step 29492, loss 0, acc 1
2017-08-08T18:55:18.580307: step 29493, loss 8.97723e-06, acc 1
2017-08-08T18:55:18.834586: step 29494, loss 1.86265e-09, acc 1
2017-08-08T18:55:19.115802: step 29495, loss 1.86265e-09, acc 1
2017-08-08T18:55:19.542214: step 29496, loss 6.70551e-08, acc 1
2017-08-08T18:55:19.968361: step 29497, loss 3.31549e-07, acc 1
2017-08-08T18:55:20.194211: step 29498, loss 7.45058e-09, acc 1
2017-08-08T18:55:20.365563: step 29499, loss 1.86265e-09, acc 1
2017-08-08T18:55:20.681374: step 29500, loss 7.45058e-09, acc 1

Evaluation:
2017-08-08T18:55:21.414954: step 29500, loss 9.32288, acc 0.724203

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-29500

2017-08-08T18:55:21.829354: step 29501, loss 4.16443e-06, acc 1
2017-08-08T18:55:22.244409: step 29502, loss 3.72529e-09, acc 1
2017-08-08T18:55:22.653063: step 29503, loss 2.6077e-08, acc 1
2017-08-08T18:55:22.961159: step 29504, loss 8.94067e-08, acc 1
2017-08-08T18:55:23.230901: step 29505, loss 5.38298e-07, acc 1
2017-08-08T18:55:23.551003: step 29506, loss 2.75354e-05, acc 1
2017-08-08T18:55:23.909234: step 29507, loss 1.79121e-05, acc 1
2017-08-08T18:55:24.185363: step 29508, loss 2.6077e-08, acc 1
2017-08-08T18:55:24.456610: step 29509, loss 3.48311e-07, acc 1
2017-08-08T18:55:24.729275: step 29510, loss 0, acc 1
2017-08-08T18:55:25.137417: step 29511, loss 5.58794e-09, acc 1
2017-08-08T18:55:25.501434: step 29512, loss 0, acc 1
2017-08-08T18:55:25.903029: step 29513, loss 1.7601e-06, acc 1
2017-08-08T18:55:26.151668: step 29514, loss 0, acc 1
2017-08-08T18:55:26.360585: step 29515, loss 3.85563e-07, acc 1
2017-08-08T18:55:26.854629: step 29516, loss 0.00188311, acc 1
2017-08-08T18:55:27.231762: step 29517, loss 3.16649e-08, acc 1
2017-08-08T18:55:27.543447: step 29518, loss 1.24275e-05, acc 1
2017-08-08T18:55:27.921143: step 29519, loss 1.49012e-08, acc 1
2017-08-08T18:55:28.296987: step 29520, loss 6.864e-05, acc 1
2017-08-08T18:55:28.593370: step 29521, loss 0, acc 1
2017-08-08T18:55:28.919563: step 29522, loss 0, acc 1
2017-08-08T18:55:29.358231: step 29523, loss 0, acc 1
2017-08-08T18:55:29.818336: step 29524, loss 4.32633e-06, acc 1
2017-08-08T18:55:30.198953: step 29525, loss 1.49012e-08, acc 1
2017-08-08T18:55:30.607026: step 29526, loss 5.58793e-09, acc 1
2017-08-08T18:55:30.924298: step 29527, loss 6.70551e-08, acc 1
2017-08-08T18:55:31.353957: step 29528, loss 7.63683e-08, acc 1
2017-08-08T18:55:31.684190: step 29529, loss 0, acc 1
2017-08-08T18:55:32.000475: step 29530, loss 2.0823e-06, acc 1
2017-08-08T18:55:32.192280: step 29531, loss 0.000146846, acc 1
2017-08-08T18:55:32.486245: step 29532, loss 0, acc 1
2017-08-08T18:55:32.744008: step 29533, loss 2.62986e-06, acc 1
2017-08-08T18:55:32.974255: step 29534, loss 1.29586e-05, acc 1
2017-08-08T18:55:33.247587: step 29535, loss 0, acc 1
2017-08-08T18:55:33.492607: step 29536, loss 1.11759e-08, acc 1
2017-08-08T18:55:33.838508: step 29537, loss 1.13621e-07, acc 1
2017-08-08T18:55:34.278433: step 29538, loss 0, acc 1
2017-08-08T18:55:34.615332: step 29539, loss 1.77686e-06, acc 1
2017-08-08T18:55:34.873986: step 29540, loss 0, acc 1
2017-08-08T18:55:35.107085: step 29541, loss 1.86264e-08, acc 1
2017-08-08T18:55:35.357147: step 29542, loss 0, acc 1
2017-08-08T18:55:35.528067: step 29543, loss 2.04891e-08, acc 1
2017-08-08T18:55:35.727776: step 29544, loss 3.42723e-07, acc 1
2017-08-08T18:55:35.993322: step 29545, loss 0, acc 1
2017-08-08T18:55:36.265322: step 29546, loss 8.28866e-07, acc 1
2017-08-08T18:55:36.490656: step 29547, loss 9.31322e-09, acc 1
2017-08-08T18:55:36.680413: step 29548, loss 1.66512e-06, acc 1
2017-08-08T18:55:36.921217: step 29549, loss 1.11759e-08, acc 1
2017-08-08T18:55:37.222211: step 29550, loss 5.15531e-05, acc 1
2017-08-08T18:55:37.384533: step 29551, loss 8.75442e-08, acc 1
2017-08-08T18:55:37.574969: step 29552, loss 2.23517e-08, acc 1
2017-08-08T18:55:37.751262: step 29553, loss 0, acc 1
2017-08-08T18:55:37.910171: step 29554, loss 0, acc 1
2017-08-08T18:55:38.185357: step 29555, loss 1.49012e-08, acc 1
2017-08-08T18:55:38.421823: step 29556, loss 1.32248e-07, acc 1
2017-08-08T18:55:38.679199: step 29557, loss 2.2146e-06, acc 1
2017-08-08T18:55:38.886242: step 29558, loss 1.86265e-09, acc 1
2017-08-08T18:55:39.137451: step 29559, loss 2.98023e-08, acc 1
2017-08-08T18:55:39.519044: step 29560, loss 7.45041e-07, acc 1
2017-08-08T18:55:39.846371: step 29561, loss 1.32285e-05, acc 1
2017-08-08T18:55:40.148474: step 29562, loss 1.86264e-08, acc 1
2017-08-08T18:55:40.369197: step 29563, loss 3.72529e-08, acc 1
2017-08-08T18:55:40.638957: step 29564, loss 2.49593e-07, acc 1
2017-08-08T18:55:40.938964: step 29565, loss 0, acc 1
2017-08-08T18:55:41.305982: step 29566, loss 2.77532e-07, acc 1
2017-08-08T18:55:41.518889: step 29567, loss 3.72529e-09, acc 1
2017-08-08T18:55:41.737355: step 29568, loss 3.72529e-09, acc 1
2017-08-08T18:55:42.116304: step 29569, loss 6.33298e-08, acc 1
2017-08-08T18:55:42.390298: step 29570, loss 1.45469e-06, acc 1
2017-08-08T18:55:42.693900: step 29571, loss 5.21582e-05, acc 1
2017-08-08T18:55:42.940208: step 29572, loss 0, acc 1
2017-08-08T18:55:43.168350: step 29573, loss 3.09197e-07, acc 1
2017-08-08T18:55:43.569291: step 29574, loss 6.50495e-06, acc 1
2017-08-08T18:55:43.882162: step 29575, loss 0, acc 1
2017-08-08T18:55:44.179979: step 29576, loss 1.08775e-06, acc 1
2017-08-08T18:55:44.439183: step 29577, loss 3.87427e-07, acc 1
2017-08-08T18:55:44.837394: step 29578, loss 0.000387477, acc 1
2017-08-08T18:55:45.165430: step 29579, loss 2.21908e-05, acc 1
2017-08-08T18:55:45.492271: step 29580, loss 3.81838e-07, acc 1
2017-08-08T18:55:45.792014: step 29581, loss 0, acc 1
2017-08-08T18:55:46.091063: step 29582, loss 2.6077e-08, acc 1
2017-08-08T18:55:46.528392: step 29583, loss 0, acc 1
2017-08-08T18:55:46.885523: step 29584, loss 0, acc 1
2017-08-08T18:55:47.126317: step 29585, loss 0, acc 1
2017-08-08T18:55:47.347752: step 29586, loss 1.64608e-05, acc 1
2017-08-08T18:55:47.766465: step 29587, loss 5.02913e-08, acc 1
2017-08-08T18:55:48.028895: step 29588, loss 8.75441e-08, acc 1
2017-08-08T18:55:48.353511: step 29589, loss 5.62363e-05, acc 1
2017-08-08T18:55:48.652901: step 29590, loss 0.000189551, acc 1
2017-08-08T18:55:49.027112: step 29591, loss 2.06753e-07, acc 1
2017-08-08T18:55:49.396443: step 29592, loss 1.36342e-06, acc 1
2017-08-08T18:55:49.717366: step 29593, loss 0, acc 1
2017-08-08T18:55:49.995281: step 29594, loss 9.872e-08, acc 1
2017-08-08T18:55:50.341030: step 29595, loss 0, acc 1
2017-08-08T18:55:50.734185: step 29596, loss 9.31322e-09, acc 1
2017-08-08T18:55:50.991868: step 29597, loss 1.86264e-08, acc 1
2017-08-08T18:55:51.325988: step 29598, loss 1.86264e-08, acc 1
2017-08-08T18:55:51.639175: step 29599, loss 0, acc 1
2017-08-08T18:55:52.012424: step 29600, loss 5.88585e-07, acc 1

Evaluation:
2017-08-08T18:55:52.796384: step 29600, loss 9.33132, acc 0.723265

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-29600

2017-08-08T18:55:53.183515: step 29601, loss 3.72529e-09, acc 1
2017-08-08T18:55:53.529383: step 29602, loss 1.86265e-09, acc 1
2017-08-08T18:55:53.849584: step 29603, loss 0, acc 1
2017-08-08T18:55:54.163880: step 29604, loss 0.00164121, acc 1
2017-08-08T18:55:54.474974: step 29605, loss 0, acc 1
2017-08-08T18:55:54.909209: step 29606, loss 7.50632e-07, acc 1
2017-08-08T18:55:55.221606: step 29607, loss 4.52616e-07, acc 1
2017-08-08T18:55:55.509398: step 29608, loss 1.97242e-06, acc 1
2017-08-08T18:55:55.767421: step 29609, loss 0, acc 1
2017-08-08T18:55:56.016178: step 29610, loss 4.09782e-08, acc 1
2017-08-08T18:55:56.416369: step 29611, loss 4.65661e-08, acc 1
2017-08-08T18:55:56.718685: step 29612, loss 4.89872e-07, acc 1
2017-08-08T18:55:57.006528: step 29613, loss 3.72529e-09, acc 1
2017-08-08T18:55:57.291190: step 29614, loss 1.86265e-09, acc 1
2017-08-08T18:55:57.610974: step 29615, loss 3.72529e-09, acc 1
2017-08-08T18:55:58.047940: step 29616, loss 0, acc 1
2017-08-08T18:55:58.467390: step 29617, loss 4.65661e-08, acc 1
2017-08-08T18:55:58.785809: step 29618, loss 7.14374e-05, acc 1
2017-08-08T18:55:59.058771: step 29619, loss 1.71195e-05, acc 1
2017-08-08T18:55:59.429240: step 29620, loss 1.44084e-05, acc 1
2017-08-08T18:55:59.782590: step 29621, loss 1.80666e-06, acc 1
2017-08-08T18:56:00.040272: step 29622, loss 3.72529e-08, acc 1
2017-08-08T18:56:00.353899: step 29623, loss 0, acc 1
2017-08-08T18:56:00.613661: step 29624, loss 1.59993e-06, acc 1
2017-08-08T18:56:00.989422: step 29625, loss 3.72525e-07, acc 1
2017-08-08T18:56:01.367433: step 29626, loss 7.07804e-08, acc 1
2017-08-08T18:56:01.760871: step 29627, loss 4.6457e-05, acc 1
2017-08-08T18:56:02.167240: step 29628, loss 1.86265e-09, acc 1
2017-08-08T18:56:02.481458: step 29629, loss 3.34051e-05, acc 1
2017-08-08T18:56:02.941212: step 29630, loss 1.43976e-06, acc 1
2017-08-08T18:56:03.219414: step 29631, loss 2.42689e-06, acc 1
2017-08-08T18:56:03.556409: step 29632, loss 7.2643e-08, acc 1
2017-08-08T18:56:03.973113: step 29633, loss 0, acc 1
2017-08-08T18:56:04.482073: step 29634, loss 0.00314945, acc 1
2017-08-08T18:56:04.906365: step 29635, loss 6.17215e-05, acc 1
2017-08-08T18:56:05.377744: step 29636, loss 8.00935e-08, acc 1
2017-08-08T18:56:05.783899: step 29637, loss 1.49012e-08, acc 1
2017-08-08T18:56:06.220490: step 29638, loss 0, acc 1
2017-08-08T18:56:06.724199: step 29639, loss 0, acc 1
2017-08-08T18:56:06.996572: step 29640, loss 0, acc 1
2017-08-08T18:56:07.296173: step 29641, loss 0, acc 1
2017-08-08T18:56:07.615964: step 29642, loss 2.04891e-08, acc 1
2017-08-08T18:56:07.905341: step 29643, loss 0, acc 1
2017-08-08T18:56:08.267529: step 29644, loss 1.3411e-07, acc 1
2017-08-08T18:56:08.688055: step 29645, loss 1.86265e-09, acc 1
2017-08-08T18:56:08.885511: step 29646, loss 0, acc 1
2017-08-08T18:56:09.179020: step 29647, loss 0, acc 1
2017-08-08T18:56:09.495070: step 29648, loss 9.31322e-09, acc 1
2017-08-08T18:56:09.714602: step 29649, loss 5.529e-05, acc 1
2017-08-08T18:56:09.933270: step 29650, loss 0, acc 1
2017-08-08T18:56:10.311942: step 29651, loss 1.75088e-07, acc 1
2017-08-08T18:56:10.590881: step 29652, loss 9.31321e-08, acc 1
2017-08-08T18:56:10.844967: step 29653, loss 0, acc 1
2017-08-08T18:56:11.057311: step 29654, loss 0, acc 1
2017-08-08T18:56:11.352961: step 29655, loss 2.23688e-06, acc 1
2017-08-08T18:56:11.679142: step 29656, loss 7.45058e-09, acc 1
2017-08-08T18:56:11.952523: step 29657, loss 0, acc 1
2017-08-08T18:56:12.227068: step 29658, loss 0, acc 1
2017-08-08T18:56:12.614281: step 29659, loss 5.58793e-09, acc 1
2017-08-08T18:56:12.893342: step 29660, loss 3.52038e-07, acc 1
2017-08-08T18:56:13.187973: step 29661, loss 0, acc 1
2017-08-08T18:56:13.422525: step 29662, loss 0, acc 1
2017-08-08T18:56:13.660178: step 29663, loss 0.000300244, acc 1
2017-08-08T18:56:14.049142: step 29664, loss 1.11759e-08, acc 1
2017-08-08T18:56:14.298867: step 29665, loss 1.93714e-07, acc 1
2017-08-08T18:56:14.702328: step 29666, loss 5.71234e-05, acc 1
2017-08-08T18:56:14.979024: step 29667, loss 0, acc 1
2017-08-08T18:56:15.363112: step 29668, loss 0, acc 1
2017-08-08T18:56:15.757627: step 29669, loss 0, acc 1
2017-08-08T18:56:16.210232: step 29670, loss 3.72529e-09, acc 1
2017-08-08T18:56:16.543965: step 29671, loss 3.72529e-09, acc 1
2017-08-08T18:56:16.841402: step 29672, loss 1.59697e-05, acc 1
2017-08-08T18:56:17.270027: step 29673, loss 1.80796e-05, acc 1
2017-08-08T18:56:17.504029: step 29674, loss 3.72529e-09, acc 1
2017-08-08T18:56:17.955507: step 29675, loss 1.86265e-09, acc 1
2017-08-08T18:56:18.216074: step 29676, loss 3.72529e-09, acc 1
2017-08-08T18:56:18.657501: step 29677, loss 0, acc 1
2017-08-08T18:56:19.075061: step 29678, loss 9.31322e-09, acc 1
2017-08-08T18:56:19.359607: step 29679, loss 1.86265e-09, acc 1
2017-08-08T18:56:19.835573: step 29680, loss 1.88963e-05, acc 1
2017-08-08T18:56:20.123265: step 29681, loss 0, acc 1
2017-08-08T18:56:20.541767: step 29682, loss 1.85925e-05, acc 1
2017-08-08T18:56:20.854591: step 29683, loss 0, acc 1
2017-08-08T18:56:21.241984: step 29684, loss 0, acc 1
2017-08-08T18:56:21.666920: step 29685, loss 0, acc 1
2017-08-08T18:56:21.930799: step 29686, loss 0, acc 1
2017-08-08T18:56:22.408945: step 29687, loss 0, acc 1
2017-08-08T18:56:22.670956: step 29688, loss 5.02404e-05, acc 1
2017-08-08T18:56:22.919308: step 29689, loss 1.47191e-05, acc 1
2017-08-08T18:56:23.155663: step 29690, loss 7.26431e-08, acc 1
2017-08-08T18:56:23.353865: step 29691, loss 0, acc 1
2017-08-08T18:56:23.653392: step 29692, loss 0, acc 1
2017-08-08T18:56:23.913607: step 29693, loss 7.89744e-07, acc 1
2017-08-08T18:56:24.165477: step 29694, loss 7.94555e-05, acc 1
2017-08-08T18:56:24.397775: step 29695, loss 7.17102e-07, acc 1
2017-08-08T18:56:24.613398: step 29696, loss 0, acc 1
2017-08-08T18:56:24.940515: step 29697, loss 8.99585e-06, acc 1
2017-08-08T18:56:25.234640: step 29698, loss 0, acc 1
2017-08-08T18:56:25.490398: step 29699, loss 0, acc 1
2017-08-08T18:56:25.731224: step 29700, loss 0, acc 1

Evaluation:
2017-08-08T18:56:26.580562: step 29700, loss 9.30863, acc 0.727017

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-29700

2017-08-08T18:56:27.035716: step 29701, loss 2.02973e-05, acc 1
2017-08-08T18:56:27.323243: step 29702, loss 1.88628e-05, acc 1
2017-08-08T18:56:27.692350: step 29703, loss 3.72529e-09, acc 1
2017-08-08T18:56:28.189552: step 29704, loss 0, acc 1
2017-08-08T18:56:28.540234: step 29705, loss 2.79397e-08, acc 1
2017-08-08T18:56:28.792360: step 29706, loss 0.000689532, acc 1
2017-08-08T18:56:29.065406: step 29707, loss 1.86265e-09, acc 1
2017-08-08T18:56:29.413988: step 29708, loss 2.84982e-07, acc 1
2017-08-08T18:56:29.710065: step 29709, loss 1.30385e-08, acc 1
2017-08-08T18:56:30.028484: step 29710, loss 4.54479e-07, acc 1
2017-08-08T18:56:30.379933: step 29711, loss 0, acc 1
2017-08-08T18:56:30.790988: step 29712, loss 0, acc 1
2017-08-08T18:56:31.236450: step 29713, loss 0, acc 1
2017-08-08T18:56:31.576119: step 29714, loss 1.86265e-09, acc 1
2017-08-08T18:56:31.815511: step 29715, loss 5.2154e-08, acc 1
2017-08-08T18:56:32.226530: step 29716, loss 3.72529e-09, acc 1
2017-08-08T18:56:32.463424: step 29717, loss 7.45058e-09, acc 1
2017-08-08T18:56:32.704729: step 29718, loss 3.72529e-09, acc 1
2017-08-08T18:56:32.971526: step 29719, loss 3.40861e-07, acc 1
2017-08-08T18:56:33.266551: step 29720, loss 0, acc 1
2017-08-08T18:56:33.633538: step 29721, loss 7.69254e-07, acc 1
2017-08-08T18:56:33.949202: step 29722, loss 0.00129972, acc 1
2017-08-08T18:56:34.352903: step 29723, loss 0, acc 1
2017-08-08T18:56:34.574380: step 29724, loss 1.38653e-05, acc 1
2017-08-08T18:56:34.838610: step 29725, loss 0, acc 1
2017-08-08T18:56:35.320980: step 29726, loss 4.47034e-08, acc 1
2017-08-08T18:56:35.564579: step 29727, loss 5.58793e-09, acc 1
2017-08-08T18:56:35.750133: step 29728, loss 1.3411e-07, acc 1
2017-08-08T18:56:36.001475: step 29729, loss 5.34571e-07, acc 1
2017-08-08T18:56:36.377846: step 29730, loss 0, acc 1
2017-08-08T18:56:36.704564: step 29731, loss 0, acc 1
2017-08-08T18:56:37.025389: step 29732, loss 0, acc 1
2017-08-08T18:56:37.283517: step 29733, loss 7.45058e-09, acc 1
2017-08-08T18:56:37.525380: step 29734, loss 5.96045e-08, acc 1
2017-08-08T18:56:37.979149: step 29735, loss 0, acc 1
2017-08-08T18:56:38.246344: step 29736, loss 7.63683e-08, acc 1
2017-08-08T18:56:38.508187: step 29737, loss 1.49011e-07, acc 1
2017-08-08T18:56:38.845354: step 29738, loss 3.37135e-07, acc 1
2017-08-08T18:56:39.206390: step 29739, loss 0, acc 1
2017-08-08T18:56:39.495059: step 29740, loss 1.21589e-05, acc 1
2017-08-08T18:56:39.736485: step 29741, loss 7.45058e-09, acc 1
2017-08-08T18:56:40.027114: step 29742, loss 0, acc 1
2017-08-08T18:56:40.365176: step 29743, loss 0, acc 1
2017-08-08T18:56:40.617479: step 29744, loss 0, acc 1
2017-08-08T18:56:40.905568: step 29745, loss 0, acc 1
2017-08-08T18:56:41.304890: step 29746, loss 3.65594e-06, acc 1
2017-08-08T18:56:41.697651: step 29747, loss 0, acc 1
2017-08-08T18:56:42.027012: step 29748, loss 0, acc 1
2017-08-08T18:56:42.295830: step 29749, loss 0, acc 1
2017-08-08T18:56:42.517267: step 29750, loss 0, acc 1
2017-08-08T18:56:42.843186: step 29751, loss 9.87199e-08, acc 1
2017-08-08T18:56:43.208387: step 29752, loss 2.59445e-06, acc 1
2017-08-08T18:56:43.554183: step 29753, loss 1.26659e-07, acc 1
2017-08-08T18:56:43.841190: step 29754, loss 1.2873e-05, acc 1
2017-08-08T18:56:44.137969: step 29755, loss 9.31322e-09, acc 1
2017-08-08T18:56:44.334852: step 29756, loss 5.02914e-08, acc 1
2017-08-08T18:56:44.632915: step 29757, loss 1.11759e-08, acc 1
2017-08-08T18:56:44.886075: step 29758, loss 1.17346e-07, acc 1
2017-08-08T18:56:45.073020: step 29759, loss 1.49012e-08, acc 1
2017-08-08T18:56:45.537388: step 29760, loss 1.98023e-05, acc 1
2017-08-08T18:56:45.786553: step 29761, loss 1.37403e-05, acc 1
2017-08-08T18:56:46.043044: step 29762, loss 0, acc 1
2017-08-08T18:56:46.264035: step 29763, loss 1.29308e-05, acc 1
2017-08-08T18:56:46.615783: step 29764, loss 2.23517e-08, acc 1
2017-08-08T18:56:47.038010: step 29765, loss 3.57624e-07, acc 1
2017-08-08T18:56:47.414721: step 29766, loss 0, acc 1
2017-08-08T18:56:47.660031: step 29767, loss 9.31322e-09, acc 1
2017-08-08T18:56:47.885324: step 29768, loss 2.04891e-08, acc 1
2017-08-08T18:56:48.269380: step 29769, loss 9.31322e-09, acc 1
2017-08-08T18:56:48.484799: step 29770, loss 2.98368e-06, acc 1
2017-08-08T18:56:48.700775: step 29771, loss 0, acc 1
2017-08-08T18:56:48.915639: step 29772, loss 0, acc 1
2017-08-08T18:56:49.161455: step 29773, loss 1.86265e-09, acc 1
2017-08-08T18:56:49.613575: step 29774, loss 0, acc 1
2017-08-08T18:56:49.893708: step 29775, loss 0, acc 1
2017-08-08T18:56:50.159649: step 29776, loss 5.58794e-09, acc 1
2017-08-08T18:56:50.402651: step 29777, loss 0, acc 1
2017-08-08T18:56:50.589950: step 29778, loss 1.86265e-09, acc 1
2017-08-08T18:56:51.012638: step 29779, loss 0, acc 1
2017-08-08T18:56:51.248033: step 29780, loss 0, acc 1
2017-08-08T18:56:51.475883: step 29781, loss 1.86265e-09, acc 1
2017-08-08T18:56:51.745689: step 29782, loss 0, acc 1
2017-08-08T18:56:52.236314: step 29783, loss 5.40166e-08, acc 1
2017-08-08T18:56:52.568145: step 29784, loss 0, acc 1
2017-08-08T18:56:52.961423: step 29785, loss 5.58793e-09, acc 1
2017-08-08T18:56:53.207264: step 29786, loss 6.39308e-05, acc 1
2017-08-08T18:56:53.481398: step 29787, loss 0.000135614, acc 1
2017-08-08T18:56:53.962283: step 29788, loss 1.86265e-09, acc 1
2017-08-08T18:56:54.233718: step 29789, loss 0.000283732, acc 1
2017-08-08T18:56:54.539000: step 29790, loss 0, acc 1
2017-08-08T18:56:54.847165: step 29791, loss 0.00153889, acc 1
2017-08-08T18:56:55.318064: step 29792, loss 0, acc 1
2017-08-08T18:56:55.705579: step 29793, loss 2.92983e-06, acc 1
2017-08-08T18:56:55.948935: step 29794, loss 1.074e-05, acc 1
2017-08-08T18:56:56.144822: step 29795, loss 4.08797e-06, acc 1
2017-08-08T18:56:56.478581: step 29796, loss 0, acc 1
2017-08-08T18:56:56.766930: step 29797, loss 8.38188e-08, acc 1
2017-08-08T18:56:56.970667: step 29798, loss 0, acc 1
2017-08-08T18:56:57.196485: step 29799, loss 9.3132e-08, acc 1
2017-08-08T18:56:57.543832: step 29800, loss 1.78813e-07, acc 1

Evaluation:
2017-08-08T18:56:58.240231: step 29800, loss 9.34336, acc 0.723265

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-29800

2017-08-08T18:56:58.804633: step 29801, loss 5.2154e-08, acc 1
2017-08-08T18:56:59.285369: step 29802, loss 5.77419e-08, acc 1
2017-08-08T18:56:59.671939: step 29803, loss 1.54914e-05, acc 1
2017-08-08T18:57:00.007050: step 29804, loss 1.2484e-05, acc 1
2017-08-08T18:57:00.408650: step 29805, loss 9.27328e-06, acc 1
2017-08-08T18:57:00.882426: step 29806, loss 1.86265e-09, acc 1
2017-08-08T18:57:01.367896: step 29807, loss 1.21509e-05, acc 1
2017-08-08T18:57:01.862242: step 29808, loss 0.00128014, acc 1
2017-08-08T18:57:02.254443: step 29809, loss 0, acc 1
2017-08-08T18:57:02.749683: step 29810, loss 0, acc 1
2017-08-08T18:57:03.212049: step 29811, loss 1.86265e-09, acc 1
2017-08-08T18:57:03.587532: step 29812, loss 0, acc 1
2017-08-08T18:57:03.916308: step 29813, loss 5.58793e-09, acc 1
2017-08-08T18:57:04.233023: step 29814, loss 3.72529e-09, acc 1
2017-08-08T18:57:04.661156: step 29815, loss 1.11759e-08, acc 1
2017-08-08T18:57:05.096437: step 29816, loss 1.07605e-05, acc 1
2017-08-08T18:57:05.539624: step 29817, loss 0, acc 1
2017-08-08T18:57:05.990491: step 29818, loss 4.63792e-07, acc 1
2017-08-08T18:57:06.303302: step 29819, loss 0, acc 1
2017-08-08T18:57:06.877356: step 29820, loss 1.17937e-05, acc 1
2017-08-08T18:57:07.335358: step 29821, loss 4.14011e-06, acc 1
2017-08-08T18:57:07.692411: step 29822, loss 6.87301e-07, acc 1
2017-08-08T18:57:08.063704: step 29823, loss 2.42144e-08, acc 1
2017-08-08T18:57:08.494540: step 29824, loss 1.86265e-09, acc 1
2017-08-08T18:57:08.986402: step 29825, loss 1.67638e-08, acc 1
2017-08-08T18:57:09.462839: step 29826, loss 5.77419e-08, acc 1
2017-08-08T18:57:09.922583: step 29827, loss 0, acc 1
2017-08-08T18:57:10.252494: step 29828, loss 6.45459e-06, acc 1
2017-08-08T18:57:10.680920: step 29829, loss 0, acc 1
2017-08-08T18:57:11.166765: step 29830, loss 1.49012e-08, acc 1
2017-08-08T18:57:11.600459: step 29831, loss 3.25959e-07, acc 1
2017-08-08T18:57:11.997591: step 29832, loss 2.70081e-07, acc 1
2017-08-08T18:57:12.384149: step 29833, loss 1.13621e-07, acc 1
2017-08-08T18:57:12.845308: step 29834, loss 8.66821e-06, acc 1
2017-08-08T18:57:13.228080: step 29835, loss 0, acc 1
2017-08-08T18:57:13.621312: step 29836, loss 5.58793e-09, acc 1
2017-08-08T18:57:13.912072: step 29837, loss 1.52662e-05, acc 1
2017-08-08T18:57:14.163766: step 29838, loss 0, acc 1
2017-08-08T18:57:14.493210: step 29839, loss 7.45058e-09, acc 1
2017-08-08T18:57:14.827530: step 29840, loss 8.38188e-08, acc 1
2017-08-08T18:57:15.148323: step 29841, loss 0.000181911, acc 1
2017-08-08T18:57:15.475627: step 29842, loss 1.50869e-06, acc 1
2017-08-08T18:57:15.789921: step 29843, loss 4.07914e-07, acc 1
2017-08-08T18:57:16.199286: step 29844, loss 0, acc 1
2017-08-08T18:57:16.641672: step 29845, loss 6.41364e-06, acc 1
2017-08-08T18:57:17.087742: step 29846, loss 0, acc 1
2017-08-08T18:57:17.451516: step 29847, loss 1.86265e-09, acc 1
2017-08-08T18:57:17.705288: step 29848, loss 1.22555e-05, acc 1
2017-08-08T18:57:18.007552: step 29849, loss 0, acc 1
2017-08-08T18:57:18.553699: step 29850, loss 0, acc 1
2017-08-08T18:57:18.980981: step 29851, loss 0, acc 1
2017-08-08T18:57:19.417508: step 29852, loss 9.2757e-07, acc 1
2017-08-08T18:57:19.751403: step 29853, loss 3.59449e-06, acc 1
2017-08-08T18:57:20.151273: step 29854, loss 0, acc 1
2017-08-08T18:57:20.627443: step 29855, loss 0, acc 1
2017-08-08T18:57:20.922446: step 29856, loss 0, acc 1
2017-08-08T18:57:21.291270: step 29857, loss 1.60552e-06, acc 1
2017-08-08T18:57:21.631009: step 29858, loss 0, acc 1
2017-08-08T18:57:22.269683: step 29859, loss 2.73235e-06, acc 1
2017-08-08T18:57:22.765693: step 29860, loss 3.91155e-08, acc 1
2017-08-08T18:57:23.425403: step 29861, loss 5.58793e-09, acc 1
2017-08-08T18:57:23.776755: step 29862, loss 0, acc 1
2017-08-08T18:57:24.327828: step 29863, loss 1.49012e-08, acc 1
2017-08-08T18:57:24.738730: step 29864, loss 3.72529e-09, acc 1
2017-08-08T18:57:25.025468: step 29865, loss 7.45058e-09, acc 1
2017-08-08T18:57:25.451812: step 29866, loss 2.42144e-08, acc 1
2017-08-08T18:57:25.849922: step 29867, loss 1.86265e-09, acc 1
2017-08-08T18:57:26.302827: step 29868, loss 0, acc 1
2017-08-08T18:57:26.879221: step 29869, loss 1.86264e-08, acc 1
2017-08-08T18:57:27.376951: step 29870, loss 0, acc 1
2017-08-08T18:57:27.844090: step 29871, loss 3.07333e-07, acc 1
2017-08-08T18:57:28.371662: step 29872, loss 4.47034e-08, acc 1
2017-08-08T18:57:28.778461: step 29873, loss 1.93714e-07, acc 1
2017-08-08T18:57:29.190866: step 29874, loss 0, acc 1
2017-08-08T18:57:29.612782: step 29875, loss 0, acc 1
2017-08-08T18:57:30.111401: step 29876, loss 9.49946e-08, acc 1
2017-08-08T18:57:30.541812: step 29877, loss 0, acc 1
2017-08-08T18:57:30.934604: step 29878, loss 0, acc 1
2017-08-08T18:57:31.278878: step 29879, loss 1.25535e-05, acc 1
2017-08-08T18:57:31.664524: step 29880, loss 7.83791e-06, acc 1
2017-08-08T18:57:32.167694: step 29881, loss 1.86264e-08, acc 1
2017-08-08T18:57:32.602103: step 29882, loss 0, acc 1
2017-08-08T18:57:33.002855: step 29883, loss 0, acc 1
2017-08-08T18:57:33.405423: step 29884, loss 0, acc 1
2017-08-08T18:57:33.945406: step 29885, loss 0.000501831, acc 1
2017-08-08T18:57:34.429893: step 29886, loss 0, acc 1
2017-08-08T18:57:34.881392: step 29887, loss 3.622e-05, acc 1
2017-08-08T18:57:35.310274: step 29888, loss 1.55418e-05, acc 1
2017-08-08T18:57:35.710587: step 29889, loss 0, acc 1
2017-08-08T18:57:36.119222: step 29890, loss 1.69874e-05, acc 1
2017-08-08T18:57:36.615555: step 29891, loss 7.45058e-09, acc 1
2017-08-08T18:57:37.022783: step 29892, loss 1.38575e-06, acc 1
2017-08-08T18:57:37.392250: step 29893, loss 0, acc 1
2017-08-08T18:57:37.784333: step 29894, loss 3.24445e-06, acc 1
2017-08-08T18:57:38.293999: step 29895, loss 0, acc 1
2017-08-08T18:57:38.838588: step 29896, loss 5.81135e-07, acc 1
2017-08-08T18:57:39.302453: step 29897, loss 1.37835e-07, acc 1
2017-08-08T18:57:39.643824: step 29898, loss 3.87425e-07, acc 1
2017-08-08T18:57:40.044606: step 29899, loss 1.35988e-05, acc 1
2017-08-08T18:57:40.337123: step 29900, loss 1.86265e-09, acc 1

Evaluation:
2017-08-08T18:57:41.394359: step 29900, loss 9.35077, acc 0.719512

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-29900

2017-08-08T18:57:41.921391: step 29901, loss 0, acc 1
2017-08-08T18:57:42.310016: step 29902, loss 5.58793e-09, acc 1
2017-08-08T18:57:42.690312: step 29903, loss 0, acc 1
2017-08-08T18:57:43.156870: step 29904, loss 5.2154e-08, acc 1
2017-08-08T18:57:43.513230: step 29905, loss 0, acc 1
2017-08-08T18:57:44.022627: step 29906, loss 6.7227e-06, acc 1
2017-08-08T18:57:44.458607: step 29907, loss 0.000166427, acc 1
2017-08-08T18:57:44.724630: step 29908, loss 5.2154e-08, acc 1
2017-08-08T18:57:45.226638: step 29909, loss 1.04308e-07, acc 1
2017-08-08T18:57:45.572152: step 29910, loss 2.18094e-05, acc 1
2017-08-08T18:57:45.998032: step 29911, loss 0, acc 1
2017-08-08T18:57:46.425442: step 29912, loss 1.67638e-08, acc 1
2017-08-08T18:57:46.824787: step 29913, loss 3.65075e-07, acc 1
2017-08-08T18:57:47.047497: step 29914, loss 2.23517e-08, acc 1
2017-08-08T18:57:47.357412: step 29915, loss 5.58507e-06, acc 1
2017-08-08T18:57:47.857445: step 29916, loss 0, acc 1
2017-08-08T18:57:48.324642: step 29917, loss 2.03202e-06, acc 1
2017-08-08T18:57:48.616965: step 29918, loss 1.18196e-05, acc 1
2017-08-08T18:57:49.106048: step 29919, loss 1.95577e-07, acc 1
2017-08-08T18:57:49.580981: step 29920, loss 0.000454521, acc 1
2017-08-08T18:57:49.961207: step 29921, loss 0, acc 1
2017-08-08T18:57:50.331905: step 29922, loss 0, acc 1
2017-08-08T18:57:50.726477: step 29923, loss 1.86265e-09, acc 1
2017-08-08T18:57:51.092196: step 29924, loss 3.72529e-09, acc 1
2017-08-08T18:57:51.486465: step 29925, loss 1.26655e-06, acc 1
2017-08-08T18:57:51.931409: step 29926, loss 1.86263e-07, acc 1
2017-08-08T18:57:52.197900: step 29927, loss 3.92783e-06, acc 1
2017-08-08T18:57:52.584749: step 29928, loss 0, acc 1
2017-08-08T18:57:53.051828: step 29929, loss 0.00026593, acc 1
2017-08-08T18:57:53.602670: step 29930, loss 5.69959e-07, acc 1
2017-08-08T18:57:54.193323: step 29931, loss 1.86264e-08, acc 1
2017-08-08T18:57:54.693269: step 29932, loss 5.2154e-08, acc 1
2017-08-08T18:57:55.065381: step 29933, loss 2.6077e-08, acc 1
2017-08-08T18:57:55.383748: step 29934, loss 7.23684e-05, acc 1
2017-08-08T18:57:56.053725: step 29935, loss 0, acc 1
2017-08-08T18:57:56.545200: step 29936, loss 3.72529e-09, acc 1
2017-08-08T18:57:56.917363: step 29937, loss 0, acc 1
2017-08-08T18:57:57.377369: step 29938, loss 0.000233268, acc 1
2017-08-08T18:57:57.855813: step 29939, loss 7.45058e-09, acc 1
2017-08-08T18:57:58.343452: step 29940, loss 1.45095e-06, acc 1
2017-08-08T18:57:58.841148: step 29941, loss 0, acc 1
2017-08-08T18:57:59.379398: step 29942, loss 0.000107533, acc 1
2017-08-08T18:57:59.812739: step 29943, loss 1.28522e-07, acc 1
2017-08-08T18:58:00.151186: step 29944, loss 1.86265e-09, acc 1
2017-08-08T18:58:00.567544: step 29945, loss 0, acc 1
2017-08-08T18:58:01.099388: step 29946, loss 0, acc 1
2017-08-08T18:58:01.576273: step 29947, loss 2.42144e-08, acc 1
2017-08-08T18:58:02.184302: step 29948, loss 6.42334e-05, acc 1
2017-08-08T18:58:02.708431: step 29949, loss 1.50411e-05, acc 1
2017-08-08T18:58:03.393439: step 29950, loss 0, acc 1
2017-08-08T18:58:03.805864: step 29951, loss 1.21067e-06, acc 1
2017-08-08T18:58:04.289992: step 29952, loss 0, acc 1
2017-08-08T18:58:04.539580: step 29953, loss 0, acc 1
2017-08-08T18:58:04.832706: step 29954, loss 1.86265e-09, acc 1
2017-08-08T18:58:05.313668: step 29955, loss 1.23606e-05, acc 1
2017-08-08T18:58:05.651529: step 29956, loss 2.19032e-06, acc 1
2017-08-08T18:58:05.944909: step 29957, loss 0, acc 1
2017-08-08T18:58:06.188682: step 29958, loss 1.36898e-06, acc 1
2017-08-08T18:58:06.603093: step 29959, loss 1.86265e-09, acc 1
2017-08-08T18:58:06.988489: step 29960, loss 0, acc 1
2017-08-08T18:58:07.317291: step 29961, loss 1.40065e-06, acc 1
2017-08-08T18:58:07.631488: step 29962, loss 1.30385e-08, acc 1
2017-08-08T18:58:07.864789: step 29963, loss 4.00464e-07, acc 1
2017-08-08T18:58:08.205104: step 29964, loss 3.72529e-09, acc 1
2017-08-08T18:58:08.560586: step 29965, loss 0, acc 1
2017-08-08T18:58:08.843291: step 29966, loss 0.00479992, acc 1
2017-08-08T18:58:09.111354: step 29967, loss 2.42144e-08, acc 1
2017-08-08T18:58:09.540339: step 29968, loss 7.71876e-06, acc 1
2017-08-08T18:58:09.839851: step 29969, loss 4.73105e-07, acc 1
2017-08-08T18:58:10.123016: step 29970, loss 5.66142e-06, acc 1
2017-08-08T18:58:10.354104: step 29971, loss 6.10936e-07, acc 1
2017-08-08T18:58:10.757917: step 29972, loss 0, acc 1
2017-08-08T18:58:10.984327: step 29973, loss 7.45058e-09, acc 1
2017-08-08T18:58:11.277387: step 29974, loss 6.70074e-06, acc 1
2017-08-08T18:58:11.569121: step 29975, loss 2.80676e-06, acc 1
2017-08-08T18:58:11.929473: step 29976, loss 0, acc 1
2017-08-08T18:58:12.358288: step 29977, loss 3.31547e-07, acc 1
2017-08-08T18:58:12.682488: step 29978, loss 9.68572e-08, acc 1
2017-08-08T18:58:12.907991: step 29979, loss 0.000455078, acc 1
2017-08-08T18:58:13.086453: step 29980, loss 6.33298e-08, acc 1
2017-08-08T18:58:13.425322: step 29981, loss 2.34692e-07, acc 1
2017-08-08T18:58:13.751621: step 29982, loss 1.17346e-07, acc 1
2017-08-08T18:58:13.989488: step 29983, loss 5.58794e-09, acc 1
2017-08-08T18:58:14.207026: step 29984, loss 7.51024e-06, acc 1
2017-08-08T18:58:14.585114: step 29985, loss 0, acc 1
2017-08-08T18:58:14.923426: step 29986, loss 3.72529e-09, acc 1
2017-08-08T18:58:15.206295: step 29987, loss 0.00228375, acc 1
2017-08-08T18:58:15.446488: step 29988, loss 1.67638e-08, acc 1
2017-08-08T18:58:15.647459: step 29989, loss 0, acc 1
2017-08-08T18:58:15.841811: step 29990, loss 6.83441e-06, acc 1
2017-08-08T18:58:16.271619: step 29991, loss 1.15666e-06, acc 1
2017-08-08T18:58:16.543154: step 29992, loss 4.97323e-07, acc 1
2017-08-08T18:58:16.823676: step 29993, loss 0, acc 1
2017-08-08T18:58:17.066908: step 29994, loss 1.86265e-09, acc 1
2017-08-08T18:58:17.439430: step 29995, loss 3.72529e-09, acc 1
2017-08-08T18:58:17.671681: step 29996, loss 2.04891e-08, acc 1
2017-08-08T18:58:17.967305: step 29997, loss 0.0622548, acc 0.984375
2017-08-08T18:58:18.185354: step 29998, loss 0, acc 1
2017-08-08T18:58:18.356824: step 29999, loss 5.58793e-09, acc 1
2017-08-08T18:58:18.534805: step 30000, loss 3.88991e-06, acc 1

Evaluation:
2017-08-08T18:58:19.215055: step 30000, loss 9.48342, acc 0.727955

Saved model checkpoint to /root/cortex-ai/src/cortex/cnn-text-classification-tf/runs/1502209352/checkpoints/model-30000

